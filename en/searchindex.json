[{"content":"APM Server extension The APM Server receives data from APM agents and transforms them into Elasticsearch documents that can be visualised in Kibana.\nUsage To include APM Server in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the apm-server-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/apm-server/apm-server-compose.yml up Meanwhile, you can navigate to the APM application in Kibana and follow the setup instructions to get started.\nConnecting an agent to APM Server The most basic configuration to send traces to APM server is to specify the SERVICE_NAME and SERVICE_URL. Here is an example Python Flask configuration:\nimport elasticapm from elasticapm.contrib.flask import ElasticAPM from flask import Flask app = Flask(__name__) app.config['ELASTIC_APM'] = { # Set required service name. Allowed characters: # a-z, A-Z, 0-9, -, _, and space 'SERVICE_NAME': 'PYTHON_FLASK_TEST_APP', # Set custom APM Server URL (default: http://localhost:8200) 'SERVER_URL': 'http://apm-server:8200', 'DEBUG': True, } Configuration settings for each supported language are available in the APM documentation: APM Agents.\nChecking connectivity and importing default APM dashboards On the Kibana home page, click Add APM under the Observability panel. Click Check APM Server status to confirm the server is up and running. Click Check agent status to verify your agent has registered properly. Click Load Kibana objects to create an index pattern for APM. Click Launch APM to be taken to the APM dashboard. See also Running APM Server on Docker\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/apm-server/readme/"},{"content":"Curator Elasticsearch Curator helps you curate or manage your indices.\nUsage If you want to include the Curator extension, run Docker Compose from the root of the repository with an additional command line argument referencing the curator-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/curator/curator-compose.yml up This sample setup demonstrates how to run curator every minute using cron.\nAll configuration files are available in the config/ directory.\nDocumentation Curator Reference\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/curator/readme/"},{"content":"Enterprise Search extension Elastic Enterprise Search is a suite of products for search applications backed by the Elastic Stack.\nRequirements 2 GB of free RAM, on top of the resources required by the other stack components and extensions. Enterprise Search exposes the TCP port 3002 for its Web UI and API.\nUsage Generate an encryption key Enterprise Search requires one or more encryption keys to be configured before the initial startup. Failing to do so prevents the server from starting.\nEncryption keys can contain any series of characters. Elastic recommends using 256-bit keys for optimal security.\nThose encryption keys must be added manually to the config/enterprise-search.yml file. By default, the list of encryption keys is empty and must be populated using one of the following formats:\nsecret_management.encryption_keys: - my_first_encryption_key - my_second_encryption_key - ... secret_management.encryption_keys: [my_first_encryption_key, my_second_encryption_key, ...] ℹ️ To generate a strong encryption key, for example using the AES-256 cipher, you can use the OpenSSL utility or any other online/offline tool of your choice:\n$ openssl enc -aes-256 -P enter aes-256-cbc encryption password: \u003ca strong password\u003e Verifying - enter aes-256-cbc encryption password: \u003crepeat your strong password\u003e ... key=\u003cgenerated AES key\u003e Enable Elasticsearch’s API key service Enterprise Search requires Elasticsearch’s built-in API key service to be enabled in order to start. Unless Elasticsearch is configured to enable TLS on the HTTP interface (disabled by default), this service is disabled by default.\nTo enable it, modify the Elasticsearch configuration file in elasticsearch/config/elasticsearch.yml and add the following setting:\nxpack.security.authc.api_key.enabled: true Configure the Enterprise Search host in Kibana Kibana acts as the management interface to Enterprise Search.\nTo enable the management experience for Enterprise Search, modify the Kibana configuration file in kibana/config/kibana.yml and add the following setting:\nenterpriseSearch.host: http://enterprise-search:3002 Start the server To include Enterprise Search in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the enterprise-search-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml up Allow a few minutes for the stack to start, then open your web browser at the address http://localhost:3002 to see the Enterprise Search home page.\nEnterprise Search is configured on first boot with the following default credentials:\nuser: enterprise_search password: changeme Security The Enterprise Search password is defined inside the Compose file via the ENT_SEARCH_DEFAULT_PASSWORD environment variable. We highly recommend choosing a more secure password than the default one for security reasons.\nTo do so, change the value ENT_SEARCH_DEFAULT_PASSWORD environment variable inside the Compose file before the first boot:\nenterprise-search: environment: ENT_SEARCH_DEFAULT_PASSWORD: {{some strong password}} ⚠️ The default Enterprise Search password can only be set during the initial boot. Once the password is persisted in Elasticsearch, it can only be changed via the Elasticsearch API.\nFor more information, please refer to User Management and Security.\nConfiguring Enterprise Search The Enterprise Search configuration is stored in config/enterprise-search.yml. You can modify this file using the Default Enterprise Search configuration as a reference.\nYou can also specify the options you want to override by setting environment variables inside the Compose file:\nenterprise-search: environment: ent_search.auth.source: standard worker.threads: '6' Any change to the Enterprise Search configuration requires a restart of the Enterprise Search container:\ndocker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml restart enterprise-search Please refer to the following documentation page for more details about how to configure Enterprise Search inside a Docker container: Running Enterprise Search Using Docker.\nSee also Enterprise Search documentation\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/enterprise-search/readme/"},{"content":"Filebeat Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.\nUsage To include Filebeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the filebeat-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up Configuring Filebeat The Filebeat configuration is stored in config/filebeat.yml. You can modify this file with the help of the Configuration reference.\nAny change to the Filebeat configuration requires a restart of the Filebeat container:\ndocker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml restart filebeat Please refer to the following documentation page for more details about how to configure Filebeat inside a Docker container: Run Filebeat on Docker.\nSee also Filebeat documentation\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/filebeat/readme/"},{"content":"Logspout extension Logspout collects all Docker logs using the Docker logs API, and forwards them to Logstash without any additional configuration.\nUsage If you want to include the Logspout extension, run Docker Compose from the root of the repository with an additional command line argument referencing the logspout-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/logspout/logspout-compose.yml up In your Logstash pipeline configuration, enable the udp input and set the input codec to json:\ninput { udp { port =\u003e 5000 codec =\u003e json } } Documentation https://github.com/looplab/logspout-logstash\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/logspout/readme/"},{"content":"Metricbeat Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server. Metricbeat takes the metrics and statistics that it collects and ships them to the output that you specify, such as Elasticsearch or Logstash.\nUsage To include Metricbeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the metricbeat-compose.yml file:\ndocker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml up Configuring Metricbeat The Metricbeat configuration is stored in config/metricbeat.yml. You can modify this file with the help of the Configuration reference.\nAny change to the Metricbeat configuration requires a restart of the Metricbeat container:\ndocker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml restart metricbeat Please refer to the following documentation page for more details about how to configure Metricbeat inside a Docker container: Run Metricbeat on Docker.\nSee also Metricbeat documentation\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/metricbeat/readme/"},{"content":"Extensions Third-party extensions that enable extra integrations with the Elastic stack.\n","description":"","title":"","uri":"/en/tracks/90daysofdevops/monitoring/elastic-stack/extensions/readme/"},{"content":"LeetCode problem\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\nYou can return the answer in any order.\nExample 1:\nInput: nums = [2,7,11,15], target = 9 Output: [0,1] Explanation: Because nums[0] + nums[1] == 9, we return [0, 1]. Topics: array, hash table\nIf we fix one of the numbers, say x, we have to scan the entire array to find the next number y which is value - x where value is the input parameter. Can we change our array somehow so that this search becomes faster?\nThe second train of thought is, without changing the array, can we use additional space somehow? Like maybe a hash map to speed up the search?\nFirst accepted class Solution: def twoSum(self, nums: List[int], target: int) -\u003e List[int]: nums_set = set(nums) # to search in a O(1) while nums: n1 = nums.pop() # get last n2 = target - n1 if n1 == n2: if n2 in nums: return [len(nums), nums.index(n2)] elif n2 in nums_set: return [len(nums), nums.index(n2)] Better solution remember indexes of “passed” n's from nums\nclass Solution: def twoSum(self, nums: List[int], target: int) -\u003e List[int]: hashmap = {} for idx, n1 in enumerate(nums): n2 = target - n1 if n2 in hashmap: return [idx, hashmap[n2]] hashmap[n1] = idx ","description":"LeetCode 1. Two Sum","title":"1. Two Sum","uri":"/en/tracks/algorithms-101/leetcode/easy/1/"},{"content":"LeetCode problem\nProblem Statement The problem is about finding a greatest common divisor (GCD) of two strings. The term “GCD” might be familiar from mathematics, as the largest number that divides two numbers without leaving a remainder. Here, we extend the idea to strings: a string x is a GCD of strings str1 and str2 if x can be repeatedly appended to itself to obtain str1 and str2.\nNaive Solution A naive approach would be to find all possible divisors of str1 and str2, and then find the largest common divisor. This would involve generating all substrings of str1 and str2 which is time-consuming and unnecessary.\nEfficient Solution Observing the problem, we see a similarity with the Euclidean algorithm for calculating the GCD of two numbers. In the Euclidean algorithm, the GCD of two numbers a and b (a \u003e b) is the same as the GCD of b and a mod b.\nWe can extend this logic to strings. If a string x is a GCD of str1 and str2, then str1 and str2 can both be written in the form x + x + ... + x. Therefore, str1 - str2 (which is similar to a mod b) should also be expressible in the form x + x + ... + x.\nThis observation allows us to use a similar approach to the Euclidean algorithm to solve this problem.\nWhy finding Greatest common divisor? In case smallest string consist multiple same parts.\nExample: str1 = “ABABAB”, str2 = “ABAB”.\nlen(str1) = 6, len(str2) = 4. We can’t use whole str2 but common minimum length -\u003e 2.\nSteps Here are the high-level steps of the algorithm:\nIf str1 + str2 is not equal to str2 + str1, return an empty string. Otherwise, find the GCD of the lengths of str1 and str2. Return the prefix substring of str1 with length equal to the GCD. Python Solution Here is a Python solution that implements the above algorithm:\nclass Solution: def gcdOfStrings(self, str1: str, str2: str) -\u003e str: def gcd(a, b): while b != 0: a, b = b, a % b return a if str1 + str2 != str2 + str1: return '' max_substr_len = gcd(len(str1), len(str2)) return str1[:max_substr_len] In the gcdOfStrings method, we first check if str1 + str2 is equal to str2 + str1. If they are not equal, no common divisor string exists, so we return an empty string. If they are equal, we find the GCD of the lengths of str1 and str2 and return the prefix substring of str1 with length equal to the GCD.\nThe gcd method is a standard implementation of the Euclidean algorithm to find the GCD of two numbers.\n","description":"Finding the greatest common divisor of two strings","title":"1071. Greatest Common Divisor of Strings","uri":"/en/tracks/algorithms-101/leetcode/easy/other/1071/"},{"content":"LeetCode problem\nYou are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).\nFind two lines that together with the x-axis form a container, such that the container contains the most water.\nReturn the maximum amount of water a container can store.\nNotice that you may not slant the container.\nExample 1:\nInput: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Example 2:\nInput: height = [1,1] Output: 1 First accepted Idea:\nTwo Pointers Max water area is limited by the height of the shorter line Get most left pointer and most right Loop until left==right class Solution: def maxArea(self, height: List[int]) -\u003e int: p1 = 0 # indexes p2 = len(height) - 1 max_water = 0 while p1 \u003c p2: area = (p2 - p1) * min(height[p1], height[p2]) max_water = max(area, max_water) if height[p1] \u003c= height[p2]: p1 += 1 else: p2 -= 1 return max_water ","description":"LeetCode 11. Container With Most Water","title":"11. Container With Most Water","uri":"/en/tracks/algorithms-101/leetcode/medium/11/"},{"content":"LeetCode problem\nfrom collections import deque class Solution: def connect(self, root: 'Optional[Node]') -\u003e 'Optional[Node]': if not root: return root queue = deque([root]) while queue: level_size = len(queue) for i in range(level_size): node = queue.popleft() if i \u003c level_size - 1: node.next = queue[0] if node.left: queue.append(node.left) if node.right: queue.append(node.right) return root LeetCode Editorial:\nEditorial ","description":"116. Populating Next Right Pointers in Each Node","title":"116. Populating Next Right Pointers in Each Node","uri":"/en/tracks/algorithms-101/leetcode/medium/116/"},{"content":"LeetCode problem\nTo solve this problem, we can use a greedy approach.\nThe idea is to keep adding the profit whenever the price on the next day is higher than the price on the current day.\nThis way, we will maximize profit.\nclass Solution: def maxProfit(self, prices: List[int]) -\u003e int: profit = 0 for i in range(1, len(prices)): if prices[i] \u003e prices[i - 1]: profit += prices[i] - prices[i - 1] return profit LeetCode Editorial:\nEditorial ","description":"122. Best Time to Buy and Sell Stock II","title":"122. Best Time to Buy and Sell Stock II","uri":"/en/tracks/algorithms-101/leetcode/medium/122/"},{"content":"LeetCode problem\nSave all numbers in set to make it possible to get number at O(1). For each element look for the current+1 element if exists. class Solution: def longestConsecutive(self, nums: List[int]) -\u003e int: res = 0 nset = set(nums) for x in nset: if x - 1 not in nset: # prevent double calculations _max = 1 cur = x while cur + 1 in nset: _max += 1 cur += 1 res = max(_max, res) return res ","description":"128. Longest Consecutive Sequence","title":"128. Longest Consecutive Sequence","uri":"/en/tracks/algorithms-101/leetcode/medium/128/"},{"content":"LeetCode problem\nRoman numerals are represented by seven different symbols: I, V, X, L, C, D and M.\nSymbol Value I 1 V 5 X 10 L 50 C 100 D 500 M 1000 For example, 2 is written as II in Roman numeral, just two ones added together. 12 is written as XII, which is simply X + II. The number 27 is written as XXVII, which is XX + V + II.\nRoman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used:\nI can be placed before V (5) and X (10) to make 4 and 9. X can be placed before L (50) and C (100) to make 40 and 90. C can be placed before D (500) and M (1000) to make 400 and 900.\nGiven a roman numeral, convert it to an integer.\nExample 1:\nInput: s = \"III\" Output: 3 Explanation: III = 3. Example 2:\nInput: s = \"MCMXCIV\" Output: 1994 Explanation: M = 1000, CM = 900, XC = 90 and IV = 4. First accepted class Solution: def romanToInt(self, s: str) -\u003e int: dict = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000} n_sum = 0 prev = 0 for c in reversed(s): n = dict[c] n = -n if n in (1,10,100) and prev in (n*5, n*10) else n n_sum += n prev = abs(n) return n_sum ","description":"Leetcode 13. Roman to Integer - Solution","title":"13. Roman to Integer","uri":"/en/tracks/algorithms-101/leetcode/easy/13/"},{"content":"LeetCode problem\nNaive Solution:\nA naive solution would be to iterate through each cell in the grid, and for each O, check if it is surrounded by X’s in all four directions (up, down, left, and right). If so, flip it to X. However, this method has a high time complexity and does not take advantage of any properties of the problem.\nApproach: The more efficient solution is to perform a Depth-First Search (DFS) starting from the border O’s.\nDFS is a way to explore a graph or tree by visiting as deep as possible in a single path before backtracking.\nLogic:\nIn this problem, we will mark the border O’s and all their adjacent O’s as not to be flipped to X. Will temporary change these cells with O!. This means that if (later) cell is marked as O! then we will change it back to O. All other cells should be X. Loop through borders. If O is in cell then check its neighbors (dfs). Border cell mark to O! Then, we can iterate through the entire grid, flipping any O’s that are not marked as not to be flipped. class Solution: def dfs(self, board, row, col): # If the current cell is out of bounds or not an 'O', return and stop DFS if ( row \u003c 0 or col \u003c 0 or row \u003e= len(board) or col \u003e= len(board[0]) or board[row][col] != \"O\" # X or O! ): return # Mark the current cell as 'O!' (Don't flip) board[row][col] = \"O!\" # Define the possible directions to move (up, down, left, right) directions = [(0, 1), (1, 0), (0, -1), (-1, 0)] # Explore each direction recursively by calling the DFS function for dr, dc in directions: self.dfs(board, row + dr, col + dc) def solve(self, board): m = len(board) n = len(board[0]) # Iterate through the border cells # rows for row in range(m): for col in [0, n - 1]: # left border coll, right border coll # If a border cell contains 'O', perform DFS on that cell if board[row][col] == \"O\": self.dfs(board, row, col) # cells for col in range(n): for row in [0, m - 1]: # upper border row, bottom border row if board[row][col] == \"O\": self.dfs(board, row, col) # Iterate through the entire grid for row in range(m): for col in range(n): if board[row][col] == \"O!\": board[row][col] = \"O\" else: board[row][col] = \"X\" return board ","description":"130. Surrounded Regions","title":"130. Surrounded Regions","uri":"/en/tracks/algorithms-101/leetcode/medium/130/"},{"content":"LeetCode problem\nNaive Solution:\nA naive solution would be to generate all possible partitions of the given string and then check if every substring in each partition is a palindrome.\nHowever, this approach would be inefficient, as there would be an exponential number of partitions to check.\nApproach:\nUsing Backtracking.\nLogic:\nDefine a helper function, is_palindrome, to check if a given substring is a palindrome. Define the backtrack function to find all palindrome partitions recursively. This function will take the current position in the string and the current partition as input arguments. current_position: An integer representing the position in the string that we are currently examining. We start at position 0 (the first character) and move towards the end of the string current_partition: A list of strings representing a partition of the string with palindromes up to the current position. We start with an empty list and build it up as we find valid palindrome substrings. In the backtrack function, if the current position is at the end of the string, add the current partition to the result list, which stores all valid palindrome partitions found so far. Iterate through the string from the current position to the end. For each character, extract the substring from the current position to the current character and check if this substring is a palindrome using the is_palindrome function. If the substring is a palindrome, add it to the current partition and call the backtrack function recursively with the next position. After the backtrack function call, remove the last substring from the current partition. This is the backtracking step, which allows the function to explore other possible palindrome substrings starting from the current position. Call the backtrack function with the initial values (current_position = 0 and current_partition = []) and return the result list. The backtrack function works by iterating through the string from the current position to the end, checking if the substring from the current position to the current character is a palindrome.\nIf it finds a palindrome, it adds this substring to the current_partition and calls itself recursively with the next position.\nThis process continues until we reach the end of the string, at which point we have found a valid partition, and we add the current_partition to the result list.\nAfter the recursive call, the function backtracks by removing the last substring from the current_partition. This step allows the function to explore other possible palindrome substrings starting from the current position.\nIn summary, the backtrack function is a recursive helper function that helps us explore all possible palindrome partitions by iterating through the string, checking for palindromes, and calling itself recursively with updated input arguments.\nclass Solution: def partition(self, s): def is_palindrome(substr): return substr == substr[::-1] result = [] def backtrack(start, current_partition): if start == len(s): result.append(current_partition[:]) return for end in range(start + 1, len(s) + 1): substr = s[start:end] if is_palindrome(substr): current_partition.append(substr) backtrack(end, current_partition) current_partition.pop() backtrack(0, []) return result Your browser does not support the video tag. Problem 131: Palindrome Partitioning ","description":"131. Palindrome Partitioning","title":"131. Palindrome Partitioning","uri":"/en/tracks/algorithms-101/leetcode/medium/131/"},{"content":"LeetCode problem\nNaive Solution:\nA naive solution would be to try starting from each gas station and check if you can complete the circuit. For each gas station, calculate the remaining gas in the tank after traveling to the next station.\nIf the gas is not enough to travel to the next station, stop and try starting from the next gas station.\nApproach:\nIn this problem, we can use a greedy algorithm\nWe can keep track of the total gas and total cost while iterating through the gas stations.\nIf the total gas is greater than or equal to the total cost, it is guaranteed that there exists a solution.\nSolution:\nclass Solution: def canCompleteCircuit(self, gas, cost) -\u003e int: total_gas = 0 total_cost = 0 star_idx = 0 current_gas = 0 for i in range(len(gas)): total_gas += gas[i] total_cost += cost[i] current_gas += gas[i] - cost[i] if current_gas \u003c 0: star_idx = i + 1 current_gas = 0 return star_idx if total_gas \u003e= total_cost else -1 Your browser does not support the video tag. Leetcode Problem 134 Video Solution ","description":"Leetcode 131. Gas Station","title":"134. Gas Station","uri":"/en/tracks/algorithms-101/leetcode/medium/other/134/"},{"content":"LeetCode problem\nThe problem asks to create a deep copy of a given linked list with a random pointer in each node. A deep copy means that the new linked list will have completely new nodes, and none of its pointers should point to the nodes in the original list. Both the next and random pointers of the new nodes should point to the new nodes in the copied list in the same order as the original list.\nNaive Solution:\nA naive solution would be to first create a copy of the original linked list without the random pointers.\nThen, for each node in the copied list, search for the node in the original list that its random pointer is pointing to, and update the random pointer in the copied list accordingly.\nThis solution would take O(n^2) time complexity, as we need to search for the random node for each node in the copied list.\nLogic:\nInitialize a hashmap to store the mapping of original nodes to new nodes Iterate through the original list to create new nodes and add their mappings to the hashmap Iterate through the original list again to update the next and random pointers of the new nodes using the hashmap Return the head of the copied linked list Solution:\nclass Solution: def copyRandomList(self, head: 'Optional[Node]') -\u003e 'Optional[Node]': if not head: return None nodes = {} cur = head new_head = Node(cur.val) new_cur = new_head nodes[cur] = new_cur while cur: # create mapping old-new linked nodes node = Node(cur.val) nodes[cur] = node cur = cur.next cur = head while cur: if cur.next: nodes[cur].next = nodes[cur.next] if cur.random: nodes[cur].random = nodes[cur.random] cur = cur.next return nodes[head] Your browser does not support the video tag. Leetcode Problem 138 Video Solution ","description":"Leetcode 138. Copy List with Random Pointer | Python soulution and explanation","title":"138. Copy List with Random Pointer","uri":"/en/tracks/algorithms-101/leetcode/medium/other/138/"},{"content":"LeetCode problem\nApproach:\nDynamic Programming.\nLogic:\nUsing DP:\nIterate through each character of string s. Generate all possible substrings ending at the current index. Check if the substring is in wordDict: If it is, check if the index before the substring’s first index is marked as True (this indicates that the part of the string before the current substring can be segmented into words in wordDict). If it is, then mark the current index as True. Solution:\nclass Solution: def wordBreak(self, s, wordDict): n = len(s) dp = [False] * n for end in range(1, n + 1): # 1. n+1 to include last char for start in range(end): # 2. Generate all substrings ending at i substring = s[start:end] # 3.1 check if previous part before substring met condition prev_substr_end_index = start - 1 # if true then everything before passed condition if prev_substr_end_index == -1 or dp[prev_substr_end_index]: # 3.1 if substring in wordDict: # 3. dp[end - 1] = True break # on current step(end index) we know that meet condition return dp[-1] Optimized solution:\nclass Solution: def wordBreak(self, s, wordDict): n = len(s) dp = [False] * (n + 1) # use n+1 list dp[0] = True for i in range(1, n + 1): for j in range(i): if dp[j] and s[j:i] in wordDict: dp[i] = True break return dp[-1] Your browser does not support the video tag. Leetcode Problem 139 ","description":"Leetcode 139. Word Break | Python soulution and explanation","title":"139. Word Break","uri":"/en/tracks/algorithms-101/leetcode/medium/other/139/"},{"content":"LeetCode problem\nWrite a function to find the longest common prefix string amongst an array of strings.\nIf there is no common prefix, return an empty string \"\".\nExample 1:\nInput: strs = [\"flower\",\"flow\",\"flight\"] Output: \"fl\" Example 2:\nInput: strs = [\"dog\",\"racecar\",\"car\"] Output: \"\" Explanation: There is no common prefix among the input strings. First accepted Idea:\nclass Solution: def longestCommonPrefix(self, strs: List[str]) -\u003e str: strs.sort() l = strs[0] r = strs[-1] if l == r: return l res = \"\" for i in range(0, len(l)): if l[i] == r[i]: res += l[i] else: return res return res ","description":"Leetcode 14. Longest Common Prefix","title":"14. Longest Common Prefix","uri":"/en/tracks/algorithms-101/leetcode/easy/14/"},{"content":"LeetCode problem\nclass Solution: def hasCycle(self, head: Optional[ListNode]) -\u003e bool: visited = set() cur = head while cur: if cur.next in visited: return True cur = cur.next visited.add(cur) return False ","description":"141. Linked List Cycle","title":"141. Linked List Cycle","uri":"/en/tracks/algorithms-101/leetcode/easy/other/141/"},{"content":"LeetCode problem\nThe operations we need to support are get and put which should both be done in O(1) time.\nget(key) should return the value if the key exists in the cache, otherwise return -1. put(key, value) should update the value of the key if the key exists; otherwise, this method should insert the key-value pair into the cache. If the cache is full, this method should also evict the least recently used key-value pair. Approach Use Doubly Linked List or Python OrderedDict\nLogic For each operation (get/put) - check if key already exists - if yes, move item to end (the way to mark this key as recent used).\nInitialization The LRUCache class is initialized with a given capacity, and an empty OrderedDict is created. This data structure maintains the keys in order of their usage.\nGet Operation - When the get method is called with a key, the function first checks if the key exists in the cache (which is an O(1) operation).\nIf it does exist, the function makes use of the move_to_end method provided by the OrderedDict to move this key to the end of the order of keys (marking it as the most recently used) and returns the corresponding value.\nIf the key is not found in the cache, the function returns -1.\nPut Operation: - When the put method is called with a key and value, the function first checks if the key is already in the cache. If it is, the function moves the key to the end of the order (making it the most recently used) and updates its value.\nIf the key isn’t already in the cache, the function checks if the cache is at its capacity. If it is, the function uses the popitem method with last=False to remove the least recently used item (which is at the start of the order).\nThe key-value pair is then added to the cache, and since this is a new addition, it is considered the most recently used item and gets added to the end.\nSolution from collections import OrderedDict class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.cache = OrderedDict() def get(self, key: int) -\u003e int: if key in self.cache: self.cache.move_to_end(key) # move to the least recently used return self.cache[key] return -1 def put(self, key: int, value: int) -\u003e None: if key in self.cache: # check if key already exists - if yes, move item to end and update the value self.cache.move_to_end(key) elif len(self.cache) == self.capacity: # if cache is full, remove least recent item self.cache.popitem(last=False) self.cache[key] = value Solution 2 Using Doubly Linked List\nclass Node: def __init__(self, key, value): self.key = key self.value = value self.prev = None self.next = None class LRUCache: def __init__(self, capacity): self.capacity = capacity self.dictionary = dict() self.head = Node(0, 0) # dummy node self.tail = Node(0, 0) # dummy node self.head.next = self.tail self.tail.prev = self.head def get(self, key): if key in self.dictionary: node = self.dictionary[key] self._remove(node) self._add(node) return node.value return -1 def put(self, key, value): if key in self.dictionary: self._remove(self.dictionary[key]) node = Node(key, value) self._add(node) self.dictionary[key] = node if len(self.dictionary) \u003e self.capacity: node = self.head.next self._remove(node) del self.dictionary[node.key] def _remove(self, node): prev = node.prev next = node.next prev.next = next next.prev = prev def _add(self, node): prev = self.tail.prev prev.next = node self.tail.prev = node node.prev = prev node.next = self.tail ","description":"Leetcode 146. LRU Cache | Python soulution and explanation","title":"146. LRU Cache","uri":"/en/tracks/algorithms-101/leetcode/medium/other/146/"},{"content":"LeetCode problem\nNaive Solution Traverse the linked list, adding each node’s value to a Python list sort that list create a new linked list from the sorted values return the head of this new list. This solution would have a time complexity of O(n log n) due to the sort operation and a space complexity of O(n) because of the extra list we’re creating.\nclass Solution: def sortList(self, head): values = [] node = head while node: values.append(node.val) node = node.next values.sort() # Create a new linked list from the sorted values node = head for val in values: node.val = val node = node.next return head Solution Using the Merge Sort algorithm.\nDivide and Conquer: Merge sort uses the divide and conquer strategy, where we continuously split the linked list in half until we have multiple sublists of length 1. A list of length 1 is technically always sorted. Merge Sublists: Once we have the sorted sublists, we start merging them together in a manner that the resultant list is also sorted. The trick to making this solution O(1) space complexity is to modify the existing nodes’ next pointers to generate the sorted list, rather than creating new nodes.\nclass Solution: def sortList(self, head: Optional[ListNode]) -\u003e Optional[ListNode]: if not head or not head.next: return head slow = head fast = head.next while fast and fast.next: slow = slow.next fast = fast.next.next mid = slow.next slow.next = None # separate left and right halves of linked list left = self.sortList(head) right = self.sortList(mid) def merge(left, right): if not left or not right: return left or right if left.val \u003e right.val: # sort left, right = right, left left.next = merge(left.next, right) return left res = merge(left, right) return res ","description":"This article provides a Python solution with an in-depth explanation for the LeetCode problem 148. Sort List.","title":"148. Sort List","uri":"/en/tracks/algorithms-101/leetcode/medium/other/148/"},{"content":"LeetCode problem\nGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\nNotice that the solution set must not contain duplicate triplets.\nExample 1:\nInput: nums = [-1,0,1,2,-1,-4] Output: [[-1,-1,2],[-1,0,1]] Explanation: nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0. nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0. nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0. The distinct triplets are [-1,0,1] and [-1,-1,2]. Notice that the order of the output and the order of the triplets does not matter. Example 2:\nInput: nums = [0,1,1] Output: [] Explanation: The only possible triplet does not sum up to 0. First accepted Idea:\nclass Solution: def threeSum(self, nums: List[int]) -\u003e List[List[int]]: nums.sort() x = 0 # index ll = len(nums) res = [] while x \u003c ll - 2: if x == 0 or nums[x] != nums[x-1]: y = x + 1 z = ll - 1 while y \u003c z: s = nums[x] + nums[y] + nums[z] if s == 0: res.append([nums[x], nums[y], nums[z]]) while y \u003c z and nums[y] == nums[y+1]: y += 1 while z \u003e y and nums[z] == nums[z-1]: z -= 1 y += 1 z -= 1 elif s \u003e 0: z -= 1 else: y += 1 x += 1 return res ","description":"LeetCode 15. 3Sum","title":"15. 3Sum","uri":"/en/tracks/algorithms-101/leetcode/medium/15/"},{"content":"LeetCode problem\nProblem Statement In this problem, we’re given an integer array nums, and our task is to find the maximum product of a contiguous subarray. A subarray is a contiguous part of an array. The interesting part of this problem is that the array can contain both positive and negative numbers, so the maximum product can be obtained by a subarray ending at any index of the array.\nNaive Solution A naive approach to this problem would be to calculate the product of all possible subarrays and return the maximum one. However, this would have a time complexity of O(n²), as there are n*(n+1)/2 subarrays of an array, where n is the length of the array.\nThis would be inefficient and time-consuming for large inputs.\nDynamic Programming We can solve this problem efficiently using Dynamic Programming.\nThe idea is to keep track of the maximum and minimum product ending at each position (as the array can contain negative numbers, and a negative number can become maximum when multiplied by another negative number).\nWe initialize two variables, max_prod and min_prod, to nums[0]. Then for each number in the array (from the second number to the end), we calculate max_prod and min_prod using the formulas:\nmax_prod = max(nums[i], max_prod * nums[i], min_prod * nums[i]) min_prod = min(nums[i], max_prod * nums[i], min_prod * nums[i]) We also keep track of res, which stores the maximum product of a subarray as a result.\nIf max_prod is greater than res, we update res.\nFinally, res will hold the maximum product of a subarray.\nSteps Initialize max_prod, min_prod, and res to nums[0]. For each number in the array (from the second number to the end): Update max_prod and min_prod. Update res if max_prod is greater. Return res. Python Solution Here is a Python solution following the described approach:\ndef maxProduct(nums): if not nums: return 0 max_prod = min_prod = res = nums[0] for num in nums[1:]: new_max = max(num, max_prod * num, min_prod * num) min_prod = min(num, max_prod * num, min_prod * num) max_prod = new_max res = max(res, max_prod) return res ","description":"Understanding the 152. Maximum Product Subarray problem from LeetCode","title":"152. Maximum Product Subarray","uri":"/en/tracks/algorithms-101/leetcode/medium/other/152/"},{"content":"LeetCode problem\nSolution 1 Using hashmap.\n# Definition for singly-linked list. class ListNode: def __init__(self, x): self.val = x self.next = None class Solution: def getIntersectionNode(self, headA: ListNode, headB: ListNode): nodes = set() cur = headA while cur: nodes.add(cur) cur = cur.next cur = headB while cur: if cur in nodes: return cur cur = cur.next return None Solution 2 Initialize two pointers, one for each head. Move each pointer to the next node in its list. If a pointer reaches the end of its list, move it to the start of the other list. Repeat steps 2 and 3 until the two pointers meet, or until both pointers have switched lists and reached the end (meaning there is no intersection). The key insight is that by switching lists when a pointer reaches the end, each pointer traverses the same total number of nodes. This means they must meet at the intersection if one exists.\nclass Solution: def getIntersectionNode(self, headA: ListNode, headB: ListNode): pointerA, pointerB = headA, headB # Continue until the pointers meet while pointerA != pointerB: # Move each pointer to the next node, # or to the start of the other list if it reached the end pointerA = pointerA.next if pointerA else headB pointerB = pointerB.next if pointerB else headA # Return the intersecting node, or None if there is no intersection return pointerA It may seem like the loop could run forever because it continues until the two pointers meet and each pointer is always moved forward (to the next node or to the start of the other list).\nIf the two linked lists intersect, then from the point of intersection, they share the same nodes until the end. The two pointers will meet at the intersection point because when each pointer has traversed its own list once and then the other list, they have both traversed exactly the same number of nodes in total (the length of list A plus the length of list B). So, they must meet at the intersection point if one exists.\nIf the linked lists do not intersect, the pointers will both reach the end of the other list after traversing both lists. They’ll be both set to None, and the loop condition pointerA != pointerB will fail, terminating the loop.\nSo, regardless of whether the linked lists intersect, the loop will eventually terminate. This is why the solution has a time complexity of O(m + n), where m and n are the lengths of the two linked lists.\n","description":"Finding the intersection of two singly linked lists.","title":"160. Intersection of Two Linked Lists","uri":"/en/tracks/algorithms-101/leetcode/easy/other/160/"},{"content":"LeetCode problem\nProblem Statement Given two integers representing the numerator and denominator of a fraction, return the fraction in string format. If the fractional part is repeating, enclose the repeating part in parentheses.\nThis problem is about converting a fraction to its decimal representation in string format. The tricky part is dealing with repeating decimals. If a decimal repeats, we should enclose the repeating part in parentheses.\nFor example, if we have a fraction 1/3, the decimal representation would be 0.3333…. In this problem, we need to represent it as “0.(3)”.\nNaive Solution A naive solution could involve simple division - you divide the numerator by the denominator and convert it to a string. However, this wouldn’t account for recurring decimals, and it would only be correct for fractions that result in a finite decimal.\nApproach A better approach to solve this problem involves using the long division method and a hash map to keep track of remainders. If the same remainder appears again, it means we have found a repeating sequence.\nIn long division, we divide the numerator by the denominator, find the remainder, and then add a zero to the remainder and repeat the process.\nWhile doing this, if we encounter the same remainder that we have seen before, it means the sequence will start to repeat from here.\nSteps First, handle the simple case where numerator is divisible by denominator. If the division isn’t exact, proceed with the long division method. Keep dividing the numerator by the denominator and track the remainder. Store the remainder and its corresponding index in the decimal part of the result in a dictionary. If the remainder repeats, stop the division and enclose the repeating part in parentheses. Solution class Solution: def fractionToDecimal(self, numerator: int, denominator: int) -\u003e str: if numerator % denominator == 0: return str(numerator // denominator) integer = str(abs(numerator) // abs(denominator)) remainder = abs(numerator) % abs(denominator) decimal = [] remainder_dict = {} while remainder != 0: if remainder in remainder_dict: decimal.insert(remainder_dict[remainder], \"(\") decimal.append(\")\") break # future index of starting repeating part , i.e. 1.12(345) remainder_dict[remainder] = len(decimal) remainder *= 10 decimal.append(str(remainder // abs(denominator))) remainder %= abs(denominator) res = integer + \".\" + \"\".join(decimal) if (numerator \u003c 0) ^ (denominator \u003c 0): # Check if the result should be negative res = \"-\" + res return res When doing the division, we are always considering the absolute value of the numerator and denominator. The remainder and the index at which it appears are stored in a dictionary. Whenever a remainder repeats, it means we have found a repeating sequence and the division process is stopped. The repeating part is then enclosed in parentheses.\nAlso, note the line if (numerator \u003c 0) ^ (denominator \u003c 0):. This is checking if the result should be negative. If either, but not both, of the numerator and denominator are negative, the result should also be negative. Here ^ is the bitwise XOR operator in Python, which returns True if exactly one of the conditions is True.\nAfter doing all this, if the numerator was negative, we add a negative sign to the front of our result. Otherwise, the result is returned as is.\nYour browser does not support the video tag. Leetcode 166 Solution ","description":"An in-depth explanation and Python solution for LeetCode problem 166: Fraction to Recurring Decimal","title":"166. Fraction to Recurring Decimal","uri":"/en/tracks/algorithms-101/leetcode/medium/other/166/"},{"content":"LeetCode problem\nGiven a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.\nA mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.\nExample 1:\nInput: digits = \"23\" Output: [\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"] Example 2:\nInput: digits = \"\" Output: [] First accepted class Solution: def letterCombinations(self, digits: str) -\u003e List[str]: if not digits: return [] letters = ['', '', 'abc', 'def', 'ghi', 'jkl', 'mno', 'pqrs', 'tuv', 'wxyz'] result = [''] for d in digits: d = int(d) tmp = [] for letter in letters[d]: for word in result: word += letter tmp.append(word) result = tmp return result ","description":"LeetCode 17. Letter Combinations of a Phone Number","title":"17. Letter Combinations of a Phone Number","uri":"/en/tracks/algorithms-101/leetcode/medium/17/"},{"content":"LeetCode problem\nThe column titles in an Excel sheet are designed similar to a base-26 number system. The columns start from ‘A’ (which is 1) to ‘Z’ (which is 26), then after ‘Z’, the column titles go to ‘AA’ (which is 27), ‘AB’ (28), and so on.\nThis problem is essentially asking us to convert a base-26 number (represented by uppercase English letters) to a decimal number.\nThat means that for each “new” index in columnTitle we already pass alphabet. Example:\nIf columnTitle length is 1 =\u003e result in range of (1-26) If columnTitle length is 2 (‘AB’) =\u003e first index passed alphabet (26), next B equals 2 in alphabet. Hence 1 * 26 + index(B) Solution 1 import string class Solution: def titleToNumber(self, columnTitle: str) -\u003e int: alphabet = list(string.ascii_uppercase) s = 0 for letter in columnTitle: letter_idx = alphabet.index(letter) + 1 s = s*26 + letter_idx return s Optimized Solution Using ord function that returns index of letter.\n\u003e\u003e\u003e ord('A') 65 Because here index is 65, will create a number to convert it to the correct one: result_number - ord('A) + 1\nExample:\nidxA = ord('A') - ord('A') + 1 idxB = ord('B') - ord('A') + 1 class Solution: def titleToNumber(self, columnTitle: str) -\u003e int: s = 0 correct_sum = - ord('A') + 1 for letter in columnTitle: s = s * 26 + ord(letter) + correct_sum return s ","description":"171. Excel Sheet Column Number","title":"171. Excel Sheet Column Number","uri":"/en/tracks/algorithms-101/leetcode/easy/other/171/"},{"content":"1772A - A+B? (implementation, 800)\nSolution def solve(): a, b = input().split('+') res = int(a) + int(b) print(res) for _ in range(int(input())): solve() ","description":"Codeforces 1772A - A+B? (implementation, 800)","title":"1772A - A+B? - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1772a/"},{"content":"1773F - Football (constructive algorithms, 800)\nLogic Solution def solve(): n = int(input()) # number of matches a = int(input()) # goals scored b = int(input()) # goals conceded if n == 1: if a == b: print(1) else: print(0) print(f\"{a}:{b}\") return matches = [[0, 0] for _ in range(n + 1)] draw = 0 match = 1 while match \u003c= n and a \u003e 0: a -= 1 matches[match] = [1, 0] match += 1 if a \u003e 0: matches[1] = [matches[1][0] + a, 0] if b \u003e 0: if match == n + 1: matches[1][0] += 1 matches[n][0] -= 1 matches[n][1] = b else: while match \u003c= n and b \u003e 0: b -= 1 matches[match][1] += 1 match += 1 if b \u003e 0: matches[n][1] += b for i in range(1, n + 1): if matches[i][0] == matches[i][1]: draw += 1 print(draw) for i in range(1, n + 1): print(f\"{matches[i][0]}:{matches[i][1]}\") solve() Codeforces Editorial PDF\n","description":"Codeforces 1773F - Football (constructive algorithms, 800)","title":"1773F - Football - 800","uri":"/en/tracks/algorithms-101/codeforces/02-combinatorics-and-geometry/1773f/"},{"content":"1777A - Everybody Likes Good Arrays! (greedy, math, 800)\nStatement You have an array of numbers, and you want to make it “good.” A good array is one where every pair of adjacent numbers has different parity (one is even, and the other is odd). You can do this by performing operations on pairs of adjacent numbers with the same parity (both even or both odd), and replacing them with their product. Logic Loop through the array Check the parity of the current element and the previous element If the parity is the same, perform the operation and increment the operation counter Repeat steps 2 and 3 until the end of the array Return the operation counter Solution def solve(n, ar): res = 0 i = 0 while i \u003c len(ar) - 1: if ar[i] % 2 == ar[i + 1] % 2: ar[i] = ar[i] * ar[i + 1] del ar[i + 1] res += 1 else: i += 1 return res Optimized Solution Check how many times the parity changes in the given array. The number of operations needed is the difference between the original length of the array and the count of parity changes. def solve(): n = int(input()) ar = list(map(int, input().split())) # Count the number of times the parity changes in the array last_parity = None parity_count = 0 for x in ar: if x % 2 != last_parity: parity_count += 1 last_parity = x % 2 # The result is the difference between the original length and the count of parity changes res = n - parity_count print(res) for _ in range(int(input())): solve() ","description":"Codeforces 1777A - Everybody Likes Good Arrays! (greedy, math, 800)","title":"1777A - Everybody Likes Good Arrays! - 800","uri":"/en/tracks/algorithms-101/codeforces/02-combinatorics-and-geometry/1777a/"},{"content":"1777B - Emordnilap (combinatorics, greedy, math, 900)\nStatement In this problem, we need to find the “beauty” of all permutations of a certain length n. The beauty of a permutation is defined as the number of inversions in an array that is created by combining the permutation and its reverse.\nLogic The key insight is that every permutation of length n has the same beauty.\nThis is because the structure of the array created by concatenating a permutation with its reverse ensures that there will always be the same number of inversions, regardless of the order of the numbers in the original permutation.\nTherefore, we can calculate the beauty of a single permutation and then multiply by the number of possible permutations (which is n!) to get the total sum of the beauties.\nSolution def solve(): n = int(input()) MOD = 10**9 + 7 # Calculate the total number of inversion pairs in the array inversion_pairs = n * (n - 1) # Find the factorial of n fact = 1 for i in range(1, n+1): fact = fact * i % MOD # requirement: size n modulo 1000000007(109+7) # The sum of the beauty of all permutations can be found by multiplying # the number of inversion pairs by the factorial of n res = (fact * inversion_pairs) % MOD print(res) for _ in range(int(input())): solve() ","description":"Codeforces 1777B - Emordnilap (combinatorics, greedy, math, 900)","title":"1777B - Emordnilap - 900","uri":"/en/tracks/algorithms-101/codeforces/02-combinatorics-and-geometry/1777b/"},{"content":"1778A - Flip Flop Sum (greedy, implementation, 800)\nThere are three possible conditions:\n-1 -1 - all negative. In this case sum -2 becomes sum 2. Plus 4. -1 1 - different, no sum change. 1 1 - all positive. 2 becomes -2. Diff is -4. Solution def solve(): n = int(input()) ar = list(map(int, input().split())) s = 0 # sum # three conditions: all 1, all -1, at least one -1 has_diff = False has2_positive = 0 has2_negative = 0 s += ar[0] for idx in range(1, n): if ar[idx] == ar[idx-1]: if ar[idx] == -1: has2_negative = 4 # -2 -\u003e +2, diff 4 else: has2_positive = -4 # +2 =\u003e +1, diff 1 else: has_diff = True s += ar[idx] if has2_negative: s += has2_negative elif has_diff: ... elif has2_positive: s += has2_positive print(s) for _ in range(int(input())): solve() Optimized solution:\ndef solve(): n = int(input()) ar = list(map(int, input().split())) res = sum(ar) for i in range(n-1): if ar[i] == ar[i+1] == -1: print(res + 4) return if res == n: res -= 4 print(res) for _ in range(int(input())): solve() ","description":"Codeforces 1778A - Flip Flop Sum (greedy, implementation, 800)","title":"1778A - Flip Flop Sum - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1778a/"},{"content":"1787A - Exponential Equation (constructive algorithms, math, 800)\nLogic The equation is a mix of multiplication and exponential operations. Given the nature of exponential operations, $𝑥^𝑦$ and $𝑦^𝑥$ can grow very large very quickly as x and y increase.\nWe can try to simplify the equation.\nIf we can somehow set one of the variables x or y to 1, the equation simplifies.\nThis is because any number (except zero) raised to the power of 1 is the number itself, and any number raised to the power of 0 is 1.\nSo, if we set x = 1, the equation simplifies to 1^y * y + y^1 * 1 = n, which further simplifies to y + y = n, or 2y = n. This is a simple linear equation, and we can see that for any even n greater than 2, it has a solution in integers. The solution is x = 1 and y = n/2.\nFor n = 2, we can set both x and y to 1, and the equation holds. It can be a base case. For odd n greater than 2, and for n = 1, there’s no solution in integers. Solution def solve(): n = int(input()) if n == 2: print(1, 1) elif n % 2 == 0: print(1, n//2) else: print(-1) for _ in range(int(input())): solve() ","description":"Codeforces 1787A - Exponential Equation (constructive algorithms, math, 800)","title":"1787A - Exponential Equation - 800","uri":"/en/tracks/algorithms-101/codeforces/02-combinatorics-and-geometry/1787a/"},{"content":"1788A - One and Two (brute force, implementation, math, 800)\nThis problem is about finding a specific index 𝑘 in a given sequence of integers $𝑎_1,𝑎_2,…,𝑎_𝑛$, where each element is either 1 or 2. The goal is to determine whether there exists an integer 𝑘 such that the product of all elements from $𝑎_1$ to $𝑎_𝑘$ is equal to the product of all elements from $𝑎_𝑘+1$ to $𝑎_𝑛$.\nBecause of product of 1 doesn’t change the result we can focus on 2. Product in left side and in the right side can be equal only if count of 2 is even or equal 0.\nWe can count number of 2. The result will be the index of the middle 2 in array. Solution def solve(ar): twos = ar.count(2) if twos % 2 != 0: return -1 passed_twos = 0 need_twos = twos // 2 for i, x in enumerate(ar): if x == 2: passed_twos += 1 if passed_twos == need_twos: return i+1 t = int(input()) for _ in range(t): n = int(input()) ar = list(map(int, input().split())) print(solve(ar)) ","description":"Codeforces 1788A - One and Two (brute force, implementation, math, 800)","title":"1788A - One and Two - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1788a/"},{"content":"Asterisk-Minor Template (implementation, strings, 1000)\nIf the first characters of both strings are the same, create a template that consists of the common character followed by an asterisk (e.g., a*). If the last characters of both strings are the same, create a template that consists of an asterisk followed by the common character (e.g., *b). If neither the first nor the last characters are the same, iterate through string a and check for any 2-character substring that also appears in string b. If a match is found, create a template that consists of an asterisk, the 2-character substring, and another asterisk (e.g., ab). If no template is found, print NO. Solution def solve(): a = input() b = input() if a[0] == b[0]: print('YES') print(f'{a[0]}*') elif a[-1] == b[-1]: print('YES') print(f'*{a[-1]}') else: for i in range(len(a)-1): if a[i:i+2] in b: print('YES') print(f'*{a[i:i+2]}*') return print('NO') for _ in range(int(input())): solve() ","description":"Codeforces Asterisk-Minor Template (implementation, strings, 1000)","title":"1796B - One and Two - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1796b/"},{"content":"1798A - Showstopper (greedy, implementation, sortings, 800)\nUpdate a and b in a such way so in a always put the largest and in b smallest.\nSolution def solve(a,b,n): a_max = a[0] for i in range(n): max_v = max(a[i], b[i]) min_v = min(a[i], b[i]) a[i] = max_v b[i] = min_v a_max = max(a_max, max_v) if a[-1] \u003e= a_max and b[-1] \u003e= max(b): return 'Yes' return 'No' t = int(input()) for _ in range(t): n = int(input()) a = list(map(int, input().split())) b = list(map(int, input().split())) print(solve(a, b, n)) ","description":"Codeforces 1798A - Showstopper (greedy, implementation, sortings, 800)","title":"1798A - Showstopper - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1798a/"},{"content":"1799A - Recent Actions (data structures, greedy, implementation, math, 800)\nExplanation On Codeforces, the “Recent Actions” field shows the last n posts with recent actions. Initially, there are posts numbered 1 to n in the field, in order from top to bottom. There are also infinitely many posts not in the field, numbered with integers n+1, n+2, and so on.\nWhen a recent action happens in post p:\nIf it is in the “Recent Actions” field, it moves from its position to the top position. Otherwise, it is added to the top position, and the post in the bottom position is removed from the “Recent Actions” field. You know that the next m recent actions (Note, that recent actions only happen with posts with numbers ≥+1.) will happen in the posts p1, p2, ..., pm (n+1 ≤ pi ≤ n+m) at moments of time 1, 2, ..., m. Note that recent actions only happen with posts with numbers ≥ n+1.\nFor each post i (1 ≤ i ≤ n), find the first time it will be removed from the “Recent Actions” field or say that it won’t be removed.\nExample Analyze example #7:\nInput:\n3 5 4 5 5 5 4 Consider there is only one test case with n = 3 and m = 5.\nThe recent actions are p1 = 4, p2 = 5, p3 = 5, p4 = 5, p5 = 4.\nInitial state of the “Recent Actions” field: [1, 2, 3].\nAt moment 1 (post 4):\nPost 4 is not in the “Recent Actions” field. So, it is added to the top position, and the post at the bottom (post 3) is removed. “Recent Actions” field becomes [4, 1, 2]. At moment 2 (post 5):\nPost 5 is not in the “Recent Actions” field. So, it is added to the top position, and the post at the bottom (post 2) is removed. “Recent Actions” field becomes [5, 4, 1]. At moment 3 (post 5):\nPost 5 is already in the “Recent Actions” field. So, it moves to the top position. “Recent Actions” field remains [5, 4, 1]. At moment 4 (post 5):\nPost 5 is already in the “Recent Actions” field. So, it moves to the top position. “Recent Actions” field remains [5, 4, 1]. At moment 5 (post 4):\nPost 4 is already in the “Recent Actions” field. So, it moves to the top position. “Recent Actions” field becomes [4, 5, 1]. In this example, post 1 is never removed, post 2 is removed at moment 2, and post 3 is removed at moment 1. m As initial “Recent Actions” was [1,2,3], the output for this input would be: -1 2 1\nLogic Initialize an array tracked_data of size n with all elements set to -1. This array will be a result array. Set -1 for all elements setting initially that no one element will be removed. Initialize an array recent_posts to keep track on current “recent actions”. Iterate through posts that got from output. If post is already in recent_posts, it means no need to remove anything, move this “found” post to index 0 and shift other elements in recent_posts. If post is not in recent_posts, then add it to index 0 of recent_posts and remove “last” post from recent_posts. if “last” post in recent_posts is \u003c=n that means that it is to be removed. In tracked_data set current moment when it is removed. Print tracked_data Solution This solution works but slow:\ndef solve(): n, m = list(map(int, inp().split())) posts = list(map(int, inp().split())) tracked_data = [-1] * n recent_posts = list(range(1, n+1)) for moment, post in enumerate(posts, 1): if post in recent_posts: idx = recent_posts.index(post) recent_posts = [post] + recent_posts[0:idx] + recent_posts[idx+1:] else: last = recent_posts[-1] # set at what moment removed if last \u003c= n: tracked_data[last-1] = moment if post \u003c= n: tracked_data[post-1] = -1 recent_posts = [post] + recent_posts[0:-1] print(*tracked_data) for _ in range(int(inp())): solve() As a result no need to keep recent_posts always uptodate. We need to follow for the posts that are in range\u003c=n.\nAll what we need is:\nto keep last post index to know what post is going to be removed. No need to rearrange array for this. keep data on “already used” posts. Optimized solution def solve(): n, m = list(map(int, input().split())) posts = list(map(int, input().split())) tracked_data = [-1] * n last = n-1 used_posts = set() for moment, post in enumerate(posts, 1): if post not in used_posts: if last \u003e= 0: tracked_data[last] = moment last -= 1 used_posts.add(post) print(*tracked_data) for _ in range(int(input())): solve() ","description":"Codeforces 1799A - Recent Actions (data structures, greedy, implementation, math, 800)","title":"1799A - Recent Actions - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1799a/"},{"content":"1807A - Plus or Minus (implementation, 800)\nSolution t = int(input()) for _ in range(t): a, b, c = map(int, input().split()) print('+' if a + b == c else '-') ","description":"Codeforces 1807A - Plus or Minus (implementation, 800)","title":"1807A - Plus or Minus - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1807a/"},{"content":"1807B - Grab the Candies (greedy, 800)\nExplanation Read the number of bags and the list of candies in the bags. Calculate the total number of candies with even and odd amounts separately. Compare the total amount of candies with even and odd amounts. If Mihai has strictly more candies than Bianca, output “YES”. Otherwise, output “NO”. Solution t = int(input()) for _ in range(t): n = int(input()) ar = list(map(int, input().split())) m = 0 b = 0 if n == 1: print('YES' if ar[0] % 2 == 0 else 'NO') else: for x in ar: if x % 2 == 0: m += x else: b += x print('YES' if m \u003e b else 'NO') ","description":"Codeforces 1807B - Grab the Candies (greedy, 800)","title":"1807B - Grab the Candies - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1807b/"},{"content":"1807C - Find and Replace (greedy, implementation, strings, 800)\nSolution t = int(input()) for _ in range(t): n = int(input()) s = input() reserved_binaries = {} binary_values = [] for c in s: val = reserved_binaries.get(c, None) if not binary_values: binary_values.append(1) if val == binary_values[-1]: print('NO') break if not val: val = 1 if binary_values[-1] == 0 else 0 reserved_binaries[c] = val binary_values.append(val) else: print('YES') ","description":"Codeforces 1807C - Find and Replace (greedy, implementation, strings, 800)","title":"1807C - Find and Replace - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1807c/"},{"content":"1809A - Garland (implementation, 800)\nExplanation If all the light bulbs have the same color, it is impossible to turn them all on, as you can’t perform the operation on the same color consecutively. In this case, print -1. If there are 3 light bulbs of the same color and one light bulb of a different color, it takes 6 operations to turn them all on: Turn on the different colored light bulb Turn on one of the other colored light bulbs Turn off the different colored light bulb Turn on the second light bulb of the same color Turn on the different colored light bulb Turn on the third light bulb of the same color In all other cases, it takes 4 operations to turn all the light bulbs on, as you can switch the light bulbs on in a sequence without violating the color restriction. Solution def solve(s): if all(c == s[0] for c in s): return -1 elif s.count(s[0]) == 3 or s.count(s[1]) == 3: return 6 else: return 4 t = int(input()) for _ in range(t): s = input() print(solve(s)) ","description":"Codeforces 1809A - Garland (implementation, 800)","title":"1809A - Garland - 800","uri":"/en/tracks/algorithms-101/codeforces/01-implementation-and-greedy/1809a/"},{"content":"LeetCode problem\nProblem Statement Given an integer array nums, rotate the array to the right by k steps, where k is non-negative.\nNaive Solution A simple, but inefficient, approach would be to rotate the array k times. In each rotation, we shift every element of the array to the right by one and move the last element to the start of the array. This solution has a time complexity of O(n*k), where n is the number of elements in the array and k is the number of rotations. This is not an optimal solution, especially when we have a large k or a large array.\nEfficient Solution An efficient solution can be found by using array reversal. Here’s the plan:\nReverse the entire array. Reverse the first k elements. Reverse the remaining n-k elements. This method allows us to achieve the desired output in O(n) time and O(1) space complexity.\nSteps Let’s break down the steps using an example: nums = [1,2,3,4,5,6,7], k = 3.\nReverse the entire array: nums = [7,6,5,4,3,2,1]. Reverse the first k elements: nums = [5,6,7,4,3,2,1]. Reverse the remaining n-k elements: nums = [5,6,7,1,2,3,4]. As you can see, we get the expected output [5,6,7,1,2,3,4].\nPython Solution Here is the Python code that implements the aforementioned logic:\nclass Solution: def rotate(self, nums, k): def reverse(start, end): while start \u003c end: nums[start], nums[end] = nums[end], nums[start] start += 1 end -= 1 n = len(nums) k = k % n # in case k \u003e len(nums) reverse(nums, 0, n - 1) # 1 reverse(nums, 0, k - 1) # 2 reverse(nums, k, n - 1) # 3 ","description":"Rotating an array to the right by k steps","title":"189. Rotate Array","uri":"/en/tracks/algorithms-101/leetcode/medium/other/189/"},{"content":"LeetCode problem\nGiven the ‘head’ of a linked list, remove the ’nth’ node from the end of the list and return its head.\nExample 1:\nInput: head = [1,2,3,4,5], n = 2 Output: [1,2,3,5] Example 2:\nInput: head = [1], n = 1 Output: [] Idea:\nTwo pointers. Second pointer starts from nth position. Run while second pointer exist. First version:\n# Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def removeNthFromEnd(self, head, n: int): p1 = head p2 = head for _ in range(n): p1 = p1.next if not p1: return head.next # in case: head=[1], n=1 -\u003e return [] while p1.next: p1 = p1.next p2 = p2.next p2.next = p2.next.next return head ","description":"LeetCode 19. Remove Nth Node From End of List","title":"19. Remove Nth Node From End of List","uri":"/en/tracks/algorithms-101/leetcode/medium/19/"},{"content":"LeetCode problem\nThis task involves understanding how binary representation works. An unsigned integer is a 32-bit value, where each bit represents a power of 2, from 2^0 (the least significant bit) to 2^31 (the most significant bit).\nUnsigned Integers (often called “uints”) are just like integers (whole numbers) but have the property that they don’t have a + or - sign associated with them. Thus they are always non-negative (zero or positive).\nNaive Solution A naive solution to this problem could involve converting the number to a binary string, reversing the string, and then converting it back to an integer. However, this would not be the most efficient solution, especially for large numbers.\nApproach Using bitwise operation.\nA better approach is to manipulate the bits of the number directly. This can be done by initializing an empty result and then repeatedly shifting the result to the left to make room for the next bit, and adding the last bit of the input number.\nSteps Initialize the result to 0. Repeat the following steps 32 times, once for each bit in the input number: Shift the result one bit to the left to make room for the next bit. This can be done with the \u003c\u003c operator. Add the last bit of the input number to the result. This can be done with the \u0026 operator, which performs a bitwise AND operation. Shift the input number one bit to the right to prepare for the next iteration. This can be done with the \u003e\u003e operator. At the end of this process, the result will be the input number with its bits reversed.\nSolution class Solution: def reverseBits(self, n: int) -\u003e int: result = 0 for _ in range(32): result = (result \u003c\u003c 1) + (n \u0026 1) n \u003e\u003e= 1 return result result \u003c\u003c 1 shifts the bits of the result one place to the left, (n \u0026 1) gets the last bit of n, and n \u003e\u003e= 1 shifts the bits of n one place to the right.\nUnderstanding Example:\nOur number: n = 0110 1010\nOur aim is to reverse these bits to get 0101 0110.\nIn the solution, we initialize our result as 0 (0000 0000 in binary). We’re going to build this result bit by bit from the binary representation of n.\nThe key point of this operation is this line of code: result = (result \u003c\u003c 1) + (n \u0026 1). This line does three things:\n(result \u003c\u003c 1): This operation is a left shift operation. It shifts all the bits in result one place to the left. In binary, this is equivalent to multiplying by 2. So if result was 0101 (5 in decimal), after this operation result will be 1010 (10 in decimal). You can see we’ve made room for a new bit on the right. (n \u0026 1): This operation is a bitwise AND operation. The \u0026 operator compares each binary digit of two integers and returns a new integer, with a 1 wherever both numbers had a 1 and a 0 anywhere else. When n is ANDed with 1 (0000 0001), the result will be 1 only if the least significant bit of n is 1. This effectively gives us the last bit of n. (result \u003c\u003c 1) + (n \u0026 1): This line combines the above two steps. It shifts the bits of result one place to the left and adds the last bit of n to result. Let’s work through the first couple of iterations of the loop:\nOn the first iteration, result is 0000 0000. We shift result left (it remains 0000 0000), and add the last bit of n (which is 0). So result remains 0000 0000. We then shift n right to become 0011 0101 (n From 0110 1010 to 0011 0101). On the second iteration, result is 0000 0000. We shift result left (it remains 0000 0000), and add the last bit of n (which is 1). So result becomes 0000 0001. We then shift n right to become 0001 1010. If we repeat this process 8 times (for an 8-bit number), or 32 times (for a 32-bit number like in the problem), result will be the binary number formed by reversing the bits of n.\nOptimization If this function is called many times, way to optimize it is to cache the results for each byte (8 bits) instead of each bit. This would divide the computation time by 8.\n","description":"190. Reverse Bits","title":"190. Reverse Bits","uri":"/en/tracks/algorithms-101/leetcode/easy/other/190/"},{"content":"LeetCode problem\nProblem Statement Given an integer value represented in binary format, we need to count the number of ‘1’ bits in its binary representation.\nNaive Solution The naive solution for this problem would be to convert the binary number into a string and then simply iterate over the string and count the number of ‘1’s. However, this solution is not optimal and is not taking advantage of the properties of binary numbers.\nAlgorithm The optimal solution for this problem involves using bitwise operation. Bitwise operations are a type of operation that works on the binary representation of numbers.\nSpecifically, we’ll use the bitwise AND operator (\u0026) and bitwise right shift operator (\u003e\u003e).\nTo count the number of 1 bits in the binary representation of a number, we can AND the number with 1. If the result is 1, that means the least significant bit of the number is 1. We then right shift the number by 1 bit to check the next bit. We continue this process until the number becomes 0.\nHigh Level Solution Logic Initialize a counter variable to 0. While the number is not 0: AND the number with 1. If the result is 1, increment the counter. Right shift the number by 1 bit. Return the counter. Python Code Here’s the Python code for this solution:\nclass Solution: def hammingWeight(self, n: int) -\u003e int: bits = 0 while n: bits += n \u0026 1 n \u003e\u003e= 1 return bits Example Let’s say we have a binary number 00000001011, which is 11 in decimal.\nInitialize a counter variable to 0. So, bits = 0. Start the loop. The number n is 11, which is not 0, so we proceed. We perform the operation n \u0026 1. In binary, 1011 \u0026 0001 equals 0001, which is 1 in decimal. This is because the bitwise AND operation returns 1 only if both bits being compared are 1. So, since our least significant bit is 1, our AND operation returns 1. We increment our counter bits by 1. Now bits = 1. We right shift our number by 1 bit using the operation n \u003e\u003e= 1. This operation moves all the bits of the number one position to the right. Our number 1011 becomes 101 in binary or 5 in decimal. Our updated number n is 5, which is not 0, so we repeat the process. Now, n \u0026 1 is 101 \u0026 001 equals 001, which is 1 in decimal. So, we increment our counter bits by 1. Now bits = 2. We right shift our number by 1 bit. Our number 101 becomes 10 in binary or 2 in decimal. Our updated number n is 2, which is not 0, so we repeat the process. Now, n \u0026 1 is 10 \u0026 01 equals 00, which is 0 in decimal. So, we do not increment our counter bits. We right shift our number by 1 bit. Our number 10 becomes 1 in binary. Our updated number n is 1, which is not 0, so we repeat the process. Now, n \u0026 1 is 1 \u0026 1 equals 1. So, we increment our counter bits by 1. Now bits = 3. We right shift our number by 1 bit. Our number 1 becomes 0 in binary. Our updated number n is 0, which stops the loop. ","description":"Solving and understanding the LeetCode problem 191. Number of 1 Bits using bitwise operations in Python.","title":"191. Number of 1 Bits","uri":"/en/tracks/algorithms-101/leetcode/easy/other/191/"},{"content":"You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself.\nExample 1:\nInput: l1 = [2,4,3], l2 = [5,6,4] Output: [7,0,8] Explanation: 342 + 465 = 807. First accepted Idea:\nLoop through lists add each value to the list reverse list calculate sum create linked list from reversed sum class Solution: def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: def createLinkedNode(values): head = ListNode(values[0]) current = head for i in values[1:]: node = ListNode(i) current.next = node current = current.next return head res = None vals_l1 = [] cur = l1 while cur: vals_l1.append(cur.val) cur = cur.next vals_l2 = [] cur = l2 while cur: vals_l2.append(cur.val) cur = cur.next s_l1 = '' for i in reversed(vals_l1): s_l1 += str(i) s_l2 = '' for i in reversed(vals_l2): s_l2 += str(i) ll_sum = int(s_l1) + int(s_l2) values = [] for val in reversed(str(ll_sum)): values.append(int(val)) res = createLinkedNode(values) return res Better solution Idea:\nJust like how you would sum two numbers on a piece of paper.\nclass Solution: def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: dummyHead = ListNode(0) curr = dummyHead carry = 0 while l1 != None or l2 != None or carry != 0: l1Val = l1.val if l1 else 0 l2Val = l2.val if l2 else 0 columnSum = l1Val + l2Val + carry carry = columnSum // 10 newNode = ListNode(columnSum % 10) curr.next = newNode curr = newNode l1 = l1.next if l1 else None l2 = l2.next if l2 else None return dummyHead.next ","description":"LeetCode 2. Add Two Numbers","title":"2. Add Two Numbers","uri":"/en/tracks/algorithms-101/leetcode/medium/2/"},{"content":"LeetCode problem\nGiven a string s containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.\nAn input string is valid if:\nOpen brackets must be closed by the same type of brackets. Open brackets must be closed in the correct order. Every close bracket has a corresponding open bracket of the same type. Example 1:\nInput: s = \"()[]{}\" Output: true Example 2:\nInput: s = \"()[]{}\" Output: true First accepted Idea:\nLoop through string If current “closes” the last in stack, then remove last from stack Else: add current to stack class Solution: def isValid(self, s: str) -\u003e bool: stack = [] par_dict = {'(': ')', '{': '}', '[': ']'} last_value = None for i in s: second_value = par_dict.get(last_value, None) if i == second_value: stack.pop() last_value = stack[-1] if stack else None else: stack.append(i) last_value = i return not stack Better solution class Solution: def isValid(self, s: str) -\u003e bool: par_dict = {'(': ')', '{': '}', '[': ']'} stack = [] for char in s: if char in par_dict: # If it's an opening bracket, add it to the stack stack.append(char) elif stack: # If there's something in the stack if char == par_dict[stack[-1]]: # If it's a closing bracket for the last opened bracket, remove it from the stack. stack.pop() else: # It's not a closing bracket for the last opened bracket. Invalid string. return False else: # Not an opening bracket or closing bracket. Invalid string. return False return stack == [] ","description":"Leetcode 20. Valid Parentheses solution","title":"20. Valid Parentheses","uri":"/en/tracks/algorithms-101/leetcode/easy/20/"},{"content":"LeetCode problem\nProblem Statement In this problem, we are given a number n. We have to determine whether this number is a “happy” number or not. A happy number is a number defined by the following process:\nStarting with any positive integer, replace the number by the sum of the squares of its digits. Repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy. Naive Solution A naive solution would be to follow the process as stated in the problem description and use a data structure such as a set to check for repetitions indicating a cycle. If during the process, the number becomes 1, we can conclude that the number is happy. However, if we encounter a number that was already visited, it means we are stuck in a cycle, and the number is not happy.\nWe calculate the sum of squares of the digits of n in each iteration, and check if this sum is 1 or a number we’ve seen before. If it’s 1, we return true. If it’s a number we’ve seen before, we return false, as this means we’re in an endless loop. Hints \u0026 Tips However, continuously checking if a number was already visited can be costly in terms of time complexity. A more efficient way to detect cycles is to use the Floyd Cycle detection algorithm (also known as the “Tortoise and the Hare” algorithm).\nThis algorithm allows us to detect a cycle in the sequence without having to store all previously seen numbers, making it more efficient in terms of space usage.\nApproach Floyd Cycle detection algorithm works by moving two pointers at different speeds - a slow pointer (tortoise) and a fast pointer (hare). If there is a cycle, the fast pointer will eventually meet the slow pointer again.\nSteps Initialize two pointers slow and fast as n. Replace slow with the sum of the squares of its digits, and fast with the sum of squares of the next number in the sequence. If fast becomes 1, return True. - n is a happy number. If slow meets fast and the number is not 1, return False. - n is not a happy number as we have detected a cycle. Solution def isHappy(n): def get_next(num): # get the next number in the sequence total_sum = 0 while num \u003e 0: # get the last digit of the number and the remaining part num, digit = divmod(num, 10) total_sum += digit ** 2 return total_sum slow = n fast = get_next(n) visited = set() while fast != 1 and slow != fast: slow = get_next(slow) fast = get_next(get_next(fast)) visited.add(slow) if fast in visited: break return fast == 1 In this solution, the function get_next(n) is used to get the next number in the sequence by replacing n with the sum of the squares of its digits.\nWe initialize slow and fast to n and get_next(n) respectively.\nThen, until fast equals 1 or slow catches up to fast, we continue moving slow one step at a time and fast two steps at a time. If fast equals 1 at the end of the loop, n is a happy number.\n","description":"Learn how to determine if a number is happy using Python.","title":"202. Happy Number","uri":"/en/tracks/algorithms-101/leetcode/easy/other/202/"},{"content":"LeetCode problem\nYou are given the heads of two sorted linked lists list1 and list2.\nMerge the two lists in a one sorted list. The list should be made by splicing together the nodes of the first two lists.\nReturn the head of the merged linked list.\nExample 1:\nInput: list1 = [1,2,4], list2 = [1,3,4] Output: [1,1,2,3,4,4] Example 2:\nInput: list1 = [], list2 = [0] Output: [0] First accepted Idea:\nGet smallest head. Loop and update its next.\n# Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: res = ListNode() current = res while l1 and l2: if l1.val \u003c= l2.val: node = ListNode(l1.val) l1 = l1.next else: node = ListNode(l2.val) l2 = l2.next current.next = node current = current.next if l1: current.next = l1 if l2: current.next = l2 return res.next Better solution Recursion\ndef mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: if l1 and l2: if l1.val \u003e l2.val: l1, l2 = l2, l1 #swap smaller and larger: make l1 the one with the smaller first value l1.next = self.mergeTwoLists(l1.next, l2) # move forward in the list which starts with the smaller value return l1 or l2 # return whichever of the two lists remains at the end Loop\ndef mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u003e Optional[ListNode]: res = ListNode() current = res while l1 and l2: if l1.val \u003c= l2.val: current.next = l1 l1 = l1.next else: current.next = l2 l2 = l2.next current = current.next class Solution: def mergeTwoLists(self, a, b): if a and b: if a.val \u003e b.val: a, b = b, a a.next = self.mergeTwoLists(a.next, b) return a or b First make sure that a is the “better” one (meaning b is None or has larger/equal value). Then merge the remainders behind a.\ndef mergeTwoLists(self, a, b): if not a or b and a.val \u003e b.val: a, b = b, a if a: a.next = self.mergeTwoLists(a.next, b) return a ","description":"Leetcode 21. Merge Two Sorted Lists","title":"21. Merge Two Sorted Lists","uri":"/en/tracks/algorithms-101/leetcode/easy/21/"},{"content":"LeetCode problem\nGiven n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.\nExample 1:\nInput: n = 3 Output: [\"((()))\",\"(()())\",\"(())()\",\"()(())\",\"()()()\"] Example 2:\nInput: n = 1 Output: [\"()\"] Prerequirements Backtracking pattern\nFirst accepted Idea:\nclass Solution: def generateParenthesis(self, n): ans = [] def dfs(l: int, r: int, s: str) -\u003e None: if l == 0 and r == 0: ans.append(s) if l \u003e 0: dfs(l - 1, r, s + '(') if l \u003c r: dfs(l, r - 1, s + ')') dfs(n, n, '') return ans ","description":"LeetCode 22. Generate Parentheses","title":"22. Generate Parentheses","uri":"/en/tracks/algorithms-101/leetcode/medium/22/"},{"content":"LeetCode problem\nProblem Statement Given the head of a singly linked list, return true if it is a palindrome or false otherwise.\nNaive Solution A simple solution to this problem is to:\ntraverse the linked list storing the value of each node in an array. Then, we could compare the array with its reversed version.\nIf they match, the linked list is a palindrome. Otherwise, it is not.\nThis solution takes O(n) time (where n is the number of nodes in the list), as we need to traverse the list once.\nHowever, it also takes O(n) space, as we store the value of each node in an array.\nApproach To solve the problem in O(n) time and O(1) space, we can use the two-pointer technique to find the middle of the linked list. Then, we can reverse the second half of the list in-place. After that, we can compare the first half with the reversed second half. If they match, the list is a palindrome.\nReversing a linked list in-place involves changing the next pointers of the nodes to point to the previous node. This process can be done with a constant amount of space.\nSteps Initialize two pointers: slow and fast at the head of the list. Move slow one step at a time and fast two steps at a time. When fast reaches the end of the list, slow will be at the middle. Reverse the second half of the list starting from slow. Compare the first half of the list with the reversed second half. If they match, return true. If they don’t, return false. Solution class Solution: def isPalindrome(self, head: ListNode) -\u003e bool: slow = fast = head # find the mid node while fast and fast.next: slow = slow.next fast = fast.next.next # reverse the second half prev = None cur = slow while cur: # 1 [1 2 3 4] nxt = cur.next # 2 cur.next = prev # 1.next = None prev = cur # 1, at the end of loop will be 4 cur = nxt # 2 # compare the first and second half nodes while prev: if prev.val != head.val: return False prev = prev.next head = head.next return True Debug of Reversing Assuming we have a linked list as [1,2,3,4,5,6] and slow initially points to 4. Result should be [6,5,4,3,2,1]\nInitial state:\nLinked list: 1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e 5 -\u003e 6 cur points to 4 prev = None First iteration: nxt is assigned 5 (the next node after cur) cur.next (the next node after 4) is assigned None prev is assigned 4 cur is assigned 5 (nxt) After first iteration:\nLinked list: 1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e None, 5 -\u003e 6 cur points to 5 prev points to 4 Second iteration: nxt is assigned 6 cur.next (the next node after 5) is assigned 4 (prev) prev is assigned 5 cur is assigned 6 (nxt) After second iteration:\nLinked list: 1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e None, 5 -\u003e 4, 6 cur points to 6 prev points to 5 Third iteration: nxt is assigned None cur.next (the next node after 6) is assigned 5 (prev) prev is assigned 6 cur is assigned None (nxt) After third iteration:\nLinked list: 1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e None, 5 -\u003e 4, 6 -\u003e 5 cur points to None prev points to 6 Since cur is None, we exit the while loop. Now prev is pointing to the head of the reversed second half of the list.\nThe list now looks like this: 1 -\u003e 2 -\u003e 3 -\u003e 4 -\u003e None and 6 -\u003e 5 -\u003e 4 -\u003e None.\n","description":"234. Palindrome Linked List","title":"234. Palindrome Linked List","uri":"/en/tracks/algorithms-101/leetcode/easy/other/234/"},{"content":"LeetCode problem\nGiven an integer array nums sorted in non-decreasing order, remove the duplicates in-place such that each unique element appears only once. The relative order of the elements should be kept the same.\nSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\nReturn k after placing the final result in the first k slots of nums.\nDo not allocate extra space for another array. You must do this by modifying the input array in-place with O(1) extra memory.\nCustom Judge:\nThe judge will test your solution with the following code:\nint[] nums = [...]; // Input array int[] expectedNums = [...]; // The expected answer with correct length int k = removeDuplicates(nums); // Calls your implementation assert k == expectedNums.length; for (int i = 0; i \u003c k; i++) { assert nums[i] == expectedNums[i]; } If all assertions pass, then your solution will be accepted.\nExample 1:\nInput: nums = [1,1,2] Output: 2, nums = [1,2,_] Explanation: Your function should return k = 2, with the first two elements of nums being 1 and 2 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). Example 2:\nInput: nums = [0,0,1,1,1,2,2,3,3,4] Output: 5, nums = [0,1,2,3,4,_,_,_,_,_] Explanation: Your function should return k = 5, with the first five elements of nums being 0, 1, 2, 3, and 4 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). First accepted Idea:\nclass Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: k = 1 if len(nums) == 1: return k p1 = 0 p2 = 1 while p2 \u003c len(nums): if nums[p1] == nums[p2]: p2 += 1 else: p1 += 1 nums[p1] = nums[p2] p2 += 1 k += 1 return k Better solution class Solution: def removeDuplicates(self, nums: List[int]) -\u003e int: if len(nums) == 1: return 1 k = 1 i = 1 for n in nums: if nums[i-1] != n: nums[i] = n i += 1 k += 1 return k ","description":"LeetCode 26. Remove Duplicates from Sorted Array","title":"26. Remove Duplicates from Sorted Array","uri":"/en/tracks/algorithms-101/leetcode/easy/26/"},{"content":"LeetCode problem\nGiven two strings needle and haystack, return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.\nExample 1:\nInput: haystack = “sadbutsad”, needle = “sad” Output: 0 Explanation: “sad” occurs at index 0 and 6. The first occurrence is at index 0, so we return 0.\nExample 2:\nInput: haystack = “leetcode”, needle = “leeto” Output: -1 Explanation: “leeto” did not occur in “leetcode”, so we return -1.\nCode class Solution: def strStr(self, haystack: str, needle: str) -\u003e int: return haystack.find(needle) class Solution: def strStr(self, haystack: str, needle: str) -\u003e int: start = 0 end = len(needle) while end \u003c= len(haystack): if haystack[start:end] == needle: return start start += 1 end += 1 return -1 ","description":"LeetCode 28. Find the Index of the First Occurrence in a String","title":"28. Find the Index of the First Occurrence in a String","uri":"/en/tracks/algorithms-101/leetcode/medium/28.en./"},{"content":"LeetCode problem\nGiven two integers dividend and divisor, divide two integers without using multiplication, division, and mod operator.\nThe integer division should truncate toward zero, which means losing its fractional part. For example, 8.345 would be truncated to 8, and -2.7335 would be truncated to -2.\nReturn the quotient after dividing dividend by divisor.\nNote: Assume we are dealing with an environment that could only store integers within the 32-bit signed integer range: [−231, 231 − 1]. For this problem, if the quotient is strictly greater than 231 - 1, then return 231 - 1, and if the quotient is strictly less than -231, then return -231.\nExample 1:\nInput: dividend = 10, divisor = 3 Output: 3 Explanation: 10/3 = 3.33333.. which is truncated to 3.\nExample 2:\nInput: dividend = 7, divisor = -3 Output: -2 Explanation: 7/-3 = -2.33333.. which is truncated to -2.\nCode Idea:\nRemove decimals from both divisor and divident Remember the result sign (positive or \u003c 0) Subtract divisor from divident until result is less or equal to zero. Works but is too slow in case small number divisor (1) and greater number dividend (-2147483648):\nclass Solution: def divide(self, dividend: int, divisor: int) -\u003e int: res = 0 dd = abs(dividend) ds = abs(divisor) sign = -1 if (dividend \u003e 0 and divisor \u003c 0) or (dividend \u003c 0 and divisor \u003e 0) else 1 while dd \u003e= ds: dd -= ds res += 1 return sign * res Improve idea:\nSum divisor after “success” subtract until result of subtract is \u003e 0 Subtract divisor back until we can subtract it from dividend class Solution: def divide(self, dividend: int, divisor: int) -\u003e int: res = 0 dd = abs(dividend) ds = abs(divisor) sign = -1 if (dividend \u003e 0 and divisor \u003c 0) or (dividend \u003c 0 and divisor \u003e 0) else 1 if divisor == -1 and dividend == -2147483648: return 2147483647 elif divisor == 1: return sign * dd while dd \u003e= ds: tmp = ds multiples = 1 # count of subtracts while dd \u003e= tmp: ## sum divisor dd -= tmp res += multiples # hense sum count of subtracts tmp += tmp multiples += multiples else: if dd \u003e= ds: dd -= ds res += 1 return sign * res Better idea Idea: Bit manipulation\nclass Solution: def divide(self, dividend, divisor): positive = (dividend \u003c 0) is (divisor \u003c 0) dividend, divisor = abs(dividend), abs(divisor) res = 0 while dividend \u003e= divisor: curr_divisor, num_divisors = divisor, 1 while dividend \u003e= curr_divisor: dividend -= curr_divisor res += num_divisors curr_divisor = curr_divisor \u003c\u003c 1 num_divisors = num_divisors \u003c\u003c 1 if not positive: res = -res return min(max(-2147483648, res), 2147483647) Explanation:\nhttps://leetcode.com/problems/divide-two-integers/discuss/715094/Python-fast-code-with-detailed-explanation Another:\nTime: $O(\\log^2 n)$ Space: $O(1)$\nclass Solution: def divide(self, dividend: int, divisor: int) -\u003e int: if dividend == -2**31 and divisor == -1: return 2**31 - 1 sign = -1 if (dividend \u003e 0) ^ (divisor \u003e 0) else 1 ans = 0 dvd = abs(dividend) dvs = abs(divisor) while dvd \u003e= dvs: k = 1 while k * 2 * dvs \u003c= dvd: k \u003c\u003c= 1 dvd -= k * dvs ans += k return sign * ans ","description":"LeetCode 29. Divide Two Integers","title":"29. Divide Two Integers","uri":"/en/tracks/algorithms-101/leetcode/medium/29/"},{"content":"LeetCode problem\nGiven a string s, find the length of the longest substring without repeating characters.\nExample 1:\nInput: s = \"abcabcbb\" Output: 3 Explanation: The answer is \"abc\", with the length of 3. First accepted Idea:\nLoop through string Calculate max count of elements in substring If get double element, then go back until get this element and do step 2. Proceed the main loop class Solution: def lengthOfLongestSubstring(self, s: str) -\u003e int: uniqs = set() len_max = 0 len_current = 0 idx = 0 for i in s: if i in uniqs: len_max = max(len_max, len_current) len_current = 1 uniqs = set(i) for j in reversed(s[:idx]): if j == i: break else: len_current += 1 uniqs.add(j) else: uniqs.add(i) len_current += 1 len_max = max(len_max, len_current) idx += 1 return len_max Better solution Sliding Window - template\nWindow Sliding Technique is a computational technique which aims to reduce the use of nested loop and replace it with a single loop, thereby reducing the time complexity. The Sliding window technique can reduce the time complexity to O(n).\nTips for identifying this kind of problem where we could use the sliding window technique:\nThe problem will be based on an array, string, or list data structure. You need to find the subrange in this array or string that should provide the longest, shortest, or target values. A classic problem: to find the largest/smallest sum of given k (for example, three) consecutive numbers in an array.\nclass Solution: def lengthOfLongestSubstring(self, s: str) -\u003e int: n = len(s) ans = 0 # mp stores the current index of a character mp = {} i = 0 # try to extend the range [i, j] for j in range(n): if s[j] in mp: i = max(mp[s[j]], i) ans = max(ans, j - i + 1) mp[s[j]] = j + 1 return ans class Solution: def lengthOfLongestSubstring(self, s: str) -\u003e int: chars = [None] * 128 left = right = 0 res = 0 while right \u003c len(s): r = s[right] index = chars[ord(r)] if index is not None and left \u003c= index \u003c right: left = index + 1 res = max(res, right - left + 1) chars[ord(r)] = right right += 1 return res class Solution(): def lengthOfLongestSubstring(self, s): max_len = 0 substr = '' for char in s: if char not in substr: substr += char max_len = max(max_len, len(substr)) else: start = substr.index(char) + 1 substr = substr[start:] + char return max_len Resources https://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2694302/JS-or-98-or-Sliding-window-or-With-exlanation https://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2133524/JavaC%2B%2B-A-reall-Detailed-Explanation ","description":"LeetCode 3. Longest Substring Without Repeating Characters","title":"3. Longest Substring Without Repeating Characters","uri":"/en/tracks/algorithms-101/leetcode/medium/3/"},{"content":"LeetCode problem\nThere is an integer array nums sorted in ascending order (with distinct values).\nPrior to being passed to your function, nums is possibly rotated at an unknown pivot index k (1 \u003c= k \u003c nums.length) such that the resulting array is [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]] (0-indexed). For example, [0,1,2,4,5,6,7] might be rotated at pivot index 3 and become [4,5,6,7,0,1,2].\nGiven the array nums after the possible rotation and an integer target, return the index of target if it is in nums, or -1 if it is not in nums.\nYou must write an algorithm with O(log n) runtime complexity.\nExample 1:\nInput: nums = [4,5,6,7,0,1,2], target = 0 Output: 4\nExample 2:\nInput: nums = [4,5,6,7,0,1,2], target = 3 Output: -1\nExample 3:\nInput: nums = [1], target = 0 Output: -1\nCode Idea:\nValues in the right part of the array are always lower than in the left part.\nUse binary search Define where to move (left or right) class Solution: def search(self, nums: List[int], target: int) -\u003e int: left = 0 right = len(nums) - 1 while left \u003c= right: mid = (left + right) // 2 if nums[mid] == target: return mid if nums[left] \u003c= nums[mid]: if nums[left] \u003c= target \u003c nums[mid]: right = mid - 1 else: left = mid + 1 else: if nums[mid] \u003c target \u003c= nums[right]: left = mid + 1 else: right = mid - 1 return -1 ","description":"LeetCode 33. Search in Rotated Sorted Array","title":"33. Search in Rotated Sorted Array","uri":"/en/tracks/algorithms-101/leetcode/medium/33/"},{"content":"LeetCode problem\nGiven an array of integers nums sorted in non-decreasing order, find the starting and ending position of a given target value.\nIf target is not found in the array, return [-1, -1].\nYou must write an algorithm with O(log n) runtime complexity.\nExample 1:\nInput: nums = [5,7,7,8,8,10], target = 8 Output: [3,4]\nExample 2:\nInput: nums = [5,7,7,8,8,10], target = 6 Output: [-1,-1]\nExample 3:\nInput: nums = [], target = 0 Output: [-1,-1]\nCode Idea:\nFind target index (target_index) using Binary Search If not exist then return [-1, -1] If exist then goto step 2 We got the middle index. For now this is the most left and most right index. Divide nums into two arrays: left_nums and right_nums: left_nums = nums[0:target_index] right_nums = nums[target_index:] Find the most left target in left_nums. (Set right border in subarray) Find the most right target in right_nums. (Set left border in subarray) class Solution: def searchRange(self, nums: List[int], target: int) -\u003e List[int]: def find_target(): left = 0 right = len(nums) - 1 while left \u003c= right: mid = (left + right) // 2 if nums[mid] == target: return mid if nums[mid] \u003c target: left = mid + 1 else: right = mid - 1 return -1 def find_most_left(right_idx): l = 0 r = right_idx while l \u003c= r: m = (l + r) // 2 if nums[m] \u003c target: l = m + 1 else: r = m - 1 return l def find_most_right(left_idx): l = left_idx r = len(nums) - 1 while l \u003c= r: m = (l + r) // 2 if nums[m] == target: # ex: [8, 8, 8, 9, 10] l = l + 1 else: # ex: [8, 8, 8, 9, 10] r = m - 1 return l - 1 target_idx = find_target() if target_idx == -1: return [-1,-1] left = find_most_left(target_idx) right = find_most_right(target_idx) return [left, right] Code Ver2 Use prebuilt Python functions:\nbisect_left bisect_right class Solution: def searchRange(self, nums: List[int], target: int) -\u003e List[int]: l = bisect_left(nums, target) if l == len(nums) or nums[l] != target: return -1, -1 r = bisect_right(nums, target) - 1 return l, r ","description":"LeetCode 34. Find First and Last Position of Element in Sorted Array","title":"34. Find First and Last Position of Element in Sorted Array","uri":"/en/tracks/algorithms-101/leetcode/medium/34/"},{"content":"LeetCode problem\nDetermine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:\nEach row must contain the digits 1-9 without repetition. Each column must contain the digits 1-9 without repetition. Each of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.\nNote:\nA Sudoku board (partially filled) could be valid but is not necessarily solvable. Only the filled cells need to be validated according to the mentioned rules. Example 1:\nInput: board = [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"] ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"] ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"] ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"] ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"] ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"] ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"] ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"] ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] Output: true Example 2:\nInput: board = [[\"8\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"] ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"] ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"] ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"] ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"] ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"] ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"] ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"] ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] Output: false Explanation: Same as Example 1, except with the 5 in the top left corner being modified to 8. Since there are two 8's in the top left 3x3 sub-box, it is invalid. Code Idea:\nclass Solution: def isValidSudoku(self, board: List[List[str]]) -\u003e bool: exist = set() for i in range(9): for j in range(9): x = board[i][j] if x != '.': uniqs = ( (i, x), (x, j), (int(i/3), int(j/3), x) ) # devide 3 because of third check in 3x3 block for z in uniqs: if z in exist: return False exist.add(z) return True ","description":"LeetCode 36. Valid Sudoku","title":"36. Valid Sudoku","uri":"/en/tracks/algorithms-101/leetcode/medium/36/"},{"content":"LeetCode problem\nThe count-and-say sequence is a sequence of digit strings defined by the recursive formula:\ncountAndSay(1) = \"1\" countAndSay(n) is the way you would “say” the digit string from countAndSay(n-1), which is then converted into a different digit string. To determine how you “say” a digit string, split it into the minimal number of substrings such that each substring contains exactly one unique digit. Then for each substring, say the number of digits, then say the digit. Finally, concatenate every said digit.\nFor example, the saying and conversion for digit string \"3322251\": Given a positive integer n, return the nth term of the count-and-say sequence.\nExample 1:\nInput: n = 1 Output: \"1\" Explanation: This is the base case. Example 2:\nInput: n = 4 Output: \"1211\" Explanation: countAndSay(1) = \"1\" countAndSay(2) = say \"1\" = one 1 = \"11\" countAndSay(3) = say \"11\" = two 1's = \"21\" countAndSay(4) = say \"21\" = one 2 + one 1 = \"12\" + \"11\" = \"1211\" Idea:\nclass Solution: def countAndSay(self, n: int) -\u003e str: res = '1' while n \u003e 1: l = len(res) new_str = '' i = 0 while i \u003c l: count = 1 while i \u003c l - 1 and res[i] == res[i+1]: count += 1 i += 1 new_str += str(count) + res[i] i += 1 res = new_str n -= 1 return res ","description":"LeetCode 38. Count and Say","title":"38. Count and Say","uri":"/en/tracks/algorithms-101/leetcode/medium/38/"},{"content":"LeetCode problem\nGiven an array nums of distinct integers, return all the possible permutations. You can return the answer in any order.\nExample 1:\nInput: nums = [1,2,3] Output: [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] Example 2:\nInput: nums = [0,1] Output: [[0,1],[1,0]] Example 3:\nInput: nums = [1] Output: [[1]] Idea:\nDraw a decigion tree Fix when branch is ready to return Implementation:\nRecursive: Go through every value in nums Pop value call perm() with updated nums from each call(step) append ‘poped’ value from step 2 class Solution: def permute(self, nums: List[int]) -\u003e List[List[int]]: result_permutation = [] if len(nums) == 1: # base case return [nums[:]] for _ in nums: tmp_removed = nums.pop(0) # remove current element before next step permutations = self.permute(nums) for perm in permutations: perm.append(tmp_removed) nums.append(tmp_removed) result_permutation.extend(permutations) return result_permutation Resources https://www.youtube.com/watch?v=s7AvT7cGdSo https://walkccc.me/LeetCode/problems/0046/ ","description":"LeetCode 46. Permutations","title":"46. Permutations","uri":"/en/tracks/algorithms-101/leetcode/medium/46/"},{"content":"LeetCode problem\nYou are given an n x n 2D matrix representing an image, rotate the image by 90 degrees (clockwise).\nYou have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation.\nExample 1:\nInput: matrix = [[1,2,3],[4,5,6],[7,8,9]] Output: [[7,4,1],[8,5,2],[9,6,3]] Example 2:\nInput: matrix = [[5,1,9,11],[2,4,8,10],[13,3,6,7],[15,14,12,16]] Output: [[15,13,2,5],[14,3,4,1],[12,6,8,9],[16,7,10,11]] Idea:\nclass Solution: def rotate(self, matrix: List[List[int]]) -\u003e None: \"\"\" Do not return anything, modify matrix in-place instead. \"\"\" l = 0 r = len(matrix) - 1 while l \u003c r: for i in range(r-l): # for not only \"corners\" t = l b = r top_left = matrix[t][l + i] matrix[t][l + i] = matrix[b - i][l] # top left=bottom left matrix[b - i][l] = matrix[b][r - i] # bottom left=bottom right matrix[b][r - i] = matrix[t+i][r] # bottom right=top right matrix[t + i][r] = top_left # top right=top left l += 1 r -= 1 Approach 2: Reverse:\nclass Solution: def rotate(self, matrix: List[List[int]]) -\u003e None: matrix.reverse() for i in range(len(matrix)): for j in range(i + 1, len(matrix)): matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j] Resources https://www.youtube.com/watch?v=fMSJSS7eO1w https://walkccc.me/LeetCode/problems/0048/ ","description":"LeetCode 48. Rotate Image","title":"48. Rotate Image","uri":"/en/tracks/algorithms-101/leetcode/medium/48/"},{"content":"LeetCode problem\nGiven an array of strings strs, group the anagrams together. You can return the answer in any order.\nAn Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\nExample 1:\nInput: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"] Output: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]] Example 2:\nInput: strs = [\"\"] Output: [[\"\"]] Example 3:\nInput: strs = [\"a\"] Output: [[\"a\"]] Idea:\nclass Solution: def groupAnagrams(self, strs: List[str]) -\u003e List[List[str]]: dd = {} for s in strs: s_sort = \"\".join(sorted(s)) values = dd.get(s_sort, []) values.append(s) dd[s_sort] = values return dd.values() Approach 2:\nIntuition:\nTwo strings are anagrams if and only if their character counts (respective number of occurrences of each character) are the same.\nAlgorithm:\nWe can transform each string s into a character count, count\\text{count}count, consisting of 26 non-negative integers representing the number of a’s, b’s, z’s, etc. We use these counts as the basis for our hash map.\nIn python, the representation will be a tuple of the counts. For example, abbccc will be (1, 2, 3, 0, 0, ..., 0), where again there are 26 entries total.\nclass Solution: def groupAnagrams(strs): ans = collections.defaultdict(list) for s in strs: count = [0] * 26 for c in s: count[ord(c) - ord('a')] += 1 ans[tuple(count)].append(s) return ans.values() Resources LeetCode expl ","description":"LeetCode 49. Group Anagrams","title":"49. Group Anagrams","uri":"/en/tracks/algorithms-101/leetcode/medium/49/"},{"content":"LeetCode problem\nGiven a string s, return the longest palindromic substring in s.\nA string is called a palindrome string if the reverse of that string is the same as the original string.\nExample 1:\nInput: s = \"babad\" Output: \"bab\" Explanation: \"aba\" is also a valid answer. Example 2:\nInput: s = \"cbbd\" Output: \"bb\" First accepted Hints How can we reuse a previously computed palindrome to compute a larger palindrome?\nHow can we reuse a previously computed palindrome to compute a larger palindrome?\nComplexity based hint: If we use brute-force and check whether for every start and end position a substring is a palindrome we have O(n^2) start - end pairs and O(n) palindromic checks. Can we reduce the time for palindromic checks to O(1) by reusing some previous computation.\nIdea:\nWe start at index = 0 and iterate through all values until n. At each index we call the function getPalindrome that will check the values to the left and right of the provided indices. It will continue to do so until the longest palindrome within the given range is found.\nLink to diagram\nclass Solution: def longestPalindrome(self, s: str) -\u003e str: def getPalindrome(left, right): while(left \u003e= 0 and right \u003c len(s) and s[left] == s[right]): left -= 1 right += 1 return left+1, right-1 pal_left = 0 pal_right = 0 len_max = 1 for i in range(len(s)): left, right = getPalindrome(i, i) pal_len= right - left + 1 if pal_len \u003e len_max: pal_left = left pal_right = right len_max = pal_len left, right = getPalindrome(i, i+1) pal_len = right - left + 1 if pal_len \u003e len_max: pal_left = left pal_right = right len_max = pal_len return s[pal_left:pal_right+1] Better solution Manacher’s algorithm There is an O(n) algorithm called Manacher’s algorithm.\nclass Solution: def longestPalindrome(self, s: str) -\u003e str: # @ and $ signs are sentinels appended to each end to avoid bounds checking t = '#'.join('@' + s + '$') n = len(t) # t[i - maxExtends[i]..i) == # t[i + 1..i + maxExtends[i]] maxExtends = [0] * n center = 0 for i in range(1, n - 1): rightBoundary = center + maxExtends[center] mirrorIndex = center - (i - center) maxExtends[i] = rightBoundary \u003e i and \\ min(rightBoundary - i, maxExtends[mirrorIndex]) # Attempt to expand palindrome centered at i while t[i + 1 + maxExtends[i]] == t[i - 1 - maxExtends[i]]: maxExtends[i] += 1 # If palindrome centered at i expand past rightBoundary, # adjust center based on expanded palindrome. if i + maxExtends[i] \u003e rightBoundary: center = i # Find the maxExtend and bestCenter maxExtend, bestCenter = max((extend, i) for i, extend in enumerate(maxExtends)) return s[(bestCenter - maxExtend) // 2:(bestCenter + maxExtend) // 2] Resources Manacher’s algorithm Errichto:Leetcode problem Longest Palindromic Substring (two solutions) https://redquark.org/leetcode/0005-longest-palindromic-substring/ RU\nРазбор задачи с интервью. Литкод 5. Longest Palindromic Substring Алгоритмика: Алгоритм Манакера Википедия:Алгоритм Манакера ","description":"Leetcode 5. Longest Palindromic Substring | Python soulution and explanation","title":"5. Longest Palindromic Substring","uri":"/en/tracks/algorithms-101/leetcode/medium/5/"},{"content":"LeetCode problem\nImplement pow(x, n), which calculates x raised to the power n (i.e., x^n).\nExample 1:\nInput: x = 2.00000, n = 10 Output: 1024.00000 Example 2:\nInput: x = 2.10000, n = 3 Output: 9.26100 Example 3:\nInput: x = 2.00000, n = -2 Output: 0.25000 Explanation: 2-2 = 1/22 = 1/4 = 0.25 Approach 1:\nclass Solution: def myPow(self, x: float, n: int) -\u003e float: return x ** n Approach 2:\nRecursive\nclass Solution: def myPow(self, x, n): if not n: return 1 if n \u003c 0: return 1 / self.myPow(x, -n) if n % 2: return x * self.myPow(x, n-1) return self.myPow(x * x, n/2) Approach 3:\nclass Solution: def myPow(self, x, n): if n \u003c 0: x = 1 / x n = -n pow = 1 while n: if n \u0026 1: pow *= x x *= x n \u003e\u003e= 1 return pow ","description":"LeetCode 50. Pow(x, n)","title":"50. Pow(x, n)","uri":"/en/tracks/algorithms-101/leetcode/medium/50/"},{"content":"LeetCode problem\nGiven an integer array nums, find the subarray which has the largest sum and return its sum.\nExample 1:\nInput: nums = [-2,1,-3,4,-1,2,1,-5,4] Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Example 2:\nInput: nums = [1] Output: 1 Example 3:\nInput: nums = [5,4,-1,7,8] Output: 23 Approach 1:\nclass Solution: def maxSubArray(self, nums: List[int]) -\u003e int: max_ = nums[0] max2 = nums[0] if len(nums) == 1: return max_ for i in range(1, len(nums)): max_ = max(nums[i], nums[i] + max_) max2 = max(max_, max2) return max2 ","description":"LeetCode 53. Maximum Subarray","title":"53. Maximum Subarray","uri":"/en/tracks/algorithms-101/leetcode/medium/53/"},{"content":"LeetCode problem\nYou are given an integer array nums. You are initially positioned at the array’s first index, and each element in the array represents your maximum jump length at that position.\nReturn true if you can reach the last index, or false otherwise.\nExample 1:\nInput: nums = [2,3,1,1,4] Output: true Explanation: Jump 1 step from index 0 to 1, then 3 steps to the last index. Example 2:\nInput: nums = [3,2,1,0,4] Output: false Explanation: You will always arrive at index 3 no matter what. Its maximum jump length is 0, which makes it impossible to reach the last index. Approach 1:\nIdea: go forward on each step and mark next cell if can achieve it.\nclass Solution: def canJump(self, nums: List[int]) -\u003e bool: last_i = len(nums) if last_i == 1: return True nn = [0] * last_i nn[0] = nums[0] for i in range(last_i): el = nums[i] if el or nn[i+1]: for j in range(el): nn[i+j+1] = el if nn[last_i - 1]: return True else: return False return False Approach 2:\nGoing forwards. m tells the maximum index we can reach so far.\nclass Solution: def canJump(self, nums): m = 0 for i, n in enumerate(nums): if i \u003e m: return False m = max(m, i + n) return True class Solution: def canJump(self, nums: List[int]) -\u003e bool: i = 0 m = 0 while i \u003c len(nums) and i \u003c= m: m = max(m, i + nums[i]) i += 1 return i == len(nums) ","description":"LeetCode 55. Jump Game","title":"55. Jump Game","uri":"/en/tracks/algorithms-101/leetcode/medium/55/"},{"content":"LeetCode problem\nGiven an array of intervals where intervals[i] = [starti, endi], merge all overlapping intervals, and return an array of the non-overlapping intervals that cover all the intervals in the input.\nExample 1:\nInput: intervals = [[1,3],[2,6],[8,10],[15,18]] Output: [[1,6],[8,10],[15,18]] Explanation: Since intervals [1,3] and [2,6] overlap, merge them into [1,6]. Example 2:\nInput: intervals = [[1,4],[4,5]] Output: [[1,5]] Explanation: Intervals [1,4] and [4,5] are considered overlapping. Approach 1:\nclass Solution: def merge(self, intervals: List[List[int]]) -\u003e List[List[int]]: intervals.sort() res = [intervals[0]] for ir in range(1, len(intervals)): if intervals[ir][0] \u003e= res[-1][0] and intervals[ir][0] \u003c= res[-1][1]: # [1,3],[2,6] res[-1][0] = min(intervals[ir][0], res[-1][0]) res[-1][1] = max(intervals[ir][1], res[-1][1]) elif res[-1][0] \u003e= intervals[ir][0] and res[-1][0] \u003c= intervals[ir][1]: # [1,3],[0,4] res[-1][0] = min(intervals[ir][0], res[-1][0]) res[-1][1] = max(intervals[ir][1], res[-1][1]) else: res.append(intervals[ir]) return res Approach 2:\nclass Solution: def merge(self, intervals: List[List[int]]) -\u003e List[List[int]]: ans = [] for interval in sorted(intervals): if not ans or ans[-1][1] \u003c interval[0]: ans.append(interval) else: ans[-1][1] = max(ans[-1][1], interval[1]) return ans ","description":"LeetCode 56. Merge Intervals","title":"56. Merge Intervals","uri":"/en/tracks/algorithms-101/leetcode/medium/56/"},{"content":"LeetCode problem\nThere is a robot on an m x n grid. The robot is initially located at the top-left corner (i.e., grid[0][0]). The robot tries to move to the bottom-right corner (i.e., grid[m - 1][n - 1]). The robot can only move either down or right at any point in time.\nGiven the two integers m and n, return the number of possible unique paths that the robot can take to reach the bottom-right corner.\nThe test cases are generated so that the answer will be less than or equal to 2 * 10^9.\nExample 1:\nInput: m = 3, n = 7 Output: 28 Example 2:\nInput: m = 3, n = 2 Output: 3 Explanation: From the top-left corner, there are a total of 3 ways to reach the bottom-right corner: 1. Right -\u003e Down -\u003e Down 2. Down -\u003e Down -\u003e Right 3. Down -\u003e Right -\u003e Down Approach 1:\nLeetCode Submission\nclass Solution: def uniquePaths(self, m: int, n: int) -\u003e int: if m == 1 or n == 1: return 1 matrix = [ [1 for j in range(n)] for i in range(m)] for i in range(1, m): for j in range(1, n): max_above = 0 max_left = 1 if i \u003e 0: max_above = matrix[i-1][j] if j \u003e 0: max_left = matrix[i][j-1] matrix[i][j] = max_above + max_left m = matrix[i][j] return m class Solution: def uniquePaths(self, m: int, n: int) -\u003e int: matrix = [[1] * n for _ in range(m)] for i in range(1, m): for j in range(1, n): matrix[i][j] = matrix[i - 1][j] + matrix[i][j - 1] return matrix[-1][-1] ","description":"LeetCode 62. Unique Paths","title":"62. Unique Paths","uri":"/en/tracks/algorithms-101/leetcode/medium/62/"},{"content":"LeetCode problem\nYou are given a large integer represented as an integer array digits, where each digits[i] is the ith digit of the integer. The digits are ordered from most significant to least significant in left-to-right order. The large integer does not contain any leading 0’s.\nIncrement the large integer by one and return the resulting array of digits.\nExample 1:\nInput: digits = [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Incrementing by one gives 123 + 1 = 124. Thus, the result should be [1,2,4]. Example 2:\nInput: digits = [4,3,2,1] Output: [4,3,2,2] Explanation: The array represents the integer 4321. Incrementing by one gives 4321 + 1 = 4322. Thus, the result should be [4,3,2,2]. First accepted class Solution: def plusOne(self, digits: List[int]) -\u003e List[int]: i = len(digits) - 1 while i \u003e= 0 and digits[i] == 9: digits[i] = 0 i -= 1 if i \u003c 0: return [1] + digits digits[i] = digits[i] + 1 return digits ","description":"LeetCode 66. Plus One","title":"66. Plus One","uri":"/en/tracks/algorithms-101/leetcode/easy/66/"},{"content":"LeetCode problem\nGiven a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.\nYou must not use any built-in exponent function or operator.\nFor example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.\nExample 1:\nInput: x = 4 Output: 2 Explanation: The square root of 4 is 2, so we return 2. Example 2:\nInput: x = 8 Output: 2 Explanation: The square root of 8 is 2.82842..., and since we round it down to the nearest integer, 2 is returned. First accepted class Solution: def mySqrt(self, x: int, div=2) -\u003e int: s = x // div s1 = (s + div) // 2 if s1 * s1 \u003e x: s1 = self.mySqrt(x, s1) return s1 else: return s1 ","description":"Leetcode 69. Sqrt(x)","title":"69. Sqrt(x)","uri":"/en/tracks/algorithms-101/leetcode/easy/69/"},{"content":"LeetCode problem\nGiven a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-231, 231 - 1], then return 0.\nAssume the environment does not allow you to store 64-bit integers (signed or unsigned).\nExample 1:\nInput: x = 123 Output: 321\nExample 2:\nInput: x = -123 Output: -321\nExample 3:\nInput: x = 120 Output: 21\nFirst accepted Idea:\nConvert number to int Remove minus if exist (or convert module of number) reverse class Solution: def reverse(self, x: int) -\u003e int: reversed_int = [] str_int = str(x) if x \u003c 0: str_int = str_int[1:] for i in reversed(range(len(str_int))): reversed_int.append(str_int[i]) res = int(''.join(reversed_int)) if x \u003c 0: res = -res return res if (res \u003e= -2147483648 and res \u003c= 2147483647) else 0 Better solution class Solution: def reverse(self, x: int) -\u003e int: s = str(abs(x)) rev = int(s[::-1]) if rev \u003e 2147483647: return 0 return rev if x \u003e 0 else (rev * -1) ","description":"LeetCode 7. Reverse Integer","title":"7. Reverse Integer","uri":"/en/tracks/algorithms-101/leetcode/medium/7/"},{"content":"LeetCode problem\nYou are climbing a staircase. It takes n steps to reach the top.\nEach time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?\nExample 1:\nInput: n = 2 Output: 2 Explanation: There are two ways to climb to the top. 1. 1 step + 1 step 2. 2 steps Example 2:\nInput: n = 3 Output: 3 Explanation: There are three ways to climb to the top. 1. 1 step + 1 step + 1 step 2. 1 step + 2 steps 3. 2 steps + 1 step First accepted Idea:\nTried to calculate by hand. There is a sequence Fibonacci here\nclass Solution: def climbStairs(self, n: int) -\u003e int: if n == 1: return 1 if n == 2: return 2 prev1 = 1 prev2 = 2 current = 2 while n \u003e 2: current = prev1 + prev2 prev1 = prev2 prev2 = current n -= 1 return current ","description":"LeetCode 70. Climbing Stairs","title":"70. Climbing Stairs","uri":"/en/tracks/algorithms-101/leetcode/easy/70/"},{"content":"LeetCode problem\nGiven an m x n integer matrix matrix, if an element is 0, set its entire row and column to 0’s.\nYou must do it in place.\nExample 1:\nInput: matrix = [[1,1,1],[1,0,1],[1,1,1]] Output: [[1,0,1],[0,0,0],[1,0,1]] Example 2:\nInput: matrix = [[0,1,2,0],[3,4,5,2],[1,3,1,5]] Output: [[0,0,0,0],[0,4,5,0],[0,3,1,0]] Approach 1:\nIdea:\nLeetCode Submission\nclass Solution: def setZeroes(self, matrix: List[List[int]]) -\u003e None: rows = len(matrix) cols = len(matrix[0]) # 1. Check first row/column for zero's first_row_has_zero = 0 in matrix[0] first_col_has_zero = 0 in list(zip(*matrix))[0] # 2. Check other cells in `matrix` and save info in the 1st row/col if cell has zero's for i in range(1, rows): for j in range(1, cols): if matrix[i][j] == 0: matrix[0][j] = 0 # 1st row matrix[i][0] = 0 # 1st col # 3. Loop again through first row/column and overwrite cells according to the data from 1st row/column # except the 1st row/col for i in range(1, rows): for j in range(1, cols): if matrix[0][j] == 0 or matrix[i][0] == 0: matrix[i][j] = 0 # Fill 0s for the 1st row if needed if first_row_has_zero: matrix[0] = [0] * cols # Fill 0s for the 1st col if needed if first_col_has_zero: for row in matrix: row[0] = 0 ","description":"LeetCode 73. Set Matrix Zeroes","title":"73. Set Matrix Zeroes","uri":"/en/tracks/algorithms-101/leetcode/medium/73/"},{"content":"LeetCode problem\nThis problem is also known as the Dutch National Flag problem. One solution is to use three pointers to partition the array into three sections: red, white, and blue.\nHere’s the algorithm:\nInitialize three pointers: left, mid, and right. Initialize left to 0, mid to 0, and right to n-1, where n is the length of the input array. While mid is less than or equal to right: If nums[mid] is 0, swap nums[mid] with nums[left], increment mid and left. If nums[mid] is 1, increment mid. If nums[mid] is 2, swap nums[mid] with nums[right], decrement right. Return the sorted array. class Solution: def sortColors(self, nums: List[int]) -\u003e None: \"\"\" Do not return anything, modify nums in-place instead. \"\"\" l, m, r = 0, 0, len(nums) - 1 while m \u003c= r: if nums[m] == 0: nums[m], nums[l] = nums[l], nums[m] l += 1 m += 1 elif nums[m] == 1: m += 1 else: nums[m], nums[r] = nums[r], nums[m] r -= 1 ","description":"LeetCode 75. Sort Colors","title":"75. Sort Colors","uri":"/en/tracks/algorithms-101/leetcode/medium/75/"},{"content":"LeetCode problem\nIn this solution, we start with an empty list in the results array.\nFor each element in the nums array, we append that element to all of the subsets in the results array to create new subsets, and then add these new subsets to the results array.\nBy doing this for all elements in nums, we generate all possible subsets.\nclass Solution: def subsets(self, nums: List[int]) -\u003e List[List[int]]: res = [[]] for i in nums: for j in range(len(res)): cur = [] cur.append(i) cur.extend(res[j]) res.append(cur) return res Approach 2:\nclass Solution: def subsets(self, nums: List[int]) -\u003e List[List[int]]: res = [] def dfs(start: int, path: List[int]) -\u003e None: res.append(path) for i in range(start, len(nums)): dfs(i + 1, path + [nums[i]]) dfs(0, []) return res This is a recursive solution that uses a depth-first search (DFS) approach to generate all possible subsets of the input list nums. The function takes two parameters start and path, where\nstart represents the starting index of the current subset path represents the current subset being constructed. The base case of the recursion is when start is greater than or equal to the length of nums, at which point the current path is added to the final result res.\nFor each recursive call, the function iterates through the remaining elements of nums starting at index start, and appends each element to the path list. Then, the function recursively calls itself with the next index i+1 as the new starting point for the next subset, and the updated path list.\nAs the recursion returns, each subset is added to the res list, and the path list is updated by removing the last element that was added in the previous recursive call.\nFinally, the function is initialized with an empty path list and a starting index start of 0, and the final res list is returned after all subsets have been generated.\nLeetCode Editorial:\nEditorial\nApproach 1: Cascading\nApproach 2: Backtracking Approach 3: Lexicographic (Binary Sorted) Subsets ","description":"LeetCode 78. Subsets","title":"78. Subsets","uri":"/en/tracks/algorithms-101/leetcode/medium/78/"},{"content":"LeetCode problem\n// .js /** * @param {string} s * @return {number} */ var myAtoi = function(s) { let res = 0; let num = parseInt(s); if(num \u003e= 2147483648){ res = 2147483647; } else if (num \u003c= -2147483648){ res = -2147483648; } else if (isNaN(num)) { res = 0; } else { res = num; } return res; }; ","description":"Leetcode 8. String to Integer (atoi)","title":"8. String to Integer (atoi)","uri":"/en/tracks/algorithms-101/leetcode/medium/other/8/"},{"content":"LeetCode problem\nFirst accepted class Solution: def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -\u003e None: \"\"\" Do not return anything, modify nums1 in-place instead. \"\"\" i = len(nums1) - n for j in nums2: nums1[i] = j i += 1 nums1.sort() ","description":"LeetCode 88. Merge Sorted Array","title":"88. Merge Sorted Array","uri":"/en/tracks/algorithms-101/leetcode/easy/88/"},{"content":"LeetCode problem\nGiven the root of a binary tree, return the inorder traversal of its nodes' values.\nExample 1:\nInput: root = [1,null,2,3] Output: [1,3,2] Example 2:\nInput: root = [] Output: [] Example 3:\nInput: root = [1] Output: [1] Thoughts Don’t understand what needed. Why:\n1-null-2-3 becomes 1-3-2 [1,2,5,7,8,9,10] becomes [7,2,8,1,9,5,10] In 1-null-2-3 1 becomes the first because we loop to its left node which is null, then come back and first value here is 1.\nFirst accepted # Definition for a binary tree node. # class TreeNode: # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution: def inorderTraversal(self, root: Optional[TreeNode]) -\u003e List[int]: # add all left, then add right def get_child(head): if head: get_child(head.left) result.append(head.val) get_child(head.right) result = [] get_child(root) return result Better solution Morris Traversal\nResources LeetCode explanation ","description":"LeetCode 94. Binary Tree Inorder Traversal","title":"94. Binary Tree Inorder Traversal","uri":"/en/tracks/algorithms-101/leetcode/easy/94/"},{"content":"Initial I had the need to implement search functionality on my site. Content on is in different languages.\nThe goal is to impelemnt search for all pages and separate search results for each and every language.\nHow it works Hugo generates the search index. In this case it means that we get json file with every static page on the site.\nTo make search works we need to create index. lunr.js takes care of it.\nClient send query -\u003e our script “tries to find” in the index\nRender the results\nThis is how the logic looks like:\nImplementation Create search form Create popup modal where will render search results Connect Lunr.js script Generate pages data Connect search/result forms with lunr.js search TL;DR Files to change/create:\n1. `/layouts/partials/header.html` \u003cform id=\"search\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\"\u003e \u003c/form\u003e 2. `/layouts/partials/components/search-list-popup.html` \u003cdiv id=\"search-result\" tabindex=\"-1\" class=\"overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \" hidden\u003e \u003cdiv class=\"relative p-4 w-full max-w-xs h-full md:h-auto\"\u003e \u003cdiv class=\"relative bg-white rounded-lg shadow dark:bg-gray-700\"\u003e \u003cdiv class=\"p-6\"\u003e \u003ch3\u003eSearch results\u003c/h3\u003e \u003cdiv id=\"search-results\" class=\"prose\"\u003e\u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 3. `/layouts/partials/footer.html` ... {{ $languageMode := .Site.Language }} \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/js/search.js?1\" languageMode={{ $languageMode }} \u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... 4. `/layouts/_default/index.json` [ {{- range $index, $page := .Site.RegularPages.ByTitle -}} {{- if gt $index 0 -}} , {{- end -}} {{- $entry := dict \"uri\" $page.RelPermalink \"title\" $page.Title -}} {{- $entry = merge $entry (dict \"description\" .Description) -}} {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}} {{- $entry | jsonify -}} {{- end -}} ] 5. `config.yaml` # config.yaml # need for search popup service / creates search.json index fo lunr.js outputFormats: SearchIndex: baseName: search mediaType: application/json outputs: home: - HTML - RSS - SearchIndex 6. `static/js/search.js` const languageMode = window.document.currentScript.getAttribute('languageMode'); const MAX_SEARCH_RESULTS = 10 let searchIndex = {} let pagesStore = {} // Need to create ONLY once , maybe before push | during build const createIndex = (documents) =\u003e { searchIndex = lunr(function () { this.field(\"title\"); this.field(\"content\"); this.field(\"description\"); this.field(\"uri\"); this.ref('uri') documents.forEach(function (doc) { pagesStore[doc['uri']] = doc['title'] this.add(doc) }, this) }) } const loadIndexData = () =\u003e { const url = `/${languageMode}/search.json`; var xmlhttp = new XMLHttpRequest(); xmlhttp.onreadystatechange = function () { if (this.readyState == 4 \u0026\u0026 this.status == 200) { const pages_content = JSON.parse(this.responseText); createIndex(pages_content) } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); } const search = (text) =\u003e { let result = searchIndex.search(text) return result } const hideSearchResults = (event, divBlock) =\u003e { event.preventDefault() if (!divBlock.contains(event.target)) { divBlock.style.display = 'none'; divBlock.setAttribute('class', 'hidden') } } // TODO refactor const renderSearchResults = (results) =\u003e { const searchResultsViewBlock = document.getElementById('search-result') // hide on move mouse from results block document.addEventListener('mouseup', (e) =\u003e hideSearchResults(e, searchResultsViewBlock)); const searchResultsDiv = document.getElementById('search-results') searchResultsDiv.innerHTML = '' searchResultsViewBlock.style.display = 'initial'; searchResultsViewBlock.removeAttribute('hidden') const resultsBlock = document.createElement('ul') for (let post of results) { const url = post['ref'] const title = pagesStore[url] let commentBlock = document.createElement('li') let link = document.createElement('a',) let linkText = document.createTextNode(title); link.appendChild(linkText) link.href = url commentBlock.appendChild(link) resultsBlock.appendChild(commentBlock) } searchResultsDiv.appendChild(resultsBlock) } const searchFormObserver = () =\u003e { var form = document.getElementById(\"search\"); var input = document.getElementById(\"search-input\"); form.addEventListener(\"submit\", function (event) { event.preventDefault(); var term = input.value.trim(); if (!term) { return } const search_results = search(term, languageMode); renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) }, false); } // create indexes loadIndexData() searchFormObserver() Search form I am going to add search form to the header part. For thios purpose edit header.html file in the path /layouts/partials/header.html\nSet form id: search. By this id script can find this form\nMinimal form for work:\n\u003cform id=\"search\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\"\u003e \u003c/form\u003e I use Tailwind, so this is how my form looks like:\n\u003cdiv class=\"relative pt-4 md:pt-0\"\u003e \u003cform id=\"search\" class=\"flex items-center\"\u003e \u003clabel for=\"search-input\" class=\"sr-only\"\u003eSearch\u003c/label\u003e \u003cdiv class=\"relative w-full\"\u003e \u003cinput type=\"text\" type=\"search\" id=\"search-input\" class=\"bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full pl-10 p-2.5 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500\" placeholder=\"Search\" required\u003e \u003c/div\u003e \u003c/form\u003e \u003c/div\u003e Modal with results By default this modal window is hidden. So don’t need to add this to any page. But need to add somewhere.\n1. Create .html component\npath: /layouts/partials/components/search-list-popup.html\nFor modal block to show or hide I use id: search-result\nFor block with search results id is: search-results\nContent:\n\u003cdiv id=\"search-result\" tabindex=\"-1\" class=\"overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \" hidden\u003e \u003cdiv class=\"relative p-4 w-full max-w-xs h-full md:h-auto\"\u003e \u003cdiv class=\"relative bg-white rounded-lg shadow dark:bg-gray-700\"\u003e \u003cdiv class=\"p-6\"\u003e \u003ch3\u003eSearch results\u003c/h3\u003e \u003cdiv id=\"search-results\" class=\"prose\"\u003e\u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 2. Add component to the site\nAdd this component to the footer. File path: /layouts/partials/footer.html\n... {{ partial \"components/search-list-popup.html\" . }} ... Connect Lunr.js Add link to this script to the footer template too\nPart of the footer template:\n... \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... Generate pages data Hugo can generate the search index the same way it generates RSS feeds for example, it’s just another output format.\n1. Generate script\nThis generator is for multilingual site\nCreates json in each language catalog in format:\n[{\"title\":\"title01\",...}] Fepends on fileds inckluded in the layout /layouts/_default/index.json\nCreate file /layouts/_default/index.json\n[ {{- range $index, $page := .Site.RegularPages.ByTitle -}} {{- if $page.IsTranslated -}} {{ if gt (index $page.Translations 0).WordCount 0 }} {{ range .Translations }} {{- if gt $translatedCount 0 -}} , {{- end -}} {{- $entry := dict \"uri\" .RelPermalink \"title\" .Title -}} {{- $entry = merge $entry (dict \"description\" .Description) -}} {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}} {{- $entry | jsonify -}} {{ $translatedCount = add $translatedCount 1 }} {{ end}} {{ end }} {{- end -}} {{- end -}} ] Creates search.json file with page indexes in /public/search.json\n2. Set index file path\nUpdate config.yaml file:\n# config.yaml # need for search popup service / creates search.json index fo lunr.js outputFormats: SearchIndex: baseName: search mediaType: application/json outputs: home: - HTML - RSS - SearchIndex Connect search/result forms with lunr.js search Create file in the path: static/js/search.js\nconst languageMode = window.document.currentScript.getAttribute('languageMode'); const MAX_SEARCH_RESULTS = 10 let searchIndex = {} let pagesStore = {} // Need to create ONLY once , maybe before push | during build const createIndex = (documents) =\u003e { searchIndex = lunr(function () { this.field(\"title\"); this.field(\"content\"); this.field(\"description\"); this.field(\"uri\"); this.ref('uri') documents.forEach(function (doc) { pagesStore[doc['uri']] = doc['title'] this.add(doc) }, this) }) } const loadIndexData = () =\u003e { const url = `/${languageMode}/search.json`; var xmlhttp = new XMLHttpRequest(); xmlhttp.onreadystatechange = function () { if (this.readyState == 4 \u0026\u0026 this.status == 200) { const pages_content = JSON.parse(this.responseText); createIndex(pages_content) } }; xmlhttp.open(\"GET\", url, true); xmlhttp.send(); } const search = (text) =\u003e { let result = searchIndex.search(text) return result } const hideSearchResults = (event, divBlock) =\u003e { event.preventDefault() if (!divBlock.contains(event.target)) { divBlock.style.display = 'none'; divBlock.setAttribute('class', 'hidden') } } // TODO refactor const renderSearchResults = (results) =\u003e { const searchResultsViewBlock = document.getElementById('search-result') // hide on move mouse from results block document.addEventListener('mouseup', (e) =\u003e hideSearchResults(e, searchResultsViewBlock)); const searchResultsDiv = document.getElementById('search-results') searchResultsDiv.innerHTML = '' searchResultsViewBlock.style.display = 'initial'; searchResultsViewBlock.removeAttribute('hidden') const resultsBlock = document.createElement('ul') for (let post of results) { const url = post['ref'] const title = pagesStore[url] let commentBlock = document.createElement('li') let link = document.createElement('a',) let linkText = document.createTextNode(title); link.appendChild(linkText) link.href = url commentBlock.appendChild(link) resultsBlock.appendChild(commentBlock) } searchResultsDiv.appendChild(resultsBlock) } const searchFormObserver = () =\u003e { var form = document.getElementById(\"search\"); var input = document.getElementById(\"search-input\"); form.addEventListener(\"submit\", function (event) { event.preventDefault(); var term = input.value.trim(); if (!term) { return } const search_results = search(term, languageMode); renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) }, false); } // create indexes loadIndexData() searchFormObserver() Next need to add this file to the site: /layouts/partials/footer.html\nNow footer looks like this:\n... {{ $languageMode := .Site.Language }} \u003cscript src=\"https://unpkg.com/lunr/lunr.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"/js/search.js?1\" languageMode={{ $languageMode }} \u003e\u003c/script\u003e {{ partial \"components/search-list-popup.html\" . }} ... ","description":"Make your multilingual Hugo static site searchable with a client-side search index","title":"Add search to Hugo multilingual static site with Lunr","uri":"/en/posts/hugo-add-search-lunr-popup/"},{"content":"","description":"AI generated","title":"AI generated","uri":"/en/photos/ai/"},{"content":" colorful abstract splashes floating around forming the loose shape of a gato, in the style of realistic anamorphic art, digital art wonders, flickr, bold yet graceful, colorful explosions, vivid realism, colorful curves colorful abstract splashes floating around forming the loose shape of a tree, in the style of realistic anamorphic art, digital art wonders, flickr, bold yet graceful, colorful explosions, vivid realism, colorful curves colorful abstract splashes floating around forming the loose shape of a rose, in the style of realistic anamorphic art, digital art wonders, flickr, bold yet graceful, colorful explosions, vivid realism, colorful curves colorful abstract splashes floating around forming the loose shape of a protea, in the style of realistic anamorphic art, digital art wonders, flickr, bold yet graceful, colorful explosions, vivid realism, colorful curves ","description":"AI Midjourney generated","title":"AI Midjourney generated","uri":"/en/photos/midjourney/"},{"content":"Intro Big-O Cheat Sheet Sort Insertion sort sorts an array by continuously picking an element, starting from the second element, and inserting it in its correct position in the sorted part of the array to its left. It does this by shifting larger elements one position ahead of their current position, making room for the new element.\ndef insertion_sort(array): for i in range(1, len(array)): value = array[i] while i \u003e 0 and array[i - 1] \u003e value: array[i] = array[i - 1] i -= 1 array[i] = value return array Selection sort works by repeatedly finding the minimum element from the unsorted part of the array and swapping it with the first unsorted element. It continues this process until the whole array is sorted, hence effectively moving the smallest unsorted element to its correct position in each iteration.\ndef selection_sort(array): for i in range(len(array) - 1): min_value = i for j in range(i + 1, len(array)): if array[j] \u003c array[min_value]: min_value = j temp = array[min_value] array[min_value] = array[i] array[i] = temp return array Merge sort sorts an array by dividing it into two halves, recursively sorting those halves, and then merging them back together in sorted order.\nIf the array has more than one element, find the middle of the array. Divide the array into two halves using the middle index: the left half (left_half) and the right half (right_half). Recursively sort both halves by calling merge_sort on left_half and right_half. Merge the sorted halves back into the original array. The merge operation walks through left_half and right_half, and at each step, it copies the smaller element from either left_half or right_half into the original array. If there are any remaining elements in left_half or right_half after one has been fully copied back into the array, those elements are copied over. This happens because those remaining elements are guaranteed to be larger than all elements already copied back into the array. First more simple example of merging already sorted two arrays:\nMerge sorted arrays:\ndef merge(left_ar, right_ar): res = [] left_index, right_index = 0 while left_index \u003c len(left_ar) and right_index \u003c len(right_ar): if left_ar[left_index] \u003c right_ar[right_index]: res.append(left_ar[left_index]) left_index += 1 else: res.append(right_ar[right_index]) right_index += 1 res += left_ar[left_index:] + right_ar[right_index:] def merge_sort(array): mid = len(array) / 2 left_ar = array[:mid] right_ar = array[mid:] return merge(left_ar, right_ar) Sort array:\ndef merge_sort(array): if len(array) \u003e 1: # Only sort if array is larger than 1 mid = len(array) // 2 # middle of the array # Split the array into two halves left_half = array[:mid] right_half = array[mid:] # Recursively sort both halves merge_sort(left_half) merge_sort(right_half) left_index = right_index = merged_index = 0 # Merge sorted halves back into the original array while left_index \u003c len(left_half) and right_index \u003c len(right_half): if left_half[left_index] \u003c= right_half[right_index]: array[merged_index] = left_half[left_index] left_index += 1 else: array[merged_index] = right_half[right_index] right_index += 1 merged_index += 1 # If any elements left in either half, append them to the array while left_index \u003c len(left_half): array[merged_index] = left_half[left_index] left_index += 1 merged_index += 1 while right_index \u003c len(right_half): array[merged_index] = right_half[right_index] right_index += 1 merged_index += 1 Your browser does not support the video tag. Merge Sort Your browser does not support the video tag. Merge Sort Binary Search Binary search template def find_target(nums, target): left = 0 right = len(nums) - 1 while left \u003c= right: mid = (left + right) // 2 if nums[mid] == target: return mid if nums[mid] \u003c target: left = mid + 1 else: right = mid - 1 return -1 Python build-in module\nfrom bisect import bisect_left sorted_fruits = ['apple', 'banana', 'orange', 'plum'] bisect_left(sorted_fruits, 'kiwi') \u003e\u003e 2 Dynamic programming (DP) Breadth First Search (BFS) BFS on Tree:\nfrom collections import deque class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def bfs_tree(root): queue = deque([root]) while queue: node = queue.popleft() print(node.val, end=' ') if node.left: queue.append(node.left) if node.right: queue.append(node.right) root = TreeNode(1) root.left = TreeNode(2) root.right = TreeNode(3) root.left.left = TreeNode(4) root.left.right = TreeNode(5) bfs_tree(root) BFS on Graph:\nfrom collections import defaultdict, deque class Graph: def __init__(self): self.graph = defaultdict(list) def add_edge(self, u, v): self.graph[u].append(v) def bfs(self, start): visited = set() queue = deque([start]) while queue: node = queue.popleft() if node not in visited: print(node, end=' ') visited.add(node) for neighbor in self.graph[node]: if neighbor not in visited: queue.append(neighbor) g = Graph() g.add_edge(0, 1) g.add_edge(0, 2) g.add_edge(1, 2) g.add_edge(2, 0) g.add_edge(2, 3) g.add_edge(3, 3) g.bfs(2) Depth-first search (DFS) DFS on Tree:\ndef dfs(root, target): if root is None: return None if root.val == target: return root left = dfs(root.left, target) if left is not None: return left return dfs(root.right, target) DFS on Graph:\ndef dfs(root, visited): for neighbor in get_neighbors(root): if neighbor in visited: continue visited.add(neighbor) dfs(neighbor, visited) DFS on two-dimensional array:\nLet’s imagine you have a big maze made of walls and corridors, and you want to find a way from the entrance to the exit. You can put a robot at the entrance, and you want to tell the robot what to do to find the exit.\nThe first thing you might tell the robot is to always go as far as it can in one direction before turning. This is what depth-first search does.\nThe robot starts at the entrance and goes as far as it can down the first corridor it finds.\nIf it comes to a dead end, it goes back to the last intersection it passed and tries the next corridor. If it comes to the exit, it stops and says “I found the exit!”. Example:\n# Define the maze as a two-dimensional array maze = [ ['.', '.', '#', '#', '#', '#', '#', '#'], ['#', '.', '.', '.', '#', '.', '.', '#'], ['#', '.', '#', '.', '#', '.', '.', '#'], ['#', '.', '.', '.', '.', '#', '.', '#'], ['#', '#', '#', '#', '.', '#', '.', '#'], ['#', '.', '.', '.', '.', '.', '.', '#'], ['#', '.', '#', '#', '#', '#', '.', '.'], ['#', '#', '#', '#', '#', '#', '#', '.'], ] # Define the starting point and the destination start = (0, 0) end = (len(maze)-1, len(maze[0])-1) # Define a function to find the exit using depth-first search def dfs(current, visited): # Mark the current cell as visited visited.add(current) # Base case: If we've reached the destination, return True # or other condition if current == end: return True # Try all possible directions from the current cell for delta in [(0, 1), (1, 0), (0, -1), (-1, 0)]: next_cell = (current[0] + delta[0], current[1] + delta[1]) if is_valid_cell(next_cell) and next_cell not in visited: if dfs(next_cell, visited): return **True** # If we couldn't find the exit from this cell, backtrack to the previous cell return False # Call the depth-first search function with the starting point and an empty set of visited cells visited = set() if dfs(start, visited): print(\"I found the exit!\") else: print(\"I couldn't find the exit.\") Base template:\ndef dfs(matrix, row, col, visited): # Check if the current cell is out of bounds or has already been visited if ( row \u003c 0 or row \u003e= len(matrix) or col \u003c 0 or col \u003e= len(matrix[0]) or visited[row][col]: ) return # Mark the current cell as visited visited[row][col] = True # Define the possible directions to move (right, down, left, up) directions = [(0, 1), (1, 0), (0, -1), (-1, 0)] # Iterate through the directions and call DFS recursively on neighboring cells for step_row, step_col in directions: new_row, new_col = row + step_row, step_col + dc dfs(matrix, new_row, new_col, visited) Sliding Window Usage: Use when need to handle the input data in specific window size.\nExample: Sliding window technique to find the largest sum of 4 consecutive numbers. Template:\nwhile j \u003c size: # Calculation's happen's here # ... if condition \u003c k: j+=1 elif condition == k: # ans \u003c-- calculation j+=1 elif condition \u003e k: while condition \u003e k: i+=1 # remove calculation for i j+=1 return ans Examples\nProblem: Find the largest sum of K consecutive entries, given an array of size N\nAdd the first K components together and save the result in the currentSum variable. Because this is the first sum, it is also the current maximum; thus, save it in the variable maximumSum. As the window size is ww, we move the window one place to the right and compute the sum of the items in the window. Update the maximum if the currentSum is greater than the maximumSum, and repeat step 2. def max_sum(arr, k): n = len(arr) # length of the array # length of array must be greater # window size if n \u003c k: print(\"Invalid\") return -1 # sum of first k elements window_sum = sum(arr[:k]) max_sum = window_sum # remove the first element of previous # window and add the last element of # the current window to calculate the # the sums of remaining windows by for i in range(n - k): window_sum = window_sum - arr[i] + arr[i + k] max_sum = max(window_sum, max_sum) return max_sum arr = [16, 12, 9, 19, 11, 8] k = 3 print(max_sum(arr, k)) Problem: Find duplicates within a range ‘k’ in an array\nInput: nums = [5, 6, 8, 2, 4, 6, 9] k = 2 Ouput: False def get_duplicates(nums, k): d = {} count = 0 for i in range(len(nums)): if nums[i] in d and i - d[nums[i]] \u003c= k: return True else: d[nums[i]] = i return False Problem/solution examples Article on LeetCode Practice questions Two Pointers A classic way of writing a two-pointer sliding window. The right pointer keeps moving to the right until it cannot move to the right (the specific conditions depend on the topic). When the right pointer reaches the far right, start to move the left pointer to release the left boundary of the window.\nUsage: Use two pointers to iterate the input data. Generally, both pointers move in the opposite direction at a constant interval.\nPractice questions Two pointers intro Backtracking Based on Depth-first search (DFS)\nUsage:\nFinding all permutations, combinations, subsets and solving sudoku are classic combinatorial problems.\nImagine you are trying to solve a puzzle, like a Sudoku. When you are solving a puzzle, sometimes you reach a point where you can’t make any more progress using the current path. That’s when you need to backtrack.\nBacktracking is a general algorithmic technique that is used to find all (or some) solutions to a problem by incrementally building candidates, and checking if the candidate is feasible or not. If the candidate is not feasible, the algorithm goes back (backtracks) to the previous step and tries again with a different candidate. The process continues until a solution is found, or all candidates have been tried.\nBacktracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point of time.\nBacktracking algorithm is derived from the Recursion algorithm, with the option to revert if a recursive solution fails, i.e. in case a solution fails, the program traces back to the moment where it failed and builds on another solution. So basically it tries out all the possible solutions and finds the correct one.\nBacktracking == DFS on a tree\nHowto:\nBacktracking is drawing tree When drawing the tree, bear in mind: how do we know if we have reached a solution? how do we branch (generate possible children)? Example:\nLet’s say we want to generate all possible combinations of 1, 2, and 3 of length 2. The possible combinations are: (1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2).\nThis process generates all possible combinations of length k:\ndef backtrack(nums, path, res, k): # nums: the list of available numbers # path: the current path of selected numbers # res: the list of all valid combinations # k: the length of each combination if len(path) == k: # base case res.append(path[:]) return for i in range(len(nums)): path.append(nums[i]) backtrack(nums[:i] + nums[i+1:], path, res, k) path.pop() nums = [1, 2, 3] k = 2 res = [] backtrack(nums, [], res, k) print(res) # [[1, 2], [1, 3], [2, 1], [2, 3], [3, 1], [3, 2]] Algorithm:\nWe start with an empty path and empty result list. We loop through the available numbers (1, 2, 3) and add the first number to the path. We make a recursive call to backtrack with the remaining numbers (2, 3) and a path that includes the first number (e.g., [1]). This adds all possible combinations of length k-1 with the first number. After the recursive call, we remove the first number from the path. We repeat this process for the other available numbers, generating all possible combinations of length k. When we reach the base case (len(path) == k), we add the current path to the result list. We return the result list of all possible combinations. The base case is when the length of the path is equal to k. At this point, we add the current path to the result list and return.\nThe recursive case involves looping through the available numbers, adding the current number to the path, making a recursive call with the remaining numbers, and removing the current number from the path after the recursive call.\npath:\nIn the backtrack function, path refers to the list of numbers that have been selected so far to form a valid combination.\nInitially, path is an empty list []. In each recursive call, a number from nums is selected and added to path.\nFor example, if nums = [1, 2, 3] and the current path is [1], the function will call backtrack([2, 3], [1], res, k) to consider all possible combinations with 1 in the first position, followed by all possible combinations of length k-1 of [2, 3] in the second position.\nOnce all possible combinations with 1 in the first position have been explored, the number 1 will be removed from path, and the function will try the next number from nums, which in this case is 2. The function continues in this way until all valid combinations of length k have been found and added to the res list.\nProblem examples:\nLeetCode 17. Letter Combinations of a Phone Number [LeetCode 22. Generate Parentheses] [LeetCode 46. Permutations] Example of LeetCode 78 problem:\nAlgorithm:\nWe define a backtrack function named backtrack(first, curr) which takes the index of first element to add and a current combination as arguments.\nIf the current combination is done, we add the combination to the final output.\nOtherwise, we iterate over the indexes i from first to the length of the entire sequence n.\nAdd integer nums[i] into the current combination curr. Proceed to add more integers into the combination: backtrack(i + 1, curr). Backtrack by removing nums[i] from curr. class Solution: def subsets(self, nums: List[int]) -\u003e List[List[int]]: def backtrack(first = 0, curr = []): # if the combination is done if len(curr) == k: output.append(curr[:]) return for i in range(first, n): # add nums[i] into the current combination curr.append(nums[i]) # use next integers to complete the combination backtrack(i + 1, curr) # backtrack curr.pop() output = [] n = len(nums) for k in range(n + 1): backtrack() return output # [ [], # [1], [3], [4], # [1, 3], [1, 4], [3, 4], # [1, 3, 4] # ] Dutch National Flag problem The Dutch National Flag problem is a sorting problem that asks us to sort an array of colors, like a bunch of different colored socks. We want to put all the socks of the same color together in the array.\nThe colors in this problem are represented by numbers. We use the numbers 0, 1, and 2 to represent the colors red, white, and blue. So, we have an array of numbers, and we want to sort them in such a way that all the 0's are at the beginning of the array, then all the 1's, and finally all the 2's are at the end.\nFor example, if we have an array [2, 0, 2, 1, 1, 0], we want to sort it so that it becomes [0, 0, 1, 1, 2, 2].\nOne way to solve this problem is to use a technique called the Dutch National Flag algorithm. The idea behind this algorithm is to use three pointers: a low pointer, a mid pointer, and a high pointer.\nThe low pointer starts at the beginning of the array, the high pointer starts at the end of the array, and the mid pointer starts at the beginning of the array.\nWe then iterate through the array with the mid pointer.\nIf the value at the mid pointer is 0, we swap it with the value at the low pointer and increment both pointers. - If the value at the mid pointer is 1, we leave it where it is and just increment the mid pointer. If the value at the mid pointer is 2, we swap it with the value at the high pointer and decrement the high pointer. We keep doing this until the mid pointer passes the high pointer, at which point the array is sorted.\nSo, in our sock example, we start with the low pointer at the beginning of the array, the mid pointer also at the beginning of the array, and the high pointer at the end of the array. Then, we iterate through the array with the mid pointer, swapping socks as needed until the array is sorted by color.\nResources https://www.geeksforgeeks.org/learn-data-structures-and-algorithms-dsa-tutorial/ https://algo.monster/templates https://interviewnoodle.com/grokking-leetcode-a-smarter-way-to-prepare-for-coding-interviews-e86d5c9fe4e1 data structures Competitive Programming Library Algorithms for Competitive Programming Solutions to Introduction to Algorithms Third Edition ","description":"LeetCode Cookbook - Algorithms","title":"Algorithms","uri":"/en/tracks/algorithms-101/algorithms/"},{"content":"About Documentation User Guide API Gateway provides the opportunity to create and expand your own REST and WebSocket APIs at any size.\nAPI endpoints can be cached to accommodate for frequent similar requests.\nUse Cases Build a network for micros­ervices archit­ectures.\nAmazon CloudWatch metrics - Collects near-real-time metrics Examples: 4XXError (client-side errors), 5XXError(server-side errors), CacheHitCount Amazon CloudWatch Logs - Debug issues related to request execution AWS CloudTrail - Record of actions taken by a user, role, or an AWS service in API Gateway AWS X-Ray - Trace your request across different AWS Services Digests Concepts REST API, HTTP API, WebSocket API\nDeployment - point-in-time snapshot of your API Gateway API\nEndpoint - https://api-id.execute-api.region-id.amazonaws.com\nEdge-optimized Private Regional Stage - A logical reference to a lifecycle state of your API. Route - URL path, Latency based routing, Integration - Lambda, HTTP, Private VPC, CORS Import/Export - Open API AM User should have permission to enable logging Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale.\nStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. The act like environment variables and can be used in your API setup and mapping templates.\nWith deployment stages in API Gateway you can manage multiple release stages for each API, such as: alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\nWhen you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through our API, you must manually add these permissions.\nIntegration timeout for AWS, Lambda, Lambda proxy, HTTP, HTTP proxy - 50 ms to 29 seconds\nYou can enable API caching to cache your endpoint’s responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API\nAWS Gateway Integration types:\nAWS_ Proxy - lambda proxy integration HTTP - http custom integration HTTP_PROXY - http proxy Practice Creating a RESTful API Using Amazon API Gateway Questions Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\nA) HTTP proxy B) mock C) AWS service proxy D) Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\nAmazon API Gateway supports mock integrations for API methods.\nB\nQ2 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ3 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\nWhich AWS service should be used to refactor this architecture?\nAWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4\nQ4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\nEnable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template\n4\nQ5 A developer is designing a full-stack serverless application. Files for the website are stored in an Amazon S3 bucket. AWS Lambda functions that use Amazon API Gateway endpoints return results from an Amazon DynamoDB table. The developer must create a solution that securely provides registration and authentication for the application while minimizing the amount of configuration.\nWhich solution meets these requirements?\nCreate an Amazon Cognito user pool and an app client. Configure the app client to use the user pool and provide the hosted web UI provided for sign-up and sign-in. Configure an Amazon Cognito identity pool. Map the users with IAM roles that are configured to access the S3 bucket that stores the website. Configure and launch an Amazon EC2 instance to set up an identity provider with an Amazon Cognito user pool. Configure the user pool to provide the hosted web UI for sign-up and sign-in. Create an IAM policy that allows access to the website that is stored in the S3 bucket. Attach the policy to an IAM group. Add IAM users to the group. Explanation 2\nQ6 A company has moved a legacy on-premises application to AWS by performing a lift and shift. The application exposes a REST API that can be used to retrieve billing information. The application is running on a single Amazon EC2 instance. The application code cannot support concurrent invocations. Many clients access the API, and the company adds new clients all the time.\nA developer is concerned that the application might become overwhelmed by too many requests. The developer needs to limit the number of requests to the API for all current and future clients. The developer must not change the API, the application, or the client code.\nWhat should the developer do to meet these requirements?\nPlace the API behind an Amazon API Gateway API. Set the server-side throttling limits. Place the API behind a Network Load Balancer. Set the target group throttling limits. Place the API behind an Application Load Balancer. Set the target group throttling limits. Place the API behind an Amazon API Gateway API. Set the per-client throttling limits. Explanation 4\n","description":"Create, maintain, and secure APIs at any scale with Amazon API Gateway","title":"API Gateway","uri":"/en/tracks/aws-certified-developer-associate/api-gateway/"},{"content":"Lab Automating Code Reviews with Amazon CodeGuru Associating Amazon CodeGuru with a CodeCommit Repository 1. Navigate to the Amazon CodeCommit console.\n2. Click java-web-app:\n3. Notice that at the moment, only a README file has been committed to the master branch. Next, you’ll associate CodeGuru with this repository, so that CodeGuru can begin to analyze the code therein.\n4. Go to the CodeGuru dashboard.\n5. Click Associate Repository and run analysis:\n6. Select AWS CodeCommit as the provider, choose java-web-app from the repository dropdown, enter _master_into Source branch and click Associate:\nIn roughly one minute, you’ll see that CodeGuru has associated with your repository:\nTriggering an Amazon CodeGuru Review 1. Navigate to :8080 in your browser.\nNote: This is the IP of an EC2 instance that can be found in the EC2 console.\n2. Click the file icon in the top left to open the file tree:\nNote: During the creation of this lab, two things were performed automatically. One is that the CodeCommit repository you visited earlier was cloned to the directory you’re looking at in the IDE now. Another is that the framework for a Java web app was added in addition to the single README you saw. This is so that you can see the benefits of CodeGuru without having to work heavily with code.\nIn this lab step, you’ll push all the new code to the nearly-empty Code Commit repository, to trigger a CodeGuru review.\n3. Open the terminal in your IDE:\n4. In the terminal, add the new files to a Git branch, and commit and push the changes:\ncd /cloudacademy/lab git add . git checkout -b trigger_branch git commit -m \"trigger a CodeGuru analysis by pushing Java code\" git push origin trigger_branch This will create a Git commit that includes all the Java files in a branch called trigger_branch, so that you can make a pull request in CodeCommit. Since CodeGuru analyses are triggered by pull requests, this is what will trigger a CodeGuru analysis.\n5. Back on the CodeCommit dashboard, click Create pull request:\n6. Set the Destination to master and the source to trigger_branch and click Compare:\n7. Type Trigger a CodeGuru Reviewer Analysis into the Title field and click Create pull request:\nThis will create a pull request and trigger a CodeGuru review.\nViewing Amazon CodeGuru Comments 1. If you weren’t automatically brought to the pull request details page after creating your pull request, click Pull Requests beneath Repositories on the left side of the page:\n2. Click the only available pull request:\n3. Notice the section mentioning CodeGuru Reviewer:\nThis section will display in each pull request made in any repository associated with CodeGuru. As of the time this lab was released, CodeGuru is still in preview. As the section on your pull request details tab mentions, because it’s in preview mode, CodeGuru can take a while to process a pull request. There isn’t a way to track its progress, and you currently won’t be alerted when that processing begins or finishes.\n4. Select the Changes tab:\n5. In the Go to file filter, enter dockerservlet and click the result to navigate to the file:\nYou may need to scroll down the page to find the DockerServlet.java file changes. This file is known to have CodeGuru Reviewer comments that usually appear a few minutes after creating the pull request.\n6. Scroll down to line 60 to see an example of a comment from CodeGuru Reviewer (If you don’t see any comment you may try refreshing the page every minute until one appears):\nYou can then make updates as you see fit, and submit more pull requests to see if you’ve addressed CodeGuru’s suggestions.\n","description":"","title":"Automating Code Reviews with Amazon CodeGuru","uri":"/en/tracks/aws-certified-developer-associate/codeguru/automating-code-reviews-amazon-codeguru/"},{"content":"About The Average True Range (ATR) is a technical analysis indicator that measures market volatility by decomposing the entire range of an asset price for that period. It was developed by J. Welles Wilder Jr. and introduced in his book “New Concepts in Technical Trading Systems” in 1978.\nCalculating Formula ATR is calculated based on the true range (TR), which is the maximum of the following:\nCurrent High less the current Low Absolute value of (Current High less the previous Close) Absolute value of (Current Low less the previous Close) The ATR is then the moving average over a given period of the TR.\nATR = Average(TR, N)\nFor the first calculation of the ATR, it’s simply the average of the TR over a specified period. But for subsequent calculations, it’s the average of the previous ATR value and the current TR.\nPros and Cons Pros:\nATR does not provide a directional bias, it purely measures volatility. It can be used in all markets such as stocks, forex, commodities etc. ATR adapts to changing market conditions, increasing during market turbulence and decreasing in calm markets. Cons:\nATR does not indicate price direction, only volatility. During periods of rapid price changes, ATR may experience sharp increases, reflecting the inherent volatility in the price series. Example of signals ATR does not provide trading signals like other oscillators such as RSI or MACD. It provides a gauge of market volatility which can assist in stop loss placement or determining trade size.\nTrue Positive:\nIn a trending market, ATR can help a trader place their stop loss further away during high volatility periods and closer during low volatility periods, reducing the likelihood of being stopped out prematurely.\nFalse Positive:\nIn a ranging market, a sudden price breakout can cause a sharp increase in ATR, possibly leading to an unnecessarily wide stop loss if the breakout turns out to be a false one.\nUse in Real Trading In real trading, ATR can be used in conjunction with other indicators for better signal confirmation. For example, ATR can be used with a trend following system to manage stop losses. As ATR increases, your stop loss can be placed further from your entry point allowing for market volatility.\nPython Implementation Click here to view this notebook in full screen ","description":"ATR Trading Indicator","title":"Average True Range (ATR) - Volatility Indicator","uri":"/en/posts/trading-indicators/atr/"},{"content":"Preface For Amplify project I use eu-west region github repo has to be ready private or public New project goto https://eu-west-1.console.aws.amazon.com/amplify/home?region=eu-west-1#/\nNew app → Host web app → Github\nAdd access to github repo Select repository Come back to Amplify and try again to choose repo Click Next\nUpdate amplify.yml for node.js project\nversion: 1 frontend: phases: preBuild: commands: - yarn install build: commands: - yarn run build artifacts: baseDirectory: build files: - '**/*' cache: paths: - node_modules/**/* Next → Save and deploy Amplify starts to build project and generates project url.\nOnce build done you can open project.\n","description":"AWS Amplify - Initial setup with Github","title":"AWS Amplify - project setup with Github","uri":"/en/posts/cloud-exam-quizz/amplify-setup-project/"},{"content":"You can use any custom domain with Amplify and no need register it with AWS Route53.\nI am adding domain at the setup app stage. Another way is from console.\nClick Domain management. or\nAdd domain Write domain name -\u003e Configure domain -\u003e Save Nest starts SSL configuration process. Amplify provides with DNS data that you need to write in the domain register account. Once SSL creation starts you can get domain data\nAction -\u003e View DNS records\nCopy provided data (DNS records) and then set it in the domain registrar panel. Go to domain registrar Set dns servers to default In my case panel looks like this: Save Go to amplify and check for updates. Amplify checks DNS server and if everything is correct (CNAME set) it will proceed to the next step. SSL configuration passed, waiting up to 30 min for domain activation\nOnce done we can check url: https://cloud-exam-prepare.com Check url: cloud-exam-prepare.com\nResources:\nhttps://docs.aws.amazon.com/amplify/latest/userguide/to-add-a-custom-domain-managed-by-a-third-party-dns-provider.html ","description":"Set custom domain on AWS Amplify","title":"AWS Amplify - Set custom domain","uri":"/en/posts/cloud-exam-quizz/amplify-custom-domain/"},{"content":"Rename all files by pattern in current directory files=(*) for file in \"${files[@]}\" do # Check if the file name contains the \"№\" symbol if [[ $file == *\"№\"* ]]; then # Remove everything before and including the \"№\" symbol new_file=${file##*\"№\"} mv \"$file\" \"$new_file\" echo \"Renamed $file to $new_file\" fi done Add substring to filename #!/bin/bash DIR=\"/path/to/folder\" cd \"$DIR\" # Rename all .png files and add \".ru\" before .png for file in *.png; do base=$(basename \"$file\" .png) mv \"$file\" \"${base}.ru.png\" done Git Push/Pull for all repos in path git pull files=(*) #!/bin/bash # For every item in the current directory for d in */; do cd \"$d\" if [[ -d \".git\" ]]; then echo $d git pull fi cd .. done git push #!/bin/bash # For every item in the current directory for d in */; do cd \"$d\" if [[ -d \".git\" ]]; then echo $d git add . git commit -m \"Auto apply\" # black . # python formatter # git add . # git commit -m \"[chore] formatter\" git push fi cd .. done git untrack #!/bin/bash items_to_untrack=(\".idea\" \".vscode\" \".DS_Store\" \"__pycache__\" \"node_modules\" \".env\" \".serverless\") # For every item in the current directory for d in */; do echo $d cd \"$d\" # Iterate over each item to untrack for item in \"${items_to_untrack[@]}\"; do # If the item exists if [[ -e \"$item\" ]]; then # Untrack the item git rm -r --cached \"$item\" fi done cd .. done ","description":"Handy collection of Bash code snippets in this tutorial, perfect for mastering Linux programming tasks. Learn to rename files by a pattern, perform Git operations across multiple repositories, and untrack specific items in your Git setup.","title":"Bash code snippets","uri":"/en/posts/bash-snippets/"},{"content":"1. What is a Binary Tree?\nA binary tree is a data structure in which each node has at most two children, which are referred to as the left child and the right child.\n2. Representing a Binary Tree in Python\nTo represent a binary tree in Python, we can create a class called Node to represent each node in the tree. Each node will have a value and references to its left and right children.\nIf a node doesn’t have a left or right child, the reference will be set to None. Here’s an example implementation:\nclass Node: def __init__(self, value, left=None, right=None): self.value = value self.left = left self.right = right On this step None will look like this:\n3. Adding Nodes to a Binary Tree\nOnce we have a representation of a node, we can start adding nodes to the tree to create the structure of the binary tree.\nTo add a node, we need to find the correct position in the tree where the new node should be added. This is typically done by starting at the root node and comparing the value of the new node to the value of the current node.\nIf the new node’s value is less than the current node’s value, we move to the left child. If the new node’s value is greater than the current node’s value, we move to the right child. We repeat this process until we find a position where there is no left or right child (i.e., the current node is a leaf node), and we can add the new node there.\nHere’s an example implementation of a function to add a node to a binary tree:\ndef insert(root, value): if root is None: return Node(value) # basicaly create a new root Node if value \u003c root.value: root.left = insert(root.left, value) else: root.right = insert(root.right, value) return root Visualize Binary Tree\n4. Full Binary Tree Class\nclass Node: def __init__(self, value): self.value = value self.left = None self.right = None def insert(self, value): if self.value: if value \u003c self.value: if self.left is None: self.left = Node(value) else: self.left.insert(value) else: if self.right is None: self.right = Node(value) else: self.right.insert(value) else: self.value = value def search(self, value): if value \u003c self.value: if self.left is None: return False else: return self.left.search(value) elif value \u003e self.value: if self.right is None: return False else: return self.right.search(value) else: return True def remove(self, value, parent=None): if value \u003c self.value: if self.left: self.left.remove(value, self) elif value \u003e self.value: if self.right: self.right.remove(value, self) else: if self.left is None and self.right is None: if parent: if parent.left == self: parent.left = None else: parent.right = None else: self.value = None elif self.left and self.right is None: if parent: if parent.left == self: parent.left = self.left else: parent.right = self.left else: self.value = self.left.value self.right = self.left.right self.left = self.left.left elif self.right and self.left is None: if parent: if parent.left == self: parent.left = self.right else: parent.right = self.right else: self.value = self.right.value self.left = self.right.left self.right = self.right.right else: min_larger_node = self.right while min_larger_node.left: min_larger_node = min_larger_node.left self.value = min_larger_node.value if self.right == min_larger_node: self.right = min_larger_node.right else: min_larger_node.parent.left = min_larger_node.right Links Visualize Binary Tree ","description":"Binary Tree","title":"Binary Tree","uri":"/en/tracks/algorithms-101/data-structures/binary-tree/"},{"content":"About Bollinger Bands is a technical indicator developed by John Bollinger in the 1980s. It provides a relative definition of high and low prices of a market instrument by creating a band of two standard deviations from a simple moving average (SMA).\nCalculating Formula Bollinger Bands consist of three lines:\nMiddle Line: 20-day simple moving average (SMA) Upper Band: 20-day SMA + (2 * 20-day standard deviation of price) Lower Band: 20-day SMA - (2 * 20-day standard deviation of price) The standard deviation measures how spread out the prices or returns of an asset are on average. It’s the most common way to gauge market volatility.\nPros and Cons Pros:\nBollinger Bands adjust themselves to market conditions. They can be used in all markets like stocks, forex, commodities, etc. The bands can provide signals for potential overbuying and overselling scenarios. Cons:\nDuring a strong trend, the price can remain at the extremes (upper or lower band) for a long time, leading to many false sell or buy signals. As a lagging indicator, Bollinger Bands might send a late signal, causing the trader to miss a big part of the trend. Example of signals Bollinger Bands are often used to identify potential buy and sell signals.\nBuy Signal: A common strategy is to buy when the price touches the lower Bollinger Band and exit when the price touches the moving average in the center of the bands.\nSell Signal: Conversely, a sell signal is present when the price touches the upper Bollinger Band and exit the position when it touches the moving average.\nTrue Positive:\nIn a ranging market, prices tend to bounce between the upper and lower band, correctly indicating good points to buy and sell.\nFalse Positive:\nIn a trending market, the price can touch the upper band (in an uptrend) or the lower band (in a downtrend) for extended periods, leading to potentially poor buy or sell signals.\nUse in Real Trading Bollinger Bands can be combined with other indicators for better signal confirmation.\nFor example, using it in conjunction with the RSI could help traders avoid false signals. When the price touches the upper band and the RSI indicates overbought conditions, there might be a good chance the price will decrease, indicating a sell signal.\nPython Implementation Click here to view this notebook in full screen ","description":"Bollinger Bands Trading Indicator","title":"Bollinger Bands - Volatility and Price Level Indicator","uri":"/en/posts/trading-indicators/bollinger_bands/"},{"content":"\nBrewMate is a macOS GUI application that makes it easy to search for, install, and uninstall Homebrew casks. You can also see the top downloaded casks for the last month.\nInstall Download the latest DMG file from the releases page or from sourceforge.net Double-click the DMG file to open it. Drag the BrewMate app to your Applications folder. Launch BrewMate from your Applications folder. or\nbrew install romankurnovskii/cask/brewmate --cask or\nbrew tap romankurnovskii/cask brew update brew install brewmate --cask FAQ Is this app free? Yes, the app is free to download and use.\nWhat operating systems does this app support? This app is designed for macOS, and it supports macOS 10.15 (Catalina) and newer versions.\n","description":"Homebrew GUI Apps Manager","title":"BrewMate","uri":"/en/apps/brewmate/"},{"content":"Lab Monitor Like a DevOps Pro: Build A Log Aggregation System in AWS Navigating to Your Cloud’s Lambda Function 1. In the AWS Management Console search bar, enter Cloud Formation, and click the CloudFormation result under Services:\nThis will bring you to the CloudFormation Stacks table.\nThere will be one stack named cloudacademylabs in the table with a Status of CREATE_COMPLETE.\nNote: If the stack hasn’t reached the Statusof CREATE_COMPLETE, try refreshing the page after a minute. It only takes a minute for the stack to fully create.\n2. To view details of the stack, under Stack name, click the cloudacademylabs link.\n3. Click the Resources tab:\nYour Physical IDs will be different than in the supplied image. Note in the Typecolumn that a DynamoDB Table, a Lambda Function, and IAMresources to grant the Lambda access to the DynamoDB Table have all been created. You will be querying the DynamoDB table via Lambda function invocations to create CloudWatch Logs, that will be aggregated and searchable via a user interface (UI).\n4. Click on the Outputs tab, and open the DynamoLambdaConsoleLink link in the Value column:\nThis takes you to the Lambda function Console.\nCreating Some Logs Using AWS Lambda 1. Briefly look around the Lambda function console:\nThe Designergives a visual representation of the AWS resources that trigger the function (there are none in this case), and the AWS resources the function has access to (CloudWatch Logs, and DynamoDB). The actual code that is executed by the function is farther down in the Function codesection. You don’t need to worry about the actual implementation details of the function for this Lab.\n2. To configure a test event to trigger the function, scroll down to the Code source section and click Test:\n3. In the Configure test event form, enter the following values before scrolling to the bottom and click Save:\nEvent name:TestPutEvent Enter the following in the code editor at the bottom of the form: Copy code\n{ \"fn\": \"PUT\", \"data\": { \"id\": \"12345\", \"name\": \"foobar\" } } The PUT object event will update the DynamoDB database with an object with the given id.\n4. To run your function with your test event, click Test again:\nAfter a few seconds, in the code editor, a tab called Execution results will load:\nThe function succeeded and the Function Logsarea displays the logs that were generated and automatically sent to CloudWatch Logs by AWS Lambda.\n5. To view the Amazon CloudWatch logs, click the Monitor tab, and then click View logs in CloudWatch:\nNote: The Lab’s CloudFormation stack outputs also include a link to the Log Group if you need to access it at a later time.\nManually Viewing Logs in Amazon CloudWatch 1. Observe the Log Streams in the CloudWatch log group for the Lambda function you invoked:\nThe rows in the table are different Log Streams for the log group.\nEach log stream corresponds to log events from the same source. AWS Lambda creates a new log event for every Lambda invocation. However, it is possible to have multiple log streams for a single Lambda function since the log stream corresponds to the container actually running the function.\nBehind the scenes, a Lambda is run in a container. After a period of inactivity, the container is unloaded and the following requests will be served by a new container, thus creating a new log stream. Depending on how many times you invoked the test command in the previous step, you will see one or more rows in the log stream.\n2. Click on the latest Log Stream.\nThe log stream is a record of event Messages ordered in Time:\n3. Enter _PUT_into the Filter eventssearch bar and click enter:\n4. Click the triangle to expand the event that matches the filter.\nYou will see the JSON formatted message:\nThe outermost data attribute wraps the test event you configured.\n5. Click custom to display the custom time range filter available in CloudWatch Logs:\nObserve the time-based options in the dialog box that displays:\nThe filter by text and by time capabilities are the tools that are available for sifting through logs in CloudWatch Logs. The text filters support some forms of conditions that can be expressed through a syntax specific to CloudWatch. These capabilities are handy, but you will see that there are more powerful tools available for log aggregation and retrieval.\nLaunching the OpenSearch Domain The first thing you need is an Amazon OpenSearch cluster/domain. Using the Amazon OpenSearch Service has the following benefits:\nIt’s distributed and resilient It supports aggregations It supports free-text search It’s managed and takes care of most of the operational complexities of operating a cluster In 2021 AWS renamed Amazon ElasticSearch Service to Amazon OpenSearch Service. You may see references to ElasticSearch in the Amazon Management Console. You should assume that ElasticSearch and OpenSearch refer to the same AWS service.\nThe following diagram illustrates the overall design of the AWS Lab environment and the part that you are building in this lab step is highlighted in the lower-left corner in the AWS cloud:\n1. In the search bar at the top, enter OpenSearch, and under Services, click the Amazon OpenSearch Service result:\n2. To begin creating your cluster, on the right-hand side of the welcome page, click Create domain:\nThe terms OpenSearch domain and an OpenSearch cluster can be used almost interchangeably. The former is the logical search resource, and the latter is the actual servers that are launched to create a domain.\nThe Create domain form will load.\n3. In the Name section, in the Domain name textbox, enter ca-labs-domain-###, replacing ### with some random numbers:\n4. In the Deployment type section, select the following:\nDeployment type: Select Development and testing Version: Select 6.8under ElasticSearch In this short-lived lab, you are using a Development and testing deployment because it allows public access and reliability isn’t a concern. In a production environment, you will want to use a Production deployment to get the full availability benefits and meet security requirements.\n5. In the Auto-Tune section, select Disable.\nIn this short-lived lab, Auto-Tune is not necessary.\n6. In the Data nodes section, enter and select the following and leave remaining defaults:\nInstance type: Select t3.small.search Number of nodes: Enter 1 The storage type values correspond to the storage types available for Amazon EC2 instances.\nWhen deploying a cluster that uses multiple nodes, you can specify that the nodes are deployed in two or three availability zones. Deploying in multiple availability zones makes the cluster highly available and more reliable in the case of failures of outages.\n7. Scroll down to the Network section, and select Public access:\nIn this lab, you are creating a publicly available Amazon OpenSearch Service cluster for convenience. Be aware that you can also deploy a cluster into an Amazon Virtual Private Cloud (VPC) and receive the network isolation and security advantages of using a VPC.\n8. In the Fine-grained access controlsection, uncheck the Enable fine-grained access controlbox.\n9. In a new browser tab, enter the following URL:\nhttps://checkip.amazonaws.com/\nYou will see an IP address displayed. This is the public IPv4 address of your internet connection. You will use this IP address to restrict access to your Amazon OpenSearch Service cluster.\n10. Scroll down to the Access Policy section and under Domain access policy, select Configure domain level access policy:\nYou will see a policy editor form display with the tabs Visual editor and JSON.\n11. In the Visual editor tab, enter and select the following:\nType: Select IPv4 address Principal: Enter the IP address you saw on the Check IP Page Action: Select Allow You are specifying an access policy that allows access to the cluster from your IP address. In a non-lab environment, you could deploy the cluster into an Amazon VPC and configure private or public access using a VPC’s networking features.\n12. To finish creating your cluster, scroll to the bottom and click Create:\nA page displaying details of your cluster will load and you will see a green notification that you have successfully created a cluster.\n13. In the General information section, observe the Domain status:\nAWS is setting up and deploying your cluster. This process can take up to 15 or 30 minutes to complete.\n12. To see the latest status of your Amazon OpenSearch Service cluster, refresh the page in your browser.\nRefresh the page for your domain periodically to check if it has finished deploying.\nWhilst waiting for the domain to finish provisioning, feel free to consult the Amazon OpenSearch Service Developer Guide to learn more about the OpenSearch service.\nWhen the cluster has been provisioned you will see the Domain status change to Active:\nSending CloudWatch Logs to OpenSearch 1. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n2. In the left-hand menu, under Logs, click on Log groups:\n3. Select the log group beginning with /aws/lambda/cloudacademylabs-DynamoLambda-:\nNext, you will create a subscription filter to send the log data to your ElasticSearch domain.\n4. Click Actions, in the menu that opens, under Subscription filters, click Create Amazon OpenSearch Service subscription filter:\nThe Create Amazon OpenSearch Service subscription filter form will load.\n5. In the Choose destination section, select the following:\nSelect account: Ensure This accountis selected Amazon OpenSearch Service cluster: Select the cluster you created previously After selecting the Amazon OpenSearchService cluster, the Lambda function section will appear.\n6. In the Lambda IAM Execution Role drop-down select LambdaElasticSearch:\n7. In the Configure log format and filters section enter the following:\nLog Format: Select Amazon Lambda Subscription filter name: ca-lab-filter The default Subscription Filter Pattern matches the timestamp, request_id, and event JSON. The Test Patternbutton is available to see which events match the pattern.\n8. To start sending the logs to ElasticSearch, at the bottom, click Start streaming:\nMomentarily, you will see a notification that the subscription filter has been created and logs are being streamed to OpenSearch:\nDiscovering and Searching Events 1. Navigate back to the Lambda function you invoked earlier and click the Test button a few times to submit more PUT events:\n2. Click the arrow on the Test button and click Configure test event:\n3. In the Configure test eventsform, click the radio button for Create new test event and enter the following non-default values:\nEvent name:TestGetEvent Enter the following in the code editor at the bottom of the form: { \"fn\": \"GET\", \"id\": \"12345\" } You will submit more test events of a different type - GET operations on the object that was PUT in the database. This gives two different event types to look at in Kibana (the Log Aggregation UI).\n4. Click Save.\n5. Click Test several times to make GET events.\n6. Return to the Amazon OpenSearch Search Console for the domain you created and click the link under Kibana URL:\n7. In the Add Data to Kibana section, on the right-hand side under Use Elasticsearch data, click Connect to your Elasticsearch index:\nThe log data is stored in OpenSearch, but you need to tell Kibana which index to use for discovering the data.\n8. In the Create an index patternwizard, enter the following value and click Next step:\nIndex pattern: cwl-* The pattern will match the daily CloudWatch Logs (cwl) indices that are created in Amazon OpenSearch.\n9. In the second step, enter the following value and click Create index pattern:\nIndex pattern: Select @timestamp The Time filter field name allows Kibana to determine which attribute represents the timestamp of each event. The confirmation page displays all of the fields in the log data index:\nNow that the index settings for Kibana are configured, you can begin using the Log Aggregation system!\n10. Click Discover in the sidebar menu on the left of the page.\n11. Explore the Discover interface:\nYou see some events and a graph. These are your aggregated log events! The system is online! Notice the search bar up top. It is initially empty so all log events will show up. But what if you only want to see the PUT events for objects containing 12345?\n12. Enter PUT 12345 in the search bar and press enter:\nThe matching terms in the event show up highlighted, and the bar graph updates to show only the count of PUT 12345 events that you made by clicking Test in the Lambda interface.\n13. Click on the timestamp range in the upper-right corner to display the time filter:\nJust as with CloudWatch Logs, you can filter the logs by time. However, in Kibana you can also drag on the bar chart to select a time range visually: Visualizing Aggregated Events 1. Click Visualize in the Kibana sidebar menu.\n2. Click Create a visualization:\n3. Select Areachart visualization:\n4. In the From a New Search, Select Indexarea, click on the *cwl- **index name:\nIf you had any saved searches in the system, you could use them to make this Visualization from this step.\nOn the left-hand side, the visualization configuration tools will appear:\n5. Enter the following values in the visualization configuration:\nSelect buckets type: X-Axis Aggregation: Date Histogram (to track log trends over time) Field: @timestamp Interval: Auto To make the graph more interesting, you will split the PUTs and GETs and display each stacked in on the chart with different colors. This requires a sub-buckets.\n6. Click Add sub-buckets below the rest of the X-Axis settings, and enter the following values:\nSelect buckets type: Split Series Sub Aggregation: Terms (Terms splits the data based on the unique values of a field) Field: $event.data.fn.keyword (The test requests used the fn key for request type, which maps to the $event.data.fn.keyword field in OpenSearch) 7. Click the play button to apply the changes and produce the visualization:\nIt will look something like the image below, with two regions in an area graph corresponding to GET and PUT event count over time:\nTo use the visualization in a Dashboard in the next step, you need to save the visualization.\n8. Click Save in the top toolbar:\n9. Enter PUTs and GETs Over Time in the Save Visualization field, and click Save:\nCreating a Kibana Dashboard 1. Click on Dashboard in the sidebar menu.\n2. Click Create a dashboard:\n3. Click Add to add saved visualizations to the dashboard:\n4. Select the PUT and GETs Over Time visualization:\nThe visualization is added to the dashboard, but the size may not be what you like. You can adjust the size of the visualization by dragging the arrow in the lower-right corner:\n5. Click Save and enter the following values before clicking the revealed Savebutton:\nTitle: Log Dashboard Description: Lambda API Logs You’ve done it! The Dashboard will always contain the up-to-date statistics for your GET and PUT events that run through the Lambda function:\n6. Return to the Lambda console and create as many test events as you want.\n7. Refresh the Kibana dashboard and see the new requests in the visualization:\nYou can also configure Auto-refresh to avoid having to refresh the view:\n","description":"Will create a distributed, scalable log aggregation system within AWS running on Amazon OpenSearch Service. This Log Aggregation System will ingest as much of your CloudWatch log stream events as you want, events generated from AWS EC2 Instances, Lambda functions, Databases, and anything else you want to submit log events from.","title":"Build A Log Aggregation System in AWS","uri":"/en/tracks/aws-certified-developer-associate/opensearch-service/build-log-aggregation-system/"},{"content":"Changing the Metadata of an Amazon S3 Object Introduction Each object in Amazon S3 has a set of key/value pairs representing its metadata. There are two types of metadata: “System metadata” (for example, Content-Type and Content-Length) and custom “User metadata”. User metadata is stored with the object and returned with it.\nAs an example, you might have your own taxonomy for various images, such as “logo”, “screenshot”, “diagram”, “flowchart” and so on. In this lab step, you will change the Content-Type of your image to “text/plain”. You will also create custom user metadata.\nNote: With the new Amazon S3 UI you can set the metadata as part of the upload process, or add it later.\nInstructions Return to the cloudfolder/ and delete the cloudacademy-logo.png from your Amazon S3 bucket by checking the checkbox and clicking Delete: The Delete objects form page will load. Because a deleted object is not retrievable, AWS asks you to confirm that you want to delete the object before deletion.\nIn the textbox at the bottom of the page, enter permanently delete and click Delete objects:\nTo return to the bucket object view, at the top-right, click Close.\nClick Upload, then Add files and browse to the logo file (or drag-and-drop it into the Upload wizard) in order to upload it again.\nNear the bottom of the page, expand the Properties section:\nScroll down to the Metadata section and click Add metadata:\nA row of form elements will appear.\nEnter the following: Type: Select System defined Key: Select Content-Type Value: Enter text/plain The drop-down list contains the System metadata that you can set.\nIn this lab, you have set the content type to text/plain as an example to see how to add metadata to an object when uploading to Amazon S3.\nNext you will add custom user metadata. User metadata must be prefaced with “x-amz-meta-”. The remaining instructions will add a custom user type for imagetype, and imagestatus.\nClick Add metadata again to add another row.\nEnter the following to define custom metadata:\nType: Select User defined Key: Enter imagetype after x-amz-meta Value: Enter logo You have added two metadata key-value pairs to the object you are going to upload. One system metadata and one user-defined.\nAt the bottom of the page, click Upload:\nTo exit the upload form, at the top-right, click Close.\nIn the Objects table click the cloudacademy-logo.png object:\nScroll down to the Metadata section and observe the key-value pairs you added:\nThis is also where you can add, remove, and edit metadata after you have uploaded objects to Amazon S3.\n","description":"Changing the Metadata of an Amazon S3 Object","title":"Change metadata of S3 Object","uri":"/en/tracks/aws-certified-developer-associate/s3/how-to-change-metadata-s3/"},{"content":"Display Chart.js diagrams/blocks\nSources\n","description":"Hugo chart.js shortcode","title":"chart","uri":"/en/posts/hugo-shortcode-examples/chart/"},{"content":"Goal: Check if you are ready to pass the Cloud exam\nThe application calculates progress after each answered question. Ability to answer at least one question and get a comment at the same time. No need to pass all questions before. It is convenient to spend 20 min a day Works from web/tablet/mobile Link: https://www.cloud-exam-prepare.com\n","description":"Check if you are ready to pass Cloud exam","title":"Cloud exam Quizz","uri":"/en/apps/cloud-exam-quizz/"},{"content":"Release notes example Changed\nfeat(exports): export mergeConfig #5151 Fixed\nfix(CancelledError): include config #4922 fix(general): removing multiple/trailing/leading whitespace #5022 fix(headers): decompression for responses without Content-Length header #5306 fix(webWorker): exception to sending form data in web worker #5139 Refactors\nrefactor(types): AxiosProgressEvent.event type to any #5308 refactor(types): add missing types for static AxiosError.from method #4956 Chores\nchore(docs): remove README link to non-existent upgrade guide #5307 chore(docs): typo in issue template name #5159 Common used\nbuild: Changes that affect the build system or external dependencies (e.g., updates to package.json, pom.xml, build.gradle, Dockerfile, etc.) chore: Regular maintenance tasks and changes that don’t modify the source code or the test suite (e.g., updating build tasks, package manager config, etc.) ci: Changes to Continuous Integration configuration files and scripts (e.g., changes in Jenkinsfile, Travis CI configuration, CircleCI, etc.) docs: Changes only affecting documentation (e.g., changes in README, API docs, comment blocks, etc.) feat: Introducing a new feature to your application. fix: A bug fix in your application code. perf: Performance improvements to your code (e.g., optimizing algorithms, improving efficiency, etc.) refactor: Changes in the code that neither fix a bug nor add a feature; typically, these changes improve code readability or structure. revert: If you are reverting a previous commit. style: Changes to the coding style (e.g., changes in whitespace, formatting, missing semi-colons, etc.) that do not affect the meaning of the code. test: Adding or updating tests, covering new or existing functionality. Code format Detailed description in git commits style Python style JavaScript style ","description":"Code style notes","title":"Code style notes","uri":"/en/posts/code-style/"},{"content":"About AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.\nDocumentation User Guide CodeAr­tifact is a secure storage, publishing, and sharing of software code packages used in a development process organisation’s software development. CodeAr­tifact makes it easy for small organisations to store, publish, and share software packages.\nCodeArtifact can be configured to automatically fetch software packages and dependencies from public artifact repositories.\nCodeArtifact works with commonly used package managers and build tools like Maven, Gradle, npm, yarn, twine, pip, and NuGet making it easy to integrate into existing development workflows.\nPrice Pay only for what you use – the size of the artifacts stored, the number of requests made, and the amount of data transferred out of an AWS Region. CodeArtifact includes a monthly free tier for storage and requests\nCurrent price\nUse Cases Type: Developer Tools\nAlternatives JFrog Artifactory Docker hub Sonatype Nexus Platform Helm Azure DevOps Services Github Usage aws codeartifact list-domains --region us-east-1 Practice Getting started using the console\n","description":"Amazon CodeArtifact","title":"CodeArtifact","uri":"/en/tracks/aws-certified-developer-associate/codeartifact/"},{"content":"About CodeBuild is a fully managed service that assembles source code, runs unit tests, \u0026 also generates artefacts ready to deploy.\nDocumentation User Guide CodeBuild is a code creation service that also produces code artefacts upon request.\nCodeBuild is an alternative to other build tools such as Jenkins.\nCodeBuild is integrated with KMS for encryption of build artifacts, IAM for build permissions, VPC for network security, and CloudTrail for logging API calls.\nCodeBuild is a fully managed build service to compile source code, run unit tests and produce artifacts that are ready for deployment. Not the best fit for serverless template deployment or serverless application initialization.\nbuildspec.yml Build instructions can be defined in the code (buildspec.yml).\nCodeBuild Local Build In case you need to do deep troubleshooting beyond analyzing log files.\nCan run CodeBuild locally on your computer using Docker.\nLeverages the CodeBuild agent.\nPrice Current price\nYou pay based on the time it takes to complete the builds.\nLab cicd-aws-code-services Chapters:\nLogging in to the Amazon Web Services Console Creating an AWS CodeCommit Repository Committing Code to Your AWS CodeCommit Repository Building and Testing with AWS CodeBuild Deploying with AWS CodeDeploy Automating Your Deployment with AWS CodePipeline Following the Continuous Deployment Pipeline Recovering Automatically from a Failed Deployment Using AWS CodeDeploy Blue/Green Deployments in Your Pipeline Questions Q1 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\nWhat is the easiest way to develop these simple applications?\nInstall AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model\nhttps://aws.amazon.com/serverless/sam/\n1\n","description":"Amazon CodeBuild - Build and test code with continuous scaling. Pay only for the build time you use.","title":"CodeBuild","uri":"/en/tracks/aws-certified-developer-associate/codebuild/"},{"content":"About CodeDeploy is a fully managed deployment service that automates software deploy­ments to a variety of compute services such as EC2, Fargate, Lambda, \u0026 on-pre­mises servers\nDocumentation User Guide CodeDeploy can also deploy a serverless Lambda function.\nCodeDeploy can be connected to CodePipeline and use artifacts from there.\nPlatforms Need to choose the compute platform:\nEC2/On-premises. AWS Lambda. Amazon ECS. AppSpec File The application specification file (AppSpec file) is a YAML-formatted, or JSON-formatted file used by CodeDeploy to manage a deployment.\nThe AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.\nDeployment types In-place deployment (EC2 only)\nBlue/green deployments:\nAWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.\nAmazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.\nEC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.\nPrice Current price\nUse Cases Type: Developer Tools\nPractice Continuous Integration and Deployment with AWS Code Services Questions Q1 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\nCan delete only if unused.\n1\nQ2 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n3\n","description":"Amazon CodeDeploy - Automate code deployments to maintain application uptime","title":"CodeDeploy","uri":"/en/tracks/aws-certified-developer-associate/codedeploy/"},{"content":"About AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\nDocumentation User Guide CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.\nYou can easily integrate AWS CodePipeline with third-party services such as GitHub or with your own custom plugin. With AWS CodePipeline, you only pay for what you use.\nAlternatives Bamboo. CircleCI. Jenkins. Travis CI. GitLab. TeamCity. Azure DevOps Server. Google Cloud Build. Terminology Pipelines\nA workflow that describes how software changes go through the release process.\nArtifacts\nFiles or changes that will be worked on by the actions and stages in the pipeline. Each pipeline stage can create “artifacts”. Artifacts are passed, stored in Amazon S3, and then passed on to the next stage. Stages\nPipelines are broken up into stages, e.g., build stage, deployment stage. Each stage can have sequential actions and or parallel actions. Stage examples would be build, test, deploy, load test etc. Manual approval can be defined at any stage. Actions\nStages contain at least one action, these actions take some action on artifacts and will have artifacts as either an input, and output, or both.\nTransitions\nThe progressing from one stage to another inside of a pipeline.\nPrice Current price\nQuestions Q1 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\n","description":"Amazon CodePipeline Automate continuous delivery pipelines for fast and reliable updates","title":"CodePipeline","uri":"/en/tracks/aws-certified-developer-associate/codepipeline/"},{"content":"About Amazon Cognito - Simple and Secure User Sign-Up, Sign-In, and Access Control\nDocumentation User Guide Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.\nUsers can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.\nAlternatives Auth0 Microsoft Azure Active Directory OneLogin Google Cloud Identity Platform IBM Security Verify Keycloak Terminology Credentials: The temporary security credentials, which include an access key ID, a secret access key, and a security token.\nAssumedRoleUser: The ARN and the assumed role ID, which are identifiers for the temporary security credentials that you can programatically refer to.\nPrice Pay only for what you use. First 50,000 (monthly active users (MAUs) - Free.\nCurrent price\nUse Cases Type: Identity \u0026 access management\nSame type services: Identity \u0026 Access Management (IAM), Single Sign-On, Cognito, Directory Service, Resource Access Manager, Organisations\nWorkflow The process of authenticating a user with Cognito is as follows:\nThe user signs in with a Web ID provider (Google, Facebook, Amazon, etc.) The Web ID provider returns a JWT token to the user The user application makes an STS API call: sts assume-role-with-web-identity STS returns an API response with the temporary credentials The user application now has AWS access e.g. for S3, DynamoDB, etc. Practice Manage Authentication with Amazon Cognito\nQuestions Q1 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\nWhat action will best solve this issue?\nUse AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n3\nQ2 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\n","description":"Amazon Cognito","title":"Cognito","uri":"/en/tracks/aws-certified-developer-associate/cognito/"},{"content":"","description":"Monitors date expiration of access and id tokens provided by Amazon Cognito. Refreshes when expired.","title":"cognito-token-observer","uri":"/en/apps/npm/cognito-token-observer/"},{"content":"Practice Lab link Creating an Amazon S3 Bucket for a Static Website 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\nYou will be placed in the Amazon S3 console.\n2. To start creating a new Amazon S3 bucket, in the top-right, click Create bucket:\nThe Amazon S3 bucket creation form will load.\n3. Under General configuration, enter the following:\nBucket name: Enter calabs-bucket-\u003cUniqueNumber\u003e(Append a unique number to the end of calabs-bucket-) Region: Ensure US West (Oregon) us-west-2 is selected You have added a unique number to the bucket name because Amazon S3 bucket names must be unique regardless of the AWS region in which the bucket is created.\nA bucket name must also be DNS compliant. Here are some of the rules it must adhere to:\nThey must be at least 3 and no more than 63 characters long. They may contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. They cannot be formatted as an IP address (for example, 192.168.1.1). The following are examples of valid bucket names:\ncalabs-bucket-1 cloudacademybucket cloudacademy.bucket calabs.1 ca-labs-bucket Make a note of the name of your bucket, you will use it later.\n4. Make sure to select ACLs Enabled:\n5. In the Block Public Access section, un-check the Block all public access check-box:\n6. To acknowledge that you want to make this bucket publicly accessible, check I acknowledge that the current settings might result in this bucket and the objects within becoming public:\n7. To finish creating your Amazon S3 bucket, scroll to the bottom of the form and click Create bucket:\nNote: If you see an error because your bucket name is not unique, append a different unique number to the bucket name. For example, change “calabs-bucket” to “calabs-bucket-1” (or a unique number/character string) and click Create bucket again.\nThe Buckets list page will load and you will see a notification that your bucket has been created:\nNext, you will enable static website hosting for your bucket.\n8. In the list, click the name of your bucket:\nYou will see an overview of your Amazon S3 bucket, and a row of tabs with Objects selected.\n9. In the row of tabs under Bucket overview, click Properties:\nThe Properties tab allows you to enable and disable various Amazon S3 bucket features, including:\nBucket Versioning: Old versions can be kept when objects are updated Default encryption: A bucket can be configured to encrypt all objects by default Server access logging: Web-server style access logs can be enabled Requester pays: When enabled, the entity downloading data from this bucket will pay data transfer costs incurred 10. Scroll to the bottom of the Properties page and in the Static website hosting section, on the right, click Edit:\nThe Edit static website hosting form will load.\n11. In the Static website hosting field, select Enable:\nThe form will expand to reveal more configuration options.\n12. Enter the following, leaving all other fields at their defaults:\nIndex document: Enter index.html Error document: Enter error/index.html 13. To finish enabling static website hosting, scroll to the bottom, and click Save changes:\nThe bucket overview page will load and you’ll see a notification that you have successfully enabled static website hosting:\nYour S3 bucket is ready to host content.\nNext, you will create a bucket policy that will apply to all objects uploaded to your bucket.\n14. In the row of tabs, click Permissions:\n15. Scroll down to the Bucket policy section, and on the right, click Edit.\nThe Edit bucket policy form will load.\nAmazon S3 bucket policies are defined in JavaScript Object Notation, commonly referred to as JSON.\n16. In the Policy editor, copy and paste the following and replace YOUR_BUCKET_NAME with the name of your S3 bucket:\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AddPerm\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\" } ] } This policy will allow public access to all objects in your S3 bucket.\nThis is a permissive policy. In a non-lab environment, security concerns may require you to implement a more restrictive policy. To learn more about bucket policies, visit the AWS Policies and Permissions in Amazon S3 documentation.\n17. To save your bucket policy, at the bottom of the page, click Save changes.\nThe bucket overview page will load and you will see a notification that the policy has been edited.\nNext, you will download a basic website from a public GitHub repository and load it into your S3 bucket.\n18. To download a zip file containing a basic website, click here.\nThis is an example website provided by CloudAcademy that is suitable for static website hosting.\n19. Extract the zip to your local file system.\nExact instructions will vary depending on your operating system and browser. In most browsers, you can click the downloaded file and a file extraction utility will open.\n20. In the row of tabs, click Objects.\n21. To begin uploading the website to your Amazon S3 bucket, scroll down and click Upload:\nThe Upload form will load.\n22. In the Files and folders section, click Add files:\nA file picker will open.\n23. Using the file picker, select all files and folders from inside the zip file you downloaded and extracted earlier.\nIf your extraction utility extracted the files to a folder called static-website-example-master, ensure you upload all the files and folders inside it but not the static-website-example-master folder itself. To be able to access the website, the index.html file must be at the top-level of your Amazon S3 bucket.\nYou may find it easier to drag and drop the files and folders onto the Upload page instead of using the file picker.\nYou may also see a browser confirmation dialog asking you to confirm you want to upload the files.\nOnce selected, the files and folders from the zip file will appear in the Files and folders section.\n24. Scroll to the bottom of the page and click Upload.\nYou will see a blue notification displaying the progress of the upload, and when complete you will see a green Upload succeeded notification.\n25. To exit the Upload form, on the right, click Close.\nThe bucket overview page will load.\nYour Objects section should look similar to:\nNext, you will test that your website is accessible.\n26. To retrieve the endpoint for your bucket, click the Properties tab, scroll to the bottom, and click the copy icon next to the Bucket website endpoint:\n27. Paste the endpoint into the address bar of a new browser tab.\nYou will see a website load that looks like this:\nThis website is being served by your Amazon S3 bucket.\nCreating an Amazon CloudFront Distribution for the Static Website 1. In the AWS Management Console search bar, enter CloudFront, and click the CloudFront result under Services:\nThe Amazon CloudFront console will load.\n2. To start creating a distribution, click Create a CloudFront Distribution:\n3. Under Origin, in the Origin Domain text-box, enter the Amazon S3 static website hosting endpoint that you created earlier:\n4. Scroll down to theSettings settings, in the Default Root Object text-box, enter index.html:\nYou are setting this field because Amazon CloudFront doesn’t always transparently relay requests to the origin. If you did not set a default root object on the distribution you would see an AccessDenied error when you access the CloudFront distribution’s domain later in this lab step.\nTo learn more, see the How CloudFront Works if You Don’t Define a Root Object section of the AWS developer guide for Specifying a Default Root Object.\n5. Leave all other settings at their default values, scroll to the bottom, and click Create Distribution.\nThe CloudFront distribution list page will load and you will see your distribution listed.\nYou will see the Last Modified of your distribution is Deploying:\nIt can take up to 15 minutes to deploy a new Amazon CloudFront distribution. Once complete, the Status will change to Enabled.\nThere are two main types of origin that Amazon CloudFront supports, Amazon S3 buckets, and custom origins. A custom origin could be a website being served by an EC2 instance, or it could be a web server outside of AWS. To learn more while your CloudFront distribution is deploying, visit the AWS Using Amazon S3 Origins, MediaPackage Channels, and Custom Origins for Web Distributions page.\nOnce your deployment is complete, continue with the instructions.\n6. To view details of your distribution, click the random alphanumeric ID:\nNote: Your ID will be different.\n7. Copy the value of the Distribution Domain Name field:\n8. Paste the domain name into the address bar of a new browser tab.\nYou will see the website that you uploaded to your Amazon S3 bucket display:\nYou are accessing the website through your Amazon CloudFront distribution.\nNote: The instructions below are optional. Perform them if there is enough time left in the lab.\n9. On the website, click through and visit the different pages a few times to generate traffic.\nIf you have a different web browser available, try accessing the site in the other browser.\n","description":"Configuring a Static Website With S3 And CloudFront","title":"Configuring a Static Website With S3 And CloudFront","uri":"/en/tracks/aws-certified-developer-associate/cloudfront/configuring-static-website-s3-and-cloudfront/"},{"content":"Creating a Folder inside an Amazon S3 Bucket Introduction The AWS S3 console allows you to create folders for grouping objects. This can be a very helpful organizational tool. However, in Amazon S3, buckets and objects are the primary resources. A folder simply becomes a prefix for object key names that are virtually archived into it.\nInstructions Return to the Buckets menu by clicking here, and click on the calabs-bucket you created earlier. (Reminder: Your bucket name will differ slightly.)\nClick Create folder:\nIn the Folder name textbox, enter cloudfolder:\nScroll to the bottom and click Create folder:\nThe folder is created inside your S3 bucket:\n","description":"Create a Folder inside an Amazon S3 Bucket","title":"Create a folder inside S3 Bucket","uri":"/en/tracks/aws-certified-developer-associate/s3/create-folder-s3/"},{"content":"Practice Creating Classic Load Balancer Planning the Classic Load Balancer When you connected to the AWS account provided in the former step, you had a few things that were already deployed. This is the current infrastructure that was already deployed for you:\nYou already have a VPC with some subnets and 2 EC2 instances running inside the VPC in different Availability Zones. Both instances are inside the same Security Group called , which is allowing HTTP access from port 80 to anywhere (0.0.0.0/0). Each EC2 instance is running the same web application. We want to configure an LB to create a central point of access to our application, and we also want to configure our architecture in a way that users can only access the application through the ELB.\nIn the end, we should have a solution similar to this one:\nTo do that we will have to create and configure a Classic Load Balancer, and properly configure the needed Security Groups to make sure that our application will work as expected.\nCreating a Classic Load Balancer and Registering EC2 Instances A Classic Load Balancer allows traffic to be balanced across many Amazon EC2 instances, it performs this balancing at the request and connection level.\n1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. In the left-hand menu, under Load Balancing, click Load Balancers:\n3. To start creating your classic load balancer, click Create Load Balancer:\nThree tiles will be displayed detailing the different types of load balancer supported by Amazon EC2.\n4. At the bottom of the page, click Classic Load Balancer:\n5. In the Classic Load Balancer tile, click Create:\nA multi-step wizard will open allowing you to configure and customize your load balancer.\n6. Under Basic Configuration, enter the following values:\nLoad Balancer name: Enter classic-elb Enable advanced VPC configuration: checked Be aware there are limitations on the name field, only the characters a-z, A-Z, 0-9 and hyphens are allowed.\nCreate LB Inside lets you select which VPC you want the load balancer to be created in, leave this at the default.\nThe Create an internal load balancer option determines whether the load balancer can accept public internet traffic or not. If checked, the load balancer will have a private IP address and will only be able to accept traffic from another source inside the VPC.\nThe default Listener Configuration, listening on port eighty (HTTP), is all that is required for this lab.\n7. Under Select Subnets, click the plus icon next to each subnet.\nAs you click for each subnet, it will move from the Available subnets table, to the Selected subnets table:\nAn Availability Zone, often referred to as an AZ, helps make your infrastructure more reliable. You can think of each zone as a separate data center (in many cases they are exactly that), they are guaranteed to have redundant power, networking, and connectivity within an AWS region.\nTo learn more about regions, availability zones, and redundancy in AWS, visit the documentation here.\nEach subnet is mapped to one availability zone. It’s important to configure the selected subnets correctly. If a subnet containing an EC2 instance is not selected, the load balancer will not be able to communicate with that EC2 instance.\n8. To move to the next step of the wizard, click Next: Assign Security Groups:\n9. In the form, enter and select the following values:\nAssign a security group: Select Create a new security group Security group name: Enter elb-sg Description: Enter Security group for the classic load balancer You will see a default security group rule allowing traffic on port eighty.\n10. In the default security group rule, in the Source drop-down, select Anywhere:\n11. To advance to the next page of the wizard, click Next: Configure Security Settings:\nThis wizard step display’s a warning that your load balancer isn’t configured to use HTTPS or SSL.\nIt’s strongly recommended that you always enable encrypted traffic on your load balancers for security reasons. Configuring SSL is beyond the scope of this lab. If you would like to learn more about SSL and load balancing, it’s covered in the Using Elastic Load Balancing \u0026 EC2 Auto Scaling to Support AWS Workloads course.\n12. To move to the next wizard step, click Next: Configure Health Check:\n13. In the Ping Path field, replace the contents with /:\nBy default, the fields on this page specify that the health check will be performed using the HTTP protocol on port eighty. This means the load balancer will assume an instance is healthy when the instance returns a 200 OK response.\nThe Advanced Details allow you to further customize different aspects of the health check:\nResponse Timeout: How long to the load balancer should wait for a response from the EC2 instance. Interval: Amount of time between health checks. Unhealthy threshold: The number of consecutive failed healthy checks that must occur before the load balancer declares the EC2 instance unhealthy. Healthy threshold: The number of consecutive health checks that must occur before declaring an EC2 instance healthy. To learn more about Elastic Load Balancing health checks, see the AWS documentation here.\n14. To move to the next wizard step, click Next: Add EC2 Instances:\nThis step of the wizard displays the EC2 instances that currently exist and can be added to the load balancer:\n15. Select the instances named web-node:\nTake a look at the configuration options on this page:\nCross-Zone Load Balancing ensures that your LB distributes incoming requests evenly across all instances in its enabled Availability Zones. This means that the LB will ignore the default of round-robin and will also take into consideration the Availability Zone in which the instance is running. This reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone and improves your application’s ability to handle the loss of one or more instances.\nConnection Draining is used to ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open.\nLeave these options at their defaults.\n16. To advance to the next wizard step, click Next: Add Tags:\nIn a non-lab environment, it is best practice to add tags to resources you create. Tags help make managing, organizing, and filtering resources in AWS easier.\nTo read more about tagging resources in AWS, see this document from AWS.\n17. To proceed to the review step, click Review and Create:\nThis page allows you to review the load balancing settings you have configured:\n18. To create your load balancer, click Create:\nYou will see a notification that your load balancer has been successfully created:\n19. To return to the EC2 management console, click Close:\nConfiguring Security Groups for Load Balanced EC2 Instances 1. In the list of load balancers, ensure your load balancer is selected:\nYou will see some tabs beneath the list and the Description tab will be selected.\nThis tab shows general information about your load balancer.\n2. To view information about instances registered with this load balancer, click the Instances tab:\nYou will see the instances and availability zones listed:\nThe instances will have a status of InService. This means the load balancer is performing successful health checks on the instances.\nNote: If you see the Status as OutOfService then the instances are still be registered. Wait a minute or two and then click the refresh icon in the top-right corner.\n3. To see the DNS of your load balancer, click the Description tab.\n4. Copy the domain name from the value of the DNS name field:\nWarning: Don’t include the (A Record) part of the value when copying.\n5. In a new browser tab, paste the domain name, and press enter.\nYou will see an instance Id displayed:\nNote: Your instance Id will be different.\nAn application has been pre-installed on the EC2 instances that will respond to web requests with the instance Id of the instance serving the request.\nTo see the Id of the other EC2 instance, refresh the page. If the Id doesn’t change, you may need to open an incognito or private browsing tab and visit the DNS name again.\nSeeing the Id change shows that the load balancer is working as expected, routing traffic to both registered instances.\nLeave this tab open and remember this is the tab for the load balancer, you will use it again later in the lab step.\n6. In the left-hand menu, under Instances, click Instances:\nYou will see two instances named web-node with a status of Running:\n7. Select one of the instances:\nYou will see tabs displayed below the list of instances.\n8. In the Details tab, in the Public IPv4 DNS field, click the copy icon:\nThe public DNS name of the EC2 has been copied to your clipboard.\n9. In a new browser tab, paste the DNS name and press enter.\nYou will see an instance Id displayed again.\nHowever, this time, because you are accessing the instance directly if you refresh or visit the DNS name in an incognito or private browsing tab, the Id won’t change.\nNote that you are accessing the instance directly, this is allowed by the security group associated with the EC2 instances. Allowing load-balanced instances to be publicly accessible is a bad security practice, and there is rarely a good reason for it.\nIn the rest of this lab step, you will modify the EC2 instance’s security group to only allow traffic from the load balancer.\nLeave this browser tab open and remember this is the tab for an EC2 instance, you will use this tab again later.\nNavigate to Load Balancers in the EC2 Management Console. 11. Ensure the classic-elb load balancer is selected.\n12. In the Description tab, scroll down to the Security section:\nThis is the security group you configured when you created the load balancer.\n13. In the left-hand menu, under Network \u0026 Security, click Security Groups:\nYou will see a list of security groups:\n14. Select the SG which has the Group Name starting with cloudacademylabs- .\nThis is the security group of the EC2 instances.\nYou will see tabs displayed beneath the list.\n15. In the row of tabs, click Inbound rules:\n16. To modify the rules of this security group, click Edit inbound rules:\nYou want to allow only connections coming from the load balancer to the instances, however, the balancer doesn’t have a particular IP address associated with it so you can’t specify an IP address here. Instead, you will restrict the access by using the security group you created for the balancer.\nYou will change the current rule to deny access to anywhere and allow it only to members of the load balancer’s security group.\n17. Delete the existing rule, and create a new one whose Type is HTTP. In the Source drop-down, ensure Custom is selected and in the box next to it, select elb-sg:\n18. To save your changes, in the bottom-right, click Save rules:\nWith your rule saved, reload the browser tab with the DNS of the load balancer.\nThis will continue to work, you will see an instance Id displayed.\n19. Reload the browser tab with the DNS of an instance in the address bar:\nThe exact behavior will vary depending upon your web browser.\nMost likely you see the loading symbol in the browser tab spinning indefinitely:\nIf you wait long enough, your browser will report that it timed out trying to reach the instance:\nChecking Your Load Balancer’s Behavior During Instance Failures Navigate to Instances in the EC2 Management Console. You will see two instances named web-node listed.\n2. To stop an instance, right-click one of them.\n3. In the menu that appears, click Instance state, and then click Stop instance:\nYou will see a dialog box asking you to confirm that you want to stop the instance.\n4. To confirm, click Stop:\nThe instance’s Instance state column will change to Stopping. A few moments later you will see it changed to Stopped:\nStopping the instance will make it fail your load balancer’s health checks.\nNavigate to Load Balancers in the EC2 Management Console. 6. Ensure the classic-elb load balancer is selected.\n7. In the row of tabs below the load balancer list, click Instances:\nLook at the Status column in the instances table, one of the instances will still be InService, and the other will be OutOfService:\nThis means that there is only one instance serving the application, and therefore all the requests will be forwarded to the same instance.\nYou can test this behavior by clicking on the Description tab and accessing the DNS nameof the load balancer in a new browser tab. Your request will be served by the instance that you didn’t stop.\nLeave the browser tab with the load balancer’s DNS name open. You will test it again after starting the stopped instance.\n8. To start the stopped instance, in the left-hand menu, under Instances, click Instances:\n9. Right-click the stopped instance.\n10. Click Instance state, and click Start instance:\nNote: You can also access this menu using the Actions button in the top-right.\nThe Instance state column will change to Pending, and a few moments later, to Running.\nTest accessing the load balancer by it’s DNS name again. This time, you will see that both instances are serving requests.\nNote: You may need to open the load balancer’s domain name in an incognito or private browsing tab to see both instance Ids.\nMonitoring your Classic Load Balancer Navigate to Load Balancers in the EC2 Management Console. 2. In the list of load balancers, ensure the classic-elb load balancer is selected, and click the Monitoring tab:\nYou will see a number of graphs of different CloudWatch metrics.\nThe Elastic Load Balancing (ELB) service reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, the load balancing service measures and sends its metrics in sixty-second intervals. If there are no requests flowing through the load balancer, or no data for a metric, the metric is not reported.\nThere are a few metrics related to a Classic Load Balancer, and most are self-explanatory if you are familiar with HTTP requests. If some of them are unfamiliar to you, visit the Amazon AWS documentation to read more.\nThe metrics called HealthyHostCount, and UnHealthyHostCountwill count the number of Healthy and Unhealthy instances respectively. These metrics can be useful for you to identify a major problem in your AWS account. A healthy instance is one that is passing the health checks performed by the load balancer.\nYou could use CloudWatch Alarms to notify you when you have less than 2 instances running your application, though to be clear this is not a general rule: the number of instances that might identify a problem will vary depending on your environment.\nAlso notice that in these metrics, there is no way of seeing the Availability Zone to which the Healthy/Unhealthy instance belongs. In our lab, we stopped an instance for a few minutes, therefore you should be able to see something like this:\nIf the Healthy Hosts metric reaches zero, that means that people won’t see anything when accessing your load balancer, and it is probable that you have a big problem in your infrastructure.\nThe Average Latency metric might be useful to identify potential issues in your setup. Maybe everything is working in your application, but you notice an increase in this metric. If you haven’t changed anything in your application, that can be a potential issue - maybe you haven’t provisioned enough EC2 instances, or you even have lots of instances but they don’t have enough power to serve your increasing traffic.\nThe other metrics can be very useful for troubleshooting specific scenarios and will vary depending on your setup.\n","description":"tutorial how to create AWS Classic Load Balancer","title":"Create Classic Load Balancer","uri":"/en/tracks/aws-certified-developer-associate/elasticloadbalancing/create-amazon-load-balancing/"},{"content":"Creating an Amazon S3 Bucket Introduction You can create an Amazon S3 bucket using the AWS Management Console. As with many other AWS services, you can use the AWS API or CLI (command-line interface) as well.\nIn this lab step, you will create a new Amazon S3 bucket.\nInstructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: You will be placed in the S3 console.\nFrom the S3 console, click the orange Create Bucket button: Enter a unique Bucket name on the Name and region screen of the wizard: Region: US West (Oregon) (This should be set for you. If not, please select this region.) **Important!**Bucket names must be globally unique, regardless of the AWS region in which you create the bucket. Buckets must also be DNS-compliant.\nThe rules for DNS-compliant bucket names are:\nBucket names must be at least 3 and no more than 63 characters long. Bucket names can contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. Bucket names must not be formatted as an IP address (for example, 192.168.1.1). The following examples are valid bucket names: calabs-bucket-1, cloudacademybucket , cloudacademy.bucket , calabs.1 or ca-labs-bucket.\nTroubleshooting Tip: If you receive an error because your bucket name is not unique, append a unique number to the bucket name in order to guarantee its uniqueness:\nFor example, change “calabs-bucket” to “calabs-bucket-1” (or a unique number/character string) and try again.\nLeave the Block public access (bucket settings) at the default values: No changes are needed. This is where you can set public access permissions.\n5. Click on Create bucket:\nA page with a table listing buckets will load and you will see a green notification that your bucket was created successfully.\nIn the Buckets table, click the name of your bucket in the Name column: A page will load with a row of tabs at the top.\nTo see details and options for your bucket, click on the Properties: This page allows you to configure your Amazon S3 bucket in many different ways. No changes are needed in this lab at this time.\nFeel free to look at the other tabs and see the configuration options that are available.\n","description":"Create an Amazon S3 Bucket","title":"Create S3 Bucket","uri":"/en/tracks/aws-certified-developer-associate/s3/create-s3-bucket/"},{"content":"The journey towards creating a light trading bot framework begins with a simple, intuitive graphical user interface (GUI). For this venture, I have chosen to rely on Python’s built-in Tkinter library to provide the fundamental building blocks for my GUI.\nEmbracing Simplicity The initial goal is to keep the GUI as straightforward as possible, focusing on the core functionalities required for a trading bot. The main elements incorporated are:\nStart/Stop Buttons: These allow the user to control the operation of the bot. Strategy Selection: An option to switch between different trading strategies. Market Price Stream: A live feed displaying the current market prices. Harnessing the Power of Tkinter Tkinter, being a standard Python interface to the Tk GUI toolkit, offers the simplicity and versatility needed for this project. It provides an array of widgets and methods to create and manage the GUI with relative ease.\nObserver Pattern To handle specific requests from the GUI and manage updates, I’ll be integrating the observer pattern into the framework. This design pattern promotes a well-organized, flexible structure that will prove invaluable as the project progresses.\nMoving Forward This is just the beginning. The GUI, while a crucial component, is merely the first step in the development of this lightweight trading bot framework. I’m looking forward to the journey ahead, as we dive deeper into the implementation and further enhance the features of the bot.\nThis is how this template looks like now:\n","description":"Creating a GUI for a Trading Bot","title":"Creating a GUI for a Trading Bot","uri":"/en/stories/003-trading-bot-gui-init-tkinter/"},{"content":"Deleting an Amazon S3 Bucket Introduction You can delete an Amazon S3 bucket using the S3 console. You will delete all objects within the bucket as well.\nInstructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: From the top level of the S3 console, notice the Delete button is not actionable.\n2. Check the name of your bucket to select it:\nWith the bucket selected, click Empty: The Empty bucket form page will load.\nIt’s not possible to delete a bucket that contains objects.\nTo confirm that you want to delete all objects in this bucket, in the textbox at the bottom, enter permanently delete and click Empty:\nTo exit the empty bucket page, at the top-right, click Exit:\nYou will be returned to the Buckets page.\nTo delete your bucket, select it in the list, and click Delete\nTo confirm that you want to delete the bucket, in the textbox, enter the name of your bucket:\nClick Delete bucket to delete the bucket.\nWarning: Make sure to delete all the files/folders inside the bucket before deleting it, otherwise AWS won’t allow you to delete the S3 bucket.\nImportant! Notice the message from AWS: “Amazon S3 buckets are unique. If you delete this bucket, you may lose the bucket name to another AWS user.”\nIf retaining the bucket name is important to you, consider using the Empty bucket feature and not actually deleting the bucket.\n","description":"Delete an Amazon S3 Bucket","title":"Delete S3 Bucket","uri":"/en/tracks/aws-certified-developer-associate/s3/delete-from-s3/"},{"content":"Web Create IB notification Login to https://www.interactivebrokers.co.uk/portal/#/ Click Deposit Click Use a new deposit method if no one exist Bank Wire -\u003e Get instructions Account Number: Bank account number\nNext you get Bank Wire Instructions These data you need to make a payment from Discount bank\nSend money from Discount bank Login start.telebank.co.il Click: ביצוע העברה\nFill the form\nClick המשך and proceed ","description":"Deposit Interactive Brokers from Israel Discount bank","title":"Deposit Interactive Brokers from Israel Discount bank","uri":"/en/posts/interactivebrokers-deposit/"},{"content":"Lab Develop and Deploy an Application with AWS CodeStar Creating an AWS CodeStar Project 1. In the AWS Management Console search bar, enter CodeStar, and click the CodeStar result under Services:\n2. On the welcome page, click Create project.\nTake a moment to see all of the different templates available in AWS CodeStar.\n3. Check the following boxes on the left filter bar to narrow down the listed templates:\nAWS services: EC2 Application category: Web application Programming languages: Node.js The choice of Application categoryand Programming language will be driven by the requirements of your project and skills available to you. The choice of AWS services may not be as easy. Some guidelines for choosing between the alternatives are:\nAWS Elastic Beanstalk: A good choice for a fully managed application environment running on EC2 servers. This option allows you to stay focused on your code. Amazon EC2: Preferable when you want to host the application on servers that you manage yourself, including on-premise servers. AWS Lambda: Choose this option if you want to run a serverless application. 4. Select the Express.jsproject template:\nExpress.js is a popular Node.js web application framework.\n5. In the next step of the Create project wizard, enter the following:\nProject name: ca-app-\u003cUnique_String\u003e (Replace \u003cUnique_String\u003e with a 6 characters. The name must be unique for the region because of AWS CodeCommit repository name restrictions) Project ID: Accept the default value The instructions in this Lab use ca-app for the project name, but you should use a different name or the project creation may fail if it is already in use.\n6. Make sure that CodeCommit is selected under Project repository:\nYou will see the EC2 Configuration section of the form.\n7. Ensure the following values are selected:\nInstance type: t2.micro(default value) VPC: Select the non-default VPC (The VPC without “(Default)”),or the VPC with only two subnets if there is no (Default) label Subnet: Select the subnet in the us-west-2a availability zone If you can’t see which subnet is in us-west-2a hover your mouse over each subnet.\n8. Click Next and thenCreate Project:\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail.\n6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nTouring the AWS CodeStar Project Website 1. Observe the tiles that are included in your Dashboard:\nIDE: References for how to get started with a variety of integrated development environments (IDEs) under Access your project code. You will simply use the EC2 instance to edit the code interact with CodeCommit in this lab.\nRepository: You can see the main details related to the code repository here. The most recent code commits for the selected branch:\nThe View commits button opens the detailed view list of the commits. Currently, there is only a master branch and the initial commit to display. The committer, AWS CodeStar, made the initial commit during the project creation. Each commit also includes a button on the right to view the code changes in AWS CodeCommit. You will look at the code in a future Lab Step.\nPipeline: This shows a graphical representation of the release pipeline for your project:\nAny time you commit a code change to the master branch, the pipeline will automatically deploy your application. As your application grows and the requirements for your release pipeline change, you can modify the pipeline by clicking Edit. For example, you may want to add an automated test stage, invoke an AWS Lambda function, or modify the deployment group to deploy to an Auto Scaling group. The Release changebutton can be used to force a deployment of the latest commit. That can be useful if you modify the pipeline or something went wrong with the release. If something does go wrong with a pipeline stage, you will see the bar on the left turn red.\nMonitoring: This shows the CPUUtilization and other metrics of the EC2 instance where your application is deployed.\nIssues: This Lab doesn’t include a JIRA project, but for projects requiring issue tracking you can find link to JIRA from here.\n3. Click View application in the upper-right to view the application included in the template:\nDepending on your time of day, the background will change. You will commit a code change later to modify the appearance of the application.\n4. Look at the Project resources tab under the Overview.\nThe most interesting thing to see here is the list of all the Project Resources created by the project template:\nAWS CodeStar saved you a lot of time compared to manually configuring everything that is included. Notice that AWS CloudFormation includes a stack resource. That is how AWS CodeStar works behind the scenes. Each project template creates a stack in AWS CloudFormation. Of course, you don’t need to know any of the details. AWS CodeStar does everything for you so you can focus on development.\nIf you need to delete an AWS CodeStar project, you can do so from the CodeStar project page. You will be given a choice of keeping the associated resources or also deleting the associated resources.\nDeveloping Your AWS CodeStar Project 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n2. Click on Users in the left navigation panel.\n3. In the Users table, click on student.\nNote: You will see error messages. This is normal. You only have the permissions required to complete the Lab.\n4. Click on the Security credentials tab.\n5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section, and click Generate credentials:\nThis will show a pop-up dialog showing you your credentials.\n6. Click Download credentials:\nYour browser will download a file containing a username and password. Keep this file, you will use the credentials to connect to your AWS CodeStar repository.\n7. Return to your AWS CodeStar project’s Repository tab and click HTTPS under Clone repository:\nThis copies the HTTPS url of the CodeCommit repository to your clipboard.\n8. Paste the repository into the file with your code repository credentials.\nYou will use this URL later to access your repository.\n9. Return to the SSH shell connected to the dev-instance EC2 instance and enter cd to ensure you are in your home directory of /home/ec2-user.\nRefresh the instance connect browser tab if the session has expired.\n10. To tell Git to cache your credentials for a few hours, enter the following command:\ngit config --global credential.helper 'cache --timeout=10800' 11. Tell Git your user name:\ngit config --global user.name \"student\" This name will show up on the commits in your project dashboard.\n12. To clone your AWS CodeStar project repository, enter:\ngit clone \u003cYOUR_PROJECT_REPOSITORY_URL\u003e Replace \u003cYOUR_PROJECT_REPOSITORY_URL\u003e with the URL you copied in a previous instruction.\nYour URL will be similar to https://git-codecommit.us-west-2.amazonaws.com/v1/repos/ca-app.\n13. When prompted, enter the Username and Password you saved in a text file earlier in this Lab Step.\nTip: The password generated by AWS is long and it is easy to make a typo when entering it. To avoid errors copy and paste the password.\n14. Change the repository directory name to ca-app:\nmv ca-app-\u003cUnique_string\u003e ca-app Note: Change ca-app-\u003cUnique_string\u003e to the name of your repository.\nThis won’t change the repository name. It will only simplify the instructions at the command-line by not having to enter your unique string following ca-app in this and later Lab Steps.\n15. Change into the directory:\ncd ca-app 16. Enter ls to get a quick overview of the project structure.\nThere are several files:\napp.js: JavaScript file that starts the server appspec.yml: Configuration file that instructs AWS CodeDeploy what steps to perform to deploy your application package.json: Metadata and dependencies related to your project README.md: Text file explaining the project template There is no need to get into the details of the file contents at this time. However, it is good to know that the appspec.yml file specifies scripts that run during the deployment of your application. The scripts are contained in one of the two project directories:\npublic: Static assets used for your application scripts: Scripts executed by AWS CodeDeploy during the deployment of your application Now you can get the server running on your development machine.\n17. Install the project dependencies using Node package manager (npm) and start the Node.js server:\nnpm install node app.js While the server is running you won’t be able to enter new commands. That won’t be a problem. Now you can test that the development server is serving the application.\nNavigate to Instances in the EC2 service in the AWS Console. 19. Select the instance named cloudacademylabs:\nIn the Description tab, you will see a field called Public DNS (IPv4).\n20. To copy the public DNS, click the click the copy icon under Public IPv4 DNS:\n21. Open a new browser taband paste the public DNS and append :3000 to the end and press enter:\nNow that you verified the application works on the development machine, you can make some code changes.\n22. Return to the SSH shell and press Ctrl+C to kill the running Node.js server.\n23. Enter the following multiline command at the shell prompt to update a file in the project:\necho 'var idx = Math.floor(new Date().getHours()); var body = document.getElementsByTagName(\"body\")[0]; var idxStep = 1; var refreshRate = 1000; function adjustIdx() { if (idx \u003c= 0) { // Start increasing idx idxStep = 1; } else if (idx \u003e= 23) { // Start decreasing idx idxStep = -1; } idx += idxStep; body.className = \"heaven-\" + idx; } body.className = \"heaven-\" + idx; setInterval(adjustIdx, refreshRate);' \u003e public/js/set-background.js 24. Test the changes by running the server again with node app.js and refresh the browser tab with your development application.\nYou will see a similar page as the previous one, but the color will change roughly once a second.\n25. Stop the Node.js server with Ctrl+C.\n26. View the local repository status:\ngit status This tells you that you are on the master branch and working from the initial code commit. The output also shows the set-background.js file was modified. You need to add the file to stage it before committing.\n27. Add the modified file to the staged changes in the commit:\ngit add public/js/set-background.js 28. Commit the staged changes to the local repository and add a short message about the changes:\ngit commit -m \"animation\" 29. Push the changes in your local repository to the remote AWS CodeStar project repository so they are synchronized:\ngit push Now that you have made a change to your code, you will see how the changes are deployed in the next Lab Step.\nSummary In this Lab Step, you committed a code change to your AWS CodeStar project repository. You created the required credentials and tested the application on your development server.\nDeploying Your AWS CodeStar Project 1. Return to your AWS CodeStar project view.\nThere are a few things to notice since you were here last:\nYour commit is now visible in the Repository\u003e Most recent committile Your Monitoring \u003e CPUUtilization tile might show some spikes if your application has already been deployed Your Pipeline tab may show one of the pipeline stages In progress or you may see a recent timestamp inside each stage box telling you the new version has been deployed. If you missed the release flowing through the stages of the pipeline, click Release change and click Continue in the pop-up.\n2. To inspect the code, in the Repository tab, click the most recent Commit ID:\nYour commit Id will be different.\n3. Look at the code changes:\nAdditions appear in green and removals would appear in red, if any were present. This is an easy way to keep track of what is happening to the code in your AWS CodeStar project.\n4. Navigate back to the Pipeline tab. Click on AWS Code Deploy under Deploy:\nThis opens your application in AWS CodeDeploy:\nYou can see the Deployment Groups created for deploying your application. In this case there will be just one with a Nameending in -Env. The Statuscolumn will tell you if your last deployment Succeededor failed. The time of the Last attempted deployment and Last successful deployment are also recorded.\n5. Click the name of your deployment group beginning with ca-app:\nNotice that by default Rollback enabled is false. That means if your deployment fails, AWS CodeDeploy will not attempt to deploy the last successful version. That is something you might consider changing when you use AWS CodeStar for one of your projects.\n6. Scroll down the page and inspect the Deployment group deployment history section.\nEach deployment that was attempted to be deployed is recorded here along with a link to where the artifacts are located on Amazon S3.\n7. Click on the most recent deployment in the Deployment Id column:\nYour deployment will have a different deployment id.\nThis opens a page with details of the most recent deployment:\nDeployment status: shows the state of the deployment operation Deployment details: shows information similar to what you saw on the AWS CodeDeploy application page Revision details: shows information about the revision deployed, including the location in AWS S3 Deployment lifecycle events: tells you the start and end times as well as the Durationof the deployment 8. To view the deployment life-cycle events, click View events down the bottom:\n9. To view the events, scroll down to the event list:\nYou will see events similar to the above.\nIn case of a failed deployment, one of the events will record the failure and provide a link under the Logs column to investigate the command and logs related to the failure. If you recall, the appspec.yml file in the code project was used to instruct AWS CodeDeploy on how to deploy your application. Your project provides different scripts to run for some of the events listed in the table.\n10. Finally, return to the AWS CodeStar and click View application.\nYou will see the latest version of your application including the animation commit deployed and available to the world.\nManaging Your AWS CodeStar Project Team 1. Return to your AWS CodeStar project’s Overview and click on Add team members:\n2. Click on the Userdrop-down menu and click on Logan.\n3. Set the team member values for Logan to:\nEmail address: test@cloudacademy.com Project Role: Contributor Remote Access: Checked (This allows the team member to upload an SSH public key to connect to EC2 instances) The difference between the default Project Roles is:\nViewer: Access to the project dashboard and able to view a few project resources Contributor: Everything Viewer can access plus view, modify, and access all project resources Owner: Everything Contributor has plus adding and removing team members, and deleting the project 4. Click Add team member:\nAfter adding a team member, you will be asked to create a profile for yourself.\n5. In the Create user profile form, enter the following values before clicking Create user profile:\nDisplay name: student Email address: student@cloudacademy.com You will see Logan and studentappear in the Team memberslist.\n6. Click Add team member and select Bessie from the drop-down menu.\n7. Enter the following values and click Add:\nEmail address: bessie@cloudacademy.com Project Role: Viewer Remote Access: Unchecked Now you can briefly experience the differences between the project roles.\n8. At the top of this Lab page, click on the Open Environment button.\nThis will sign you out of the student user and allow you to sign in as a different user.\n9. Log in to AWS using the team member in the viewer role:\nUser Name: Bessie Password: Lab-Viewer1 Navigate to AWS CodeStar in the AWS Console. 11. Click on your project name.\nObserve that the viewer role has access to view the same tabs as your student user.\n13. Click on the Repository \u003e Commit ID and see that a viewer is allowed to view code changes.\n14. Return to the project Pipeline section and click Release change, then Release.\nYou will receive an error message stating that you are not authorized to perform that action:\n15. At the top of this Lab page, click on the Open Environment button and sign in again with the following credentials:\nUser Name: Logan Password: Lab-Contributor1 The user Logan is in the contributor role, which has additional permissions than the viewer role.\n16. Click Release change, then Continue.\nThe contributor has permission to perform this action:\n17. Click Team in the left sidebar.\nNotice that you can only remove yourself from the team and not other members. That is a distinction between the contributor and owner roles.\n18. One last time, in the lab, click on the Open Environment button and sign in with the student credentials given in the Credentialssection of the Lab.\nCleaning Up Your AWS CodeStar Project 1. Return to your AWS CodeStar project dashboard and click on Settings:\n2. Click Delete project.\n3. Enter delete in the pop-up dialog:\n4. Click Delete:\nIn a few seconds you will return to the AWS CodeStar start page and all of the resources in the project will begin terminating.\n","description":"Develop and Deploy an Application with AWS CodeStar","title":"Develop and Deploy an Application with AWS CodeStar","uri":"/en/tracks/aws-certified-developer-associate/codestar/develop-and-deploy-app-with-codestar/"},{"content":"Despite their similar names, these structures serve different purposes, and understanding their differences is crucial to utilizing them effectively.\nTree A tree data structure is a collection of entities, called nodes, connected by edges.\nEach node contains a value, and a list of references to its child nodes. The first node of the tree is called the root. If we visualize it, a tree data structure resembles an inverted tree, with the root at the top and the leaves (nodes without children) at the bottom.\nTrees are hierarchical, non-linear data structures.\nThey are excellent for representing relationships between objects, and their operations usually have a logarithmic time complexity, making them efficient for search operations.\nLet’s create a simple binary tree in Python, where each node can have at most two children:\nclass Node: def __init__(self, data): self.data = data self.left = None self.right = None root = Node(1) root.left = Node(2) root.right = Node(3) Here, we have a tree with the root node storing the value 1. The root node has two children: the left child stores the value 2, and the right child stores the value 3.\nTrie A trie, also known as a prefix tree, is a type of tree that specializes in managing sequences, typically strings. In a trie, every node (except the root) corresponds to a character or a string, and each path down the tree can represent a word or a prefix.\nThe key characteristic of tries is that they provide a fast retrieval of data. They can check if a word or prefix exists in a dataset in O(M) time, where M is the length of the word.\nHere’s a simple Python example of a trie data structure:\nclass TrieNode: def __init__(self): self.children = {} self.end_of_string = False class Trie: def __init__(self): self.root = TrieNode() def insert(self, word): node = self.root for ch in word: if ch not in node.children: node.children[ch] = TrieNode() node = node.children[ch] node.end_of_string = True In this example, each node in the trie has a dictionary called children to keep references to its child nodes. The end_of_string flag helps determine if the current concatenation of characters forms a valid word.\nTries vs Trees Despite their shared properties (being tree-based structures), tries and trees are designed for different use cases.\nData Storage: A general-purpose tree can store any data type—numbers, strings, objects, whereas a trie is specifically used for storing sequences, like strings or arrays.\nNode Value: In a tree, each node holds a value. In a trie, nodes themselves don’t hold a value—instead, the value is the path from the root to that node.\nEfficiency: Tries are incredibly efficient when it comes to searching for a word or prefix in a dictionary. Trees, on the other hand, are more efficient for a wide range of operations, like searching, inserting, and deleting arbitrary values.\nMemory Usage: Tries can use more memory because of references in each node, especially when dealing with a large alphabet. Each node in a trie maintains a collection (often a dictionary or array) of all its child nodes. However, in a binary tree, each node only needs to keep a reference to at most two child nodes.\nLookup Time: Tries have a faster lookup time for certain tasks. For instance, finding a word in a trie takes O(M) time, where M is the length of the word. For a balanced binary search tree, the time complexity would be O(log N), where N is the number of elements in the tree.\n","description":"Difference between Tries and Trees?","title":"Difference between Tries and Trees?","uri":"/en/posts/tree-vs-trie-data-structures/"},{"content":" Docs EN | RU Posts EN | RU ","description":"","title":"Docs","uri":"/en/tracks/archive/"},{"content":"About AWS EC2 AWS EC2 User Guide Amazon Elastic Compute Cloud (EC2) - one of the most popular AWS services.\nAllows:\nto run different types of cloud instances and pay-per-use models. to control computing resources at the operating system level working in an Amazon computing environment. Digest EC2 \u0026 EBS EC2 (Elastic Compute Cloud) Instance EBS (Elastic Block Store) - Persistent storage volume AMI (Amazon Machine Image) - Packages OS and additional installations in a reusable template Instance and Instance Types: General Purpose (t-type and m-type), Compute Optimized(c-type), GPU Graphics, GPU Compute, Memory Optimized(r, × and z-type), and Storage Optimized(d, h and i-type) Purchasing Options: On Demand, Reserved, Scheduled, Spot, Dedicated Instance and Dedicated Host Spot: Partial hours are not billed if terminated by AWS EC2 Secure login information for your instances using key pairs Placement group: Cluster and Spread For root:\nGeneral purpose SSD (balances price \u0026 performance) Provisioned OPS SD (Highest performance for mission critical low-latency or high throughput workloads) Magnetic HDD (previous generation) For other:\nThroughput Provisioned HDD (low cost for frequently accessed, throughput intensive workloads) Cold HDD (lowest cost for less frequently workloads) Instance Store - temporary storage volume in which data is deleted when you STOP or TERMINATE your instance Price Pricing models:\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time. Reserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper. Spot - Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use. Current price\nPractice TL;DR Choose a region close to you Go to EC2 service Click on “Instances” in the menu and click on “Launch instances” Choose image: Amazon Linux 2 Choose instance type: t2.micro Make sure “Delete on Termination” is checked in the storage section Under the “User data” field the following: yum update -y yum install -y httpd systemctl start httpd systemctl enable httpd echo \"\u003ch1\u003eHello from web!\u003c/h1\u003e\" \u003e /var/www/html/index.html Add tags with the following keys and values: key “Type” and the value “web” key “Name” and the value “web-1” In the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere Click on “Review” and then click on “Launch” after reviewing. If you don’t have a key pair, create one and download it. Now HTTP traffic (port 80) should be accepted from anywhere Create an EC2 Instance Go to EC2 page -\u003e Launch Instance\nEC2 image Choose the image we want Create keys Let’s create a key to use to connect to the instance externally\nEnter any name you want. Leave all other parameters by default\nAfter the key is created it will start automatic downloading. You need it to connect to EC2 from your local terminal\nNetwork Settings Under Network Settings I leave Allow SSH traffic from\nCreate Click Launch Instance\nThe Instance has been created and is available for connection\nConnecting to EC2 from the terminal Connect to EC2 from a local terminal\nLet’s move previously created and downloaded mykey key to home folder of current user and give permissions to file CHMOD 400\ncd ~ cd Downloads/ mv mykey.pem $HOME cd .. chmod 400 mykey.pem To connect, we need a public iPv4 address. Find it on the instance page\nConnect with the command ssh.\nssh -i mykey.pem ec2-user@52.24.109.78 Questions Q1 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\nWhich approach meets these requirements?\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager\nSecrets Manager offers secret rotation\n4\nQ2 An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.\nIn this scenario, which of the following can be used to fulfill this requirement?\nDirectly attach multiple Instance Store volumes in an EC2 instance to deliver maximum IOPS performance. Launch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Launch an Amazon EFS file system and mount it to a Nitro-based Amazon EC2 instance and set the performance mode to Max I/O. Launch any type of Amazon EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Explanation An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible.\nThe AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.\nAmazon EBS is a persistent block storage volume. It can persist independently from the life of an instance. Since the scenario requires you to have an EBS volume with up to 64,000 IOPS, you have to launch a Nitro-based EC2 instance.\nAmazon EBS volume types\n2\nQ3 A Database Specialist manages an EBS-Optimized Amazon RDS for MySQL DB instance with Provisioned IOPS storage. The users recently raised a database IO latency issue during peak hours when it was always under a heavy workload. Upon review, the Specialist noticed that the RDS DB instance was barely using the maximum IOPS configured but was fully utilizing the maximum bandwidth for the required throughput. CloudWatch metrics showed that CPU and Memory utilization were at optimum levels.\nWhich action should the Database Specialist take to fix the performance issue?\nChange the underlying EBS storage type of the instance to General Purpose (SSD). Modify the DB instance to an EBS-Optimized instance class with higher maximum bandwidth. Disable EBS optimization on the MySQL DB instance to allow higher maximum bandwidth. Modify the DB instance to increase the size and corresponding Provisioned IOPS allocated to the storage. Explanation Amazon RDS volumes are built using Amazon EBS volumes, except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads. RDS currently supports both magnetic and SSD-based storage volume types. There are two supported Amazon EBS SSD-based storage types, Provisioned IOPS (called io1) and General Purpose (called gp2).\nProvisioned IOPS storage is a storage type that delivers predictable performance and consistently low latency. If your workload is I/O constrained, using Provisioned IOPS SSD storage can increase the number of I/O requests that the system can process concurrently.\nProvisioned IOPS SSD storage provides a way to reserve I/O capacity by specifying IOPS. However, as with any other system capacity attribute, its maximum throughput under load is constrained by the resource that is consumed first. That resource might be network bandwidth, CPU, memory, or database internal resources.\nEBS–optimized instances deliver dedicated bandwidth to Amazon EBS. When attached to an EBS–optimized instance, Provisioned IOPS SSD (io1) volumes are designed to achieve their provisioned performance, 99.9% of the time. Choose an EBS–optimized instance that provides more dedicated Amazon EBS throughput than your application needs; otherwise, the connection between Amazon EBS and Amazon EC2 can become a performance bottleneck.\n2\nQ4 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.\nHow can the application find this information?\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1\nQ5 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\nResources EC2 Linux Hands-On Lab EB FAQ EC2 Digest EB Digest Community posts https://dev.to/romankurnovskii/aws-ec2-cheat-sheet-2mhp ","description":"A step-by-step guide to Amazon EC2","title":"EC2","uri":"/en/tracks/aws-certified-developer-associate/ec2/"},{"content":"About AWS Elastic Beanstalk AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nDeploying new application versions to existing resources in AWS Elastic Beanstalk happens much faster (typically under a minute) and once again is mostly dependent on the size of the new application version.\nDigest When you want to use new run time capabilities with elastic bean stalk, it is better to use blue-green deployment Security group will not be removed when removing the stack with elastic bean stalk For long running tasks - Use Elastic Beanstalk worker environment to process the tasks asynchronously Launch configuration is used for modifying instance type, key pair, elastic block storage and other settings that can be configured only when launching the instance Rolling with Additional Batch and Immutable both involve provisioning new servers to ensure capacity is not reduced. All At Once means the application will be offline for the duration of the update. Performing a Rolling Update without an additional batch of servers means a reduction in capacity. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html For Blue green deployment - Use Elastic beanstalk swap URL feature or route 53 with weighted routing policies You create your own Elastic Beanstalk platform using Packer, which is an open-source tool for creating machine images for many platforms, including AMIs for use with Amazon Elastic Compute Cloud (Amazon EC2). Price There is no additional charge for AWS Elastic Beanstalk. Only the AWS resources required to store and run applications are charged.\nConcepts AWS doc Applications An application is a collection of different elements, such as environments, environment configurations, and application versions.\nYou can have multiple application versions held within an application.\nApplication Version An application version is a very specific reference to a section of deployable code. The application version will point typically to simple storage service (S3) where the deployable code may reside.\nEnvironment Configurations An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.\nEnvironment An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage the application is deployed as a solution and becomes operational within your environment.\nThe “environment” is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.\nEnvironment Tier Reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment.\nConfiguration Template This is the template that provides the baseline for creating a new, unique, environment configuration.\nPlatform Culmination of components in which you can build your application upon using Elastic Beanstalk. These are comprised of the OS of the instance, the programming language, the server type (web or application), and components of Elastic Beanstalk\nDeployment policies All at once – deploys the new version to all instances simultaneously and will be out of service for a short time. Rolling – deploys the new version in batches. Rolling with additional batch – deploys the new version in batches, but first launch a new batch of instances. Immutable – deploys the new version to a new set of instances. Traffic splitting – deploys the new version to a new set of instances and temporarily split incoming client traffic. Practice Controlled deployment with AWS Elastic Beanstalk Lab Controlled deployment with AWS Elastic Beanstalk\nIn this lab, we will deploy several application version updates in a load-balanced, auto-scaling environment.\nThe first update is deployed using a simple deployment. The second update is deployed using a `blue-green’ deployment, where a separate environment is created to run the new version of the application, and the DNS switch switches incoming traffic to the new environment.\nThe final deployment architecture will look like this\nLoading the application In this review, I’m using the code that Cloudacademy provided me, but I have a ready-made launch script that you can download from Elastic Beanstalk: download\nCreate Go to Elastic Beanstalk page and click Create Application.\nSet Name Specify a name for the new application Choose platform Under Platform choose the desired platform of the application. In our case - Node.js. Download source code Under Source code origin specify the version of the application and download the archive with the application. Example\nApplication Configuration Change the preset Configuration to Custom configuration:\nClick Edit under Rolling updates and deployments\nIn the default configuration, updates are distributed to all instances at the same time. This leads to application downtime, which is unacceptable for production environments.\nWe will set Rolling and Batch size to 30%\nNetwork Back in the main application form, click Edit in the Network configuration.\nOn the Modify network form, configure the following values, then Save.\nVPC: Select VPC with CIDR block 10.0.0.0/16. This will not be the default VPC. Load balancer settings: Load balancer subnets: Select subnets with CIDR blocks 10.0.100.0/24(us-west-2a)and 10.0.101.0/24 (us-west-2b). These are public subnets. The application load balancer requires at least two subnets in different availability zones Instance settings: * Instance subnets: Select a subnet with CIDR block 10.0.1.0/24. This is a private subnet.\nConfirmation Press Create app.\nThe app creation process takes from 5 minutes.\nThen go to Dasboard This concludes the loading phase of the app in Elastic Beanstalk. Next, let’s break down how to switch the downloading of the new version of the application to the clients.\nDownloading version 2 of the app Downloading version 2.0 Press Upload and deploy and download the updated code. For example, you can change the text in the same source code for comparison.\nSpecify new version and publication settings Version comparison Now we can compare both versions by following the links. In my case the applications look like this\nChanging the url of the apps Now let’s swap the apps around. So that a user who previously went to one address will now see the 2nd version of the app.\nUnder Actions, click on Swap environment URLs and then select the app you want to swap\nRemoving Elastic Beanstalk resources Elastic Beanstalk runs EC2 instances as well as other services to deploy applications. But you can remove all services from a single window.\ngo to the Applications section Select an application.f Click on Actions -\u003e Terminate environment Translated with \u003cwww.DeepL.com/Translator\u003e (free version) Questions Q1 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\nQ2 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\nWhich of the following options would meet this requirement?\nUse an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n2\nQ3 An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.\nIn Elastic Beanstalk, where does it store the application files and server log files?\nApplication files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk. Application files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs. Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs. Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs. Explanation AWS Elastic Beanstalk stores your application files and optionally, server log files in Amazon S3. If you are using the AWS Management Console, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account and the files you upload will be automatically copied from your local client to Amazon S3.\nOptionally, you may configure Elastic Beanstalk to copy your server log files every hour to Amazon S3. You do this by editing the environment configuration settings.\nWith CloudWatch Logs, you can monitor and archive your Elastic Beanstalk application, system, and custom log files from Amazon EC2 instances of your environments. You can also configure alarms that make it easier for you to react to specific log stream events that your metric filters extract.\nThe CloudWatch Logs agent installed on each Amazon EC2 instance in your environment publishes metric data points to the CloudWatch service for each log group you configure.\nEach log group applies its own filter patterns to determine what log stream events to send to CloudWatch as data points. Log streams that belong to the same log group share the same retention, monitoring, and access control settings. You can configure Elastic Beanstalk to automatically stream logs to the CloudWatch service.\nThe option that says: Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs is incorrect because the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to CloudTrail as this service is primarily used for auditing API calls.\n4\nQ4 A former colleague reached out to you for consultation. He uploads a Django project in Elastic Beanstalk through CLI using instructions he read in a blog post, but for some reason he could not create the environment he needs for his project. He encounters an error message saying “The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist.”\nWhat are the possible causes of this issue? (Select TWO.)\nHe selected the wrong platform for the Django code. Elastic Beanstalk CLI did not create one because your IAM role has no permission to create roles. Instance profile container for the role needs to be manually replaced every time a new environment is launched. You have not associated an Elastic Beanstalk role to your CLI. IAM role already exists but has insufficient permissions that Elastic Beanstalk needs. Explanation AWS EB CLI cannot create the instance profile for your beanstalk environment if your IAM role has no access to creating roles.\nThis error is also thrown when the instance profile has insufficient or outdates policies that beanstalk needs to function. More details on this can be seen on the references provided.\n2, 5\nResources https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html Tutorials and samples Community posts https://dev.to/romankurnovskii/todo-aws-aws-elastic-beanstalk-cheat-sheet-1718 https://dev.to/romankurnovskii/aws-elastic-beanstalk-top-questions-certified-developer-exam-478g ","description":"AWS Elastic Beanstalk","title":"Elastic Beanstalk","uri":"/en/tracks/aws-certified-developer-associate/elasticbeanstalk/"},{"content":"About Amazon Elastic Container Registry (Amazon ECR) - Fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere\nDocumentation User Guide Hosted private Docker registry\nAlternatives Docker Hub JFrog Artifactory Azure Container Registry Harbor Google Container Registry Red Hat Quay JFrog Container Registry Price Current price\nUse Cases Store, encrypt, and manage container images\nManage software vulnerabilities Streamline your deployment workloads Manage image lifecycle policies Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nPractice This commands returns the command to execute to be able to login to ECR:\nLogin\nget-login-password:aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository:\naws ecr create-repository \\ --repository-name hello-repository \\ --image-scanning-configuration scanOnPush=true \\ --region region Tag image\ndocker tag hello-world:latest aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Push\ndocker push aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Pull\ndocker pull aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository:latest Delete an image\naws ecr batch-delete-image \\ --repository-name hello-repository \\ --image-ids imageTag=latest \\ --region region Delete a repository\naws ecr delete-repository \\ --repository-name hello-repository \\ --force \\ --region region Labs:\nUse AWS Fargate for Serverless Deployment of Container Applications Quick start: Publishing to Amazon ECR Public using the AWS CLI Notes:\nIf you get a 503 Service Temporarily Unavailable error, try again after 30 seconds to let the load balancer finish adding the task to the target group. ","description":"Run highly secure, reliable, and scalable containers","title":"Elastic Container Registry","uri":"/en/tracks/aws-certified-developer-associate/ecr/"},{"content":"About Documentation User Guide Highly secure, reliable, \u0026 scalable way to run contai­ners\nAlternatives Google Container Engine (GKE) Azure Container Service IBM Bluemix Container Service Jelastic Multi-Cloud PaaS Terminology Amazon ECS Term Definition Cluster Logical Grouping of EC2 Instances Container Instance EC2 instance running the ECS agent Task Definition Blueprint that describes how a docker container should launch Task A running container using settings in a Task Definition Service Defines long running tasks – can control task count with Auto Scaling and attach an ELB Digest Microservices are built in multiple programming languages Containers simplify deployment of microservices: Step 1 : Create a self contained Docker image Application Runtime (JDK or Python), Application code and Dependencies Step 2 : Run it as a container any where Local machine OR Corporate data center OR Cloud Use On-Demand instances or Spot instances Launch type: EC2 or Fargate Data volumes attached to containers Deployment type: Rolling update Blue/green deployment (powered by AWS CodeDeploy) Task Placement Strategies: binpack - Leave least amount of unused CPU or memory. Minimizes number of container instances in use random - Random task placement spread - Based on specified values: Host (instanceId) (OR) Availability Zone(attribute:ecs.availability-zone) (Alowed) Combine strategies and prioritize How do you manage 100s of containers? ECS - Fully managed service for container orchestration Step 1 : Create a Cluster (Group of one or more EC2 instances) Step 2: Deploy your microservice containers AWS Fargate: Serverless ECS. DON’T worry about EC2 instances. Cloud Neutral: Kubernetes AWS - AWS Elastic Kubernetes Service (EKS) Load balancing: Performed using Application Load Balancers Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (\u003cwww.myapp.com/microservice-a\u003e and \u003cwww.myapp.com/microservice-b\u003e) Price Current price\nUse Cases Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nBest practice:\n10 Microservices =\u003e 10 Task Definitions =\u003e 10 Task IAM Roles with individual permissions needed by each microservice ECS vs EKS Amazon also provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\nAmazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Questions Q1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.\nmemory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_monitoring.html\n1, 2\n","description":"Run highly secure, reliable, and scalable containers","title":"Elastic Container Service","uri":"/en/tracks/aws-certified-developer-associate/ecs/"},{"content":"About Kubernetes (K8) Docker Container/Cluster management\nRun highly secure, reliable, and scalable containers\nDocumentation User Guide Alternatives Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Google Kubernetes Engine (GKE) Oracle Cloud Infrastructure Container Engine for Kubernetes Mirantis Kubernetes Engine (formerly Docker Enterprise) Kubernetes Cloud Foundry Price Current price\nUse Cases Build and run web applications Deploy across hybrid environments Model machine learning (ML) workflows ECS vs EKS Amazon provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\nAmazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Practice Building a Cloud Native Application\n","description":"Amazon Elastic Kubernetes Service","title":"Elastic Kubernetes Service","uri":"/en/tracks/aws-certified-developer-associate/eks/"},{"content":"About Documentation User Guide Amazon Elasticache is a fully managed Redis or Memcached in-memory data store.\nIt’s great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\nYou can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don’t require durability like session stores, gaming leaderboards, streaming, and analytics.\nCompatible with Redis and Memcached\nPrice Current price\nUse Cases Type: In-memory\nUse Case Benefit Web session store In cases with load-balanced web servers, store web session information in Redis so if a server is lost, the session info is not lost, and another web server can pick it up Database caching Use Memcached in front of AWS RDS to cache popular queries to offload work from RDS and return results faster to users Leaderboards Use Redis to provide a live leaderboard for millions of users of your mobile app Streaming data dashboards Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays Caching Engines Memcached Redis Simple, no-frills You need encryption You need to elasticity (scale out and in) You need HIPAA compliance You need to run multiple CPU cores and threads Support for clustering You need to cache objects (e.g. database queries) You need complex data types You need HA (replication Backup and restore features Pub/Sub capability Multi-AZ with Auto-Failover Non persistent. No backups Multi-node for partitioning of data (sharding) Memcached ElastiCache manages Memcached nodes as a pool that can grow and shrink (similar to an EC2 Auto Scaling group); individual nodes are expendable and non-persistent.\nMemcached provides a simple caching solution that best supports object caching and lets you scale out horizontally. Ideal for offloading a DB’s contents into a cache.\nRedis ElastiCache manages Redis more as a relational database, i.e. Redis clusters are managed as persistent, stateful entities that include using multi-AZ redundancy for handling failover (similar to RDS).\nRedis supports complex data structures, hence would be ideal in cases where sorting and ranking datasets in memory are important (e.g. such as in leaderboards for games).\nCaching Strategies Lazy Loading The data that is read from the DB is stored in the cache. The data can become stale The data becomes stale because there are no updates to the cache when data is changed in the database Only cache data when it is requested. Cache miss penalty on initial request. Chance to produce stale data; can be mitigated by setting a TTL. Shorter TTL = less stale data.\nWrite-Through The data is added/updated into the cache everytime the data is written to the DB (no stale data) Because the data in the cache is updated every time it’s written to the database, the data in the cache is always current. Every database write will write to the cache as well. Data is never stale however there will be alot more operations to perform; and these resources are wasted if most of the data is never used.\nSession Store Stores temporary session data in cache (with TTL) - Time to Live. Data expires after the given time Practice Configuring a Lambda function to access Amazon ElastiCache in an Amazon VPC\nQuestions Q1 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\nThe metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to Elasticache is required for test or development purposes, it can be done through a VPN.\n2\nQ2 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\n","description":"Amazon ElastiCache","title":"ElastiCache","uri":"/en/tracks/aws-certified-developer-associate/elasticache/"},{"content":"About Unlike the SMA, the EMA gives more weight to recent data, making it quicker to respond to price changes.\nCalculating Formula EMA = (Close - Previous EMA) * Multiplier + Previous EMA\nWhere:\nClose: the closing price for a given period (Price today/now). Here close prices is for example N: the period of the EMA. Multiplier: 2 / (N + 1) Previous EMA is the EMA of the previous period. The EMA for the first period is just the Close price.\nBut for subsequent periods, it’s calculated as follows:\nEMA = (Close - Previous EMA) * Multiplier + Previous EMA EMA[i] = (Close[i] - EMA[i-1]) * 2/(N+1) + EMA[i-1] However, the EMA’s calculation is slightly more complex for the initial period because there is no previous EMA. In this case, we use the SMA as the first EMA:\nEMA(first period) = SMA Example:\nLet’s calculate a 5-minute EMA at Minute 6 with the following market data:\nMinute Open High Low Close EMA 1 $10.0 $11.0 $9.5 $10.0 - 2 $10.1 $12.1 $10.0 $12.0 - 3 $12.2 $15.2 $12.0 $15.0 - 4 $15.1 $15.1 $13.9 $14.0 - 5 $14.1 $16.1 $14.0 $16.0 13.4(SMA) 6 $16.1 $16.1 $14.9 $15.0 15.67 7 $15.1 $17.1 $15.0 $17.0 … 8 $17.1 $17.1 $15.9 $16.0 … 9 $16.1 $18.1 $16.0 $18.0 … Can be calculated from first price\nHere, N = 5, so Multiplier = 2 / (5 + 1) = 0.33.\nSMA(5) = (10.0 + 12.0 + 15.0 + 14.0 + 16.0) / 5 = 13.4 This value becomes the first EMA (EMA[5]). Now, to calculate the EMA for the 6th minute, we use the EMA formula: EMA[6] = (Close[6] - EMA[5]) * multiplier + EMA[5] = (15.0 - 13.4) * 0.33 + 13.4 = 14.13 Pros and Cons Pros:\nMore responsive: By giving more weight to recent prices, the EMA can adapt faster to price changes. Combines trend and momentum: The EMA not only captures the overall trend but also shows the asset’s momentum. Often used for High frequency trading. Cons:\nMore prone to false signals: The sensitivity of the EMA can sometimes lead to false signals, especially in volatile markets. Complex calculation: Compared to the SMA, the EMA’s calculation is slightly more complex, especially for longer periods. Example of signals Like the SMA, traders often use two EMAs: a short-term one and a long-term one. When the short-term EMA crosses above the long-term EMA, it’s a bullish (buy) signal, and when it crosses below, it’s a bearish (sell) signal.\nTrue Positive:\nIn minute 7, the short EMA crosses above the long EMA, which is a buy signal. The price then goes up, confirming this was a correct signal.\nIn a stable uptrend, the short-term EMA might cross above the long-term EMA, correctly suggesting that it’s a good time to enter a long position.\nFalse Positive:\nIn minute 11, the short EMA dips below the long EMA, suggesting a sell signal. However, the price increases in the next minute, making this a false signal.\nIn a volatile market, the price might swing up and down sharply, causing the short-term EMA to cross the long-term EMA back and forth, generating multiple buy and sell signals that could be misleading.\nUse in Real Trading In real trading, EMA can be used in combination with other indicators such as MACD (Moving Average Convergence Divergence) or Bollinger Bands.\nFor instance, a trader might look for the short-term EMA to cross above the long-term EMA and the MACD to cross above its signal line as a confirmation for a long position.\nPython Implementation Click here to view this notebook in full screen ","description":"EMA Trading indicator","title":"EMA - Exponential Moving Average","uri":"/en/posts/trading-indicators/ema/"},{"content":"Lab Encrypting S3 Objects Using SSE-KMS Creating a Customer Master Key (CMK) 1. In the AWS Management Console search bar, enter KMS, and click the KMS result under Services:\n2. Select Customer managed** keys** in the left pane of the KMS console.\nWarning: Cloud Academy cleans up the lab environment for you after a lab is completed or terminated. As a precaution, AWS prevents keys from being deleted immediately. Rather, they are queued for deletion, and an expiration period is set (of 7-30 days). For this reason, you may see residual keys from other students within the last week. For this reason, you may need to append a unique number to the Alias field in the next instruction.\n3. Click Create Key, then expand Advanced Optionsand set the following values:\nKey type: Symmetric(Symmetric keys are suitable for most data encryption applications. The same key is used for both encrypt and decrypt operations with symmetric key algorithms.) Key usage: Encrypt and decrypt Advanced options: Key Material Origin: Leave as KMS (default). AWS will generate the key material for encryption. Note that another common use case is for customers to generate their own keys, and have AWS keep a back up encrypted copy and help manage them with KMS. Regionality:Single-Region key 4. Click Next to advance to the Add Labels page of the wizard.\n5. Set the following values before clicking Next (leave the default values for other fields)\nAlias: calabs-CMK-key(Append a unique number to the key’s Alias if needed to be unique. For example, calabs-CMK-key2.) Description: 6. Click Next to advance to Define Key Administrative Permissions and leave the default values.\nAdministrative permissions allow users and roles to administer CMKs but not to perform cryptographic operations. In production environments, this is sometimes used to easily grant limited access to other users. The Allow key administrators to delete this keycheckbox makes it explicit if deleting keys is allowed, since the key can’t be recovered once deleted, making recovery of encrypted data impossible. Note that key deletion is not immediate and first enters into a pending state before the key is deleted. The delete operation can be canceled while in the pending state.\nThese settings generate a key policy. The default policy allows IAM policies to grant access the key, which is why you don’t require selecting your student user as an administrator. The lab IAM policy of your student user allows you to perform the required actions of the lab.\n7. Click Next to advance to Define Key Usage Permissions.\nUsage permissions grant access to perform cryptographic operations such as encrypting and decrypting. Enterprises usually have different permissions for administrators and users, hence the wizard walks you through defining both.\nNotice that you can grant access to the key so other AWS accounts can use it for encryption/decryption.\n8. Click Nextto preview the key policy and then click Finish when ready. The CMK is created.\n9. Confirm the key created correctly and that the Status is Enabled:\nEncrypting S3 Data using Server-Side Encryption with KMS Managed Keys (SSE-KMS) You will upload a file and encrypt it using SSE-KMS in this lab step.\n1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n2. Click the name of the bucket the Cloud Academy lab environment created for you (name begins with cloudacademylabs-ssekms):\n3. Click Upload.\n4. Click Add files and select a small file, or download this sample file and select it.\n5. Expand the Properties tab and scroll until the Server-side encryption settings.\n6. Check the Specify an encryption key checkbox.\n7. Check the AWS Key Management Service key (SSE-KMS) checkbox and then the Choose from your AWS KMS keys checkbox:\n8. Choose the AWS KMS key you previously generated:\n9. Click on Upload.\n10. Click Close and then click the name of the object to open its properties panel:\nYou can verify the object is encrypted using SSE-KMS by checking that the Encryption field is AWS-KMS.\nEnforcing S3 Encryption Using Bucket Policies 1. In the S3 bucket console, click the Permissions tab followed by Bucket Policy to open the Bucket policy editor:\nBucket policies are IAM policies applied to a bucket rather than to a user or role as is conventionally done with IAM policies. Similar to how a key policy applied to the CMK. These are examples of resource-based policies in AWS.\n2. Paste the following bucket policy into the policy editor:\n{ \"Version\": \"2012-10-17\", \"Id\": \"RequireSSEKMS\", \"Statement\": [ { \"Sid\": \"DenyUploadIfNotSSEKMSEncrypted\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::\u003cYour_Bucket_Name\u003e/*\", \"Condition\": { \"StringNotEquals\": { \"s3:x-amz-server-side-encryption\": \"aws:kms\" } } } ] } This policy denies (\"Effect\": \"Deny\") all users’ (\"Principal\": \"*\") uploads (\"Action\": \"s3:PutObject\") to the bucket (\"Resource\": \"arn:aws:s3:::\u003cYour_Bucket_Name\u003e/*\") if the s3:x-amz-server-side-encryption is not set to aws:kms, which corresponds to SSE-KMS. The lab provides you with the policy but you could recreate it using the policy generator linked to beneath the policy editor.\n3. Replace \u003cYour_Bucket_Name\u003e with the name of your lab bucket (it begins with cloudacademylabs-ssekms- and can be copied from the S3 console):\n4. Click Save changes to save the policy and have it start being enforced.\n5. Click the Objectstab followed by Upload.\n6. Click Add files and select a small file, or download this sample file and select it.\n7. Click Upload and observe the image does not appear in the bucket contents table.\nClicking upload without configuring any properties of the object uses the default of no encryption.\nYou can see the upload Failed.\n8. Retry the upload but this time use the Set properties step to configure Encryptionto AWS KMS master-key using your CMK.\nThe upload now succeeds since the bucket policy condition is satisfied:\nThe policy does not require the use of your CMK however, so the default S3 KMS key in the region is also allowed. You can change the policy condition to enforce a specific CMK is used.\n","description":"How to encrypt S3 Objects Using SSE-KMS","title":"Encrypting S3 Objects Using SSE-KMS","uri":"/en/tracks/aws-certified-developer-associate/kms/encrypting-s3-objects-using-sse-kms/"},{"content":"Abstraction Today worked on refining some abstraction level in framework. The main objective of today’s work was to enhance the abstraction in the classes and improve the overall code structure.\nThe goal was to make the code more adaptable and easier to maintain.\nFor example, instead of having Binance hardcoded in the from_binance_order method, now use the Exchanges enum, which makes the code more versatile and ready for additional exchanges in the future.\ndef from_binance_order(binance_order: BinanceOrder): exchange = Exchanges.BINANCE ... Orders Orders are at the heart of any trading bot, and handling them efficiently is crucial for the bot’s performance. In the Order class, I added the side property to capture whether the order is a buy or sell order, and also refined the Order creation method to accept side and order_type as arguments.\nImplemented a new method, update, in the Order class. This function checks if the incoming order is an instance of the Order class and updates specific fields based on certain conditions.\ndef update(self, other_order): if not isinstance(other_order, Order): raise ValueError(\"The given object is not an instance of Order\") # update only specific fields if other_order.time_to_cancel: self.time_to_cancel = other_order.time_to_cancel if other_order.status != OrderStatus.NEW: self.status = other_order.status Async One of the major changes made was to the way I handle async operations. In the Exchange class, the methods on_new_order_request and on_cancel_order_request were updated to run in separate tasks. This change helps in improving the overall efficiency of the bot as multiple tasks can run concurrently without blocking the main event loop.\nasyncio.create_task( self._exchange.on_new_order_request(order_request, **kwargs) ) Summury Today’s changes focused on enhancing the bot’s framework by making it more abstract, improving order handling, and making async operations more efficient.\n","description":"In this part of our trading bot series, we dive into the importance of enhancing abstraction in code structure, efficient order handling, and the implementation of asynchronous operations for optimized performance.","title":"Enhancing Trading Bot with Abstraction and Async Management","uri":"/en/stories/004-trading-bot-refactor-orders/"},{"content":"About EventB­ridge is a serverless event bus that makes it easy to connect applic­ations together using data from apps, integrated SaaS apps, \u0026 AWS services.\nDocumentation User Guide EventB­ridge is a low-cost alternative to building a new backend infrastructure for every new app. With Serverless EventB­ridge, you can connect your existing apps with a few lines of code. You don’t have to build a new backend for every new app you want to connect to.\nYou can use existing infrastructure as a provider of event data, and connect your apps using Serverless EventB­ridge.\nAlternatives Azure Service Bus TIBCO Cloud Integration (including BusinessWorks and Scribe) IBM App Connect Google Cloud Pub/Sub Apache Camel Peregrine Connect Software AG webMethods IBM Cloud Pak for Integration Price Current price\nUse Cases Type: Applic­ation integr­ation\nSame type services: SNS, SQS, AppSync, EventBridge\nRe-architect for speed Extend functionality via SaaS integrations Monitoring and Auditing Customize SaaS with AI/ML EventBridge vs Amazon SNS In comparison with Amazon SNS, EventBridge:\nIntegrates with more AWS services than SNS Supports registering message schemas Has sophisticated third-party integrations available Supports transforming event messages before sending them You should choose to use Amazon EventBridge over Amazon SNS when the system you are building is expected to:\nSupport significant asynchronous functionality Grow significantly in terms of both usage and complexity Have changing requirements over time Have components built by different teams that interact Need support for disparate event sources and targets Amazon EventBridge vs CloudWatch Events Amazon EventBridge extends CloudWatch Events - Build event-driven architectures Original goal with CloudWatch Events was to help with monitoring usecases specific to AWS services. React to events from Your Applications, AWS services and Partner Services Example: EC2 status change, change in your application or SaaS partner application Event Targets can be a Lambda function, an SNS Topic, an SQS queues etc Rules map events to targets (Make sure that IAM Roles have permissions) Event buses receive the events: Default event bus (for AWS services) Custom event bus (custom applications) Partner event bus (partner applications) Over time, Amazon EventBridge will replace Amazon CloudWatch Events Practice Processing File Uploads Asynchronously with Amazon EventBridge\nQuestions Q1 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\n","description":"Amazon EventBridge - Build event-driven applications at scale across AWS, existing systems, or SaaS apps","title":"EventBridge","uri":"/en/tracks/aws-certified-developer-associate/eventbridge/"},{"content":"Lab Fan-Out Orders using Amazon SNS and SQS Creating an Amazon SNS Topic and Amazon SQS Queues Here’s a diagram of what you will build and configure in this lab step:\nIn the search bar at the top, enter SNS and under Services, click the Simple Notification Service result: In the Create topic card on the right, in the Topic name textbox, enter new-orders and click Next step: The Create topic form will load.\nBy default, the Type of topic selected will be Standard. This is the most scalable topic type. The cost of this scalability is that message order and exactly-once delivery attempts can not be guaranteed.\nIf you are building a solution requires strict message ordering and exactly-once message delivery, you should use a FIFO type topic.\nStandard is fine for this lab.\nClick the black triangle next to Access policy - optional to expand the section: In the Access policy section, under Define who can publish messages to the topic, select Everyone: Under Define who can subscribe to this topic, select Everyone: You are using a permissive access policy to save time and because the focus of this lab is on demonstrating the fan-out scenario.\nIn a non-lab environment, you should carefully consider the access policy required and make sure if conforms with your company or organization’s security requirements.\nScroll to the bottom of the page, and click Create topic: You will see a page load displaying details of your newly created topic:\nIn the order processing system your are building, this Amazon SNS topic is where orders are published to. In a non-lab environment it would most likely be a web application or other application that accepts orders that will publish messages to this topic.\nNext, you will create two queues using Amazon Simple Queue Service and subscribe them to your Amazon SNS topic.\nOpen a new tab by right-clicking the AWS icon in the top-left and selecting Open in new tab. Note: The above instruction may vary slightly depending upon the web browser you are using.\nIn the search bar at the top, enter SQS, and under Services, click the Simple Queue Service result: In the middle right of the screen, in the Get started card, click Create queue: The Create queue form will open.\nIn the Name textbox, enter orders-for-inventory: Scroll down to the bottom, click Create queue: You will see a web page load showing you details of your newly created Amazon SQS queue:\nYou will now create a second Amazon SQS queue for analytics.\nTo navigate to the Queues list page, at the top-left, click Queues: On the right-hand side, click Create queue.\nRepeat the queue creation process, only this time enter orders-for-analytics as the Name of the queue.\nReturn to the Queues list page by clicking Queues in the top-left.\nYou will see the two queues you have created:\nClick the radio button for the orders-for-analytics queue.\nOn the right-hand side, click Actions and click Subscribe to Amazon SNS topic:\nThe Subscribe to Amazon SNS topic form will load.\nIn the Choose a topic drop down, select the topic ending with new-orders: This is the Amazon SNS topic you created earlier.\nClick Save to finish subscribing this queue to your topic.\nAt the top-left, click Queues again.\nRepeat the topic subscription process for your orders-for-inventory Amazon SQS queue.\nYou now have both of your Amazon SQS queues subscribed to your Amazon SNS topic. Any messages published to the topic will fan-out to both queues.\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail.\n6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nPublishing and Processing Messages In the terminal, enter the following command: aws sns list-topics You will see one topic displayed:\nNote: Your TopicArn will have a different account identifier.\nBy default, the AWS command-line interface tool uses the JSON format for responses. This response contains an array of Topics with one element. The element consists of a TopicArn.\nArn is short for Amazon Resource Name. An ARN is used to uniquely identify resources in AWS.\nIn this lab, the EC2 instance has been configured with an IAM role that has permissions to interact with Amazon SNS topics and Amazon SQS queues.\nStore the value of the TopicArn attribute in a shell variable (topic_arn): topic_arn=$(aws sns list-topics --query 'Topics[0].TopicArn' --output text) The above command uses the --query option to select only the value of the TopicArn and the --output option is used to specify plaintext format which removes the quotation marks from the value.\nTo publish a message, enter the following, utilizing the ARN you stored in the topic_arn shell variable: aws sns publish \\ --topic-arn $topic_arn \\ --message \"1 x Widget @ 21.99 USD\\n2 x Widget Cables @ 5.99 USD\" In response, you will see a MessageId:\nNote: Your message identifier will be different.\nYou have successfully published an order message to your Amazon Simple Notification Service topic.\nIn this lab, you are using the AWS command-line interface tool to simulate an application publishing an order message.\nIn a non-lab environment, the message could be published by a web application that accepts orders from customers.\nTo list Amazon Simple Queue Service queues, enter the following command: aws sqs list-queues You will see a JSON response:\nThe queues that you created earlier are listed.\nStore each of the QueueUrls in shell variables: analytics_queue_url=$(aws sqs list-queues --query 'QueueUrls[0]' --output text) inventory_queue_url=$(aws sqs list-queues --query 'QueueUrls[1]' --output text) To retrieve a message from the orders-for-analytics queue, enter the following command, utilizing the analytics queue URL you stored previously: aws sqs receive-message \\ --queue-url $analytics_queue_url You will see a JSON response containing an array with one Message:\nThe response contains the following fields:\nBody: A JSON representation of the message ReceiptHandle: You are required to supply this to delete a message after processing MD5OfBody: An MD5 hash of the message body MessageId: The message identifier that Amazon SNS saw when pushing the message to the queues Note that this is not the same as the MessageId that Amazon SNS returned to you when you published to the topic Repeat the previous instruction, using the orders-for-inventory queue but store the message response in a shell variable (for use later) and output the shell variable (using Python’s JSON tool to pretty print it): inventory_message=$(aws sqs receive-message --queue-url $inventory_queue_url) echo $inventory_message | python -m json.tool You will see the same message displayed again.\nThe message you published to the Amazon SNS topic has been sent to the Amazon SQS queues you subscribed the topic. This is an example of fanning out a message to multiple receivers.\nIn a non-lab environment, you could have worker applications constantly running and asking the Amazon SQS queues for more messages. One worker may be updating an inventory database for the order, whilst another worker could be recording the order details in a data lake for future analysis.\nUsing Amazon SNS and Amazon SQS like this allows you to build scalable systems that are decoupled and resilient. If a worker went offline, messages would queue up in the Amazon SQS queues. When the worker is available again, it can pick up new messages where it left off.\nYou can also have multiple worker applications, to help ensure there’s no downtime in message processing.\nAfter successfully processing a message, a worker application should delete the message to prevent it from being processed again.\nStore the value of the ReceiptHandle attribute in a shell variable: receipt_handle=$(echo $inventory_message | python -m json.tool | grep ReceiptHandle | cut -d\\\" -f 4) To delete a message, enter the following command for the orders-for-inventory queue: aws sqs delete-message \\ --queue-url $inventory_queue_url \\ --receipt-handle $receipt_handle Return to your browser tab with the Amazon SQS management console open. Note: If the SQS management console appears to only have one SQS queue, click the refresh button above the table:\nThe correct number of SQS queues will be displayed after a refresh.\nNavigate to the Queues list and click the orders-for-inventory queue.\nIn the top-right, click Send and receive messages:\nVerify that in that Receive messages section, under Messages available, it says 0. This is the queue you deleted a message for, simulating a long-running background application that receives an Amazon SQS message and then deletes the message after processing.\nRepeat the last three instructions for the orders-for-analytics queue and verify Messages available is 1: This is the queue you did not delete the message for. The message is still available to be picked up for processing by an application receiving messages from the queue.\n","description":"Fan-Out Orders using Amazon SNS and SQS","title":"Fan-Out Orders using Amazon SNS and SQS","uri":"/en/tracks/aws-certified-developer-associate/sqs/fan-out-orders-with-sns-sqs/"},{"content":"About Serverless version of ECS.\nServerless compute for contai­ners.\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.\nDeploy and manage your applications, not infrastructure. Fargate removes the operational overhead of scaling, patching, securing, and managing servers.\nCompatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\nDocumentation User Guide Alternatives Google Kubernetes Engine (GKE) Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Azure Container Instances Cloud Foundry Oracle Cloud Infrastructure Container Engine for Kubernetes Price Current price\nUse Cases Web apps, APIs, and microservices Run and scale container workloads Support AI and ML training applications Type: Containers\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\nQuestions Q1 How AWS Fargate different from AWS ECS?\nExplanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don’t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n","description":"Serverless compute for containers","title":"Fargate","uri":"/en/tracks/aws-certified-developer-associate/fargate/"},{"content":"About AWS Fault Injection Simulator (FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application’s performance, observability, and resiliency.\nDocumentation User Guide Price Current price\nWith AWS FIS, you pay only for what you use. There are no upfront costs or minimum fees. You are charged based on the duration that an action is active. The AWS FIS price is $0.10 per action-minute.\nTerminology and Concepts Everything starts with an experiment template. The experiment template defines the targets that participate in the experiment. Supported targets are:\nEC2 Instances EKS node groups RDS clusters \u0026 instances IAM roles The actions define the injected faults. You can run actions in parallel or sequence.\nSome action examples:\nAWS API level errors for the EC2 service Stop/reboot/terminate EC2 instances Run SSM commands on EC2 instances to stress CPU or memory, add network latency, or kill a process Reboot RDS instance Failover RDS cluster Drain ECS container instance Terminate EKS node group instance Use Cases Periodic Game Days Continuous Delivery Pipeline Integration Practice Test instance stop and start using\nQuestions Q1 What is Chaos Engineering?\nExplanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n","description":"Improve resiliency and performance with controlled experiments with AWS Fault Injection Simulator","title":"Fault Injection Simulator","uri":"/en/tracks/aws-certified-developer-associate/fis/"},{"content":"Commits More info about commits style\nGithub Actions Submodules Sync name: 'Submodules Sync' on: schedule: - cron: \"0 * * * *\" jobs: sync: runs-on: ubuntu-latest steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 with: submodules: true - name: Pull \u0026 update submodules recursively run: | git pull --recurse-submodules git submodule update --remote --recursive - name: Commit \u0026 push changes run: | git config --global user.name 'Git bot' git config --global user.email 'bot@noreply.github.com' git commit -am \"Auto updated submodule references\" \u0026\u0026 git push || echo \"No changes to commit\" Free space in git repo Download BFG\nor\nwget https://repo1.maven.org/maven2/com/madgag/bfg/1.14.0/bfg-1.14.0.jar Remove history files bigger than 100Kb:\ncd repo java -jar bfg-1.14.0.jar --strip-blobs-bigger-than 100K . git reflog expire --expire=now --all \u0026\u0026 git gc --prune=now --aggressive Removing an entire commit Replace “SHA” with the reference you want to get rid of. The “^” in that command is literal.\ngit rebase -p --onto SHA^ SHA We want to remove commits 2 \u0026 4 from the repo. (Higher the the number newer the commit; 0 is the oldest commit and 4 is the latest commit)\ncommit 0 : b3d92c5 commit 1 : 2c6a45b commit 2 : \u003cany_hash\u003e commit 3 : 77b9b82 commit 4 : \u003cany_hash\u003e Note: You need to have admin rights over the repo since you are using --hard and -f.\ngit checkout b3d92c5 Checkout the last usable commit. git checkout -b repair Create a new branch to work on. git cherry-pick 77b9b82 Run through commit 3. git cherry-pick 2c6a45b Run through commit 1. git checkout master Checkout master. git reset --hard b3d92c5 Reset master to last usable commit. git merge repair Merge our new branch onto master. git push -f origin master Push master to the remote repo. If didn’t publish changes, to remove the latest commit, do:\ngit rebase -i HEAD~\u003cnumber of commits to go back\u003e git rebase -i \u003cCommitId\u003e~1 git reset --hard HEAD^ git reset --hard commitId git rebase -i HEAD~5 If already published to-be-deleted commit:\ngit revert HEAD\nCleanups git stash clear git reflog expire --expire-unreachable=now --all git fsck --full git fsck --unreachable # Will show you the list of what will be deleted git gc --prune=now # Cleanup unnecessary files and optimize the local repository Common git commands git rev-list --all --count # count commits git clean -fd # To remove all untracked (non-git) files and folders! Resources Git commits style On undoing, fixing, or removing commits in git Truncating git history clean unused branches ","description":"Git snippets","title":"Git snippets","uri":"/en/posts/git-snippets/"},{"content":"Granting Public Access to an Amazon S3 Object Introduction All uploaded files are private by default and can only be viewed, edited, or downloaded by you. In order to illustrate this point, complete the instructions below.\nNote: The terms “file” and “object” are often used interchangeably when discussing Amazon S3. Technically, Amazon S3 is an object-store. It is not a block storage device and does not contain a file system as your local computer does. However, files such as images, movies, and sound clips are often uploaded from your file system to Amazon S3.\nInstructions 1. Click on the object you just uploaded to the S3 bucket.\nTake a look at the Object overview section:\nUnder Object URL, right-click the link and open the URL in a new browser tab: You will see an XML (eXtensible Markup Language) response telling you that access is denied for this object:\nNote: The response may appear differently depending upon your web browser.\nLeave the browser tab open. You will return to it shortly.\nTo allow public access to objects, you need to disable the default safety guards that prevent them from being made publicly accessible.\nTo return to the bucket view, at the top of the page, click the name of your bucket in the bread crumb trail:\nClick the Permissions tab and click Edit in the Block public access section:\nUncheck all of the options to allow all kinds of public access:\nYou should carefully consider anytime you allow public access to S3 buckets. AWS has implemented these security features to help prevent data breaches. For this lab, there is no sensitive data and you do want to allow public access.\nPoorly managed Amazon S3 permissions have been a contributing factor to many unauthorized data access events. AWS is making sure you understand the implications of allowing public access to an Amazon S3 bucket.\nAt the bottom of the page, click Save changes: A confirmation dialog box will appear.\nEnter confirm in the confirmation dialog box and click Confirm: You will see a green notification that the public access settings have been edited.\nTurning off Block all public access does not automatically make objects in an Amazon S3 bucket public. There are several ways of of explicitly granting public access including:\nBucket policies IAM policies Access control lists Pre-signed URLs In this lab, you will use a bucket policy to grant public access to your Amazon S3 bucket.\nScroll down to the Bucket policy section and click Edit: The Edit bucket policy page will load. Here you can specify a JSON (JavaScript Object Notation) policy to control access to your Amazon S3 bucket.\nReplace the contents of the Policy editor with the following: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:GetObject\" ], \"Effect\": \"Allow\", \"Resource\": \"BUCKET_ARN/*\", \"Principal\": \"*\" } ] } This is a permissive policy that allows GetObject access to anyone. More restrictive policies are possible such as\nRestricting access to specific principals Allow cross AWS account access Using conditions to restrict access to a specific IP address Notice the Resource is currently “BUCKET_ARN/*\", which is causing an error. We need to replace this with the ARN of the bucket we created:\nClick the copy icon under Bucket ARNand replace BUCKET_ARN in the value of the Resource key with the ARN you just copied : Note: Ensure that you preserve the /* at the end of the value. This means that the policy will apply to all objects inside the bucket recursively. Public access won’t be granted if this is not present.\nAt the bottom of the page, click Save changes: You will see a green notification that the bucket policy was edited.\nReturn to the browser tab where access was denied and fresh the browser tab. You will see the response change from “Access Denied” to the logo:\n","description":"Grant Public Access to an Amazon S3 Object","title":"Grant public access to S3 Object","uri":"/en/tracks/aws-certified-developer-associate/s3/grant-access-s3/"},{"content":"function addCopyButtonToCodeBlocks() { // Get all code blocks with a class of \"language-*\" const codeBlocks = document.querySelectorAll('code[class^=\"language-\"]'); // For each code block, add a copy button inside the block codeBlocks.forEach(codeBlock =\u003e { // Create the copy button element const copyButton = document.createElement('button'); copyButton.classList.add('copy-code-button'); copyButton.innerHTML = '\u003ci class=\"far fa-copy\"\u003e\u003c/i\u003e'; // Add a click event listener to the copy button copyButton.addEventListener('click', () =\u003e { // Copy the code inside the code block to the clipboard const codeToCopy = codeBlock.innerText; navigator.clipboard.writeText(codeToCopy); // Update the copy button text to indicate that the code has been copied copyButton.innerHTML = '\u003ci class=\"fas fa-check\"\u003e\u003c/i\u003e'; setTimeout(() =\u003e { copyButton.innerHTML = '\u003ci class=\"far fa-copy\"\u003e\u003c/i\u003e'; }, 1500); }); // Add the copy button to the code block codeBlock.parentNode.insertBefore(copyButton, codeBlock); }); } ","description":"Learn how to quickly add a copy button to code highlight blocks in Hugo to make it easier for users to share code snippets on your site.","title":"How to add copy code button on HUGO highligh code block","uri":"/en/posts/hugo-add-copy-button-on-highlight-block/"},{"content":"We can use recursion. Use Object.assign() and an empty object ({}) to create a shallow clone of the original. Use Object.keys() and Array.prototype.forEach() to determine which key-value pairs need to be deep cloned.\nconst deepClone = obj =\u003e { let clone = Object.assign({}, obj); Object.keys(clone).forEach( key =\u003e (clone[key] = typeof obj[key] === 'object' ? deepClone(obj[key]) : obj[key]) ); return Array.isArray(obj) \u0026\u0026 obj.length ? (clone.length = obj.length) \u0026\u0026 Array.from(clone) : Array.isArray(obj) ? Array.from(obj) : clone; }; const a = { foo: 'bar', obj: { a: 1, b: 2 } }; const b = deepClone(a); // a !== b, a.obj !== b.obj ","description":"How to create a deep clone of an object in JavaScript","title":"How to create a deep clone of an object in JavaScript","uri":"/en/posts/howto-create-deepclone-js/"},{"content":"In this step-by-step tutorial, we’ll create a simple React-Electron application from scratch. Electron is a framework that allows you to build cross-platform desktop applications using web technologies such as HTML, CSS, and JavaScript. React is a popular JavaScript library for building user interfaces.\nI use this clean template as a boilerplate for my projects.\nTLDR Go to repo electron-react-template click Use this template to create a new repo and start coding. Prerequisites Before starting this tutorial, make sure you have the following tools installed on your system:\nNode.js (version 12 or newer) npm or yarn (npm is bundled with Node.js, and yarn can be installed separately) A code editor (e.g., Visual Studio Code) Step 1: Set up the project structure Create a new directory for your project and navigate to it in your terminal:\nmkdir react-electron-app cd react-electron-app Next, create the following directories and files to set up the project structure:\nmkdir -p src electron public touch src/App.tsx src/index.tsx electron/main.ts touch public/index.html Your project should now have the following structure:\nreact-electron-app ├── electron │ └── main.ts ├── public │ └── index.html └── src ├── App.tsx └── index.tsx Step 2: Initialize the project Run the following command in the terminal to initialize the project with a package.json file:\nnpm init -y Step 3: Install dependencies Install the necessary dependencies for the project:\nnpm install --save react react-dom typescript electron npm install --save-dev concurrently electron-builder electron-is-dev wait-on cross-env Step 4: Configure TypeScript Create a tsconfig.json file in the root of the project:\ntouch tsconfig.json Add the following content to the tsconfig.json file:\n{ \"compilerOptions\": { \"target\": \"ES2023\", \"lib\": [\"dom\", \"dom.iterable\", \"esnext\"], \"allowJs\": true, \"skipLibCheck\": true, \"esModuleInterop\": true, \"allowSyntheticDefaultImports\": true, \"strict\": true, \"forceConsistentCasingInFileNames\": true, \"noFallthroughCasesInSwitch\": true, \"module\": \"esnext\", \"moduleResolution\": \"node\", \"resolveJsonModule\": true, \"isolatedModules\": true, \"noEmit\": true, \"jsx\": \"react-jsx\" }, \"include\": [\"src\"] } Step 5: Set up the React app Replace the contents of src/App.tsx with the following code:\nfunction App() { return ( \u003cdiv className='App'\u003e \u003cp\u003e Hello World! \u003c/p\u003e \u003c/div\u003e ); } export default App; Replace the contents of src/index.tsx with the following code:\nimport React from 'react'; import ReactDOM from 'react-dom'; import App from './App'; ReactDOM.render( \u003cReact.StrictMode\u003e \u003cApp /\u003e \u003c/React.StrictMode\u003e, document.getElementById('root') ); Replace the contents of public/index.html with the following code:\n\u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\" /\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /\u003e \u003ctitle\u003eReact-Electron App\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id=\"root\"\u003e\u003c/div\u003e \u003c/body\u003e \u003c/html\u003e Step 6: Set up the Electron main process Replace the contents of electron/main.ts with the following code:\nimport { app, BrowserWindow } from 'electron'; import * as path from 'path'; import * as isDev from 'electron-is-dev'; function createWindow() { const win = new BrowserWindow({ width: 800, height: 600, webPreferences: { nodeIntegration: true, }, }); win.loadURL( isDev ? 'http://localhost:3000' : `file://${path.join(__dirname, '../build/index.html')}` ); win.webContents.openDevTools(); } app.whenReady().then(createWindow); app.on('window-all-closed', () =\u003e { if (process.platform !== 'darwin') { app.quit(); } }); app.on('activate', () =\u003e { if (BrowserWindow.getAllWindows().length === 0) { createWindow(); } }); Step 7: Configure scripts in package.json Open your package.json file and update the scripts section to include the following:\n\"scripts\": { \"start\": \"react-scripts start\", \"build\": \"react-scripts build\", \"eject\": \"react-scripts eject\", \"electron:dev\": \"concurrently \\\"cross-env BROWSER=none yarn start\\\" \\\"wait-on http://127.0.0.1:3000 \u0026\u0026 tsc -p electron -w\\\" \\\"wait-on http://127.0.0.1:3000 \u0026\u0026 tsc -p electron \u0026\u0026 electron .\\\"\", \"electron:build\": \"yarn build \u0026\u0026 tsc -p electron \u0026\u0026 electron-builder\", \"electron:dist\": \"yarn build \u0026\u0026 tsc -p electron \u0026\u0026 electron-builder --mac --dir\" } These scripts will allow you to run the React app in development mode, build the app for production, and package the Electron app for distribution.\nStep 8: Run the application You’re now ready to run your React-Electron application in development mode. In the terminal, run:\nnpm run electron:dev This command will start the React development server, compile the Electron main process, and launch the Electron app. You should see a new window with your “Hello World!” message displayed.\n","description":"How to Create a React-Electron Typescript Application From Scratch in 2023","title":"How to Create a React-Electron Application From Scratch","uri":"/en/posts/howto-create-react-electron-app-ts/"},{"content":"How to Create Interactive Financial Charts using Tkinter and Plotly Data visualization is an integral part of data analysis. Python, with its strong set of libraries, has emerged as a go-to language for data visualization. In this tutorial, we will create an application that generates and displays interactive financial charts using Tkinter and Plotly.\nPrerequisites Before we get started, ensure you have the following installed on your machine:\nPython 3.6 or newer Dash, Plotly’s Python framework for building analytical web applications. Tkinter, Python’s standard GUI package. You can install Dash using pip:\npip install dash plotly Step 1: Python Script Imports import tkinter as tk import threading import webbrowser import random import plotly.graph_objs as go import dash_html_components as html from dash import Dash, dcc from dash.dependencies import Output, Input Step 2: Creating a Dash Application Thread We’ll be running the Dash application in a separate thread. This allows the Tkinter GUI and Dash app to run simultaneously. To do this, we create a new DashThread class that inherits from Python’s threading.Thread:\nclass DashThread(threading.Thread): def __init__(self, data_list): threading.Thread.__init__(self) self.data_list = data_list self.app = Dash(__name__) # Initialize an empty graph self.app.layout = html.Div( [ dcc.Graph(id=\"live-graph\", animate=True), dcc.Interval( id=\"graph-update\", interval=1 * 1000, ), ] ) @self.app.callback( Output(\"live-graph\", \"figure\"), [Input(\"graph-update\", \"n_intervals\")] ) def update_graph(n): data = [ go.Scatter( x=list(range(len(self.data_list[symbol]))), y=self.data_list[symbol], mode=\"lines+markers\", name=symbol, ) for symbol in self.data_list.keys() ] fig = go.Figure(data=data) # Update x-axis range to show last 120 data points fig.update_xaxes(range=[max(0, n - 120), n]) return fig def run(self): self.app.run_server(debug=False) Step 3: Creating the Main Application Class The App class will initialize the Tkinter window and generate random prices for each of the financial symbols:\nclass App: def __init__(self, root): self.root = root self.data_list = {\"ETHUSDT\": [], \"BTCUSD\": [], \"BNBUSDT\": []} # Start the Dash application in a separate thread dash_thread = DashThread(self.data_list) dash_thread.start() # Open Dash app in web browser webbrowser.open(\"http://localhost:8050\") # Start the price generation in tkinter after Dash app is launched self.root.after(1000, self.generate_prices) def generate_prices(self): for symbol in self.data_list.keys(): new_price = random.randint(1, 100) # Generate random price self.data_list[symbol].append(new_price) # Store the price in list # Schedule the function to run again after 1 second self.root.after(1000, self.generate_prices) Step 4: Running the Application Finally, we create a Tkinter root window, instantiate the App class, and start the Tkinter event loop:\nif __name__ == \"__main__\": root = tk.Tk() app = App(root) root.mainloop() Conclusion You’ve successfully created an application that generates and displays interactive financial charts using Tkinter and Plotly. This project can be extended with real-time data feeds and additional interactive features to fit your needs. Happy coding!\n","description":"This tutorial guides you step-by-step to create an application with interactive financial charts using Tkinter and Plotly.","title":"How to Create Interactive Financial Charts using Tkinter and Plotly","uri":"/en/posts/howto-tkinter-interactive-plotly-chart/"},{"content":"Red Hat Enterprise Linux 9 (RHEL 9), codenamed Plow, has gone public (GA). Red Hat announced it on May 18, 2022. It replaced the beta version, which had been in existence since November 3, 2021.\nRHEL 9 is the first few releases in the Red Hat family. It is the first major release since IBM acquired Red Hat in July 2019, and the first major release since abandoning the CentOS project in favor of CentOS Stream, which is now RHEL’s predecessor.\nRHEL 9 is the latest major version of RHEL and comes with a 5.14 kernel, lots of new software packages and a host of improvements. It emphasizes security, stability, flexibility and reliability.\nDescription RHEL 9 ships with new versions of software including Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0, and many more.\nPreparing for installation Registration on the Red Hat portal Red Hat Developer Subscription is a free Red Hat Developer Program offer designed for individual developers who want to take full advantage of Red Hat Enterprise Linux.\nIt gives developers access to all versions of Red Hat Enterprise Linux, as well as other Red Hat products such as add-ons, software updates and security bugs.\nFirst of all, make sure you have an active Red Hat account. If you don’t already have an account, go to the Red Hat Customer Portal, click on “Register” and fill out your information to create a Red Hat account. Downloading the installation image After creating a Red Hat account, you can start downloading RHEL 9. To download Red Hat Enterprise Linux 9 absolutely free, go to Red Hat Developer Portal and log in with your account credentials. Then go to the download RHEL 9 page and click on the download button shown below.\nI’m using a MacBook M1, so I download the RHEL 9 image for the M1 processor aarch64 Virtual machine I use the free UTM virtual machine as a virtual machine to install RHEL 9. You can install using Homebrew by running the command brew install --cask utm.\nInstalling Red Hat Enterprise Linux 9 Setting up the UTM virtual machine In UTM click Create a New Virtual Machine -\u003e Virtualize Choose the downloaded RHEL 9 image and click Continue. Main Setup Menu The marked fields need to be filled in\nCreate Root Password User Creation. Create the user you want to log in with. Connect to Red Hat. Here we will use the account created above.\nHere you will enter your account data and click Register. Press Done\nUnder Installation Destination choose your default drive.\nWe can now continue with the installation. A Begin installation button will appear on the main screen\nAfter installation is complete, we will have to reboot the system. Sometimes rebooting will unload the installation image again. It’s necessary to either disable the disk in the installer setup or reboot the UTM.\nRunning Red Hat Enterprise Linux 9 Enter your password and see the RHEL 9 desktop To access the applications, click the Activities button in the upper left corner\nConfiguring Red Hat Enterprise Linux 9 Checking the ROOT user In a Linux system users belong to different groups which have certain rights. If during the installation process we did not check the checkbox to make the user an administrator, by default he will not be able to install some system programs.\nExit and log in as root (the same user we created earlier on the main screen). Press Log out Now log in as root. The user may not be listed. Press Not listed and enter the account data. Open terminal and check Configuring system settings Button to minimize the application The first thing that seems unusual about using the GUI is that there are no buttons to minimize windows Install the necessary package\nyum install gnome-tweaks -y After installation, the Tweaks application will appear. Find it by searching.\nThere are many other tweaks in the app as well. We will show you the minimize buttons for the applications.\nLet’s go to Windows titlebars and set the Maximize, Minimize options\nUser access to install applications To avoid constantly switching to a root user to install applications, we can give the normal user access to install applications. We will continue to do this as root. Open /etc/sudoers and add the user\nsudo vi /etc/sudoers Add user data to the end of the file. My user name: rhel-user\nrhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf Let’s install Visual Studio Code as a normal user Installation consists of the following steps:\nadding the desired repository. Rights to add the repository (changing the files in the directory is still only for root user) Downloading and installing. First step is done as root user Go to https://code.visualstudio.com/docs/setup/linux\nCopy the code and run it in the terminal\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" \u003e /etc/yum.repos.d/vscode.repo' Switch to user rhel-user. This can also be done in the terminal. Updating the repositories Install VSCode su rhel-user dnf check-update sudo dnf install code References https://developers.redhat.com/products/rhel/getting-started https://www.redhat.com/sysadmin/install-linux-rhel-9 ","description":"How to Download and Install Linux RHEL 9 for Free","title":"How to Download and Install Linux RHEL 9 for Free","uri":"/en/posts/howto-install-rhel-9-free/"},{"content":"In this post, we’ll dive deeper into the process of creating, structuring, and publishing a JavaScript package to the npm registry. We’ll use the repository https://github.com/romankurnovskii/npm-js-package-template as a reference for this guide.\nTLDR Open template: https://github.com/romankurnovskii/npm-js-package-template Click Use this template Create repo from this template. git clone new repo update code src/index.ts, src/bin.ts How to check if bin script works from terminal:\nnpm link \u0026\u0026 npm link mypackage mypackage Publish:\nnpm run prepublishOnly npm publish Folder Structure Let’s go through the folder structure of the repository:\n.eslintignore .eslintrc.json .gitattributes .gitignore .npmrc .prettierrc ├── .github │ ├── dependabot.yml │ └── workflows │ ├── create-release.yml │ └── test-environments.yml ├── CHANGELOG.md ├── LICENSE ├── README.md ├── jestconfig.json ├── package.json ├── src | ├── bin.js | └── index.js └── tests └── index.test.js File Descriptions .github/dependabot.yml: This file configures Dependabot for your project. Dependabot will automatically create pull requests to update your npm dependencies and GitHub Actions workflows as specified in the file. .github/workflows/create-release.yml: This GitHub Actions workflow is triggered when you push to the main branch or create a release. It builds, tests, and releases your package. The workflow is configured to create a release with the version specified in package.json. .github/workflows/test-environments.yml: This GitHub Actions workflow is triggered when you push to the main branch or create a pull request. It sets up a matrix of Node.js versions (14, 16, 18) and runs the build and test steps for each version. This ensures your package works correctly across different Node.js environments. .eslintignore: Specifies the files and directories that should be ignored by ESLint. .eslintrc.json: Configuration file for ESLint, a popular linting tool for JavaScript. .gitattributes: Specifies the attributes for paths in the repository. .gitignore: Specifies the files and directories that should be ignored by Git. .npmrc: Configuration file for npm, the package manager for JavaScript. .prettierrc: Configuration file for Prettier, a popular code formatter for JavaScript. CHANGELOG.md: A file that lists the notable changes made to the project. LICENSE: The license file for the project, in this case, the MIT License. README.md: The main documentation file for the project. jestconfig.json: Configuration file for Jest, a popular testing framework for JavaScript. package.json: Lists the project’s metadata, dependencies, and scripts. src/bin.js: The main entry point for the command line interface (CLI) of the package. src/index.js: The main entry point for the package when imported as a module. tests/index.test.js: The test file for the package, containing test cases for the package’s functions. Publish to NPM Before you publish the package, make sure you have followed these steps:\nUpdate the package.json file with the correct name, version, description, and other metadata.\nEnsure your code is in the src folder, and the main entry points are src/index.js (for modules) and src/bin.js (for CLI).\nUpdate the test cases in the tests folder.\nAfter making all the necessary updates to your package, such as updating the README.md, package.json, and other files as needed, ensure that you’ve built the package by running:\nnpm run build Before publishing, you should test your package and ensure that all tests pass: npm test Also, make sure your code is properly formatted and follows the linting rules: npm run prettier npm run lint If there are any linting issues, you can try to automatically fix them using: npm run lint:fix Once everything is set up and ready, you can publish your package to the npm registry by running: npm run prepublishOnly npm publish This will build, test, and format your code, ensuring that your package is ready for publishing. Once published, your package will be available for others to install and use through the npm registry. ","description":"A detailed guide on how to create, structure, and publish a JavaScript package to the npm registry","title":"How to publish JavaScript package to npm registry","uri":"/en/posts/howto-publish-js-npm-project/"},{"content":" Open template: https://github.com/romankurnovskii/npm-typescript-package-template Click Use this template Create repo from this template. git clone new repo update code src/index.ts, src/bin.ts How to check if bin script works from terminal:\nnpm link \u0026\u0026 npm link mypackage mypackage Publish:\nnpm run prepublishOnly npm publish ","description":"How to publish typescript package to npm registry","title":"How to publish typescript package to npm registry","uri":"/en/posts/howto-publish-ts-npm-project/"},{"content":"Learn different ways to rename files in Python using the os and pathlib modules.\nos.rename Rename files with os\nYou can use\nos.rename(old_name, new_name) For example we can combine it with os.path.splitext() to get the base name and file extension, and then combine it to a new name:\nimport os for file in os.listdir(): name, ext = os.path.splitext(file) new_name = f\"{name}_new{ext}\" os.rename(file, new_name) pathlib Rename files with pathlib\nThe same could be achieved with the pathlib module and\nPath.rename(new_name) With a Path object we can access .stem and .suffix:\nfrom pathlib import Path for file in os.listdir(): f = Path(file) new_name = f\"{f.stem}_new{f.suffix}\" f.rename(new_name) shutil.move The shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module.\nimport shutil old_source = '/Users/r/Desktop/old_source.txt' new_source = '/Users/r/Desktop/new_source.txt' newFileName = shutil.move(old_source, new_source) print(\"New file:\", newFileName) # New file: /Users/r/Desktop/new_source.txt ","description":"How to rename files in Python","title":"How to rename files in Python","uri":"/en/posts/howto-rename-files-in-python/"},{"content":"Jupyter notebooks are a fantastic tool for data scientists and programmers, allowing you to write code, visualize results, and write documentation all in one place. Hugo is a powerful and flexible static site generator that’s great for blogging and building websites. But what if you want to share your Jupyter notebooks on your Hugo website?\nThis step-by-step guide will walk you through how to render Jupyter notebooks in Hugo using a custom shortcode and automate the process using GitHub Actions.\nPrerequisites Before you start, make sure you have:\nA Hugo website up and running. Jupyter installed on your machine. A GitHub account. Basic knowledge of Hugo, Jupyter, and GitHub Actions. Step 1: Create Your Jupyter Notebook Create a new Jupyter notebook in the static_files/jupyter/ directory of your Hugo website. Write your code and generate the plots or data visualizations you want to include.\nStep 2: Install nbconvert nbconvert is a Python library that allows you to convert Jupyter notebooks to other formats, including HTML, which we’ll need for this guide. Install it with pip:\npip install nbconvert Step 3: Convert cuaderno to HTML # Replace your_notebook.ipynb with the name of your notebook. jupyter nbconvert --to html static_files/jupyter/your_notebook.ipynb Step 4: Create a Custom Shortcode in Hugo Updated version of shortcode you can find in github.\nFirst, create a new file named notebook.html in your theme’s layouts/shortcodes/ directory. Next, copy the code into notebook.html.\nStep 5: Use the notebook Shortcode in Your Page # remove space between “{{” and “}}” { {\u003c notebook \"jupyter/your_notebook\" 1200 \u003e} } Replace your_notebook with the name of your notebook (without the .html extension), and 1200 with the desired height of the iframe in pixels.\nStep 6: Automate the Process with GitHub Actions GitHub Actions can automatically convert your Jupyter notebooks to HTML whenever you push changes to your repository.\nYou can find a sample GitHub Actions workflow in the notebook shortcode’s repository. Copy this workflow into a new file in your repository’s .github/workflows/ directory, and commit and push the changes.\n","description":"Step-by-step guide to rendering Jupyter notebooks in Hugo using a custom shortcode and GitHub Actions.","title":"How to Render Jupyter Notebooks in Hugo with a Custom Shortcode","uri":"/en/posts/howto-render-notebook-in-hugo/"},{"content":"Step 1: Create a SourceForge account Go to the SourceForge website at https://sourceforge.net/ Click on the “Join” button in the top-right corner. Fill in the required fields, such as username, email, and password, then click “Register” You’ll receive a confirmation email from SourceForge. Click on the link provided to confirm your account. Step 2: Start a new project Log in to your SourceForge account. Click on the “Create” button in the top-right corner of the page. Select “Create Your Project Now” from the dropdown menu. Step 3: Configure your project Enter a unique name for your project in the “Project Name” field. This name will also serve as your project’s URL. Provide a Phone number if required. You will get a pin for verification. Next you will see a quick tour that could help you to fulfill all the required fields. Provide a brief description of your project in the “Short Summary” field.\nUpload Project Logo\nClick “Save”.\nFill other fields from left sidebar menu.\nChoose an appropriate “License” for your open-source project from the dropdown menu. If you’re unsure which license to choose, you can refer to the Open Source Initiative’s list of approved licenses (https://opensource.org/licenses). Select the “Programming Language” and “Operating System” that your project is built for. Add any relevant “Tags” to help users find your project. Click the “Create” button at the bottom of the form to create your project. Step 4: Configure your project’s Source Control Management (SCM) SourceForge supports several SCM options, including Git, Mercurial, and Subversion. Choose the one that best suits your needs.\nGo to your project’s main page. Click on the “Buttons \u0026 Badges” link in the left side bar. Click “GitHub Integration”. Enter your GitHub username/repo. Click “Set up” or choose “Set up integration manually”. Step 5: Set up release files (optional) If you want to provide compiled binaries or other release files for users to download, follow these steps:\nGo to your project’s main page. Click on the “Files” tab in the top navigation bar. Click on the “Add Folder” button to create a new folder for your release files (e.g., “v1.0”). Click on the newly created folder and then click the “Upload” button. Select the release files you want to upload, and click “Open” to start the upload process. Summary Now we have a project page on SourceForge.\nHere is a button for download:\n[![Download BrewMate](https://a.fsdn.com/con/app/sf-download-button)](https://sourceforge.net/projects/brewmate/files/latest/download) Project url: https://sourceforge.net/projects/brewmate/\n","description":"Step-by-Step Guide how to upload and manage your open-source application on SourceForge with this detailed, step-by-step guide, ensuring maximum visibility and accessibility for your project.","title":"How to upload an opensource application to SourceForge","uri":"/en/posts/how-to-upload-app-to-sourceforge/"},{"content":"Introduction Hugo by default uses parsing of markdown files. This means that we get the html code as it is written in markdown.\nIn order to understand which images we can enhance, we add a separate tag/key/id to those images\nTools To implement the functionality, we need to:\nWrite/connect a script/handler that will perform the zoomin effect on the images we need Add the necessary metadata to the images, so the script can find them zoomin script To add the ability to zoom on click, we will use the medium-zoom package.\nThis package provides this functionality in a non-loaded, handy style.\nDemo\nScript logic The script finds images with id and so understands to apply the zoomin property to those images\nPossible id:\nzoom-default zoom-margin zoom-background zoom-scrollOffset zoom-trigger zoom-detach zoom-center Connecting the scripts In order for the script to work, we need to connect the logic as well as the handler.\nHugo has a static folder in the root of the project, which can be used to store static files (styles, scripts) and used to connect them to the site. If there is no such folder, you can create one.\nIn the static folder create a folder zoom-image and add two scripts to it\nstatic/js/zoom-image/index.js const zoomDefault = mediumZoom('#zoom-default') const zoomMargin = mediumZoom('#zoom-margin', { margin: 48 }) const zoomBackground = mediumZoom('#zoom-background', { background: '#212530' }) const zoomScrollOffset = mediumZoom('#zoom-scrollOffset', { scrollOffset: 0, background: 'rgba(25, 18, 25, .9)', }) // Trigger the zoom when the button is clicked const zoomToTrigger = mediumZoom('#zoom-trigger') const button = document.querySelector('#button-trigger') button.addEventListener('click', () =\u003e zoomToTrigger.open()) // Detach the zoom after having been zoomed once const zoomToDetach = mediumZoom('#zoom-detach') zoomToDetach.on('closed', () =\u003e zoomToDetach.detach()) // Observe zooms to write the history const observedZooms = [ zoomDefault, zoomMargin, zoomBackground, zoomScrollOffset, zoomToTrigger, zoomToDetach, ] // Log all interactions in the history const history = document.querySelector('#history') observedZooms.forEach(zoom =\u003e { zoom.on('open', event =\u003e { const time = new Date().toLocaleTimeString() history.innerHTML += `\u003cli\u003eImage \"\u003cem\u003e${event.target.alt }\u003c/em\u003e\" was zoomed at ${time}\u003c/li\u003e` }) zoom.on('detach', event =\u003e { const time = new Date().toLocaleTimeString() history.innerHTML += `\u003cli\u003eImage \u003cem\u003e\"${event.target.alt }\"\u003c/em\u003e was detached at ${time}\u003c/li\u003e` }) }) static/js/zoom-image/placeholders.js // Show placeholders for paragraphs const paragraphs = [].slice.call(document.querySelectorAll('p.placeholder')) paragraphs.forEach(paragraph =\u003e { // eslint-disable-next-line no-param-reassign paragraph.innerHTML = paragraph.textContent .split(' ') .filter(text =\u003e text.length \u003e 4) .map(text =\u003e `\u003cspan class=\"placeholder__word\"\u003e${text}\u003c/span\u003e`) .join(' ') }) CDN script You can download the script, or you can upload it\nScript Link\nAdding to template In order for these scripts to work in the website template, they must be connected.\nI use for this the template baseof.html. I simply add links to the scripts in body of the template.\n# baseof.html ... \u003c/footer\u003e \u003cscript src=\"https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js\" defer\u003e\u003c/script\u003e \u003cscript src=\"/js/zoom-image/placeholders.js\" defer\u003e\u003c/script\u003e \u003cscript src=\"/js/zoom-image/index.js\" defer\u003e\u003c/script\u003e \u003c/body\u003e \u003c/html\u003e image ID Hugo allows you to change the parsing behavior of markdown files with hooks. You can read more about render-hooks at website.\nIn the *layouts folder.\nLet’s add the file render-image.html to the following path layouts -\u003e _default -\u003e _markup file code:\n\u003cp class=\"md__image\"\u003e \u003cimg src=\"{{ .Destination | safeURL }}\" id=\"zoom-default\" alt=\"{{ .Text }}\" {{ with .Title}} title=\"{{ . }}\" {{ end }} /\u003e \u003c/p\u003e We only added id=\"zoom-default\" to the default code\nResult Your browser does not support the video tag. Process ","description":"Script will zoom in on a picture on click in Hugo","title":"Hugo resize a picture on click","uri":"/en/posts/hugo-add-image-zoomin/"},{"content":"","description":"Create lunr index file for multilingual hugo static site","title":"hugo-lunr-ml","uri":"/en/apps/npm/hugo-lunr-ml/"},{"content":"About IAM - AWS Identity and Access Management\nAWS IAM AWS IAM User Guide AWS Identity and Access Management (IAM) allows to securely control user access to AWS services and resources.\nDesigned for organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon RDS, and AWS Management Console.\nWith IAM, you can centrally manage users, security credentials such as access keys, and permissions that control user access to AWS resources.\nThere are three ways IAM authenticates a principal:\nUser Name/Password Access Key Access Key/Session Token Digest IAM consists of the following: Users Groups Roles Policy Documents IAM is Global. It doesn’t apply to any specific region. There is no charge to use IAM. IAM is compliant with Payment Card Industry (PCI) Data Security Standard (DSS) The “root account” has complete Admin access. Don’t use “root account” for everyday use. Instead, create users. A new user will have NO permissions by default. Grant least privilege needed for their job. New user will be assigned with password, Access Key ID \u0026 Secret Access Keys. The password will be used to login to AWS management console. Access Key ID \u0026 Secret Access Key will be used to login via the APIs and CLI Always setup MFA on your root account. Use Groups to assign permissions to IAM users Use Roles to Delegate permissions. Role is more secure than creating individual user. Roles gives temporary credentials for access; whereas User has long term credentials. Create and customize password rotation policies Policies can be attached to users, groups and roles. Use AWS defined policies, assign permissions wherever possible. Policy is defined in JSON format and contains version, statements, - effect, action, resource, principal, and condition. STS Security Token Service provides temporary security credentials to the trusted users. STS is global and there is no charge to use it. Digest: https://tutorialsdojo.com/aws-identity-and-access-management-iam/ IAM best practices - Question might ask you to identify best practices among the given choices. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Difference between when to use Role and User. IAM Policy Simulator - service for testing and troubleshooting IAM Policies. Details Practice Go to IAM page\nCreating IAM groups On the User Groups page, click Create group\nSpecify the name of the group. Mine is: DevOps. Add permission to view EC2: AmazonEC2ReadOnlyAccess. create The group was created\nCreating IAM users On the Users page, click Create user Type in user name (login) Permissions Add user to the group Tags Skip section or put tags. It is useful and popular to set tags for resources in companies with a lot of connected AWS resources\nLogin/Password At the last step, download the .csv file with login, keys and password. You will need the password later to log in as this user. On this page there is a link to log in. We will use it in the next step Logging in as a new user Checking privileges This user has access to view EC2 instances. Let’s check whether or not the S3 garbage cans have access.\nLet’s try to create an S3 bucket After trying to create a recycle bucket, we get a window indicating no permissions Questions Q1 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\nWhat are the best practice guidelines for use of the root account?\nNever use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1\nQ2 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\nAssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1\nQ3 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\nWhich of the following APIs will be leveraged to provide temporary security credentials?\nAssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\nAssumeRoleWithWebIdentity - does not support MFA\n3\nQ4 A leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC.\nThe access should allow the users to create and configure various AWS resources, such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user’s organization, including the master account email and organization limitations.\nWhich of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?\nAttach the PowerUserAccess AWS managed policy to the IAM users. Attach the AdministratorAccess AWS managed policy to the IAM users. Create a new IAM role and attach the SystemAdministrator AWS managed policy to it. Assign the IAM Role to the IAM users. Create a new IAM role and attach the AdministratorAccess AWS managed policy to it. Assign the IAM Role to the IAM users. Explanation AWS managed policies for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function.\nThese policies consolidate permissions for many services into a single policy that’s easier to work with than having permissions scattered across many policies.\nFor Developer Power Users, you can use the AWS managed policy name: PowerUserAccess if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development.\nThe first statement of this policy uses the NotAction element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role.\nThis is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user’s organization, including the master account email and organization limitations.\n1\nQ5 A company has 100 AWS accounts that are consolidated using AWS Organizations. The accountants from the finance department log in as IAM users in the TD-Finance AWS account. The finance team members need to read the consolidated billing information in the TD-Master AWS master account that pays the charges of all the member (linked) accounts. The required IAM access to the AWS billing services has already been provisioned in the master account.\nThe Security Officer should ensure that the finance team must not be able to view any other resources in the master account.\nWhich of the following grants the finance team the necessary permissions for the above requirement?\nSet up an IAM group for the finance users in the TD-Finance account then attach a ViewBilling permission and AWS managed ReadOnlyAccess IAM policy to the group. Set up individual IAM users for the finance users in the TD-Master account then attach the AWS managed ReadOnlyAccess IAM policy to the group with cross-account access. Set up an AWS IAM role in the TD-Finance account with the ViewBilling permission then grant the finance users in the TD-Master account the permission to assume that role. Set up an IAM role in the TD-Master account with the ViewBilling permission then grant the finance users in the TD-Finance account the permission to assume the role. Explanation You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Internet Services Pvt. Ltd (AISPL) accounts. Every organization in AWS Organizations has a master (payer) account that pays the charges of all the member (linked) accounts.\nModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyBilling – Allow or deny IAM users permission to modify billing settings. ModifyPaymentMethods – Allow or deny IAM users permission to modify payment methods. ViewAccount – Allow or deny IAM users permission to view account settings. ViewBilling – Allow or deny IAM users permission to view billing pages in the console. ViewPaymentMethods – Allow or deny IAM users permission to view payment methods. ViewUsage – Allow or deny IAM users permission to view AWS usage reports. Use policies to grant permissions to perform an operation in AWS. When you use an action in a policy, you usually allow or deny access to the API operation or CLI command with the same name. However, in some cases, a single action controls access to more than one operation.\n4\nResources Security best practices in IAM IAM Hands-On Lab IAM Workshops Security workshop tutorialsdojo digest Community posts https://dev.to/romankurnovskii/aws-iam-cheet-sheet-3if4 ","description":"A step-by-step guide to setting up AWS Identity and Access Management (IAM)","title":"IAM","uri":"/en/tracks/aws-certified-developer-associate/iam/"},{"content":" My image float right, tall Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nMy image float right Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nMy image text Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\nMy image float left Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n","description":"Insert resizable image in post","title":"img","uri":"/en/posts/hugo-shortcode-examples/img/"},{"content":"Lab Initializing Amazon EC2 Instances with AWS CloudFormation Init Establishing Desired EC2 Instance State with AWS CloudFormation Init 1. In the AWS Console search bar, search for cloudformation and click the CloudFormation result under Services:\n2. Click the Create stack dropdown menu and select With new resources:\n3. In the Create stack form, in the Specify template section, ensure Amazon S3 URLis selected for the Template source.\n4. Paste in the following URL in the Amazon S3 URL field:\nAWSTemplateFormatVersion: '2010-09-09' Description: Provision a Single Amazon EC2 Instance with CFN Helper Scripts Parameters: AmiID: Description: The ID of the AMI. Type: AWS::SSM::Parameter::Value\u003cAWS::EC2::Image::Id\u003e Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 Resources: WebServer: Type: AWS::EC2::Instance Properties: ImageId: !Ref AmiID InstanceType: t3.micro SecurityGroupIds: - !Ref WebServerSecurityGroup UserData: # Update aws-cfn-bootstrap # Run cfn-init to initialize WebServer content # Return cfn-init run result to CloudFormation upon completion Fn::Base64: !Sub | #!/bin/bash -xe yum update -y aws-cfn-bootstrap /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region} /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region} CreationPolicy: ResourceSignal: Count: 1 Timeout: PT5M Metadata: AWS::CloudFormation::Init: config: packages: yum: httpd: [] files: \"/var/www/html/index.html\": content: | \u003ccenter\u003e \u003ch1\u003eCloud Academy EC2 Instance\u003c/h1\u003e \u003ch3\u003eThis content has been initialized with \u003ca href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html\" target=\"_blank\"\u003eAWS CloudFormation Helper Scripts\u003c/a\u003e\u003c/h3\u003e \u003c/center\u003e mode: '000644' services: sysvinit: httpd: enabled: 'true' ensureRunning: 'true' WebServerSecurityGroup: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: SSH and HTTP SecurityGroupIngress: - CidrIp: 0.0.0.0/0 FromPort: 22 IpProtocol: tcp ToPort: 22 - CidrIp: 0.0.0.0/0 FromPort: 80 IpProtocol: tcp ToPort: 80 Outputs: WebServerPublicDNS: Description: Public DNS of EC2 instance Value: !GetAtt WebServer.PublicDnsName The CloudFormation stack template is stored in a public S3 bucket. The EC2 instance resource definition is shown below:\nThe WebServer EC2 instance is defined above. It is a size t3.micro instance that references a WebServerSecurityGroup resource for its security group and the AmiID parameter for its image ID. Both of these referenced configurations are defined elsewhere in the template.\nThe UserData script defined next performs the following tasks once the EC2 instance is created:\nUpdates the aws-cfn-bootstrappackage to retrieve the latest version of the helper scripts Runs the cfn-init helper script to execute the WebServer instance Metadata scripts Runs the cfn-signalhelper script to notify CloudFormation after all the service(s) (Apache in this case) is installed and configured on the EC2 instance Note: The cfn-init helper script is not executed automatically. You must run the cfn-init script within the EC2 instance UserData in order to execute your metadata scripts.\nThe cfn-signal helper script works hand-in-hand with the CreationPolicy configuration. The ResourceSignal property has a Count of 1 and a Timeout of PT5M. This instructs CloudFormation to wait for up to 5 minutes to receive 1 resource signal from the EC2 instance.\nThe cfn-signal helper script call in the UserData uses $? to retrieve the return code of the previous script. If the cfn-init script is successful and the EC2 instance is configured properly, cfn-signal returns a success to CloudFormation which then transitions the EC2 instance to the CREATE_COMPLETEstatus. If the cfn-init script is unsuccessful or the timeout of 5 minutes expires before receiving a signal, then the EC2 instance will be transitioned to a CREATE_FAILEDstatus and the stack deployment will fail.\nThe EC2 instance Metadata configuration is the same as the previous lab step. It defines a AWS::CloudFormation::Init script to install the httpd package using yum, generate an index.html file within /var/www/html/ and start the httpd service to serve the content from the EC2 instance.\n5. Click Nextto continue:\n6. Enter web-server-stack for the Stack name and click Next:\n7. You will not be configuring additional stack options. Scroll to the bottom of the page and click Next.\n8. On the review page, scroll to the bottom and click Create stackto deploy your stack:\nYour stack will begin deploying and you will be brought to the Events page of your web-server-stack:\nThe stack can take up to 3 minutes to deploy successfully.\n9. If the Events section does not automatically refresh after 3 minutes, click the refresh icon:\nThe WebServer instance remains in a CREATE_IN_PROGRESS status until CloudFormation receives a SUCCESS signal from the instance. In the screenshot above, the UniqueId of i-0fd18c8deb52983d5 belongs to the WebServer instance.\nAfter the success signal is received, the WebServer instance is transitioned into the CREATE_COMPLETE status.\nWithout the CloudFormation signal helper script, CloudFormation would have transitioned the EC2 instance to a completed status when the resource was created instead of waiting until the Apache service has been installed and running on the instance.\n10. Click the Outputs tab on the web-server-stack page:\n11. Right-click and open the WebServerPublicDNS URL in a new browser tab:\nThe HTMLpage generated in the cfn-init script is now being served from the Apache server running within your WebServer EC2 instance:\n","description":"Initializing Amazon EC2 Instances with AWS CloudFormation Init","title":"Initializing Amazon EC2 Instances with AWS CloudFormation Init","uri":"/en/tracks/aws-certified-developer-associate/cloudformation/initializing-ec2-with-cloudformation/"},{"content":"Lab Introduction to CloudWatch Explore CloudWatch 1. AWS has done an excellent job defining CloudWatch key concepts. Read the abbreviated excerpt from their official documentation below to obtain an understanding of Metrics, Namespaces and Alarms:\nMetrics\nA metric is the fundamental concept in CloudWatch and represents a time-ordered set of data points. These data points can be either your custom metrics or metrics from other services in AWS. You or AWS products publish metric data points into CloudWatch and you retrieve statistics about those data points as an ordered set of time-series data. Metrics exist only in the region in which they are created.\nThink of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another.\nNamespaces\nCloudWatch namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\nNote: In this lab you will see namespaces that AWS has created for you, and a custom namespace created by the steps performed in this lab.\nAlarms\nYou can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.\nAlarms invoke actions for sustained state changes only. CloudWatch alarms will not invoke actions simply because they are in a particular state. The state must have changed and been maintained for a specified number of periods.\nThe interested student can take a look at the full version of the documentation here. Due to time constraints, you should look at additional documentation once you have completed the lab.\n2. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n3. Click Metrics \u003e All metrics in the left navigation pane. At this point, there are most likely no custom namespaces. But several AWS namespaces may already be established for you. What metrics are listed on the All metricstab depends on a couple of factors:\nHow quickly you arrived at this view after starting your lab. This lab creates an EC2 instance and EBS volume when you start the lab. After a couple of minutes of delay, metrics for the EC2 and EBS namespaces are included. How recently your Cloud Academy AWS account has been used to complete other Cloud Academy labs. If the AWS account you logged in to recently completed other labs, you may see namespace related to metrics collected in those labs. 4. Spend a few minutes to explore what metrics and namespaces look like in the CloudWatch console. Simply select any namespace and then any particular metric. As an example, the EC2 namespace and CPUUtilizationmetric for the HighCPUInstance are selected in the image below:\nNote: The image above is for illustrative purposes only, you do not need to choose the same instance or metric to explore CloudWatch metrics.\nThe longer the instance has been running, the more data points will appear in the graph. By default, EC2 metrics are collected every five minutes. You may need to adjust the displayed timeline to 1 week (1w) or further in the past to see some metrics.\nMonitoring EC2 Instances 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. Click Instancesfrom the navigation pane and select the box near the instance name. A wealth of instance information is displayed in the Details tab:\nWhen you started the Lab, Cloud Academy configured the lab environment for you. This includes a medium instance named HighCPUInstance.\nNote: Your information will vary. There is additional instance information not shown in the example above.\n3. Switch to the Monitoring tab and take a look at the standard metrics:\nNote: If you don’t see an instance yet, it’s possible that it’s still provisioning in the background. Refresh the page every minute or so until it appears.\nThese are the standard metrics that CloudWatch monitors for all your EC2 instances. Please refer to the documentation for details. (Due to possible time constraints, please look up additional information in the documentation after completing this lab.)\nYou should be aware that all the metrics in this tab related to Disk (Disk Reads, Disk Read Operations, Disk Writes, Disk Write Operations) pertain to ephemeral storage disks. Those metrics will not represent anything if you have launched an EBS backed instance. To see the metrics related to EBS volumes you need to look elsewhere. Next you will take a look at the metrics of the EBS volume for this particular instance.\nNote: Ephemeral storage is also known as instance storage. It is temporary storage that is added to your instance, unlike EBS which is an attached volume that is permanent in nature.\n4. To enable and disable detailed monitoring, click Manage detailed monitoring:\nThe Detailed monitoring page will open :\nHere you can enable and disable detailed monitoring by checking or unchecking the Enable checkbox followed by clicking Save.\n5. Click Cancel as we will not be enabling detailed monitoring in this lab:\n6. Reselect the HighCPUInstance , click the Storage tab. Scroll down and click on the Volume Id (lower right):\n7. Select the volume and click on the Monitoring tab to see the metrics for this EBS volume:\nAs you can see, Amazon does quite a bit out of the box with respect to monitoring EC2 Instances and EBS volumes. However, you can enable Detailed Monitoring for even more control over the monitoring frequency of EC2 instances. CloudWatch monitors EC2 instances every 5 minutes by default. If you need more frequent monitoring, you can enable CloudWatch’s Detailed Monitoring feature to monitor your instances every minute. You can enable Detailed Monitoring during the instance launch or change it anytime afterwards. Note: Detailed Monitoring does come with an associated cost.\nInstall the EC2 Monitoring Scripts 1. Navigate to EC2 Instances by clicking here.\n2. Click on Launch instances:\n3. In the Application and OS Images section, select the Amazon Linux option under Quick Start:\n4. In the Instance Type section, you should not change any options. Simply make sure the default t2.microis selected:\n5. In the Key pair section, select the keypair:\nNote: Your keypair may differ from the screenshot. Reminder: The PEM or PPK formatted key pair can be downloaded directly from the Your lab data section of the Cloud Academy Lab page at any time.\n6. Scroll down and expand the Advanced details section. Under IAM instance profile, select the IAM role provided. It will have a name that looks similar to cloudacademylabs-EC2MonitoringRole-XXXXXXXXXX :\n7. Scroll down to Detailed CloudWatch monitoring and select Enable:\n8. Scroll down to User data and copy and paste the following bash script code in the User data (As text) field:\nThis is where the magic happens. Next you will insert the code to execute during the instance launch. However, in order to send metrics to CloudWatch, you need to configure some credentials first. You can use either Access Keys or IAM roles for this task. In this Lab, you will follow the best practices and use IAM roles. There is an instance role already created in you account configured with the proper permissions.\nCopy code\n!/bin/bash yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 wget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip unzip CloudWatchMonitoringScripts-1.2.2.zip rm CloudWatchMonitoringScripts-1.2.2.zip echo “/1 ** * /aws-scripts-mon/mon-put-instance-data.pl –mem-util –disk-space-util –disk-path=/ –from-cron” \u003e monitoring.txt crontab monitoring.txt rm monitoring.txt\nThis bash script will get executed the first time the instances launches. In summary, the script will:\nInstall Perl libraries Retrieve and install the AWS CloudWatch Monitoring scripts Configure crontab to run the monitoring script every minute 9. In the Summary section, click Launch instance:\nA confirmation page will let you know that your instance is launching:\n10. Click View all instances.\nNotice the Name for the new instance is blank by default. Although not mandatory, it is helpful to have a name. Move your mouse into the blank space in the Name column. It turns to an edit pencil. Use the pencil to change your Instance Name to Monitoring Scripts:\nWait until theInstance State is R****unningfor the new Instance. It typically takes less than one minute for the state to transition fromP****ending to R****unning.\n11. Navigate back to CloudWatch by clicking hereand clickAll metricsfrom the navigation pane. Notice that there is a new namespace called System/LinuxunderCustom namespaces:\nThis name is configured when you send the custom metrics.\nNote: If you don’t see the new Namespace wait a few minutes and refresh the page. CloudWatch takes some time to display the information in the dashboard. Recall that the newly installed monitoring scripts send data every minute based on the crontab configuration setup in the User data bash script for the instance.\n12. Click on the new System/Linux namespace:\nThere are two metrics being monitored by CloudWatch in the custom System/Linux namespace. (Filesystem, InstanceId, MountPath and InstanceId)\n13. Click the metric on the left (Filesystem…), then select the checkbox so the first metric is graphed.\n14. Click Linux System, so the Metrics path is All \u003e Linux System again. Now select the metric on the right (InstanceId) and select its checkbox as well.\n15. Switch to the Graphed metrics tab. If you selected both metrics correctly the tab will include a “(2)” at the end of it indicating how many metrics are graphed. Your graph should look similar to the following:\nIt is simple to customize the display to meet your needs for the metrics displayed.\n16. Click the custom graph period drop-down above the graph display area and select 15from the Minutesrow:\n17. Select the Period drop-down column menu for each metric in the lower Graphed metrics tab and choose 1 Minute:\nYou can now see the highest resolution metrics that are being sent to CloudWatch every minute. (You may need to refresh the chart after setting the new periods)\n18. Select Maximum for the Statisticcolumn. Instead of an average of the datapoints, the maximum will be graphed. (Note: In the lab example it is probably the same since the disk really has not been touched) Your configuration should look like:\nCreating Your First CloudWatch Alarm 1. Navigate to CloudWatch by clicking here, click on Alarms\u003eAll Alarms in the left pane:\nThere are no Alarms configured, so there are no records found. Further, the three types of Alarms are all at zero (0).\nNote: More information on Alarm states will be covered soon.\n2. Click Create Alarm and click Select metric. Select the EC2 namespace:\nMany different metrics are displayed for both the HighCPUInstance and the Monitoring Scripts instances.\n3. Click Per-instance metrics, scroll down and select the metric with HighCPUInstanceunder Instance NameandCPUUtilizationunder Metric Name:\nTip: You may need to use the arrows in the upper right to find the HighCPUInstance on another page. Alternatively, you can make note of the last 3 or 4 characters in the InstanceId from the EC2 console, then enter those in the Search Metrics field. The search applies to all pages of information.\nOnce selected it is graphed immediately. Notice that you could tailor the graph to a specific Time Range (upper-right). For example, the time range can be specified in Relative or Absolute terms.\n4. Click Select metric when ready.\n5. Under Conditions, set the following values leaving the defaults for the rest:\n_Whenever High CPU is…:_Greater/Equal Than…: 50 An alarm watches for a metric to go beyond an allowable value range when monitored over time. If violated the alarm’s state is changed. There are three possibles states for an alarm:\nOK—The metric is within the defined threshold\nIn alarm—The metric is outside of the defined threshold\nInsufficient data—The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state\n6. Click Nextand fill out the form as described:\nAlarm state trigger: In alarm **Select an SNS topic:**Create new topic Insert a valid e-mail and click on Create topic.\n7. Click Next and fill the form as described before clicking Next:\nDefine a unique name: High CPU Alarm description: When CPU utilization \u003e= 50% Tip: Be sure to use your valid email address in the Email list field so you can verify the Alarm later. AWS Simple Notification Service (SNS) is used to send the email when the alarm is triggered. However, you will not need to configure anything in SNS.\n8. Click Create alarm when ready.\n9. Check for an email from AWS Notifications. Open up the email and click the Confirm subscription link:\nYou should receive a subscription confirmation. (For example, a confirmation message from Amazon Simple Notification Service (SNS) in a new browser tab if using a browser-based email client like Gmail.) To summarize, you have created a new alarm, along with a new SNS topic. Since you subscribed to this new SNS topic, every time the state of the alarm switches to ALARM you will receive an email. You may not receive an alarm email if the time it took to confirm the SNS topic subscription took longer than the time it took for the alarm to trigger. Emails will only be sent to subscribers at the time of the alarm transition.\n10. You should be put to the Alarm page:\nNote your Alarm state may differ. For example, you may be in an Insufficient datastate briefly and then transition either to In alarm or OK.\nTroubleshooting Tip: If the state of your alarm does not change to In alarm almost immediately, it is probably because you picked the incorrect instance. The HighCPUInstance is designed to trigger an alarm due to a high CPU utilization metric. The Monitoring Scripts instance is not taxed at all. To remedy the situation you can either create a new alarm with the correct instance, lower the threshold to something artificially low (1), or change the \u003e= to \u003c= (which is not very realistic but will test the alarm).\n11. Click the Alarm. You can see very useful information about the alarm itself. In the Details tab, there is a general overview of the alarm, and in the History tab you can see up to the last 50 states of the alarm:\n12. After an In alarmstate is raised, check for an email titled ALARM: “High CPU Alarm” in US West - Oregon from AWS Notifications.\nAgain, you may not have received an email because the alarm triggers before you had time to subscribe to the notification topic. Don’t worry, in the next Lab Step, you will reuse the topic for another alarm. Because you are already subscribed, you will receive an email. You could also retrigger the alarm by editing the alarm to trigger when CPUUtilization is \u003e= 500 (which can never happen for the single CPU instance). Wait five minutes until the alarm is disabled, then edit the alarm to trigger when CPUUtilization \u003e= 50.\n13. Now move to the History tab:\nYour History is likely similar to the example shown above. The oldest event is the furthest down. In succession, the Alarm was created; the state changed from INSUFFICIENT DATA to ALARM; SNS sent off an email notification.\n15. Spend a few minutes exploring the latest alarm history and try to understand what is going on with each entry. You can see more details for each entry by clicking the date.\nCreate an Alarm using the EC2 console 1. Navigate to EC2 Instances by clicking here.\n2. Select the Monitoring Scripts instance, then switch to the Status Checks tab:\n3. Click Actions\u003e Create Status Check Alarm:\nThis dialog is similar in function to the create alarm wizard you saw in an earlier Lab Step.\n4. For the Alarm notification, select the SNS topic name you set up before.\nOther fields can be kept at their defaults. The Alarm thresholds section uses Status check failed: either to trigger the alarm for either instance or system status check failures:\n5. Click Create when ready. An alarm creation confirmation message is displayed:\nNow you know two different ways to create alarms: one from CloudWatch and the other from the EC2 console. Next, you will learn how to attach EC2 actions to alarms.\n6. Return to the**Alarms by clicking here.**Notice that the first alarm you created is stuck in the In alarm state.\nThe alarm is stuck in the In alarm state because the instance is running an application that consumes 100% of the CPU utilization. Clearly an indicator that something may have gone wrong with the instance. Imagine that you are managing a production environment and you have an instance that is becoming unavailable intermittently because of high CPU utilization. You would like to receive a notification every time the CPU utilization is high, but this can happen anytime, in the middle of the night, or during a weekend or holiday. It would be helpful to have a pre-defined action in this case – at least until you find a definitive solution for the problem.\nTo help you address the scenario, you can set EC2 actions on your alarms.\n8. Select the High CPU alarm and then Actions \u003e Edit:\nTo make your alarm more suitable to the training environment needs, set a new EC2 Action to Reboot this instance whenever the state of this alarm is ALARM.\n9. Click on Next and click on Add EC2 actionunder EC2 action. Select Reboot this instance.\n10. Click Update alarm when ready.\nAlthough the changes have been made to the alarm, the alarm remains in the In alarm state. CloudWatch will only perform actions when the state transitions to the In alarmstate from another state. In the next instruction, you will modify the alarm to quickly have it change to the OKstate and then change it back to return to the In alarmstate.\n11. Select the High CPUAlarm and choose Actions \u003e Edit. Toggle the relationship from \u003e= to \u003c= and click Update alarm:\n12. Refresh the page to ensure the alarm has transitioned to the OKstate. Then toggle the condition back to \u003e= and save the alarm to have it transition to the In alarm state.\n_Note:_The state change may not be immediate and may take up to 2 minutes.\n13. Navigate back to the Instances by clicking here and watch CloudWatch reboot the instance when the Alarm Status changes to In alarm.\nIn case you miss it, you can return to the alarm in CloudWatch and see the Reboot Action listed in the Historysection:\n14. Check your email client and confirm that you received a notification of the alarm:\nSharing CloudWatch Metrics with others 1. Go to CloudWatch by clicking here and click on Metrics\u003e All metrics.\n2. Select an interesting metric, such as the DiskspaceUtilization metric for the Monitoring Scripts instance, and click Actions \u003e Share:\nThis metric can be found underSystem/Linux \u003e Filesystem, InstanceId, MountPath.\n3. In the Share Graph URL dialog, right-click and copy the URL, then Close the dialog:\n4. The URL for the specific graph you were looking at is copied into the clipboard. You can paste it into a test email to confirm this. For example:\nThe URL is quite complex. To confirm that it is indeed correct, test it out in another browser tab.\n5. Open another browser tab. Paste the URL into the address field and refresh your browser. You should see the exact same graph as the one you shared earlier. Notice that you need to be logged into the AWS console in order to view the information referenced by the URL. For security reasons, you can only share URLs with other AWS Identity and Access Management (IAM) users who have the appropriate CloudWatch IAM permissions in your AWS account.\n","description":"Introduction to CloudWatch","title":"Introduction to CloudWatch","uri":"/en/tracks/aws-certified-developer-associate/cloudwatch/introduction-to-cloudwatch/"},{"content":"Lab https://cloudacademy.com/lab/introduction-codecommit/\nCreate a repository 1. In the AWS Management Console search bar, enter CodeCommit, and click the CodeCommit result under Services:\n2. Click Create repository:\n3. In the Create repository form enter the following values accepting the defaults for values not specified:\nRepository name: lab-repository You can leave the Description field empty for this lab. Usually this field would contain a short description of the purpose of the repository. Attaching meaningful descriptions to repositories makes managing large numbers of repositories easier.\n4. Click Create to create the repository.\nCreating credentials to access your repository 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n2. Under Access Management, click Users in the left-hand sidebar menu:\n3. In the IAM user list, click student:\n4. Click the Security credentials tab:\n5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section and click Generate credentials:\n6. In the box that opens, click Download credentials:\nYour browser will download a file called credentials.csv.\nKeep these credentials saved, you will use them in later steps.\nAccessing a shell with Git available 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. In the left-hand side menu, click Instances:\n3. Select the instance and in the row of buttons above the instance list, click Connect:\n4. In the Connect to instance dialog, ensure EC2 Instance Connect is selected and enter in the User name field:\n5. To open a shell on this instance, click Connect.\nEC2 Instance Connect allows you to connect to the instance over SSH using your web browser. With EC2 Instance Connect a new browser window opens an SSH shell on a Linux host that has git installed.\nKeep this window open, you will use it in later steps.\nAdding files to your repository 1. Navigate to the CodeCommit Console.\n2. In the list of repositories, click lab-repository:\n3. Click Clone URL and select Clone HTTPS in the drop-down menu that opens:\nThe URL of the repository has been copied to your clipboard.\n4. To copy the repository to your Linux host, in your shell, type git clone followed by a space and paste the repository URL:\ngit clone https://git-codecommit.us-west-2.amazonaws.com/v1/repos/lab-repository 5. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\nYou can ignore the warning about cloning an empty repository.\nYou have copied the repository from AWS CodeCommit and stored it locally on the Linux host.\n6. To change to the directory of the repository, enter the cd command:\ncd lab-repository 7. To create a file, enter the following command:\necho \"lab\" \u003e lab.txt With this command you have created a file called lab.txt that can be added to your local repository.\n8. To add the file to your local repository, enter the following commands:\ngit add lab.txt git commit -m \"Lab commit\" In Git terminology, with the first command you are “staging” the file before you commit it. This process enables you to specify which files you want to add to the repository and which ones you want to ignore when committing.\nYou will see output similar to the following:\nYou can ignore the message about your name and email address. Usually when using Git you will configure the name and email address so that your commits are labeled with these details.\nYou have added the lab.txt file to your local copy of the repository on the Linux host.\nPushing your commit to your remote repository 1. In the shell on the Linux host, enter the following command:\ngit push In Git terminology, with this command you are “pushing” your local commit from your “local” repository to the “remote” repository that you “cloned” from.\n2. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\nYou have copied the file from the local repository on the Linux host, to the repository hosted in AWS CodeCommit.\n3. Navigate to the CodeCommit Console.\n4. In the Repositories list click lab-repository:\nYou will see the lab.txt file you pushed in the previous Lab step listed.\n","description":"Tutorial Introduction to CodeCommit","title":"Introduction to CodeCommit","uri":"/en/tracks/aws-certified-developer-associate/codecommit/introduction-codecommit/"},{"content":"Lab\nCreating a DynamoDB Table with a Partition Key 1. From the AWS Management Console, in the search bar at the top, enter DynamoDB, and under Services, click the DynamoDB result:\nThe Amazon DynamoDB product overview page will load.\n2. To start creating a new DyanmoDB table, on the right-hand side, click Create table:\n3. In the Table details section, enter the following:\nTable Name: Partition Key: Enter Name and ensure type is 4. In the Settings section, select Customize settings:\nChoosing this option allows you to specify values for the table’s read and write capacities.\n5. In the Read/write capacity settings section, under Capacity mode, select Provisioned and enter the following:\nRead Capacity: Provisioned capacity units: Write Capacity: Provisioned capacity units: Accept the defaults for all other options on this page.\n6. Scroll to the bottom and click Create table:\nThe Tables list view will load and you will see a notification that your table is being created. After a 30 seconds or so, you will see a success notification:\nCreating a DynamoDB Table with Local and Global Secondary Indexes 1. On the right-hand side of the page, click Create table:\n2. Enter the following in the Table details section:\nTable name: Partition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select 3. In the Settings section, select Customize settings.\n4. Under Read/write capacity settings, ensure Provisioned is selected for Capacity mode, and enter the following:\nRead capacity: Provisioned capacity units: Write capacity: Provisioned capacity units: 5. Scroll down to the Secondary indexes section and click Create local index:\nThe New local secondary index dialog box will appear.\n6. Enter the following to configure your local secondary index:\nSort Key: Name: Enter Type: Select Attribute projections: Select An LSI (Local Secondary Index) has the same partition key as the table’s primary key and will share the provisioned capacity of the table in contrast to global secondary indexes which provision their own capacity.\n7. To finish creating the local secondary index, at the bottom, click Create index:\n8. Scroll to the bottom and click Create table.\nAfter roughly 30 seconds you will the table become active:\nIn contrast to a Local Secondary Index, a Global Secondary Index is an index with a partition and sort key that can be different from those in the table. It is considered “global” because queries on the index can span all of the data in a table, across all partitions.\n9. Click Create table once more to start creating another table.\n10. Enter the following in the Table details section:\nTable Name: Partition key: Name: Enter Type_:_Select Sort key: Name: Enter Type: Select 11. In the Settings section, select Customize settings.\n12. In the Read/write capacity settings section, ensure the Capacity mode is Provisioned, and enter the following:\nRead capacity: Provisioned capacity units: Enter Write capacity: Provisioned capacity units: Enter 13. Scroll down to the Secondary indexes section, and click Create global index:\nThe New global secondary index dialog form will appear.\n14. Enter the following:\nPartition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 15. To finish creating the global secondary index, at the bottom, click Create index.\n16. Click Create global index again and enter the following:\nPartition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 17. To finish creating the global secondary index, at the bottom, click Create index.\n18. Scroll to the bottom and click Create table.\nOnce again, you will see your table created after roughly 30 seconds.\nInserting Items Into a DynamoDB Table 1. In the left-hand menu, click Explore items:\n2. In the Tables list, select\nYou will see nothing under Items returned because there are no items stored.\n3. On the right-hand side, click Create item:\nThe Create item form will load and you will see a list of Attributes.\n4. In the Value textbox next to Name - Partition key, enter a name for your forum (can be anything you wish):\n5. To add another attribute for this item, click Add new attribute and select String from the list of types:\n6. In the Attribute name textbox, enter Description and in the Value textbox, enter any value you’d like:\n7. At the bottom, click Create item:\n8. Repeat steps 3-7 three more times so that end up with four entries in the table:\n9. Select the table and click Create Item.\n10. Provide any values you’d like for , and , keeping in mind that the value must match the name of one of your forums.\nNote: is a \" \" table with the Local Secondary Index. For being able to save a item, you have to provide:\n(the table Primary Key) (the table Sort Key) (the Local Secondary Index Sort Key) Note: You will have to click Add new attribute to add the CreationDate attribute and specify a value.\n11. At the bottom, click Create item.\n12. Repeat steps 9-11 three more times until you have four items in the table:\nEditing DynamoDB Table Items 1. On the Explore items page, select the table:\n2. Select any item in the table and click on its name to get to the Item editor page:\n3. Click inside any value and make an update to its contents:\nWarning: Note that modifying the partition key will result in changing the values of the item keys. This will delete and recreate the item with new keys.\n4. At the bottom of the page, click Save changes:\nQuerying a DynamoDB Table 1. In the left-hand menu, click PartiQL editor:\nThe PartiQL editor page will load.\nPartiQL is a SQL (Structured Query Language) compatible language for Amazon DynamoDB. As well as querying tables, you can use it to insert new items and update existing ones.\n2. Under Tables, click the three dots next to the and click Scan table:\nThe Query 1 editor will be populated with a PartiQL query that selects all items from the .\n3. To execute the PartiQL table, under the editor, click Run:\n4. Scroll down to see the results under Items returned:\nNotice that you have a choice of viewing the results in tabular form or in JSON (Java Script Object Notation):\n5. To query for a specific item, replace the contents of the Query 1 editor with the following, and click Run:\nSELECT * FROM \"Thread\" WHERE \"Subject\" = 'Intro to cool stuff' This time, you will only see items returned that satisfy the value of the WHERE condition.\nNote: Change the value of the WHERE condition to match an item you created if you don’t see a result.\nPartiQL supports most standard features of SQL which means you can query, select, and sort your data in sophisticated ways.\nTypically, using the Amazon DynamoDB Console to query items is useful for one-off reports and debugging or troubleshooting. Like most databases, DynamoDB can be accessed programmatically by other systems and software applications through either the AWS SDK (software development kit) or DyanmoDB’s HTTP API (application programming interface).\nYou can learn more about using PartiQL with Amazon DynamoDB by visiting the Working with PartiQL Query Language section of the Amazon DynamoDB developer guide.\nDeleting a DynamoDB Table 1. In the left-hand menu, click Tables:\n2. In the Tables table, select the Thread table:\n3. On the right-hand side, click Delete:\nThe Delete table confirmation modal will appear.\nNotice that you have the ability to create a backup for a table before deleting it.\n4. In the confirmation textbox, enter delete and click Delete table:\nYou will see a message summarizing the deletion:\n5. To continue, click Go to tables:\n6. To update the Tables table, click the refresh icon:\nYou will now see only two tables listed.\n","description":"Creating a DynamoDB Amazon DynamoDB Table","title":"Introduction to DynamoDB","uri":"/en/tracks/aws-certified-developer-associate/dynamodb/introduction-dynamodb/"},{"content":"Google maps Route\n","description":"Israel - Haifa - Bahai Gardens","title":"Israel - Haifa - Bahai Gardens","uri":"/en/photos/22-07-02-israel-haifa-bahai-gardens/"},{"content":"Interim metrics still in process\nFor 2020:\nTime spent studying/practicing: ~5500 hours ","description":"Certified IT knowledge for the year 2020","title":"IT courses 2020","uri":"/en/posts/diploma/"},{"content":"Web / Browser get base URL const getBaseURL = url =\u003e url.replace(/[?#].*$/, ''); getBaseURL('http://url.com/page?name=Adam\u0026surname=Smith'); // 'http://url.com/page' const url = new URL(\"https://example.com/login?user=someguy\u0026page=news\"); url.origin // \"https://example.com\" url.host // \"example.com\" url.protocol // \"https:\" url.pathname // \"/login\" url.searchParams.get('user') // \"someuser\" get URL parameters as object const getURLParameters = url =\u003e (url.match(/([^?=\u0026]+)(=([^\u0026]*))/g) || []).reduce( (a, v) =\u003e ( (a[v.slice(0, v.indexOf('='))] = v.slice(v.indexOf('=') + 1)), a ), {} ); getURLParameters('google.com'); // {} getURLParameters('http://url.com/page?name=Adam\u0026surname=Smith'); // {name: 'Adam', surname: 'Smith'} // One line Object.fromEntries('http://url.com/page?name=Adam\u0026surname=Smith'.split('?')[1].split('\u0026').map(x=\u003ex.split('='))) if DOC element contains another element const elementContains = (parent, child) =\u003e parent !== child \u0026\u0026 parent.contains(child); elementContains( document.querySelector('head'), document.querySelector('title') ); // true elementContains(document.querySelector('body'), document.querySelector('body')); // false Date const {locale, timeZone} = Intl.DateTimeFormat().resolvedOptions(); is Date valid const isDateValid = (...val) =\u003e !Number.isNaN(new Date(...val).valueOf()); isDateValid('December 17, 1995 03:24:00'); // true isDateValid('1995-12-17T03:24:00'); // true isDateValid('1995-12-17 T03:24:00'); // false isDateValid('Duck'); // false isDateValid(2023, 01, 22); // true isDateValid(1995, 11, 17, 'Duck'); // false isDateValid({}); // false UNIX timestamp from Date const getTimestamp = (date = new Date()) =\u003e Math.floor(date.getTime() / 1000); getTimestamp(); // 1602162242 Compare dates / sort export function compareDates(date1, date2) { if (!date1) { return 1; // move date1 to the end of the array } if (!date2) { return -1; // move date2 to the end of the array } // Convert the date strings to Date objects const d1 = new Date(date1); const d2 = new Date(date2); // Compare the dates if (d1.getTime() === d2.getTime()) { return 0; // dates are equal } if (d1 \u003c d2) { return -1; // date1 is earlier than date2 } return 1; // date1 is later than date2 } const sortedItems = allItems.sort((itemA, itemB) =\u003e compareDates(itemA.date, itemB.date) ); Login Secure Your Node.js App with JSON Web Tokens\nclient.ts // client.ts import axios, { AxiosInstance } from 'axios'; export class Client { private _client: AxiosInstance; constructor(accessToken?: string, url?: string) { const apiUrl = this.selectApiTarget(); let headers = {}; if (accessToken !== undefined) { headers = { 'Authorization': `Bearer ${accessToken}` }; } this._client = axios.create({ baseURL: url || apiUrl, headers: headers, }); } private selectApiTarget(): string { let backendUrl = config.backend.url; if (window.location.host.includes(\"node.sharedtodos.com\")) { backendUrl = config.backend.url.slice().replace(\"api.sharedtodos.com\", \"node-api.sharedtodos.com\"); } return `${backendUrl}/api/v1/`; } async getLoggedInUser(): Promise\u003cUser\u003e { return await this._client.get('/user/me').then((response) =\u003e response.data); } async forgetLoggedInUser(): Promise\u003cvoid\u003e { return await this._client.delete('/user/me').then((response) =\u003e response.data); } async getTasks(listId: number): Promise\u003cTask[]\u003e { return await this._client.get(`boards/${listId}/tasks`).then((response) =\u003e response.data); } async deleteTask(listId: number, taskId: number) { return await this._client.delete(`boards/${listId}/tasks/${taskId}`).then((response) =\u003e response.data); } async createTask(listId: number, title: string, description: string) { const task: Task = { title: title, description: description, }; return await this._client.post(`boards/${listId}/tasks`, task); } async updateTask(listId: number, taskId: string, task: Task) { return await this._client.put(`boards/${listId}/tasks/${taskId}`, task); } async login(email: string): Promise\u003cstring\u003e { let data = new FormData(); data.append('user_email', email); return await this._client.post(`login`, data, { headers: {'Content-Type': 'multipart/form-data' } }).then((response) =\u003e response.data.access_token); } } export const getClient = (accessToken?, url?): Client =\u003e new Client(accessToken, url); config.ts // config.ts interface ConfigOptions { backend: { url: string }; auth0: any; authentication: { provider: string }; authorization: { embedUrl: string }; } declare global { interface Window { _env_: any; } } const Config: ConfigOptions = { backend: { url: process.env.REACT_APP_BACKEND_URL || window?._env_?.BACKEND_URL || \"http://localhost:8008\", }, auth0: { domain: process.env.AUTH0_DOMAIN || window?._env_?.AUTH0_DOMAIN || \"acalla-demoapp.us.auth0.com\", clientId: process.env.AUTH0_CLIENT_ID || window?._env_?.AUTH0_CLIENT_ID || \"myClientID\", audience: process.env.AUTH0_AUDIENCE || window?._env_?.AUTH0_AUDIENCE || \"https://demoapi.server.com/v1/\", }, authentication: { provider: \"auth0\", }, authorization: { embedUrl: window?._env_?.AUTHZ_EMBED_URL || \"http://localhost:3000\", } }; export default Config; Data Structures export const shuffleArray = \u003cT\u003e(array: T[]): T[] =\u003e { for (let i = array.length - 1; i \u003e 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [array[i], array[j]] = [array[j], array[i]]; } return array; }; Cheat Sheet // Single-line comments start with two slashes. /* Multiline comments start with slash-star, and end with star-slash */ // Statements can be terminated by ; doStuff(); // ... but they don't have to be, as semicolons are automatically inserted // wherever there's a newline, except in certain cases. doStuff() // Because those cases can cause unexpected results, we'll keep on using // semicolons in this guide. /////////////////////////////////// // 1. Numbers, Strings and Operators // JavaScript has one number type (which is a 64-bit IEEE 754 double). // Doubles have a 52-bit mantissa, which is enough to store integers // up to about 9✕10¹⁵ precisely. 3; // = 3 1.5; // = 1.5 // Some basic arithmetic works as you'd expect. 1 + 1; // = 2 0.1 + 0.2; // = 0.30000000000000004 8 - 1; // = 7 10 * 2; // = 20 35 / 5; // = 7 // Including uneven division. 5 / 2; // = 2.5 // And modulo division. 10 % 2; // = 0 30 % 4; // = 2 18.5 % 7; // = 4.5 // Bitwise operations also work; when you perform a bitwise operation your float // is converted to a signed int *up to* 32 bits. 1 \u003c\u003c 2; // = 4 // Precedence is enforced with parentheses. (1 + 3) * 2; // = 8 // There are three special not-a-real-number values: Infinity; // result of e.g. 1/0 -Infinity; // result of e.g. -1/0 NaN; // result of e.g. 0/0, stands for 'Not a Number' // There's also a boolean type. true; false; // Strings are created with ' or \". 'abc'; \"Hello, world\"; // Negation uses the ! symbol !true; // = false !false; // = true // Equality is === 1 === 1; // = true 2 === 1; // = false // Inequality is !== 1 !== 1; // = false 2 !== 1; // = true // More comparisons 1 \u003c 10; // = true 1 \u003e 10; // = false 2 \u003c= 2; // = true 2 \u003e= 2; // = true // Strings are concatenated with + \"Hello \" + \"world!\"; // = \"Hello world!\" // ... which works with more than just strings \"1, 2, \" + 3; // = \"1, 2, 3\" \"Hello \" + [\"world\", \"!\"]; // = \"Hello world,!\" // and are compared with \u003c and \u003e \"a\" \u003c \"b\"; // = true // Type coercion is performed for comparisons with double equals... \"5\" == 5; // = true null == undefined; // = true // ...unless you use === \"5\" === 5; // = false null === undefined; // = false // ...which can result in some weird behaviour... 13 + !0; // 14 \"13\" + !0; // '13true' // You can access characters in a string with `charAt` \"This is a string\".charAt(0); // = 'T' // ...or use `substring` to get larger pieces. \"Hello world\".substring(0, 5); // = \"Hello\" // `length` is a property, so don't use (). \"Hello\".length; // = 5 // There's also `null` and `undefined`. null; // used to indicate a deliberate non-value undefined; // used to indicate a value is not currently present (although // `undefined` is actually a value itself) // false, null, undefined, NaN, 0 and \"\" are falsy; everything else is truthy. // Note that 0 is falsy and \"0\" is truthy, even though 0 == \"0\". /////////////////////////////////// // 2. Variables, Arrays and Objects // Variables are declared with the `var` keyword. JavaScript is dynamically // typed, so you don't need to specify type. Assignment uses a single `=` // character. var someVar = 5; // If you leave the var keyword off, you won't get an error... someOtherVar = 10; // ...but your variable will be created in the global scope, not in the scope // you defined it in. // Variables declared without being assigned to are set to undefined. var someThirdVar; // = undefined // If you want to declare a couple of variables, then you could use a comma // separator var someFourthVar = 2, someFifthVar = 4; // There's shorthand for performing math operations on variables: someVar += 5; // equivalent to someVar = someVar + 5; someVar is 10 now someVar *= 10; // now someVar is 100 // and an even-shorter-hand for adding or subtracting 1 someVar++; // now someVar is 101 someVar--; // back to 100 // Arrays are ordered lists of values, of any type. var myArray = [\"Hello\", 45, true]; // Their members can be accessed using the square-brackets subscript syntax. // Array indices start at zero. myArray[1]; // = 45 // Arrays are mutable and of variable length. myArray.push(\"World\"); myArray.length; // = 4 // Add/Modify at specific index myArray[3] = \"Hello\"; // Add and remove element from front or back end of an array myArray.unshift(3); // Add as the first element someVar = myArray.shift(); // Remove first element and return it myArray.push(3); // Add as the last element someVar = myArray.pop(); // Remove last element and return it // Join all elements of an array with semicolon var myArray0 = [32,false,\"js\",12,56,90]; myArray0.join(\";\"); // = \"32;false;js;12;56;90\" // Get subarray of elements from index 1 (include) to 4 (exclude) myArray0.slice(1,4); // = [false,\"js\",12] // Remove 4 elements starting from index 2, and insert there strings // \"hi\",\"wr\" and \"ld\"; return removed subarray myArray0.splice(2,4,\"hi\",\"wr\",\"ld\"); // = [\"js\",12,56,90] // myArray0 === [32,false,\"hi\",\"wr\",\"ld\"] // JavaScript's objects are equivalent to \"dictionaries\" or \"maps\" in other // languages: an unordered collection of key-value pairs. var myObj = {key1: \"Hello\", key2: \"World\"}; // Keys are strings, but quotes aren't required if they're a valid // JavaScript identifier. Values can be any type. var myObj = {myKey: \"myValue\", \"my other key\": 4}; // Object attributes can also be accessed using the subscript syntax, myObj[\"my other key\"]; // = 4 // ... or using the dot syntax, provided the key is a valid identifier. myObj.myKey; // = \"myValue\" // Objects are mutable; values can be changed and new keys added. myObj.myThirdKey = true; // If you try to access a value that's not yet set, you'll get undefined. myObj.myFourthKey; // = undefined /////////////////////////////////// // 3. Logic and Control Structures // The `if` structure works as you'd expect. var count = 1; if (count == 3){ // evaluated if count is 3 } else if (count == 4){ // evaluated if count is 4 } else { // evaluated if it's not either 3 or 4 } // As does `while`. while (true){ // An infinite loop! } // Do-while loops are like while loops, except they always run at least once. var input; do { input = getInput(); } while (!isValid(input)); // The `for` loop is the same as C and Java: // initialization; continue condition; iteration. for (var i = 0; i \u003c 5; i++){ // will run 5 times } // Breaking out of labeled loops is similar to Java outer: for (var i = 0; i \u003c 10; i++) { for (var j = 0; j \u003c 10; j++) { if (i == 5 \u0026\u0026 j ==5) { break outer; // breaks out of outer loop instead of only the inner one } } } // The for/in statement allows iteration over properties of an object. var description = \"\"; var person = {fname:\"Paul\", lname:\"Ken\", age:18}; for (var x in person){ description += person[x] + \" \"; } // description = 'Paul Ken 18 ' // The for/of statement allows iteration over iterable objects (including the built-in String, // Array, e.g. the Array-like arguments or NodeList objects, TypedArray, Map and Set, // and user-defined iterables). var myPets = \"\"; var pets = [\"cat\", \"dog\", \"hamster\", \"hedgehog\"]; for (var pet of pets){ myPets += pet + \" \"; } // myPets = 'cat dog hamster hedgehog ' // \u0026\u0026 is logical and, || is logical or if (house.size == \"big\" \u0026\u0026 house.colour == \"blue\"){ house.contains = \"bear\"; } if (colour == \"red\" || colour == \"blue\"){ // colour is either red or blue } // \u0026\u0026 and || \"short circuit\", which is useful for setting default values. var name = otherName || \"default\"; // The `switch` statement checks for equality with `===`. // Use 'break' after each case // or the cases after the correct one will be executed too. grade = 'B'; switch (grade) { case 'A': console.log(\"Great job\"); break; case 'B': console.log(\"OK job\"); break; case 'C': console.log(\"You can do better\"); break; default: console.log(\"Oy vey\"); break; } /////////////////////////////////// // 4. Functions, Scope and Closures // JavaScript functions are declared with the `function` keyword. function myFunction(thing){ return thing.toUpperCase(); } myFunction(\"foo\"); // = \"FOO\" // Note that the value to be returned must start on the same line as the // `return` keyword, otherwise you'll always return `undefined` due to // automatic semicolon insertion. Watch out for this when using Allman style. function myFunction(){ return // \u003c- semicolon automatically inserted here {thisIsAn: 'object literal'}; } myFunction(); // = undefined // JavaScript functions are first class objects, so they can be reassigned to // different variable names and passed to other functions as arguments - for // example, when supplying an event handler: function myFunction(){ // this code will be called in 5 seconds' time } setTimeout(myFunction, 5000); // Note: setTimeout isn't part of the JS language, but is provided by browsers // and Node.js. // Another function provided by browsers is setInterval function myFunction(){ // this code will be called every 5 seconds } setInterval(myFunction, 5000); // Function objects don't even have to be declared with a name - you can write // an anonymous function definition directly into the arguments of another. setTimeout(function(){ // this code will be called in 5 seconds' time }, 5000); // JavaScript has function scope; functions get their own scope but other blocks // do not. if (true){ var i = 5; } i; // = 5 - not undefined as you'd expect in a block-scoped language // This has led to a common pattern of \"immediately-executing anonymous // functions\", which prevent temporary variables from leaking into the global // scope. (function(){ var temporary = 5; // We can access the global scope by assigning to the \"global object\", which // in a web browser is always `window`. The global object may have a // different name in non-browser environments such as Node.js. window.permanent = 10; })(); temporary; // raises ReferenceError permanent; // = 10 // One of JavaScript's most powerful features is closures. If a function is // defined inside another function, the inner function has access to all the // outer function's variables, even after the outer function exits. function sayHelloInFiveSeconds(name){ var prompt = \"Hello, \" + name + \"!\"; // Inner functions are put in the local scope by default, as if they were // declared with `var`. function inner(){ alert(prompt); } setTimeout(inner, 5000); // setTimeout is asynchronous, so the sayHelloInFiveSeconds function will // exit immediately, and setTimeout will call inner afterwards. However, // because inner is \"closed over\" sayHelloInFiveSeconds, inner still has // access to the `prompt` variable when it is finally called. } sayHelloInFiveSeconds(\"Adam\"); // will open a popup with \"Hello, Adam!\" in 5s /////////////////////////////////// // 5. More about Objects; Constructors and Prototypes // Objects can contain functions. var myObj = { myFunc: function(){ return \"Hello world!\"; } }; myObj.myFunc(); // = \"Hello world!\" // When functions attached to an object are called, they can access the object // they're attached to using the `this` keyword. myObj = { myString: \"Hello world!\", myFunc: function(){ return this.myString; } }; myObj.myFunc(); // = \"Hello world!\" // What this is set to has to do with how the function is called, not where // it's defined. So, our function doesn't work if it isn't called in the // context of the object. var myFunc = myObj.myFunc; myFunc(); // = undefined // Inversely, a function can be assigned to the object and gain access to it // through `this`, even if it wasn't attached when it was defined. var myOtherFunc = function(){ return this.myString.toUpperCase(); }; myObj.myOtherFunc = myOtherFunc; myObj.myOtherFunc(); // = \"HELLO WORLD!\" // We can also specify a context for a function to execute in when we invoke it // using `call` or `apply`. var anotherFunc = function(s){ return this.myString + s; }; anotherFunc.call(myObj, \" And Hello Moon!\"); // = \"Hello World! And Hello Moon!\" // The `apply` function is nearly identical, but takes an array for an argument // list. anotherFunc.apply(myObj, [\" And Hello Sun!\"]); // = \"Hello World! And Hello Sun!\" // This is useful when working with a function that accepts a sequence of // arguments and you want to pass an array. Math.min(42, 6, 27); // = 6 Math.min([42, 6, 27]); // = NaN (uh-oh!) Math.min.apply(Math, [42, 6, 27]); // = 6 // But, `call` and `apply` are only temporary. When we want it to stick, we can // use `bind`. var boundFunc = anotherFunc.bind(myObj); boundFunc(\" And Hello Saturn!\"); // = \"Hello World! And Hello Saturn!\" // `bind` can also be used to partially apply (curry) a function. var product = function(a, b){ return a * b; }; var doubler = product.bind(this, 2); doubler(8); // = 16 // When you call a function with the `new` keyword, a new object is created, and // made available to the function via the `this` keyword. Functions designed to be // called like that are called constructors. var MyConstructor = function(){ this.myNumber = 5; }; myNewObj = new MyConstructor(); // = {myNumber: 5} myNewObj.myNumber; // = 5 // Unlike most other popular object-oriented languages, JavaScript has no // concept of 'instances' created from 'class' blueprints; instead, JavaScript // combines instantiation and inheritance into a single concept: a 'prototype'. // Every JavaScript object has a 'prototype'. When you go to access a property // on an object that doesn't exist on the actual object, the interpreter will // look at its prototype. // Some JS implementations let you access an object's prototype on the magic // property `__proto__`. While this is useful for explaining prototypes it's not // part of the standard; we'll get to standard ways of using prototypes later. var myObj = { myString: \"Hello world!\" }; var myPrototype = { meaningOfLife: 42, myFunc: function(){ return this.myString.toLowerCase(); } }; myObj.__proto__ = myPrototype; myObj.meaningOfLife; // = 42 // This works for functions, too. myObj.myFunc(); // = \"hello world!\" // Of course, if your property isn't on your prototype, the prototype's // prototype is searched, and so on. myPrototype.__proto__ = { myBoolean: true }; myObj.myBoolean; // = true // There's no copying involved here; each object stores a reference to its // prototype. This means we can alter the prototype and our changes will be // reflected everywhere. myPrototype.meaningOfLife = 43; myObj.meaningOfLife; // = 43 // The for/in statement allows iteration over properties of an object, // walking up the prototype chain until it sees a null prototype. for (var x in myObj){ console.log(myObj[x]); } ///prints: // Hello world! // 43 // [Function: myFunc] // true // To only consider properties attached to the object itself // and not its prototypes, use the `hasOwnProperty()` check. for (var x in myObj){ if (myObj.hasOwnProperty(x)){ console.log(myObj[x]); } } ///prints: // Hello world! // We mentioned that `__proto__` was non-standard, and there's no standard way to // change the prototype of an existing object. However, there are two ways to // create a new object with a given prototype. // The first is Object.create, which is a recent addition to JS, and therefore // not available in all implementations yet. var myObj = Object.create(myPrototype); myObj.meaningOfLife; // = 43 // The second way, which works anywhere, has to do with constructors. // Constructors have a property called prototype. This is *not* the prototype of // the constructor function itself; instead, it's the prototype that new objects // are given when they're created with that constructor and the new keyword. MyConstructor.prototype = { myNumber: 5, getMyNumber: function(){ return this.myNumber; } }; var myNewObj2 = new MyConstructor(); myNewObj2.getMyNumber(); // = 5 myNewObj2.myNumber = 6; myNewObj2.getMyNumber(); // = 6 // Built-in types like strings and numbers also have constructors that create // equivalent wrapper objects. var myNumber = 12; var myNumberObj = new Number(12); myNumber == myNumberObj; // = true // Except, they aren't exactly equivalent. typeof myNumber; // = 'number' typeof myNumberObj; // = 'object' myNumber === myNumberObj; // = false if (0){ // This code won't execute, because 0 is falsy. } if (new Number(0)){ // This code will execute, because wrapped numbers are objects, and objects // are always truthy. } // However, the wrapper objects and the regular builtins share a prototype, so // you can actually add functionality to a string, for instance. String.prototype.firstCharacter = function(){ return this.charAt(0); }; \"abc\".firstCharacter(); // = \"a\" // This fact is often used in \"polyfilling\", which is implementing newer // features of JavaScript in an older subset of JavaScript, so that they can be // used in older environments such as outdated browsers. // For instance, we mentioned that Object.create isn't yet available in all // implementations, but we can still use it with this polyfill: if (Object.create === undefined){ // don't overwrite it if it exists Object.create = function(proto){ // make a temporary constructor with the right prototype var Constructor = function(){}; Constructor.prototype = proto; // then use it to create a new, appropriately-prototyped object return new Constructor(); }; } // ES6 Additions // The \"let\" keyword allows you to define variables in a lexical scope, // as opposed to a function scope like the var keyword does. let name = \"Billy\"; // Variables defined with let can be reassigned new values. name = \"William\"; // The \"const\" keyword allows you to define a variable in a lexical scope // like with let, but you cannot reassign the value once one has been assigned. const pi = 3.14; pi = 4.13; // You cannot do this. // There is a new syntax for functions in ES6 known as \"lambda syntax\". // This allows functions to be defined in a lexical scope like with variables // defined by const and let. const isEven = (number) =\u003e { return number % 2 === 0; }; isEven(7); // false // The \"equivalent\" of this function in the traditional syntax would look like this: function isEven(number) { return number % 2 === 0; }; // I put the word \"equivalent\" in double quotes because a function defined // using the lambda syntax cannnot be called before the definition. // The following is an example of invalid usage: add(1, 8); const add = (firstNumber, secondNumber) =\u003e { return firstNumber + secondNumber; }; Cheat Sheet Typescript // There are 3 basic types in TypeScript let isDone: boolean = false; let lines: number = 42; let name: string = \"Anders\"; // But you can omit the type annotation if the variables are derived // from explicit literals let isDone = false; let lines = 42; let name = \"Anders\"; // When it's impossible to know, there is the \"Any\" type let notSure: any = 4; notSure = \"maybe a string instead\"; notSure = false; // okay, definitely a boolean // Use const keyword for constants const numLivesForCat = 9; numLivesForCat = 1; // Error // For collections, there are typed arrays and generic arrays let list: number[] = [1, 2, 3]; // Alternatively, using the generic array type let list: Array\u003cnumber\u003e = [1, 2, 3]; // For enumerations: enum Color { Red, Green, Blue }; let c: Color = Color.Green; console.log(Color[c]); // \"Green\" // Lastly, \"void\" is used in the special case of a function returning nothing function bigHorribleAlert(): void { alert(\"I'm a little annoying box!\"); } // Functions are first class citizens, support the lambda \"fat arrow\" syntax and // use type inference // The following are equivalent, the same signature will be inferred by the // compiler, and same JavaScript will be emitted let f1 = function (i: number): number { return i * i; } // Return type inferred let f2 = function (i: number) { return i * i; } // \"Fat arrow\" syntax let f3 = (i: number): number =\u003e { return i * i; } // \"Fat arrow\" syntax with return type inferred let f4 = (i: number) =\u003e { return i * i; } // \"Fat arrow\" syntax with return type inferred, braceless means no return // keyword needed let f5 = (i: number) =\u003e i * i; // Interfaces are structural, anything that has the properties is compliant with // the interface interface Person { name: string; // Optional properties, marked with a \"?\" age?: number; // And of course functions move(): void; } // Object that implements the \"Person\" interface // Can be treated as a Person since it has the name and move properties let p: Person = { name: \"Bobby\", move: () =\u003e { } }; // Objects that have the optional property: let validPerson: Person = { name: \"Bobby\", age: 42, move: () =\u003e { } }; // Is not a person because age is not a number let invalidPerson: Person = { name: \"Bobby\", age: true }; // Interfaces can also describe a function type interface SearchFunc { (source: string, subString: string): boolean; } // Only the parameters' types are important, names are not important. let mySearch: SearchFunc; mySearch = function (src: string, sub: string) { return src.search(sub) != -1; } // Classes - members are public by default class Point { // Properties x: number; // Constructor - the public/private keywords in this context will generate // the boiler plate code for the property and the initialization in the // constructor. // In this example, \"y\" will be defined just like \"x\" is, but with less code // Default values are also supported constructor(x: number, public y: number = 0) { this.x = x; } // Functions dist(): number { return Math.sqrt(this.x * this.x + this.y * this.y); } // Static members static origin = new Point(0, 0); } // Classes can be explicitly marked as implementing an interface. // Any missing properties will then cause an error at compile-time. class PointPerson implements Person { name: string move() {} } let p1 = new Point(10, 20); let p2 = new Point(25); //y will be 0 // Inheritance class Point3D extends Point { constructor(x: number, y: number, public z: number = 0) { super(x, y); // Explicit call to the super class constructor is mandatory } // Overwrite dist(): number { let d = super.dist(); return Math.sqrt(d * d + this.z * this.z); } } // Modules, \".\" can be used as separator for sub modules module Geometry { export class Square { constructor(public sideLength: number = 0) { } area() { return Math.pow(this.sideLength, 2); } } } let s1 = new Geometry.Square(5); // Local alias for referencing a module import G = Geometry; let s2 = new G.Square(10); // Generics // Classes class Tuple\u003cT1, T2\u003e { constructor(public item1: T1, public item2: T2) { } } // Interfaces interface Pair\u003cT\u003e { item1: T; item2: T; } // And functions let pairToTuple = function \u003cT\u003e(p: Pair\u003cT\u003e) { return new Tuple(p.item1, p.item2); }; let tuple = pairToTuple({ item1: \"hello\", item2: \"world\" }); // Including references to a definition file: /// \u003creference path=\"jquery.d.ts\" /\u003e // Template Strings (strings that use backticks) // String Interpolation with Template Strings let name = 'Tyrone'; let greeting = `Hi ${name}, how are you?` // Multiline Strings with Template Strings let multiline = `This is an example of a multiline string`; // READONLY: New Feature in TypeScript 3.1 interface Person { readonly name: string; readonly age: number; } var p1: Person = { name: \"Tyrone\", age: 42 }; p1.age = 25; // Error, p1.age is read-only var p2 = { name: \"John\", age: 60 }; var p3: Person = p2; // Ok, read-only alias for p2 p3.age = 35; // Error, p3.age is read-only p2.age = 45; // Ok, but also changes p3.age because of aliasing class Car { readonly make: string; readonly model: string; readonly year = 2018; constructor() { this.make = \"Unknown Make\"; // Assignment permitted in constructor this.model = \"Unknown Model\"; // Assignment permitted in constructor } } let numbers: Array\u003cnumber\u003e = [0, 1, 2, 3, 4]; let moreNumbers: ReadonlyArray\u003cnumber\u003e = numbers; moreNumbers[5] = 5; // Error, elements are read-only moreNumbers.push(5); // Error, no push method (because it mutates array) moreNumbers.length = 3; // Error, length is read-only numbers = moreNumbers; // Error, mutating methods are missing // Tagged Union Types for modelling state that can be in one of many shapes type State = | { type: \"loading\" } | { type: \"success\", value: number } | { type: \"error\", message: string }; declare const state: State; if (state.type === \"success\") { console.log(state.value); } else if (state.type === \"error\") { console.error(state.message); } // Template Literal Types // Use to create complex string types type OrderSize = \"regular\" | \"large\"; type OrderItem = \"Espresso\" | \"Cappuccino\"; type Order = `A ${OrderSize} ${OrderItem}`; let order1: Order = \"A regular Cappuccino\"; let order2: Order = \"A large Espresso\"; let order3: Order = \"A small Espresso\"; // Error // Iterators and Generators // for..of statement // iterate over the list of values on the object being iterated let arrayOfAnyType = [1, \"string\", false]; for (const val of arrayOfAnyType) { console.log(val); // 1, \"string\", false } let list = [4, 5, 6]; for (const i of list) { console.log(i); // 4, 5, 6 } // for..in statement // iterate over the list of keys on the object being iterated for (const i in list) { console.log(i); // 0, 1, 2 } // Type Assertion let foo = {} // Creating foo as an empty object foo.bar = 123 // Error: property 'bar' does not exist on `{}` foo.baz = 'hello world' // Error: property 'baz' does not exist on `{}` // Because the inferred type of foo is `{}` (an object with 0 properties), you // are not allowed to add bar and baz to it. However with type assertion, // the following will pass: interface Foo { bar: number; baz: string; } let foo = {} as Foo; // Type assertion here foo.bar = 123; foo.baz = 'hello world' Resources react cheatsheet https://learnxinyminutes.com/docs/typescript/ ","description":"JavaScript code snippets","title":"JavaScript code snippets","uri":"/en/posts/js-snippets/"},{"content":"type ObjectWithKeyName = { [key: string]: any; }; // transform array of objects to dict // use object provided keyName as a key of dict // expected for each object in array this keyName value is unique const transformArrayToDict = (objects: ObjectWithKeyName[], keyName: string) =\u003e { const res: ObjectWithKeyName = {}; objects.forEach((obj) =\u003e { res[obj[keyName]] = obj; }); return res; }; ","description":"JavaScript: convert array of objects to dictionary","title":"JavaScript: convert array of objects to dictionary","uri":"/en/posts/js-convert-array-to-dict/"},{"content":"About AWS Lambda AWS Lambda User Guide AWS Lambda is a serverless computing service that runs program code in response to certain events and is responsible for automatically allocating the necessary computing resources.\nAWS Lambda automatically runs program code in response to various events, such as HTTP requests through Amazon API Gateway, changing objects in Amazon Simple Storage Service garbage cans (Amazon S3), updating tables in Amazon DynamoDB or changing states in AWS Step Functions.\nSupports for Java, Go, PowerShell, Node.js, C#, Python and Ruby. It also provides a Runtime API which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function’s handler method when the function is invoked. You can include a runtime in your function’s deployment package in the form of an executable file named bootstrap\nWhen you publish a version, AWS Lambda makes a snapshot copy of the Lambda function code (and configuration) in the $LATEST version. A published version is immutable.\nLambda execution role is an IAM role that grants the function permission to access AWS services and resources. Under Attach permissions policies, choose the AWS managed policies AWSLambdaBasicExecutionRole and AWSXRayDaemonWriteAccess.\nAWS managed policies for Lambda features\nDigest Types of lambda invocation RequestResponse. Event. Dryrun. Lambda execution context is a temporary runtime environment that initializes any external dependencies of our Lambda function code, such as database connections or HTTP endpoints Lambda Environment variables are variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration. Lambda concurrent executions = (invocations per second) x (average execution duration in seconds). Concurrency limit of lambda execution, Default 1000 Reserved - 900 unreserved 100. Will get throttled if it exceeds concurrency limit AWS_PROXY in API gateway is primarily used for Lambda proxy integration. A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. Lambda authorizer can be used for custom authorization scheme. 2 types: Token based. Request parameter based Lambda authorizer. Lambda deployment configuration: HalfAtATime Canary Linear. AWS Lambda compute platform deployments cannot use an in-place deployment type Increasing memory in lambda will increase CPU in lambda Lambda Versioning: By default, each AWS Lambda function has a single current version of the code. Clients of Lambda function can call a specific version or at the latest implementation Lambda Alias: You can create one or more aliases for our AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency Lambda Layer - Layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package Amazon DynamoDB is integrated with AWS Lambda so that you can trigger pieces of code that automatically respond to events in DynamoDB Streams. AWSLambdaDynamoDBExecutionRole is required to enable Lambda to work with DynamoDB API Gateway - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. Integrating Cloud Watch Events with lambda can be used for scheduling events If there is an incompatible output returned from a Lambda proxy integration backend, it will return 502 To resolve lambda throttled exception when using Cognito events, perform retry on sync. Lambda Event hook running order: start -\u003e BeforeAllowTraffic -\u003e AllowTraffic -\u003e After AllowTraffic -\u003e End AWS Lambda runs function code securely within a VPC b default. To enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC Lambda Asynchronous invocation can be triggered by Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config. Lambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html Lambda provides 500 MB of additional disk space as a workspace. Lambda logs all stout for a lambda function to CloudWatch Logs. Any additional logging calls used in the function will also be sent to CloudWatch Logs. To connect to a VPC, lambda function execution role must have the following permissions: ec2:Create Networkinterface, ec2:DescribeNetworkinterfaces, ec2:Delete Networkinterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy When lambda execution is hit by concurrency limit, you need to request AWS to increase concurrency limit For stream-based services like Dynamo b streams, that don’t invoke Lambda functions directly, the event source mapping configuration should be made on the Lambda side. A deployment package is a ZIP archive that contains your function code and dependencies. You can unload the package directly to lambda. Or you can use an Amazon S3 bucket and then upload it to lambda. If the deployment package is larger than 50 MB. you must use Amazon 53 Lambda can incur a first run penalty also called cold starts. Cold starts can cause slower than expected behavior on infrequently run functions or functions with high concurrency demands Price Price\nPrice x86\n0.000016667 USD per gigabyte-second 0,20 USD per 1 million requests Arm price\n0,0000133334 USD for each gigabyte-second 0,20 USD for 1 million queries Practice In the AWS Management Console search bar, type Lambda and select Lambda under “Services”:\nhttps://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#\nOn page Functions click Create a function\nAuthor from scratch is selected and enter the following values in the bottom form:\nFunction name: *MyCustomFunc Runtime: Node.js 16.X I select this section because I use the cloudacademy account. This role gives you permission to create functions\nPermissions: Change default execution role. Execution Role: Select Use an existing role. Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole Create function I’m writing a function to view the log, I’ll add a print to the terminal. And I’ll also add processing of the message I receive (In the next step in the testing section)\nThe function takes as an object event which contains an array of Records. On the 1st (0) position the object Sns (name of the service SNS Notifications).\nIn the object itself there will be 2 values:\ncook_secs - cooking time (microwave) req_secs - cooking time (prepare) console.log('Loading function'); exports.handler = function(event, context) { console.log(JSON.stringify(event, null, 2)); const message = JSON.parse(event.Records[0].Sns.Message); if (message.cook_secs \u003c message.req_secs) { if (message.pre) { context.succeed(\"User ended \" + message.pre + \" preset early\"); } else { context.succeed(\"User ended custom cook time early\"); } } context.succeed(); }; Deploy Test This functionality allows you to test how the function reacts to certain events. Let’s try to add an event from SNS Notifications.\nLet’s choose from the list\nWe get a template in which we make some changes, adjust the field Message - the one that we will process in our function.\nField Message - string, so our object will need to be wrapped in quotes\nTo make the handler understand that we put quotation marks inside quotation marks, we must put a special symbol \\ before the quotation mark.\nFinally we update one line and save it - Create\nNow we click the Test button.\nSince cook_secs in our event was less than req_secs, the function printed the first condition, and below in Function Logs we see the message that we print when we initialize the Loading function\nQuestions Q1 When working with a published version of the AWS Lambda function, you should note that the _____.\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager\nC\nQ2 A developer is building a streamlined development process for Lambda functions related to S3 storage.The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\nWhich of the following AWS solutions offers is best suited for this objective?\nAWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model\n2\nQ3 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\nQ4 A developer is designing a web application that allows the users to post comments and receive a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ5 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\nResources Community posts ","description":"A step-by-step guide to AWS Lambda","title":"Lambda","uri":"/en/tracks/aws-certified-developer-associate/lambda/"},{"content":"AI The curated list of AI tools for marketing Discover the newest AIs for any given task Create 1,000 AI art images a day for free openai examples openai answers from file AI-powered video summaries Chrome extension Free AI-generated Stock Images of People Soft/Architecture Block Diagram Maker Theme for Docs - MkDocs Media Fast and simple way to visualize your story - free 1 Storyboard and 10 Frames Video from Screenshot Other Create and sell your own personalized books and journals Books nice covers IL https://www.cashcow.co.il/ интернет магазин\n","description":"","title":"Links","uri":"/en/p/links/"},{"content":"Python To set up Python with Black and Flake8, you will need a couple of configuration files: pyproject.toml for Black, and .flake8 for Flake8.\nBlack is a code formatter for Python. It reformats your entire file in place according to the Black code style, which is a strict subset of PEP 8. Black makes code review faster by producing the smallest diffs possible.\nFlake8 is a powerful tool that checks your Python code against some of the style conventions in PEP 8. It combines the capabilities of several other tools including:\nPyFlakes: Checks for logical errors. pycodestyle: Checks for the format errors. Ned Batchelder’s McCabe script: Checks the complexity of your code. Step 1: Install Python python --version Step 2: Install Black and Flake8 pip install black pip install flake8 Step 3: Install Python Extension for VS Code Go to the Extensions view in VS Code (View -\u003e Extensions), search for “Python,” and then install it.\nStep 4: Configure Black and Flake8 In the pyproject.toml file, you have the following settings:\nline-length: This is the maximum line length that Black will enforce. You’ve set it to 130. target-version: This specifies the Python versions your project supports. Black may change the way it formats code depending on the Python version targetted. include and exclude: These options define the files Black should format and exclude from formatting, respectively. Create a pyproject.toml file in your project root directory with the following content for Black:\n[tool.black] line-length = 130 target-version = ['py38'] include = '\\.pyi?$' exclude = ''' /( \\.git | \\.venv | \\.eggs | \\.hg | \\.mypy_cache | \\.nox | \\.tox | \\.venv | _build | buck-out | build | dist )/ ''' And a .flake8 file with the following content for Flake8:\n[flake8] max-line-length = 150 ignore = E203, E266, E501, W503, F403, F401 max-complexity = 18 select = B,C,E,F,W,T4,B9 max-complexity: This is complexity checker setting. It’s a measure of the complexity of your functions, methods, and classes. The lower this number, the less complex your code is allowed to be. Step 5: Configure VS Code Settings Go to the Settings in VS Code (File -\u003e Preferences -\u003e Settings) and search for “Python Formatting Provider.” Select “black” from the dropdown list.\nTo set Flake8 as the linter, search for “Python Linting” in the settings and select “flake8.”\nNow, VS Code will automatically use Black and Flake8 to format and lint your Python code, respectively.\nStep 6: Format on file save To run the formatter each time you save a Python file, you’ll need to modify the VS Code settings. Here’s how:\nOpen the Command Palette with Ctrl+Shift+P MacOs(Cmd+Shift+P). Type “Preferences: Open Settings (JSON)” and select it. Choose Default Settings or Workspace Settings depend on global or project specific setup. For global “Search for “Editor: Format On Save” in Command Palette search bar and check the box to enable it. Add the following lines in the JSON file for local project/workspace setup: \"python.editor.formatOnSave\": true, \"python.formatting.provider\": \"black\" This tells VS Code to run the Python formatter (which you’ve set to Black) every time you save a Python file.\nYou can do the same with Flake8 by enabling linting on save:\n\"python.linting.flake8Enabled\": true, \"python.linting.lintOnSave\": true Groovy with CodeNarc Step 1: Install Groovy Ensure you have Groovy installed on your system. You can verify the installation by running the following command in your terminal:\ngroovy --version Step 2: Install the Groovy Extension for VS Code In your build.gradle file, add:\nStep 3: Set Up CodeNarc apply plugin: 'codenarc' dependencies { codenarc 'org.codenarc:CodeNarc:1.6' } codenarc { configFile = file('config/codenarc/rules.groovy') } Create a config/codenarc/rules.groovy file in your project root directory and add the following content:\nruleset { LineLength { enabled = true priority = 1 maximumLineLength = 150 } } Now, when you run your Gradle build, CodeNarc will check your Groovy files against the rules defined in config/codenarc/rules.groovy.\nJS/TypeScript with ESLint and Prettier Step 1: Install Node.js and npm Before you install ESLint and Prettier, ensure you have Node.js and npm installed on your system. You can verify the installation by running the following commands in your terminal:\nnode --version npm --version Step 2: Install ESLint and Prettier You can install ESLint and Prettier as devDependencies in your project by running the following command in your terminal:\nnpm install eslint prettier --save-dev Step 3: Install the ESLint and Prettier Extensions for VS Code Search for ESLint and Prettier - Code formatter in the Extensions view in VS Code (View -\u003e Extensions) and install them.\nStep 4: Configure ESLint and Prettier To configure ESLint and Prettier, you need to create two configuration files, .eslintrc.json for ESLint, and .prettierrc for Prettier, in your project root directory.\nFor example, you can create an .eslintrc.json file with the following content for ESLint:\n{ \"env\": { \"browser\": true, \"es6\": true }, \"extends\": \"eslint:recommended\", \"rules\": { \"indent\": [\"error\", 2], \"linebreak-style\": [\"error\", \"unix\"], \"quotes\": [\"error\", \"double\"], \"semi\": [\"error\", \"always\"] } } And a .prettierrc file with the following content for Prettier:\n{ \"semi\": true, \"trailingComma\": \"all\", \"singleQuote\": true, \"printWidth\": 80, \"tabWidth\": 2 } Step 5: Configure VS Code Settings Go to the Settings in VS Code (File -\u003e Preferences -\u003e Settings) and search for “Format On Save.” Check the box to enable it.\nIn the settings, search for “Default Formatter” and select “Prettier - Code formatter” from the dropdown list.\nYou can also add these settings directly to your settings.json file:\n{ \"editor.formatOnSave\": true, \"editor.defaultFormatter\": \"esbenp.prettier-vscode\", \"editor.codeActionsOnSave\": { \"source.fixAll.eslint\": true }, \"[javascript]\": { \"editor.formatOnSave\": false, \"editor.defaultFormatter\": null }, \"[javascriptreact]\": { \"editor.formatOnSave\": false, \"editor.defaultFormatter\": null }, \"[typescript]\": { \"editor.formatOnSave\": false, \"editor.defaultFormatter\": null }, \"[typescriptreact]\": { \"editor.formatOnSave\": false, \"editor.defaultFormatter\": null } } These settings tell VS Code to run Prettier as the default formatter, and also to perform any auto-fixes that ESLint can handle on save.\nNow, VS Code will automatically use ESLint and Prettier to lint and format your JavaScript and TypeScript code, respectively.\n","description":"Setting Up Python and Groovy Linters and Formatters in Visual Studio Code","title":"Linters \u0026 Formatters Setup for Python, Groovy, JavaScript in VSCode","uri":"/en/posts/python-groovy-lint-format-setup/"},{"content":"MacBook Pro Specification 13-inch Apple M1 Pro M1 2020 16 GB RAM 512 GB SSD QWERTY = English/Hebrew macOS Monterey (Update always) Homebrew Install Homebrew as package manager for macOS:\n## paste in terminal and follow the instructions /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Update everything in Homebrew to recent version:\nbrew update Add additional source for casks:\nbrew tap homebrew/cask-versions Install GUI applications (read more about these in GUI Applications):\nbrew install --cask \\ appcleaner \\ dbeaver-community \\ deepl \\ disk-inventory-x \\ google-chrome \\ google-drive \\ grammarly \\ itsycal \\ firefox \\ visual-studio-code \\ all-in-one-messenger \\ sublime-text \\ docker \\ rectangle \\ discord \\ vlc \\ figma \\ grammarly \\ macx-youtube-downloader \\ notion \\ postman \\ tor-browser \\ transmission \\ utm \\ mongodb-compass \\ obs \\ spotify \\ iterm2 \\ rectangle \\ sublime-text \\ syncthing \\ viber \\ visual-studio-code \\ yandex-disk \\ zoom Install terminal applications (read more about these in Terminal Applications):\nbrew install \\ git \\ jupyterlab \\ ffmpeg \\ nvm Additional GUI Applications GUI Applications Google Chrome Google Chrome (web development, web browsing)\nPreferences\nset default browser always show bookmarks import bookmarks from previous machine Chrome Developer Tools\nNetwork -\u003e only “Fetch/XHR” Search Shortcuts. Add Shortucts for different search engines.\nchrome://settings/searchEngines Yandex, search only in Russia. Shortcut: vv url: https://yandex.ru/{yandex:searchPath}?text=%s\u0026{yandex:referralID}\u0026lr=101443\u0026rstr=-225 Youtube Shortcut: yy url: https://www.youtube.com/results?search_query=%s\u0026page={startPage?}\u0026utm_source=opensearch Chrome Extensions\nChatGPT for Search Engines - Show ChatGPT output on every search request DeepL Translate - AI translator DoubleSubs - dual subs on youtube/netflix + web translator Google Translate React Developer Tools Pocket - The easiest, fastest way to capture articles, videos, and more. Session Buddy (Manage Browser Tabs and Bookmarks) LanguageTool (multilingual grammar, style, and spell checker) RSS Feed Reader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) Inoreader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) 30 Seconds of Knowledge (random code snippet on a new tab) JSON Formatter picture-in-picture (youtube/video above other screens) Visual CSS Editor (Customize any website visually) Squish - AI-powered summary tool. Turn any body of text into a few sentences with one click. Zotero - Add/sync scientific PDF documents Video Downloader Plus Opus Guide (Step-by-step for instructions) Disk Inventory X Disk Inventory X (disk usage utility for macOS)\nDocker Docker (Docker, see setup)\nused for running databases (e.g. PostgreSQL, MongoDB) in container without cluttering the Mac Preferences enable “Use Docker Compose” Firefox Firefox (web development)\nVisual Studio Code Visual Studio Code (web development IDE)\nSettings / Synced\nSublime Text Sublime Text (editor)\nMaccy Maccy (clipboard manager)\nenable “Launch at Login” Rectangle Move and resize windows in macOS using keyboard shortcuts or snap areas\nhttps://rectangleapp.com OBS OBS (for video recording and live streaming)\nfor Native Mac Screen recorder Base (Canvas) 2880x1800 (Ratio: 16:10) Output 1728x1080 ### Spotify Spotify\nSoundcloud https://soundcloud.com Syncthing syncthing - Sync folders/files between devices. I use to backup all photos/video from mobile to PC\nTransmission Transmission (A torrent client that I use. Very minimal in its UI but very powerful and has all the features that I need)\nUTM UTM (Virtual machines UI using QEMU)\ndownload ubuntu for arm, doc On error with shared folder: Could not connect: Connection refused open in browser: http://127.0.0.1:9843/ For Debian install spice-webdavd for shared folder. https://packages.debian.org/search?keywords=spice-webdavd, https://github.com/utmapp/UTM/issues/1204 sudo apt install spice-vdagent spice-webdavd -y VLC VLC (video player)\nuse as default for video files Terminal Applications nvm nvm (node version manager)\njupyterlab jupyterlab (Jupyter - python development, fast code snippets)\njupyter notebook - to start jupyter notebook ffmpeg ffmpeg (Converting video and audio)\ncompress video:\nffmpeg -i input.mp4 -c:v libx264 -crf 23 -preset slow -c:a aac -b:a 192k output.mp4 # or ffmpeg -i input.mp4 output.avi convert video to .gif:\n- ffmpeg \\ -i input.mp4 \\ -ss 00:00:00.000 \\ -pix_fmt rgb24 \\ -r 10 \\ -s 960x540 \\ -t 00:00:10.000 \\ output.gif NVM for Node/npm The node version manager (NVM) is used to install and manage multiple Node versions. After you have installed it via Homebrew in a previous step, type the following commands to complete the installation:\necho \"source $(brew --prefix nvm)/nvm.sh\" \u003e\u003e ~/.zshrc source ~/.zshrc ## or alias ## zshsource Now install the latest LTS version on the command line:\nnvm install \u003clatest LTS version from https://nodejs.org/en/\u003e Afterward, check whether the installation was successful and whether the node package manager (npm) got installed along the way:\nnode -v \u0026\u0026 npm -v Update npm to its latest version:\nnpm install -g npm@latest And set defaults for npm:\nnpm set init-author-name \"Roman Kurnovskii\" npm set init-author-email \"you@example.com\" npm set init-author-url \"https://romankurnovskii.com\" If you are a library author, log in to npm too:\nnpm adduser That’s it. If you want to list all your Node.js installation, type the following:\nnvm list If you want to install a newer Node.js version, then type:\nnvm install \u003cversion\u003e --reinstall-packages-from=$(nvm current) nvm use \u003cversion\u003e nvm alias default \u003cversion\u003e Optionally install yarn if you use it as alternative to npm:\nnpm install -g yarn yarn -v If you want to list all globally installed packages, run this command:\nnpm list -g --depth=0 That’s it. You have a running version of Node.js and its package manager.\nOH MY ZSH MacOS already comes with zsh as default shell. Install Oh My Zsh for an improved (plugins, themes, …) experience. Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.\nInstall:\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" Update everything (e.g. plugins) in Oh My Zsh to recent version:\nomz update Install fonts for themes:\nbrew tap homebrew/cask-fonts brew install --cask font-hack-nerd-font iTerm2 Install theme Theme description\nbrew install romkatv/powerlevel10k/powerlevel10k echo \"source $(brew --prefix)/opt/powerlevel10k/powerlevel10k.zsh-theme\" \u003e\u003e~/.zshrc Enable suggestions git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions echo \"plugins=(zsh-autosuggestions)\" \u003e\u003e~/.zshrc Open new tab(CMD+T)/restart iTerm to proceed with theme setup.\nTerminal Script and Aliases Update .zprofile. The changes will take effect after restarting the terminal\nvi ~/.zprofile Automatic software updates Add script to zprofile that updates everything:\nUpdate, upgrade all and cleanup softwareupdate - system software update tool We can execute this command on strartup, but i prefer handle it. When I kick of upd command in terminal, it will update everythin I need:\nalias upd='brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i' Add aliases to latest versions pip \u0026 python\n# А # ❯ which pip # /opt/homebrew/bin/pip # ❯ which python # /opt/homebrew/anaconda3/bin//python # snippet creates python virtual env in current folder alias penv='python -m venv venv \u0026\u0026 source ./venv/bin/activate \u0026\u0026 pip install --upgrade pip \u0026\u0026 echo \"\\n\" \u003e\u003e requirements.txt \u0026\u0026 pip install -r requirements.txt \u0026\u0026 pip freeze \u003e requirements_freeze.txt \u0026\u0026 echo \"venv/\" \u003e\u003e .gitignore' Links https://www.robinwieruch.de/mac-setup-web-development/ https://sourabhbajaj.com/mac-setup/iTerm/ack.html https://www.engineeringwithutsav.com/blog/spice-up-your-macos-terminal ","description":"How I set up my M1 MacBook Pro software development...","title":"Mac Setup 2022","uri":"/en/posts/mac-setup-development/"},{"content":"About The Moving Average Convergence Divergence (MACD) is a versatile trading indicator used in various forms of trading, including high-frequency trading (HFT).\nMACD is a trend-following momentum indicator that shows the relationship between two moving averages of a security’s price.\nCalculating Formula The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA. A 9-day EMA of the MACD, called the “signal line”, is then plotted on top of the MACD, functioning as a trigger for buy and sell signals.\nHere’s the formula for the MACD:\nMACD = 12-Period EMA − 26-Period EMA To calculate the signal line, you take the 9-period EMA of the MACD.\nSignal Line = 9-Period EMA of MACD Example:\nSince MACD requires 26 periods to start, we’ll calculate from the 26th minute:\nMinute Open High Low Close EMA12 EMA26 EMA9 (Signal) MACD 1 $10.0 $11.0 $9.5 $10.0 - - - - 2 $10.1 $12.1 $10.0 $12.0 - - - - 3 $12.2 $15.2 $12.0 $15.0 - - - - 4 $15.1 $15.1 $13.9 $14.0 - - - - 5 $14.1 $16.1 $14.0 $16.0 - - - - 6 $16.1 $16.1 $14.9 $15.0 - - - - 7 $15.1 $17.1 $15.0 $17.0 - - - - 8 $17.1 $17.1 $15.9 $16.0 - - - - 9 $16.1 $18.1 $16.0 $18.0 - - Starts here - 10 $18.1 $18.1 $16.9 $17.0 - - - 11 $17.1 $19.1 $17.0 $19.0 - - - 12 $19.1 $19.1 $17.9 $18.0 Starts here - - 13 $18.1 $20.1 $18.0 $20.0 - - 14 $20.1 $21.1 $20.0 $21.0 - - 15 $21.1 $21.1 $19.9 $20.0 - - 16 $20.1 $22.1 $20.0 $21.0 - - 17 $21.1 $23.1 $21.0 $22.0 - - 18 $22.1 $24.1 $22.0 $23.0 - - 19 $23.1 $25.1 $23.0 $24.0 - - 20 $24.1 $26.1 $24.0 $25.0 - - 21 $25.1 $27.1 $25.0 $26.0 - - 22 $26.1 $28.1 $26.0 $27.0 - - 23 $27.1 $29.1 $27.0 $28.0 - - 24 $28.1 $30.1 $28.0 $29.0 - - 25 $29.1 $31.1 $29.0 $30.0 - - 26 $30.1 $32.1 $30.0 $31.0 Starts here Starts here 27 $31.1 $33.1 $31.0 $32.0 28 $32.1 $34.1 $32.0 $33.0 29 $33.1 $35.1 $33.0 $34.0 30 $34.1 $36.1 $34.0 $35.0 In python it can be calculated in the following way:\nimport pandas as pd # Assuming 'df' is your DataFrame and 'Close' is the column with closing prices df['EMA12'] = df['Close'].ewm(span=12, adjust=False).mean() df['EMA26'] = df['Close'].ewm(span=26, adjust=False).mean() df['MACD'] = df['EMA12'] - df['EMA26'] df['Signal'] = df['MACD'].ewm(span=9, adjust=False).mean() Pros and Cons Pros:\nTrend Identification: MACD can identify the start of a trend, providing good entry points. Signal Line Crossovers: The MACD’s signal line can provide clear buy and sell signals. Cons:\nFalse Signals: Like any indicator, MACD can produce false signals, particularly in volatile markets. Lag: MACD can sometimes lag behind the market because it’s a trend-following indicator. It may not work well in ranging (non-trending) markets, where price movements can be random. Example of signals True Positive:\nA true positive in MACD is a situation where the MACD line crosses above the signal line and the price goes up, indicating a strong bullish signal, or the MACD line crosses below the signal line and the price goes down, indicating a strong bearish signal.\nThese signals can help traders decide when to enter or exit trades.\nFalse Positive:\nA false positive in MACD is typically a situation where the MACD line crosses above the signal line (potential buy signal), but the price does not go up, or the MACD line crosses below the signal line (potential sell signal), but the price does not go down. It’s important to confirm MACD signals with other indicators or patterns to avoid false positives.\nUse in Real Trading In a real trading scenario, traders often use MACD in conjunction with other indicators to confirm signals and minimize risks. For example, a trader may use the Relative Strength Index (RSI) together with MACD.\nIf MACD gives a buy signal (MACD line crosses above the signal line), and RSI is below 30 (indicating an oversold condition), the trader may consider this as a strong buy signal.\nPython Implementation Click here to view this notebook in full screen ","description":"A comprehensive guide to the Moving Average Convergence Divergence (MACD) trading indicator.","title":"MACD - Moving Average Convergence Divergence","uri":"/en/posts/trading-indicators/macd/"},{"content":"This article offers an example of the basic Markdown syntax that can be used and also shows whether the basic elements of HTML are decorated with CSS.\nHeaders Header 1 ======== Header 2 -------- Header 1 Header 2 # h1 ## h2 ### h3 #### h4 ##### h5 ###### h6 h1 h2 h3 h4 h5 h6 Paragraph To insert an empty string, you need to put the word wrap symbol twice (press Enter)\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit! Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae. Images ![Image alt text](/path/to/img.jpg) ![Image alt text](/path/to/img.jpg \"title\") ![Image alt text][img] [img]: http://foo.com/img.jpg Emphasis *italic* _italic_ **bold** __bold__ ***bold italic*** ___bold italic___ ~~strikethrough~~ `code` italic italic\nbold bold bold italic bold italic\nstrikethrough\ncode\nLinks [link](http://google.com) [link][google] [google]: http://google.com \u003chttp://google.com\u003e Blockquotes The blockquote element represents the content that is quoted from another source, optionally with a quotation that must be in the element footer or cite, and optional line changes such as annotations and abbreviations.\nBlock quote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Notethat you can use the syntax Markdown inside the block quote.\nBlock quote with authorship Don’t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n\u003eThis is an example quote, \u003ein which before each line \u003eangle bracket is used. \u003eThis is an example quote, in which the corner bracket is placed only before the beginning of the new paragraph. \u003eSecond paragraph. This is an example quote, in which before each line angle bracket is used.\nThis is an example quote, in which the corner bracket is placed only before the beginning of the new paragraph. Second paragraph.\n\u003e Level One Citation \u003e\u003e Second Level Citation \u003e\u003e\u003e Third Level Citation \u003e \u003eLevel One Citation Level One Citation\nSecond Level Citation\nThird Level Citation\nLevel One Citation\nTables | Name | Age | | ----- | --- | | Bob | 27 | | Alice | 23 | Name Age Bob 27 Alice 23 The cells in the delimitation row use only symbols - and :. The symbol : is placed at the beginning, at the end, or on both sides of the cell contents of the dividing row to indicate the alignment of the text in the corresponding column on the left, right side, or center.\n| Column on the left | Column on the right | Column on the center | | :----------------- | ------------------: | :------------------: | | Text | Text | Text | Column on the left Column on the right Column on the center Text Text Text Markdown inside the table | Italics | Bold | Code | | --------- | -------- | ------ | | *italics* | **bold** | `code` | Italics Bold Code italics bold code Code Blocks Code block with inverted quotes \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code block with four spaces indent \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Code Unit with Hugo Internal Shorted Backlight \u003c!doctype html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003ctitle\u003eExample HTML5 Document\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eTest\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e Lists * Item 1 * Item 2 - Item 1 - Item 2 - [ ] Checkbox off - [x] Checkbox on 1. Item 1 2. Item 2 Item 1\nItem 2\nItem 1\nItem 2\nCheckbox off\nCheckbox on\nItem 1 Item 2 Make the headers uniform. At the end of the title, do not put a point.\nCorrect Wrong Getting the Creating a Cluster Get the Creating a Cluster Get Create Cluster If you want to describe the sequence of actions, use the numbered list. At the end of the lines, put a period.\nIf the order of items is not important, use the marked list. Make it one of the ways:\nIf the entries in the list are separate sentences, start them with a capital letter and put a period at the end. If the introductory phrase and the list make up one sentence, the entries in the list should start with a lowercase letter and end with a semicolon. The last list item ends with a dot. If the list consists of parameter names or values (without explanation), do not put characters at the end of lines. Ordered list First item Second item Third item To create an ordered numbered list, use the digits with the symbol . or ). The recommended markup format is 1 and ..\n1. First item 1. Second item 1. Third item First item Second item Third item To create a nested ordered list, add a indent to the entries in the child list. The allowed indentation is from two to five spaces. The recommended indent size is four spaces.\nFor example, markup:\n1. First paragraph 1. Sub-paragraph 1. Sub-paragraph 1. Second paragraph First paragraph Sub-paragraph Sub-paragraph Second paragraph Unordered list List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other elements - abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nMost \u003cmark\u003esalamanders\u003c/mark\u003e are nocturnal 💡 Data structure is a container that stores data in a specific format. This container decides how the outside world can read or change this data.\nMath LaTeX syntax Math Element Code Pronunciation $x_{12345}$ $x_{12345}$ $\\quad x_12$ $\\quad x_12$ $a_{ij}b_{kl}=\\delta_{i}^{l}$ $a_{ij}b_{kl}=\\delta_{i}^{l}$ $\\alpha$ $\\alpha$ \\alpha $\\beta$ $\\beta$ \\beta $\\gamma$ $\\gamma$ \\gamma $\\delta$ $\\delta$ \\delta $\\epsilon$ $\\epsilon$ \\epsilon $\\pi$ $\\pi$ \\pi $\\theta$ $\\theta$ \\theta $\\lambda$ $\\lambda$ \\lambda $\\mu$ $\\mu$ \\mu $\\sigma$ $\\sigma$ \\sigma $\\omega$ $\\omega$ \\omega $\\Gamma$ $\\Gamma$ \\Gamma $\\Delta$ $\\Delta$ \\Delta $\\Sigma$ $\\Sigma$ \\Sigma $\\Theta$ $\\Theta$ \\Theta $\\Omega$ $\\Omega$ \\Omega $\\infty$ $\\infty$ \\infty $\\sum$ $\\sum$ \\sum $\\prod$ $\\prod$ \\prod $\\int$ $\\int$ \\int $\\oint$ $\\oint$ \\oint $\\partial$ $\\partial$ \\partial $\\nabla$ $\\nabla$ \\nabla $\\pm$ $\\pm$ \\pm $\\times$ $\\times$ \\times $\\div$ $\\div$ \\div $\\approx$ $\\approx$ \\approx $\\geq$ $\\geq$ \\geq $\\leq$ $\\leq$ \\leq $\\neq$ $\\neq$ \\neq Resources Style from Google The above quote is taken from Rob Pike’s book talk during Gopherfest, November 18, 2015. ↩︎\n","description":"Markdown cheatsheet","title":"Markdown Cheat Sheet","uri":"/en/posts/markdown-syntax/"},{"content":"Stage 1: Implementation and Greedy 1809A - Garland (implementation, 800) 1807A - Plus or Minus (implementation, 800) 1807B - Grab the Candies (greedy, 800) 1807C - Find and Replace (greedy, implementation, strings, 800) 1798A - Showstopper (greedy, implementation, sortings, 800) 1799A - Recent Actions (data structures, greedy, implementation, math, 800) 1788A - One and Two (brute force, implementation, math, 800) 1778A - Flip Flop Sum (greedy, implementation, 800) 1772A - A+B? (implementation, 800) 1796B - Asterisk-Minor Template (implementation, strings, 1000) Stage 2: Combinatorics and Geometry 1777A - Everybody Likes Good Arrays! (greedy, math, 800) 1787A - Exponential Equation (constructive algorithms, math, 800) 1777B - Emordnilap (combinatorics, greedy, math, 900) 1773F - Football (constructive algorithms, 800) 1774A - Add Plus Minus Sign (constructive algorithms, math, 800) 1776A - Walking Boy (greedy, 800) 1776L - Controllers (binary search, math, 1500) ","description":"Codeforces contests solutions","title":"Plan","uri":"/en/tracks/algorithms-101/codeforces/plan/"},{"content":"Q1 # Problem Difficulty Topics 1 1. Two Sum Easy Array, Hash Table 2 13. Roman to Integer Easy Hash Table, Math, String 3 14. Longest Common Prefix Easy String 4 20. Valid Parentheses Easy String, Stack 5 21. Merge Two Sorted Lists Easy Linked List, Recursion 6 26. Remove Duplicates from Sorted Array Easy Array, Two pointers 7 66. Plus One Easy Array, Math 8 69. Sqrt(x) Easy Math, Binary Search, 9 70. Climbing Stairs Easy Math, Dynamic Programming, Memoization 10 88. Merge Sorted Array Easy Array, Two pointers, Sorting 11 94. Binary Tree Inorder Traversal Easy Stack, Tree, Depth-First Search, Binary Tree 12 2. Add Two Numbers Medium Linked List, Math, Recursion 13 3. Longest Substring Without Repeating Characters Medium Hash Table, String, Sliding Window 14 5. Longest Palindromic Substring Medium String, Dynamic Programming 15 7. Reverse Integer Medium Math 16 11. Container With Most Water Medium Array, Two pointers, Greedy 17 15. 3Sum Medium Array, Two pointers, Sorting 18 17. Letter Combinations of a Phone Number Medium Hash Table, String, Backtracking 19 19. Remove Nth Node From End of List Medium Linked List, Two pointers 20 22. Generate Parentheses Medium String, Dynamic Programming, Backtracking Intermediate results Appeared intuitive understanding of algorithms. In most cases, one hour is not enough to solve the problem. If you start sketching an intuitive algorithm, then in the process comes an understanding and an improved solution.\nUpdate plan by solution:\nAfter reading, if there is no exact solution: Assume/analyze/draw the proposed algorithm View solutions with explanations Compare with your own / analyze Code Q2 # Problem Difficulty Topics 21 28. Find the Index of the First Occurrence in a String Medium String, Two pointers, String Matching 22 29. Divide Two Integers Medium Math, Bit Manipulation 23 33. Search in Rotated Sorted Array Medium Array, Binary Search, 24 34. Find First and Last Position of Element in Sorted Array Medium Array, Binary Search, 25 36. Valid Sudoku Medium Array, Hash Table, Matrix 26 38. Count and Say Medium String 27 46. Permutations Medium Array, Backtracking 28 48. Rotate Image Medium Array, Math, Matrix 29 49. Group Anagrams Medium Array, Hash Table, String, Sorting 30 50. Pow(x, n) Medium Math, Recursion 31 53. Maximum Subarray Medium Array, Divide and Conquer, Dynamic Programming Intermediate results Revise training tactics. Prepare list of top coding patterns.\nPractice on each coding pattern.\nQ3 # Problem Difficulty Topics 32 55. Jump Game Medium Array, Greedy, Dynamic Programming 33 56. Merge Intervals Medium Array, Sorting, 34 62. Unique Paths Medium Math, Dynamic Programming, Combinatorics] 35 73. Set Matrix Zeroes Medium Array, Hash Table, Matrix 36 75. Sort Colors Medium Array, Two Pointers, Sorting 37 78. Subsets Medium Array, Backtracking, Bit Manipulation Started participating in contests.\nLinks Top Interview 150 LeetCode Companies problems ","description":"","title":"Plan","uri":"/en/tracks/algorithms-101/plan/"},{"content":" Docs EN | RU Posts EN | RU ","description":"","title":"Posts Archive","uri":"/en/posts/archive/"},{"content":"Lab Process Amazon SNS Notifications with AWS Lambda Creating an Amazon SNS Topic 1. In the AWS Management Console search bar, enter SNS, and click the Simple Notification Service result under Services:\nIn the left-hand side menu, click Topics: If you can’t see the left-hand menu, to expand it, click the following:\nClick Create topic: In the Create topic form, ensure to have selected the Standard type, and enter the following values accepting the defaults for values not specified: Name: lab-topic You can leave the Display name field empty for this Lab. When you create topics where the recipients receive messages over SMS (Short Message Service) you are required to provide a value.\nAt the bottom of the form, click Create topic: Creating an AWS Lambda Function 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\nYou will see the Functions list page.\nClick Create function: In the Create function form, ensure Author from scratch is selected: In the Create function form, enter lab-function in the Function name field: In the Create function form, in the Runtime drop-down, select Python 3.8: In the Create function form, click Change default execution role and select Use an existing role: In the Existing role drop-down, select lambda_s3_put: The role you have selected has been pre-populated for this Lab. Usually when using Lambda you will create a specific role for your function.\nTo create your function, click Create function: Implementing an AWS Lambda Function to Upload to S3 Scroll down to the Code source section and double-click lambda_function.py.\nIn the code editor, replace the contents with the following Python code:\nfrom datetime import datetime import boto3 account_id = boto3.client('sts').get_caller_identity()[\"Account\"] s3 = boto3.resource('s3') def lambda_handler(event, context): record = event['Records'][0]['Sns'] message = record['Message'] subject = record['Subject'] print(\"Subject: %s\" % subject) print(\"Message: %s\" % message) s3.Object(f\"sns-lab-bucket-{account_id}\", subject).put(Body=message) return \"SUCCESS\" The function code you entered processes a message from SNS. The code uploads a file into an S3 Bucket which was pre-created as a part of this lab. The name of the file will be the subject of the message and the content of the file will be the message body.\nYou can use a Lambda function to do many different things. Some examples include:\nProcess web-requests Put a custom metric into AWS CloudWatch Add or update a record in a database Post a web-request to an external service To save your changes and deploy your function, at the top of the Code source section, click Deploy: You will see a notification that your function has been deployed:\n4. To add an SNS trigger, in the Function overview section, click Add trigger:\n​\nIn the Select a trigger dropdown, enter SNS, and click the SNS result: ​\nIn the SNS topic drop-down, select lab-topic: ​\nThe SNS topic field will be filled with the ARN (Amazon Resource Name) of your SNS topic.\nTo add your SNS trigger, click Add: ​\nYou will see a notification that your trigger has been added:\nIn SNS terminology, by adding an SNS trigger you have “subscribed” your Lambda function to the SNS topic.\nPublishing a Message to an Amazon SNS Topic Navigate to the AWS SNS service.\nIn the left-hand side menu, click Topics:\nIn the list of topics, click lab-topic: Click Publish message: In the Message details section of the form, in the Subject field, enter lab-subject: In the Message body section of the form, in the Message body to send to the endpoint textbox, enter Lab Message: Usually when you publish a message to an SNS topic, you would include meaningful data in the message body. The content of the message body is often called the “payload” of a message. In SNS, the payload can be plain text, or it can be a structured payload such as JSON, XML, or some other format. The service or device subscribed to your topic can use the data in the payload to determine what action to take in response to receiving a message.\nTo publish your message, click Publish message: You will see a notification, similar to the following, confirming your message has been published:\nVerifying the AWS Lambda Function Processed the Message 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\nIn the list of S3 Buckets, click the Bucket beginning with sns-lab-bucket-: In the list of objects you will see a file called lab-subject: This file was uploaded to the S3 bucket by your Lambda function.\n","description":"Process Amazon SNS Notifications with AWS Lambda","title":"Process Amazon SNS Notifications with AWS Lambda","uri":"/en/tracks/aws-certified-developer-associate/sns/aws-lambda-sns-notifications/"},{"content":"« Left Shift Moves the bits of its first operand to the left by the number of places specified in its second operand.\nFormula: $a « n = a * 2^n$\nShifting a single bit to the left by one place doubles its value. Shifting to two places to the left by one place quadruple its value. \u003e\u003e\u003e 100 \u003c\u003c 1 200 \u003e\u003e\u003e 100 \u003c\u003c 2 400 \u003e\u003e\u003e 100 \u003c\u003c 3 800 » Right Shift The rightmost bits always get dropped. Every time you shift a bit to the right by one position, you halve its underlying value. Formula: $a » n = [a/2^n]$\n\u003e\u003e\u003e 100 \u003e\u003e 1 50 \u003e\u003e\u003e 100 \u003e\u003e 2 25 \u003e\u003e\u003e 100 \u003e\u003e 3 12 \u003e\u003e\u003e 5 \u003e\u003e 10 0 the right shift operator automatically floors the result. \u003e\u003e\u003e 5 \u003e\u003e 1 # Bitwise right shift 2 \u003e\u003e\u003e 5 // 2 # Floor division (integer division) 2 \u003e\u003e\u003e 5 / 2 # Floating-point division 2.5 \u003e\u003e\u003e -2 \u003e\u003e 5 -1 \u0026 Operator 0 \u0026 0 = 0 0 \u0026 1 = 0 1 \u0026 0 = 0 1 \u0026 1 = 0 For numbers:\n27 \u0026 23\nCovert to binary\n27 -\u003e 11011 23 -\u003e 10111\nturns to (in binary)\n11011 \u0026 10111 = 10011 -\u003e 19\n27 \u0026 23 = 19\nResources https://realpython.com/python-bitwise-operators/ ","description":"Python bitwise operators","title":"Python bitwise operators","uri":"/en/posts/python-bitwise-operators/"},{"content":"def sum_of_digits(n): sum = 0 while n: sum += n % 10 n //= 10 return sum # Single line comments start with a number symbol. \"\"\" Multiline strings can be written using three \"s, and are often used as documentation. \"\"\" #################################################### ## 1. Primitive Datatypes and Operators #################################################### # You have numbers 3 # =\u003e 3 # Math is what you would expect 1 + 1 # =\u003e 2 8 - 1 # =\u003e 7 10 * 2 # =\u003e 20 35 / 5 # =\u003e 7.0 # Integer division rounds down for both positive and negative numbers. 5 // 3 # =\u003e 1 -5 // 3 # =\u003e -2 5.0 // 3.0 # =\u003e 1.0 # works on floats too -5.0 // 3.0 # =\u003e -2.0 # The result of division is always a float 10.0 / 3 # =\u003e 3.3333333333333335 # Modulo operation 7 % 3 # =\u003e 1 # i % j have the same sign as j, unlike C -7 % 3 # =\u003e 2 # Exponentiation (x**y, x to the yth power) 2**3 # =\u003e 8 # Enforce precedence with parentheses 1 + 3 * 2 # =\u003e 7 (1 + 3) * 2 # =\u003e 8 # Boolean values are primitives (Note: the capitalization) True # =\u003e True False # =\u003e False # negate with not not True # =\u003e False not False # =\u003e True # Boolean Operators # Note \"and\" and \"or\" are case-sensitive True and False # =\u003e False False or True # =\u003e True # True and False are actually 1 and 0 but with different keywords True + True # =\u003e 2 True * 8 # =\u003e 8 False - 5 # =\u003e -5 # Comparison operators look at the numerical value of True and False 0 == False # =\u003e True 1 == True # =\u003e True 2 == True # =\u003e False -5 != False # =\u003e True # None, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False. # All other values are True bool(0) # =\u003e False bool(\"\") # =\u003e False bool([]) # =\u003e False bool({}) # =\u003e False bool(()) # =\u003e False bool(set()) # =\u003e False bool(4) # =\u003e True bool(-6) # =\u003e True # Using boolean logical operators on ints casts them to booleans for evaluation, but their non-cast value is returned # Don't mix up with bool(ints) and bitwise and/or (\u0026,|) bool(0) # =\u003e False bool(2) # =\u003e True 0 and 2 # =\u003e 0 bool(-5) # =\u003e True bool(2) # =\u003e True -5 or 0 # =\u003e -5 # Equality is == 1 == 1 # =\u003e True 2 == 1 # =\u003e False # Inequality is != 1 != 1 # =\u003e False 2 != 1 # =\u003e True # More comparisons 1 \u003c 10 # =\u003e True 1 \u003e 10 # =\u003e False 2 \u003c= 2 # =\u003e True 2 \u003e= 2 # =\u003e True # Seeing whether a value is in a range 1 \u003c 2 and 2 \u003c 3 # =\u003e True 2 \u003c 3 and 3 \u003c 2 # =\u003e False # Chaining makes this look nicer 1 \u003c 2 \u003c 3 # =\u003e True 2 \u003c 3 \u003c 2 # =\u003e False # (is vs. ==) is checks if two variables refer to the same object, but == checks # if the objects pointed to have the same values. a = [1, 2, 3, 4] # Point a at a new list, [1, 2, 3, 4] b = a # Point b at what a is pointing to b is a # =\u003e True, a and b refer to the same object b == a # =\u003e True, a's and b's objects are equal b = [1, 2, 3, 4] # Point b at a new list, [1, 2, 3, 4] b is a # =\u003e False, a and b do not refer to the same object b == a # =\u003e True, a's and b's objects are equal # Strings are created with \" or ' \"This is a string.\" 'This is also a string.' # Strings can be added too \"Hello \" + \"world!\" # =\u003e \"Hello world!\" # String literals (but not variables) can be concatenated without using '+' \"Hello \" \"world!\" # =\u003e \"Hello world!\" # A string can be treated like a list of characters \"Hello world!\"[0] # =\u003e 'H' # You can find the length of a string len(\"This is a string\") # =\u003e 16 # You can also format using f-strings or formatted string literals (in Python 3.6+) name = \"Reiko\" f\"She said her name is {name}.\" # =\u003e \"She said her name is Reiko\" # You can basically put any Python expression inside the braces and it will be output in the string. f\"{name} is {len(name)} characters long.\" # =\u003e \"Reiko is 5 characters long.\" # None is an object None # =\u003e None # Don't use the equality \"==\" symbol to compare objects to None # Use \"is\" instead. This checks for equality of object identity. \"etc\" is None # =\u003e False None is None # =\u003e True # None, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False. # All other values are True bool(0) # =\u003e False bool(\"\") # =\u003e False bool([]) # =\u003e False bool({}) # =\u003e False bool(()) # =\u003e False bool(set()) # =\u003e False #################################################### ## 2. Variables and Collections #################################################### # Python has a print function print(\"I'm Python. Nice to meet you!\") # =\u003e I'm Python. Nice to meet you! # By default the print function also prints out a newline at the end. # Use the optional argument end to change the end string. print(\"Hello, World\", end=\"!\") # =\u003e Hello, World! # Simple way to get input data from console input_string_var = input(\"Enter some data: \") # Returns the data as a string # There are no declarations, only assignments. # Convention is to use lower_case_with_underscores some_var = 5 some_var # =\u003e 5 # Accessing a previously unassigned variable is an exception. # See Control Flow to learn more about exception handling. some_unknown_var # Raises a NameError # if can be used as an expression # Equivalent of C's '?:' ternary operator \"yay!\" if 0 \u003e 1 else \"nay!\" # =\u003e \"nay!\" # Lists store sequences li = [] # You can start with a prefilled list other_li = [4, 5, 6] # Add stuff to the end of a list with append li.append(1) # li is now [1] li.append(2) # li is now [1, 2] li.append(4) # li is now [1, 2, 4] li.append(3) # li is now [1, 2, 4, 3] # Remove from the end with pop li.pop() # =\u003e 3 and li is now [1, 2, 4] # Let's put it back li.append(3) # li is now [1, 2, 4, 3] again. # Access a list like you would any array li[0] # =\u003e 1 # Look at the last element li[-1] # =\u003e 3 # Looking out of bounds is an IndexError li[4] # Raises an IndexError # You can look at ranges with slice syntax. # The start index is included, the end index is not # (It's a closed/open range for you mathy types.) li[1:3] # Return list from index 1 to 3 =\u003e [2, 4] li[2:] # Return list starting from index 2 =\u003e [4, 3] li[:3] # Return list from beginning until index 3 =\u003e [1, 2, 4] li[::2] # Return list selecting every second entry =\u003e [1, 4] li[::-1] # Return list in reverse order =\u003e [3, 4, 2, 1] # Use any combination of these to make advanced slices # li[start🔚step] # Make a one layer deep copy using slices li2 = li[:] # =\u003e li2 = [1, 2, 4, 3] but (li2 is li) will result in false. # Remove arbitrary elements from a list with \"del\" del li[2] # li is now [1, 2, 3] # Remove first occurrence of a value li.remove(2) # li is now [1, 3] li.remove(2) # Raises a ValueError as 2 is not in the list # Insert an element at a specific index li.insert(1, 2) # li is now [1, 2, 3] again # Get the index of the first item found matching the argument li.index(2) # =\u003e 1 li.index(4) # Raises a ValueError as 4 is not in the list # You can add lists # Note: values for li and for other_li are not modified. li + other_li # =\u003e [1, 2, 3, 4, 5, 6] # Concatenate lists with \"extend()\" li.extend(other_li) # Now li is [1, 2, 3, 4, 5, 6] # Check for existence in a list with \"in\" 1 in li # =\u003e True # Examine the length with \"len()\" len(li) # =\u003e 6 # Tuples are like lists but are immutable. tup = (1, 2, 3) tup[0] # =\u003e 1 tup[0] = 3 # Raises a TypeError # Note that a tuple of length one has to have a comma after the last element but # tuples of other lengths, even zero, do not. type((1)) # =\u003e \u003cclass 'int'\u003e type((1,)) # =\u003e \u003cclass 'tuple'\u003e type(()) # =\u003e \u003cclass 'tuple'\u003e # You can do most of the list operations on tuples too len(tup) # =\u003e 3 tup + (4, 5, 6) # =\u003e (1, 2, 3, 4, 5, 6) tup[:2] # =\u003e (1, 2) 2 in tup # =\u003e True # You can unpack tuples (or lists) into variables a, b, c = (1, 2, 3) # a is now 1, b is now 2 and c is now 3 # You can also do extended unpacking a, *b, c = (1, 2, 3, 4) # a is now 1, b is now [2, 3] and c is now 4 # Tuples are created by default if you leave out the parentheses d, e, f = 4, 5, 6 # tuple 4, 5, 6 is unpacked into variables d, e and f # respectively such that d = 4, e = 5 and f = 6 # Now look how easy it is to swap two values e, d = d, e # d is now 5 and e is now 4 # Dictionaries store mappings from keys to values empty_dict = {} # Here is a prefilled dictionary filled_dict = {\"one\": 1, \"two\": 2, \"three\": 3} # Note keys for dictionaries have to be immutable types. This is to ensure that # the key can be converted to a constant hash value for quick look-ups. # Immutable types include ints, floats, strings, tuples. invalid_dict = {[1,2,3]: \"123\"} # =\u003e Raises a TypeError: unhashable type: 'list' valid_dict = {(1,2,3):[1,2,3]} # Values can be of any type, however. # Look up values with [] filled_dict[\"one\"] # =\u003e 1 # Get all keys as an iterable with \"keys()\". We need to wrap the call in list() # to turn it into a list. We'll talk about those later. Note - for Python # versions \u003c3.7, dictionary key ordering is not guaranteed. Your results might # not match the example below exactly. However, as of Python 3.7, dictionary # items maintain the order at which they are inserted into the dictionary. list(filled_dict.keys()) # =\u003e [\"three\", \"two\", \"one\"] in Python \u003c3.7 list(filled_dict.keys()) # =\u003e [\"one\", \"two\", \"three\"] in Python 3.7+ # Get all values as an iterable with \"values()\". Once again we need to wrap it # in list() to get it out of the iterable. Note - Same as above regarding key # ordering. list(filled_dict.values()) # =\u003e [3, 2, 1] in Python \u003c3.7 list(filled_dict.values()) # =\u003e [1, 2, 3] in Python 3.7+ # Check for existence of keys in a dictionary with \"in\" \"one\" in filled_dict # =\u003e True 1 in filled_dict # =\u003e False # Looking up a non-existing key is a KeyError filled_dict[\"four\"] # KeyError # Use \"get()\" method to avoid the KeyError filled_dict.get(\"one\") # =\u003e 1 filled_dict.get(\"four\") # =\u003e None # The get method supports a default argument when the value is missing filled_dict.get(\"one\", 4) # =\u003e 1 filled_dict.get(\"four\", 4) # =\u003e 4 # \"setdefault()\" inserts into a dictionary only if the given key isn't present filled_dict.setdefault(\"five\", 5) # filled_dict[\"five\"] is set to 5 filled_dict.setdefault(\"five\", 6) # filled_dict[\"five\"] is still 5 # Adding to a dictionary filled_dict.update({\"four\":4}) # =\u003e {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4} filled_dict[\"four\"] = 4 # another way to add to dict # Remove keys from a dictionary with del del filled_dict[\"one\"] # Removes the key \"one\" from filled dict # From Python 3.5 you can also use the additional unpacking options {'a': 1, **{'b': 2}} # =\u003e {'a': 1, 'b': 2} {'a': 1, **{'a': 2}} # =\u003e {'a': 2} # Sets store ... well sets empty_set = set() # Initialize a set with a bunch of values. Yeah, it looks a bit like a dict. Sorry. some_set = {1, 1, 2, 2, 3, 4} # some_set is now {1, 2, 3, 4} # Similar to keys of a dictionary, elements of a set have to be immutable. invalid_set = {[1], 1} # =\u003e Raises a TypeError: unhashable type: 'list' valid_set = {(1,), 1} # Add one more item to the set filled_set = some_set filled_set.add(5) # filled_set is now {1, 2, 3, 4, 5} # Sets do not have duplicate elements filled_set.add(5) # it remains as before {1, 2, 3, 4, 5} # Do set intersection with \u0026 other_set = {3, 4, 5, 6} filled_set \u0026 other_set # =\u003e {3, 4, 5} # Do set union with | filled_set | other_set # =\u003e {1, 2, 3, 4, 5, 6} # Do set difference with - {1, 2, 3, 4} - {2, 3, 5} # =\u003e {1, 4} # Do set symmetric difference with ^ {1, 2, 3, 4} ^ {2, 3, 5} # =\u003e {1, 4, 5} # Check if set on the left is a superset of set on the right {1, 2} \u003e= {1, 2, 3} # =\u003e False # Check if set on the left is a subset of set on the right {1, 2} \u003c= {1, 2, 3} # =\u003e True # Check for existence in a set with in 2 in filled_set # =\u003e True 10 in filled_set # =\u003e False # Make a one layer deep copy filled_set = some_set.copy() # filled_set is {1, 2, 3, 4, 5} filled_set is some_set # =\u003e False #################################################### ## 3. Control Flow and Iterables #################################################### # Let's just make a variable some_var = 5 # Here is an if statement. Indentation is significant in Python! # Convention is to use four spaces, not tabs. # This prints \"some_var is smaller than 10\" if some_var \u003e 10: print(\"some_var is totally bigger than 10.\") elif some_var \u003c 10: # This elif clause is optional. print(\"some_var is smaller than 10.\") else: # This is optional too. print(\"some_var is indeed 10.\") \"\"\" For loops iterate over lists prints: dog is a mammal cat is a mammal mouse is a mammal \"\"\" for animal in [\"dog\", \"cat\", \"mouse\"]: # You can use format() to interpolate formatted strings print(\"{} is a mammal\".format(animal)) \"\"\" \"range(number)\" returns an iterable of numbers from zero to the given number prints: 0 1 2 3 \"\"\" for i in range(4): print(i) \"\"\" \"range(lower, upper)\" returns an iterable of numbers from the lower number to the upper number prints: 4 5 6 7 \"\"\" for i in range(4, 8): print(i) \"\"\" \"range(lower, upper, step)\" returns an iterable of numbers from the lower number to the upper number, while incrementing by step. If step is not indicated, the default value is 1. prints: 4 6 \"\"\" for i in range(4, 8, 2): print(i) \"\"\" To loop over a list, and retrieve both the index and the value of each item in the list prints: 0 dog 1 cat 2 mouse \"\"\" animals = [\"dog\", \"cat\", \"mouse\"] for i, value in enumerate(animals): print(i, value) \"\"\" While loops go until a condition is no longer met. prints: 0 1 2 3 \"\"\" x = 0 while x \u003c 4: print(x) x += 1 # Shorthand for x = x + 1 # Handle exceptions with a try/except block try: # Use \"raise\" to raise an error raise IndexError(\"This is an index error\") except IndexError as e: pass # Pass is just a no-op. Usually you would do recovery here. except (TypeError, NameError): pass # Multiple exceptions can be handled together, if required. else: # Optional clause to the try/except block. Must follow all except blocks print(\"All good!\") # Runs only if the code in try raises no exceptions finally: # Execute under all circumstances print(\"We can clean up resources here\") # Instead of try/finally to cleanup resources you can use a with statement with open(\"myfile.txt\") as f: for line in f: print(line) # Writing to a file contents = {\"aa\": 12, \"bb\": 21} with open(\"myfile1.txt\", \"w+\") as file: file.write(str(contents)) # writes a string to a file with open(\"myfile2.txt\", \"w+\") as file: file.write(json.dumps(contents)) # writes an object to a file # Reading from a file with open('myfile1.txt', \"r+\") as file: contents = file.read() # reads a string from a file print(contents) # print: {\"aa\": 12, \"bb\": 21} with open('myfile2.txt', \"r+\") as file: contents = json.load(file) # reads a json object from a file print(contents) # print: {\"aa\": 12, \"bb\": 21} # Python offers a fundamental abstraction called the Iterable. # An iterable is an object that can be treated as a sequence. # The object returned by the range function, is an iterable. filled_dict = {\"one\": 1, \"two\": 2, \"three\": 3} our_iterable = filled_dict.keys() print(our_iterable) # =\u003e dict_keys(['one', 'two', 'three']). This is an object that implements our Iterable interface. # We can loop over it. for i in our_iterable: print(i) # Prints one, two, three # However we cannot address elements by index. our_iterable[1] # Raises a TypeError # An iterable is an object that knows how to create an iterator. our_iterator = iter(our_iterable) # Our iterator is an object that can remember the state as we traverse through it. # We get the next object with \"next()\". next(our_iterator) # =\u003e \"one\" # It maintains state as we iterate. next(our_iterator) # =\u003e \"two\" next(our_iterator) # =\u003e \"three\" # After the iterator has returned all of its data, it raises a StopIteration exception next(our_iterator) # Raises StopIteration # We can also loop over it, in fact, \"for\" does this implicitly! our_iterator = iter(our_iterable) for i in our_iterator: print(i) # Prints one, two, three # You can grab all the elements of an iterable or iterator by calling list() on it. list(our_iterable) # =\u003e Returns [\"one\", \"two\", \"three\"] list(our_iterator) # =\u003e Returns [] because state is saved #################################################### ## 4. Functions #################################################### # Use \"def\" to create new functions def add(x, y): print(\"x is {} and y is {}\".format(x, y)) return x + y # Return values with a return statement # Calling functions with parameters add(5, 6) # =\u003e prints out \"x is 5 and y is 6\" and returns 11 # Another way to call functions is with keyword arguments add(y=6, x=5) # Keyword arguments can arrive in any order. # You can define functions that take a variable number of # positional arguments def varargs(*args): return args varargs(1, 2, 3) # =\u003e (1, 2, 3) # You can define functions that take a variable number of # keyword arguments, as well def keyword_args(**kwargs): return kwargs # Let's call it to see what happens keyword_args(big=\"foot\", loch=\"ness\") # =\u003e {\"big\": \"foot\", \"loch\": \"ness\"} # You can do both at once, if you like def all_the_args(*args, **kwargs): print(args) print(kwargs) \"\"\" all_the_args(1, 2, a=3, b=4) prints: (1, 2) {\"a\": 3, \"b\": 4} \"\"\" # When calling functions, you can do the opposite of args/kwargs! # Use * to expand tuples and use ** to expand kwargs. args = (1, 2, 3, 4) kwargs = {\"a\": 3, \"b\": 4} all_the_args(*args) # equivalent to all_the_args(1, 2, 3, 4) all_the_args(**kwargs) # equivalent to all_the_args(a=3, b=4) all_the_args(*args, **kwargs) # equivalent to all_the_args(1, 2, 3, 4, a=3, b=4) # Returning multiple values (with tuple assignments) def swap(x, y): return y, x # Return multiple values as a tuple without the parenthesis. # (Note: parenthesis have been excluded but can be included) x = 1 y = 2 x, y = swap(x, y) # =\u003e x = 2, y = 1 # (x, y) = swap(x,y) # Again parenthesis have been excluded but can be included. # Function Scope x = 5 def set_x(num): # Local var x not the same as global variable x x = num # =\u003e 43 print(x) # =\u003e 43 def set_global_x(num): global x print(x) # =\u003e 5 x = num # global var x is now set to 6 print(x) # =\u003e 6 set_x(43) set_global_x(6) # Python has first class functions def create_adder(x): def adder(y): return x + y return adder add_10 = create_adder(10) add_10(3) # =\u003e 13 # There are also anonymous functions (lambda x: x \u003e 2)(3) # =\u003e True (lambda x, y: x ** 2 + y ** 2)(2, 1) # =\u003e 5 # There are built-in higher order functions list(map(add_10, [1, 2, 3])) # =\u003e [11, 12, 13] list(map(max, [1, 2, 3], [4, 2, 1])) # =\u003e [4, 2, 3] list(filter(lambda x: x \u003e 5, [3, 4, 5, 6, 7])) # =\u003e [6, 7] # We can use list comprehensions for nice maps and filters # List comprehension stores the output as a list which can itself be a nested list [add_10(i) for i in [1, 2, 3]] # =\u003e [11, 12, 13] [x for x in [3, 4, 5, 6, 7] if x \u003e 5] # =\u003e [6, 7] # You can construct set and dict comprehensions as well. {x for x in 'abcddeef' if x not in 'abc'} # =\u003e {'d', 'e', 'f'} {x: x**2 for x in range(5)} # =\u003e {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} #################################################### ## 5. Modules #################################################### # You can import modules import math print(math.sqrt(16)) # =\u003e 4.0 # You can get specific functions from a module from math import ceil, floor print(ceil(3.7)) # =\u003e 4.0 print(floor(3.7)) # =\u003e 3.0 # You can import all functions from a module. # Warning: this is not recommended from math import * # You can shorten module names import math as m math.sqrt(16) == m.sqrt(16) # =\u003e True # Python modules are just ordinary Python files. You # can write your own, and import them. The name of the # module is the same as the name of the file. # You can find out which functions and attributes # are defined in a module. import math dir(math) # If you have a Python script named math.py in the same # folder as your current script, the file math.py will # be loaded instead of the built-in Python module. # This happens because the local folder has priority # over Python's built-in libraries. #################################################### ## 6. Classes #################################################### # We use the \"class\" statement to create a class class Human: # A class attribute. It is shared by all instances of this class species = \"H. sapiens\" # Basic initializer, this is called when this class is instantiated. # Note that the double leading and trailing underscores denote objects # or attributes that are used by Python but that live in user-controlled # namespaces. Methods(or objects or attributes) like: __init__, __str__, # __repr__ etc. are called special methods (or sometimes called dunder methods) # You should not invent such names on your own. def __init__(self, name): # Assign the argument to the instance's name attribute self.name = name # Initialize property self._age = 0 # An instance method. All methods take \"self\" as the first argument def say(self, msg): print(\"{name}: {message}\".format(name=self.name, message=msg)) # Another instance method def sing(self): return 'yo... yo... microphone check... one two... one two...' # A class method is shared among all instances # They are called with the calling class as the first argument @classmethod def get_species(cls): return cls.species # A static method is called without a class or instance reference @staticmethod def grunt(): return \"*grunt*\" # A property is just like a getter. # It turns the method age() into a read-only attribute of the same name. # There's no need to write trivial getters and setters in Python, though. @property def age(self): return self._age # This allows the property to be set @age.setter def age(self, age): self._age = age # This allows the property to be deleted @age.deleter def age(self): del self._age # When a Python interpreter reads a source file it executes all its code. # This __name__ check makes sure this code block is only executed when this # module is the main program. if __name__ == '__main__': # Instantiate a class i = Human(name=\"Ian\") i.say(\"hi\") # \"Ian: hi\" j = Human(\"Joel\") j.say(\"hello\") # \"Joel: hello\" # i and j are instances of type Human, or in other words: they are Human objects # Call our class method i.say(i.get_species()) # \"Ian: H. sapiens\" # Change the shared attribute Human.species = \"H. neanderthalensis\" i.say(i.get_species()) # =\u003e \"Ian: H. neanderthalensis\" j.say(j.get_species()) # =\u003e \"Joel: H. neanderthalensis\" # Call the static method print(Human.grunt()) # =\u003e \"*grunt*\" # Static methods can be called by instances too print(i.grunt()) # =\u003e \"*grunt*\" # Update the property for this instance i.age = 42 # Get the property i.say(i.age) # =\u003e \"Ian: 42\" j.say(j.age) # =\u003e \"Joel: 0\" # Delete the property del i.age # i.age # =\u003e this would raise an AttributeError #################################################### ## 6.1 Inheritance #################################################### # Inheritance allows new child classes to be defined that inherit methods and # variables from their parent class. # Using the Human class defined above as the base or parent class, we can # define a child class, Superhero, which inherits the class variables like # \"species\", \"name\", and \"age\", as well as methods, like \"sing\" and \"grunt\" # from the Human class, but can also have its own unique properties. # To take advantage of modularization by file you could place the classes above in their own files, # say, human.py # To import functions from other files use the following format # from \"filename-without-extension\" import \"function-or-class\" from human import Human # Specify the parent class(es) as parameters to the class definition class Superhero(Human): # If the child class should inherit all of the parent's definitions without # any modifications, you can just use the \"pass\" keyword (and nothing else) # but in this case it is commented out to allow for a unique child class: # pass # Child classes can override their parents' attributes species = 'Superhuman' # Children automatically inherit their parent class's constructor including # its arguments, but can also define additional arguments or definitions # and override its methods such as the class constructor. # This constructor inherits the \"name\" argument from the \"Human\" class and # adds the \"superpower\" and \"movie\" arguments: def __init__(self, name, movie=False, superpowers=[\"super strength\", \"bulletproofing\"]): # add additional class attributes: self.fictional = True self.movie = movie # be aware of mutable default values, since defaults are shared self.superpowers = superpowers # The \"super\" function lets you access the parent class's methods # that are overridden by the child, in this case, the __init__ method. # This calls the parent class constructor: super().__init__(name) # override the sing method def sing(self): return 'Dun, dun, DUN!' # add an additional instance method def boast(self): for power in self.superpowers: print(\"I wield the power of {pow}!\".format(pow=power)) if __name__ == '__main__': sup = Superhero(name=\"Tick\") # Instance type checks if isinstance(sup, Human): print('I am human') if type(sup) is Superhero: print('I am a superhero') # Get the Method Resolution search Order used by both getattr() and super() # This attribute is dynamic and can be updated print(Superhero.__mro__) # =\u003e (\u003cclass '__main__.Superhero'\u003e, # =\u003e \u003cclass 'human.Human'\u003e, \u003cclass 'object'\u003e) # Calls parent method but uses its own class attribute print(sup.get_species()) # =\u003e Superhuman # Calls overridden method print(sup.sing()) # =\u003e Dun, dun, DUN! # Calls method from Human sup.say('Spoon') # =\u003e Tick: Spoon # Call method that exists only in Superhero sup.boast() # =\u003e I wield the power of super strength! # =\u003e I wield the power of bulletproofing! # Inherited class attribute sup.age = 31 print(sup.age) # =\u003e 31 # Attribute that only exists within Superhero print('Am I Oscar eligible? ' + str(sup.movie)) #################################################### ## 6.2 Multiple Inheritance #################################################### # Another class definition # bat.py class Bat: species = 'Baty' def __init__(self, can_fly=True): self.fly = can_fly # This class also has a say method def say(self, msg): msg = '... ... ...' return msg # And its own method as well def sonar(self): return '))) ... (((' if __name__ == '__main__': b = Bat() print(b.say('hello')) print(b.fly) # And yet another class definition that inherits from Superhero and Bat # superhero.py from superhero import Superhero from bat import Bat # Define Batman as a child that inherits from both Superhero and Bat class Batman(Superhero, Bat): def __init__(self, *args, **kwargs): # Typically to inherit attributes you have to call super: # super(Batman, self).__init__(*args, **kwargs) # However we are dealing with multiple inheritance here, and super() # only works with the next base class in the MRO list. # So instead we explicitly call __init__ for all ancestors. # The use of *args and **kwargs allows for a clean way to pass arguments, # with each parent \"peeling a layer of the onion\". Superhero.__init__(self, 'anonymous', movie=True, superpowers=['Wealthy'], *args, **kwargs) Bat.__init__(self, *args, can_fly=False, **kwargs) # override the value for the name attribute self.name = 'Sad Affleck' def sing(self): return 'nan nan nan nan nan batman!' if __name__ == '__main__': sup = Batman() # Get the Method Resolution search Order used by both getattr() and super(). # This attribute is dynamic and can be updated print(Batman.__mro__) # =\u003e (\u003cclass '__main__.Batman'\u003e, # =\u003e \u003cclass 'superhero.Superhero'\u003e, # =\u003e \u003cclass 'human.Human'\u003e, # =\u003e \u003cclass 'bat.Bat'\u003e, \u003cclass 'object'\u003e) # Calls parent method but uses its own class attribute print(sup.get_species()) # =\u003e Superhuman # Calls overridden method print(sup.sing()) # =\u003e nan nan nan nan nan batman! # Calls method from Human, because inheritance order matters sup.say('I agree') # =\u003e Sad Affleck: I agree # Call method that exists only in 2nd ancestor print(sup.sonar()) # =\u003e ))) ... ((( # Inherited class attribute sup.age = 100 print(sup.age) # =\u003e 100 # Inherited attribute from 2nd ancestor whose default value was overridden. print('Can I fly? ' + str(sup.fly)) # =\u003e Can I fly? False #################################################### ## 7. Advanced #################################################### # Generators help you make lazy code. def double_numbers(iterable): for i in iterable: yield i + i # Generators are memory-efficient because they only load the data needed to # process the next value in the iterable. This allows them to perform # operations on otherwise prohibitively large value ranges. # NOTE: `range` replaces `xrange` in Python 3. for i in double_numbers(range(1, 900000000)): # `range` is a generator. print(i) if i \u003e= 30: break # Just as you can create a list comprehension, you can create generator # comprehensions as well. values = (-x for x in [1,2,3,4,5]) for x in values: print(x) # prints -1 -2 -3 -4 -5 to console/terminal # You can also cast a generator comprehension directly to a list. values = (-x for x in [1,2,3,4,5]) gen_to_list = list(values) print(gen_to_list) # =\u003e [-1, -2, -3, -4, -5] # Decorators # In this example `beg` wraps `say`. If say_please is True then it # will change the returned message. from functools import wraps def beg(target_function): @wraps(target_function) def wrapper(*args, **kwargs): msg, say_please = target_function(*args, **kwargs) if say_please: return \"{} {}\".format(msg, \"Please! I am poor :(\") return msg return wrapper @beg def say(say_please=False): msg = \"Can you buy me a beer?\" return msg, say_please print(say()) # Can you buy me a beer? print(say(say_please=True)) # Can you buy me a beer? Please! I am poor :( ","description":"Python Cheat Sheet","title":"Python Cheat Sheet","uri":"/en/posts/python-snippets/"},{"content":"Template def function_name(param1: Type1, param2: Type2, ...) -\u003e ReturnType: \"\"\"Brief description of the function. More detailed explanation of the function if necessary. This can span multiple lines as needed. Args: param1 (Type1): Description of param1. param2 (Type2): Description of param2. ... Returns: ReturnType: Description of the return value. Raises: ExceptionType: Explanation of the conditions under which this exception is raised. Example: \u003e\u003e\u003e function_name(param1_value, param2_value) Expected output \"\"\" ... Example With Type Hints def add_numbers(num1: int, num2: int = 5) -\u003e int: \"\"\"Adds two numbers together. Args: num1 (int): The first number to add. num2 (int, optional): The second number to add. Defaults to 5. Returns: int: The sum of num1 and num2. Example: \u003e\u003e\u003e add_numbers(3, 2) 5 \"\"\" return num1 + num2 Without Type Hints def add_numbers(num1, num2=5): \"\"\"Adds two numbers together. Args: num1: The first number to add. Should be of type int. num2: The second number to add. Should be of type int. Defaults to 5. Returns: The sum of num1 and num2. The return value will be of type int. Example: \u003e\u003e\u003e add_numbers(3, 2) 5 \"\"\" return num1 + num2 Resources Google Python Style Guide: This is a widely adopted style guide in the Python community. It has a specific section on comments and docstrings that I found particularly helpful: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings\nPEP 257 - Docstring Conventions: This is the Python Enhancement Proposal that describes the conventions for writing good docstrings in Python: https://www.python.org/dev/peps/pep-0257/\nPEP 484 - Type Hints: This PEP introduced the concept of type hints to Python, and provides guidelines on how to use them: https://www.python.org/dev/peps/pep-0484/\n","description":"Python docstring templates","title":"Python docstring templates","uri":"/en/posts/python-docstring-templates/"},{"content":"Clean template ############################################# Libraries ###################################################### import bisect import sys import math import os import time from queue import PriorityQueue from io import BytesIO, IOBase from collections import defaultdict, Counter from bisect import bisect_right ############################################# Definitions ###################################################### INF = sys.maxsize BUFSIZE = 4096 ############################################# Inputs ###################################################### # def inp(): return sys.stdin.readline().rstrip(\"\\r\\n\") # read line as string # def inp_int(): return int(inp()) # read input as integer. '1' -\u003e 1 # def inp_int_list(): return list(map(int, inp().split())) # def inp_str_list(): return list(inp()) ############################################# Solution ###################################################### def solve(): n = list(map(int, input().split())) res = 0 print(res) for _ in range(int(input())): solve() Full template ############################################# Libraries ###################################################### import bisect import sys import math import os import time from queue import PriorityQueue from io import BytesIO, IOBase from collections import defaultdict, Counter from bisect import bisect_right ############################################# Definitions ###################################################### INF = sys.maxsize BUFSIZE = 4096 ############################################# Inputs ###################################################### def inp(): return sys.stdin.readline().rstrip(\"\\r\\n\") # read line as string def inp_int(): return int(inp()) # read input as integer. '1' -\u003e 1 def inp_int_list(): return list(map(int, inp().split())) def inp_str_list(): return list(inp()) ############################################# Data Structures ###################################################### class SegmentTree: # //O(logn) for operations and O(n) for building// def init(arr): # n shld be a power of 2...hence add extra zeros before itself if needed //O(n)// n = len(arr) tree = [0]*(2*n) for i in range(n): # The actual array is between indices n to 2*n-1 the first nodes store sums tree[n+i] = arr[i] for i in range(n-1, -1, -1): # parent node value = child node's sum i\u003c\u003c1 = 2*i, i\u003c\u003c1 |1 = 2*i+1 tree[i] = tree[i \u003c\u003c 1]+tree[(i \u003c\u003c 1) | 1] return tree def add(tree, i, v): # Sets vertex i to value v (i shld be 0 based indexing) //O(logn)// # As the actual array is between n and 2*n-1, we add n to i (n = len(tree)//2) i += len(tree) // 2 tree[i] = v while i \u003e 1: tree[i \u003e\u003e 1] = tree[i] + tree[i ^ 1] i \u003e\u003e= 1 # Calculating the values of prev nodes. (eg if node 9 is changed 9\u003e\u003e1 = 4 takes values of node i(9) and node i^1(8)) # calculates the sum of values in the range [l,r-1] (l and r take 0 based indexing) //O(logn)// def range_sum(tree, l, r): l += len(tree)//2 r += len(tree)//2 sum = 0 while l \u003c r: if l \u0026 1: # If the index is odd, add its value to sum. if the index is even it means there would be a parent sum += tree[l] l += 1 # of this with odd index if r \u0026 1: r -= 1 sum += tree[r] l \u003e\u003e= 1 r \u003e\u003e= 1 return sum ############################################# Solution ###################################################### def solve(): s = inp() print() def run(): for _ in range(inp_int()): solve() if __name__ == \"__main__\": CODE_DEBUG = 1 if os.environ.get(\"CODE_DEBUG\") or CODE_DEBUG: sys.stdin = open(\"./input.txt\", \"r\") start_time = time.time() run() print(\"\\n--- %s seconds ---\\n\" % (time.time() - start_time)) else: run() ","description":"Python template for contests","title":"Python template for contests","uri":"/en/tracks/algorithms-101/codeforces/cp-template/"},{"content":"On this page you can find 50 random questions.\nTo get prepared for exam you can use cloud-exam-prepare.com\nQ1 - Q10 Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\nHTTP proxy mock AWS service proxy Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\nAmazon API Gateway supports mock integrations for API methods.\n2\nQ2 You are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.\nWhat steps can you take in this situation? (Choose 2 answers)\nUse a template to first create Resource A with the ARN as an output value. Use a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt. Hard code the ARN value output from creating Resource A into the second template. Just create Resource B. Explanation http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n2\nQ3 A company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.\nWhich API command can a developer use to ensure cache storage consistency with minimal duplication?\nCreateReusableDelegationSet with Route 53 CreateStackSet with CloudFormation CreateGlobalReplicationGroup with ElastiCache CreateCachePolicy with CloudFront Explanation https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html\n4\nQ4 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\nWhat is the easiest way to develop these simple applications?\nInstall AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model\nhttps://aws.amazon.com/serverless/sam/\n1\nQ5 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\nCan delete only if unused.\n1\nQ6 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n3\nQ7 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\nWhich actions should you take to make this function perform correctly? (2 answers)\nRestart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n2, 3\nQ8 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\nWhat action will best solve this issue?\nUse AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n3\nQ9 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\nUse Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n1, 5\nQ10 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\nWhich architectures meet these requirements? (Select TWO.)\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n1, 2\nQ11 - Q20 1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.\nmemory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation 4\n2 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\nThe metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to ElastiCache is required for test or development purposes, it can be done through a VPN.\n2\n3 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n3\n4 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\nWhich of the following options would meet this requirement?\nUse an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n2\n5 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\nWhich AWS service should be used to refactor this architecture?\nAWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4\n6 Which load balancer would you use for services which use HTTP or HTTPS traffic?\nExplanation Application Load Balancer (ALB). 7 What are possible target groups for ALB (Application Load Balancer)?\nExplanation EC2 tasks ECS instances Lambda functions Private IP Addresses 8 Your would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.\nWhich solution below would be best to implement for this?\nEnable X-Forwarded For on the web servers and use a Classic Load Balancer Configure proxy servers to forward the traffic to the correct instances Use Classic Load Balancer with path-based routing rules to forward the traffic to the correct instances Use Application Load Balancer with host-based routing rules to forward the traffic to the correct instances Explanation Application Load Balancer with host-based routing rules\nhttps://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\n4\n9 A company uses Amazon DynamoDB for managing and tracking orders. DynamoDB table is partitioned based on the order date. The company receives a huge increase in orders during a sales event, causing DynamoDB writes to throttle, and the consumed throughput is below the provisioned throughput.\nAccording to AWS best practices, how can this issue be resolved with MINIMAL costs?\nCreate a new Dynamo DB table for every order date Add a random number suffix to the partition key values Add a global secondary index to the DynamoDB table Increase the read and write capacity units of the DynamoDB table Explanation A randomizing strategy can greatly improve write throughput. But it’s difficult to read a specific item because you don’t know which suffix value was used when writing the item.\nChoosing the Right DynamoDB Partition Key\nUsing write sharding to distribute workloads evenly\n2\n10 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\nWhich approach best meets these requirements?\nUse EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html\n3\nQ21 - Q30 1 How AWS Fargate different from AWS ECS?\nExplanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don’t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n2 What is Chaos Engineering?\nExplanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n3 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\nWhat are the best practice guidelines for use of the root account?\nNever use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1\n4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\nEnable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template\n4\n5 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\nAssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1\n6 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\nWhich of the following APIs will be leveraged to provide temporary security credentials?\nAssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\nAssumeRoleWithWebIdentity - does not support MFA\n3\n7 You built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.\nWhat steps will you take to keep a strong performance?\nRemove the hot shards Merge the hot shards Split the hot shards Increase the shard capacity Explanation https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\nSplit the hot shards\n3\n8 Jasmin needs to perform ad-hoc business analytics queries on well-structured dat1. Data comes in constantly at a high velocity. Jasmin’s team can understand SQL.\nWhat AWS service(s) should Jasmin look to first?\nEMR using Hive EMR running Apache Spark Kinesis Firehose + RDS Kinesis Firehose + RedShift Explanation RedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol\nhttps://aws.amazon.com/kinesis/data-firehose/features/\n4\n9 Key rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?\nKMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key. KMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys. Key rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed. Key rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key. Explanation When you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key’s older cryptographic material so it can be used to decrypt data that it has encrypted.\n10 Alan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.\nWhich statements are true with regard to this scenario? (Choose 2 answers)\nCreate Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts. Can share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys. Make encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts. You can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used. Explanation https://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n1, 2\nQ31 - Q40 1 When working with a published version of the AWS Lambda function, you should note that the _____.\nUse the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager\n3\n2 A Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.\nWhich of the following enables monitoring of the metric in Amazon CloudWatch?\nRetrieve the log data from AWS CloudTrail using the LookupEvents API Call Retrieve the log data from CloudWatch using the GetMetricData API call Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application Install the Amazon CloudWatch logs agent on the EC2 instance that the application is running on Explanation 4\n3 A developer is building a streamlined development process for Lambda functions related to S3 storage. The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\nWhich of the following AWS solutions offers is best suited for this objective?\nAWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model\n2\n4 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically 5 Developer wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.\nWhich methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)\nControl access based on the time of day Control access based on IP Address Control access based on Active Directory group Control access based on CIDR block Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html CIDRs - A set of Classless Inter-Domain Routings\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html\n1, 2, 4\n6 To ensure that an encryption key was not corrupted in transit, Elastic Transcoder uses a(n) ____ digest of the decryption key as a checksum.\nBLAKE2 SHA-1 SHA-2 MD5 Explanation https://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html\nMD5 digest (or checksum)\n4\n7 Dan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.\nWhat are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)\nUse the encrypted SSL/TLS endpoint. Enable encryption in the bucket policy. Encrypt it on the client-side before uploading. Set the server-side encryption option on upload. Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n1, 3\n8 In a move toward using microservices, a company’s Management team has asked all Development teams to build their services so that API requests depend only on that services data store. One team is building a Payments service that has its own database. The service floods data that originates in the Accounts database. Both are using Amazon DynamoDB.\nWhat approach will result in the simplest, decoupled, and reliable method to get near-real-time updates from the Accounts database?\nUse Amazon Glue to perform frequent updates from the Accounts database to the Payments database Use Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database. Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database. Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database. Explanation 3\n9 You’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.\nWhich options are supported notification methods? (Choose 3 answers)\nHTTP or HTTPS POST notifications Email using SMTP or plain text Kinesis Stream Invoking of a Lambda function Explanation https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html\n1, 2, 4\n10 Which endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?\nSNS E-Mail SQS Kinesis Explanation https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html\n3\nQ41 - Q50 1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\nWhich of the following Step Function workflows can be used to meet requirements?\nAsynchronous Express Workflow Synchronous Express Workflow Standard Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n2\n2 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\nWhich AWS service will be your main tool?\nAWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks\n1\n3 A team of Developers must migrate an application running inside an AWS Elastic Beastalk environment from a Classic Load Balancer to an Application Load Balancer.\nWhich steps should be taken to accomplish the task using the AWS Management Console?\n1 Select a new load balancer type before running the deployment. Update the application code in the existing deployment. Deploy the new version of the application code to the environment. 2 Create a new environment with the same configurations except for the load balancer type. Deploy the same application versions as used in the original environment. Run the Swap-environment-cnames action. 3 Clone the existing environment, changing the associated load balancer type. Deploy the same application version as used in the original environment. Run the Swap-environment-cnames action. 4 Edit the environment definitions in the existing deployment. Change the associated load balancer type according to the requirements. Rebuild the environment with the new load balancer type. Explanation 4\n4 A developer is deploying an application that will store files in an Amazon S3 bucket. The files must be encrypted at rest. The developer wants to automatically replicate the files to an S3 bucket in a different AWS Region for disaster recovery.\nHow can the developer accomplish this task with the LEAST amount of configuration?\nEncrypt the files by using server-side encryption with S3 managed encryption keys (SSE-S3). Enable S3 bucket replication. Encrypt the files by using server-side encryption (SSE) with an AWS Key Management Service (AWS KMS) customer master key (CMK). Enable S3 bucket replication. Use the s3 sync command to sync the files to the S3 bucket in the other Region. Configure an S3 Lifecycle configuration to automatically transfer files to the S3 bucket in the other Region. Explanation 2\n5 A serverless application is using AWS Step Functions to process data and save it to a database. The application needs to validate some data with an external service before saving the dat1. The application will call the external service from an AWS Lambda function, and the external service will take a few hours to validate the dat1. The external service will respond to a webhook when the validation is complete.\nA developer needs to pause the Step Functions workflow and wait for the response from the external service.\nWhat should the developer do to meet this requirement?\nUse the .waitForTaskToken option in the Lambda function task state. Pass the token in the body. Use the .waitForTaskToken option in the Lambda function task state. Pass the invocation request. Call the Lambda function in synchronous mode. Wait for the external service to complete the processing. Call the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing. Explanation 4\n6 A company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.\nWhat is causing the duplicate entries?\nThe S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket. The Lambda function did not run correctly, and Lambda retried the invocation with a delay. Amazon S3 is delivering the same event multiple times. The application stopped intermittently and then resumed, splitting the logs into multiple smaller files. Explanation 1\n7 An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function.\nHow can the developer inspect the timing of the DynamoDB API calls?\nAdd DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs Limit Lambda to no more than five concurrent invocations. Monitor from the Lambda console. Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service. Explanation 4\n8 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance. How can the application find this information?\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1\n9 A developer is creating a serverless application that uses an AWS Lambda function The developer will use AWS CloudFormation to deploy the application. The application will write logs to Amazon CloudWatch Logs. The developer has created a log group in a CloudFormation template for the application to use. The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime.\nWhich solution will meet this requirement?\nUse the AWS::Include transform in CloudFormation to provide the log group’s name to the application. Pass the log group’s name to the application in the user data section of the CloudFormation template Use the CloudFormation template’s Mappings section to specify the log group’s name for the application. Pass the log group’s Amazon Resource Name (ARN) as an environment variable to the Lambda function. Explanation 4\n10 A developer needs to use the AWS CLI on an on-premises development server temporarily to access AWS services while performing maintenance. The developer needs to authenticate to AWS with their identity for several hours.\nWhat is the MOST secure way to call AWS CLI commands with the developer’s IAM identity?\nSpecify the developer’s IAM access key ID and secret access key as parameters for each CLI command. Run the AWS configure CLI command. Provide the developer’s IAM access key ID and secret access key. Specify the developer’s IAM profile as a parameter for each CLI command. Run the get-session-token CLI command with the developer’s IAM user. Use the returned credentials to call the CLI. Explanation 4\nQ51 - Q60 6 A developer notices timeouts from the AWS CLI when the developer runs list commands.\nWhat should the developer do to avoid these timeouts?\nUse the --page-size parameter to request a smaller number of items. Use shorthand syntax to separate the list by a single space. Use the yaml-stream output for faster viewing of large datasets. Use quotation marks around strings to enclose data structure. Explanation 1\n7 A company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.\nWhich CodeDeploy predefined configuration will meet these requirements?\nCodeDeployDefault.ECSCanary10Percent15Minutes CodeDeployDefault.LambdaCanary10Percent5Minutes CodeDeployDefault.LambdaCanary10Percent15Minutes CodeDeployDefault.ECSLinear10PercentEvery1 Minutes Explanation 1\n8 Explanation 9 A microservices application is deployed across multiple containers in Amazon Elastic Container Service (Amazon ECS). To improve performance, a developer wants to capture trace information between the microservices and visualize the microservices architecture.\nWhich solution will meet these requirements?\nBuild the container from the amazon/aws-xray-daemon base image. Use the AWS X-Ray SDK to instrument the application. Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices. Install the AWS X-Ray daemon on each of the ECS instances. Configure AWS CloudTrail data events to capture the traffic between the microservices. Explanation 3\n10 A company is running an application on Amazon Elastic Container Service (Amazon ECS). When the company deploys a new version of the application, the company initially needs to expose 10% of live traffic to the new version. After a period of time, the company needs to immediately route all the remaining live traffic to the new version.\nWhich ECS deployment should the company use to meet these requirements?\nRolling update Blue/green with canary Blue/green with all at once Blue/green with linear Explanation 2\n","description":"AWS exam questions Certified Developer 2023","title":"Questions","uri":"/en/tracks/aws-certified-developer-associate/questions/"},{"content":"About Relational Database Service Managed DB service that uses SQL as query language Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.\nDocumentation User Guide Supports engines:\nAmazon Aurora with MySQL compatibility: 5432 Amazon Aurora with PostgreSQL compatibility: 5432 MySQL: 3306 MariaDB: 3306 PostgreSQL: 5432 Oracle: 1521 SQL Server: 1433 Engine modes:\nUsed in CreateDBCluster\nglobal parallelquery serverless multimaster Backups Backups are enabled by default in RDS Automated backups\nDaily full backup (during maintenance window) Backups of transaction logs (every 5 minutes) 7 days retention (can increase upto 35) DB Snapshots\nManually triggered by the user Can retain backup as long as you want Auto scaling When RDS detects you’re running out of space, it scales automatically Digest To verify slowly running queries enable slow query log. TDE (Transparent data encryption) supports encryption on Microsoft SQL server AWS Systems Manager Parameter Store provides secure, hierarchical storage for confiquration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values AWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases. Price Current price\nUse Cases Type: Relational\nThis type services: Aurora, Redshift, RDS\nEcommerce websites, Traditional sites etc.\nAmazon Relational Database Service (Amazon RDS) on [AWS Outposts](AWS Outposts) allows you to deploy fully managed database instances in your on-premises environment\nQuestions Q1 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically Q2 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\nWhich approach meets these requirements?\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager\nSecrets Manager offers secret rotation\n4\nQ3 Explain RDS Multi Availability Zone\nExplanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically ","description":"Set up, operate, and scale a relational database in the cloud with just a few clicks.","title":"RDS","uri":"/en/tracks/aws-certified-developer-associate/rds/"},{"content":"TODO G2 Contest date: 2023-02-03\nContest problems\nEditorial\nTemplate with function snippets\nA. Codeforces Checking https://codeforces.com/contest/1791/problem/A\nSolution:\ndef solve(letter): code = \"codeforces\" if letter in code: print(\"YES\") else: print(\"NO\") for _ in range(int(inp())): letter = inp() solve(letter) B. Following Directions https://codeforces.com/contest/1791/problem/B\ngeometry, implementation, *800\nSolution:\ndef solve(n, s): x = 0 y = 0 for move in s: if move == 'L': x -= 1 elif move == 'R': x += 1 elif move == 'U': y += 1 elif move == 'D': y -= 1 if x == 1 and y == 1: print(\"YES\") break else: print(\"NO\") for _ in range(int(inp())): # attempts num = int(inp()) letter = inp() solve(num, letter) C. Prepend and Append https://codeforces.com/contest/1791/problem/C\nimplementation, two pointers, *800\nIn this problem we are allowed:\nto remove first letter of the binary string and last. We can do this while first letter is not equal to the last according to the definition of the problem. (Add 𝟶 to one end of the string and 𝟷 to the other end of the string.) Solution:\ndef solve(n, s): res = n if res \u003c= 1: return 1 while res \u003e 0: first = s[0] last = s[-1] if first == last: return res s = s[1:-1] res -= 2 return res for _ in range(int(inp())): # attempts n = int(inp()) s = inp() print(solve(n, s)) Optimized:\nUse deque from collection module\n\u003e\u003e\u003e deque('123') deque(['1', '2', '3']) from collections import deque def solve(n, s): s = deque(s) while len(s) and s[0] != s[-1]: s.popleft() s.pop() print(len(s)) D. Distinct Split https://codeforces.com/contest/1791/problem/D\nbrute force, greedy, strings, *1000\nIf we get a string abcabcd we need to split into two strings.\nNote 1: result number could not be more than string length.\nabcabcd can be splited into abc and abcd. len(‘abc’) = 3 #3 distinct letters len(‘abcd’) = 4 3 + 4 = 7\nOutput: maximum possible value of 𝑓(𝑎)+𝑓(𝑏) such that 𝑎+𝑏=𝑠.\nLet’s look at other examples and possible edge cases.\nNote 2: function for a string 𝑥 - is the number of distinct characters. (From problem statement.)\n'aaaaa' =\u003e 1 # 1 distinct If we concatenate two strings into one s, we need to keep order of the letters. Need to split this string s in a such a way so that repeated letters fall into different parts of string s (a and b). abcab =\u003e abc and ab better that abca and b because:\nf('abc') = 3 f('abca') = 3 f('ab') = 2 f('b') = 1 5 \u003e 4 Approach 1:\nDivide string into two parts starting left part from len 1. For example: s = ‘abcabc’ a = ‘a’ b = ‘bcabc’ Increase left part and decrease right part. On each step calculate sum of distinct letters. For example: set(a) + set(b) max_result = max(max_result, len(set(a)) + len(set(b))) Solution:\ndef solve(): # input n = int(inp()) s = inp() point = 0 max_n = 0 a_set = set(s[point]) b_set = set(s[point+1:]) while point \u003c n - 1: max_n = max(max_n, len(a_set) + len(b_set)) a_set.add(s[point + 1]) b_set = set(s[point+2:]) point += 1 if max_n == n: break return max_n def run(): for _ in range(inp_int()): print(solve()) if __name__ == \"__main__\": if os.environ.get(\"CODE_DEBUG\"): sys.stdin = open(\"./input.txt\", \"r\") start_time = time.time() run() print(\"\\n--- %s seconds ---\\n\" % (time.time() - start_time)) else: run() Optimization:\nb_set = set(s[point+2:]) is very heavy on each step in the loop.\nUse Counter instead.\ndef solve(): # input n = int(inp()) s = inp() point = 0 max_n = 0 a = set(s[point]) b = Counter(s[point+1:]) while point \u003c n - 1: max_n = max(max_n, len(a) + len(b)) point += 1 b_letter = s[point] a.add(b_letter) b[b_letter] -= 1 if b[b_letter] \u003c= 0: del b[b_letter] if max_n == n: break return max_n Explanation from Codeforces:\nLet’s check all splitting points 𝑖 for all (1≤𝑖≤𝑛−1). We denote a splitting point as the last index of the first string we take (and all the remaining characters will go to the second string). We need to keep a dynamic count of the number of distinct characters in both strings 𝑎 (the first string) and 𝑏 (the second string). We can do this using two frequency arrays (and adding one to the distinct count of either string 𝑎 or 𝑏 when the frequency of a character is greater than zero.\nE. Negatives and Positives https://codeforces.com/contest/1791/problem/E\ndp, greedy, sortings, *1100\nExplanation from Codeforces:\nWe can notice that by performing any number of operations, the parity of the count of negative numbers won’t ever change.\nThus, if the number of negative numbers is initially even, we can make it equal to 0 by performing some operations.\nSo, for an even count of negative numbers, the answer is the sum of the absolute values of all numbers (since we can make all of them positive). And if the count of negative numbers is odd, we must have one negative number at the end.\nWe will choose the one smallest by absolute value and keep the rest positive (for simplicity, we consider −0 as a negative number).\nSolution:\ndef solve(): n = inp_int() a = inp_int_list() count_minus = 0 count_zeros = 0 min_val = abs(a[0]) s = 0 # sum for i in range(n): if a[i] \u003c 0: count_minus += 1 if a[i] == 0: count_zeros += 1 v = abs(a[i]) s += v min_val = min(min_val, v) count_minus = count_minus % 2 # if count of odd numbers is negative count_minus -= count_zeros if count_minus \u003c= 0: return s if count_minus == 1: s -= abs(min_val * 2) return s return s def run(): for _ in range(inp_int()): print(solve()) run() Optimization:\ndef solve(): n = inp_int() a = inp_int_list() count_minus = 0 for i in range(n): if a[i] \u003c 0: count_minus += 1 a[i] = abs(a[i]) s = sum(a) # sum if count_minus % 2: return s - min(a) * 2 else: return s F. Range Update Point Query https://codeforces.com/contest/1791/problem/F\nThere are two types of inputs (cases) (in addition to array a and n of test cases):\nline with two elements: 2 x. Starts with 2 line with three elements: 1 l r. Starts with 1 In 1st case: print array a\nIn 2nd case: update the value of 𝑎𝑖 to the sum of its digits.\nSlow Solution:\ndef sum_of_digits(n): sum = 0 while n: sum += n % 10 n //= 10 return sum def solve(): n, q = map(int, inp().split()) a = list(map(int, inp().split())) while q: q -= 1 t, *params = map(int, inp().split()) if t == 1: l, r = params for i in range(l-1, r): a[i] = sum_of_digits(a[i]) else: x, = params print(a[x-1]) t = int(inp().strip()) for _ in range(t): # attempts solve() This solution is slow because of loop:\nfor i in range(l-1, r): a[i] = sum_of_digits(a[i]) The key here is the following: after the operation is applied on ai thrice, it won’t change after any further operations.\nProblem here is to implement a solution how to save information how many times there was a change of a[i].\nOne way is to use Fenwick Tree\nUse FenwickTree template Save there count of a[i] changes. No need to calculate more than 3 times When need to print a[x] calculate up to three times and print result. class Fenwick: #also known as Binary Indexed Tree (BIT) # no need to change here anything def __init__(self, n): self.n = n self.bit = [0] * (n+1) def add(self, idx, val): while idx \u003c= self.n: self.bit[idx] += val idx += idx \u0026 -idx def add_range(self, l, r, val): self.add(l, val) self.add(r+1, -val) def query(self, idx): #Calculates the sum of the elements from the beginning to idx ret = 0 while idx \u003e 0: ret += self.bit[idx] idx -= idx \u0026 -idx return ret def range_sum(self, l, r): # Return the sum of the elements from l (inclusive) to r (exclusive) return self.prefix_sum(r - 1) - self.prefix_sum(l - 1) def prefix_sum(self, x): # return sum upto and including element x z = x res = 0 while z \u003e= 0: res += self.bit[z] # Strip trailing zeros from z, and then take away one z = (z \u0026 (z + 1)) - 1 return res def solve(): n, q = list(map(int, inp().split())) a = list(map(int, inp().split())) tree = BIT(n) while q: q -= 1 t, *params = map(int, inp().split()) if t == 1: l, r = params tree.add(l, 1) tree.add(r + 1, -1) else: x, = params need = min(3, tree.query(x)) # get count of times need to change a[i] cur = a[x - 1] for i in range(need): cur = sum_of_digits(cur) print(cur) Good to know\nSegment Tree template tutorial A Visual Introduction to Fenwick Tree | medium Fenwick Tree | cp-algorithms Segment Tree | cp-algorithms Дерево отрезков | algorithmica Дерево Фенвика | algorithmica Дерево Фенвика | habr G1. Teleporters (Easy Version) https://codeforces.com/contest/1791/problem/G1\ngreedy, sortings *1100\nStatement:\nYou are playing a game where you are given a list of teleporters (0,2,…,𝑛), each located at a point on the number line. The number line includes all integers from 0 to n.\nAt point i, you can do one of three actions:\nMove left one unit: this action costs 1 coin. Move right one unit: this action also costs 1 coin. Use a teleporter at point i: this action costs $a_i$ coins. When you use a teleporter, you are immediately teleported back to point 0. Last statement in the problem means that at any point i on the number line, you have the option to use a teleporter located at that point, which will immediately transport you back to point 0. However, using a teleporter costs a certain number of coins, denoted by $a_i$. For example, if you are at point i = 5, and you use the teleporter located at point 5, you will be immediately transported back to point 0, but you will have to pay the cost $a_i$ in coins to do so.\nEssentially, the teleporters provide a way for you to quickly move back to the starting point (point 0) without having to take multiple steps, but this convenience comes at a cost. You can only use each teleporter once, and you must have enough coins to pay the cost of using it.\nYou start at point 0, and you have c coins to spend. Your goal is to use as many teleporters as possible while still having at least 1 coin left over. You cannot use a teleporter more than once.\nWrite a function max_teleporters(n, c, a) that takes in\nthe length of the number line n the number of coins you have c the list of teleporter costs a. The function should return the maximum number of teleporters you can use given your starting position at 0 and the number of coins you have to spend.\nYou may assume that n, c, and all elements of a are positive integers.\nFor example, the following input should return 2:\nn = 8 c = 32 a = [100, 52, 13, 6, 9, 4, 100, 35] max_teleporters(n, c, a) =\u003e 2 Will use greedy algorithm.\nApproach:\nGiven input: n = 8, c = 32, a = [100, 52, 13, 6, 9, 4, 100, 35]\nStep 1: Calculate the cost of each teleporter\nFor each teleporter at position i, add the index i and the cost a[i] The resulting array cost contains the costs to use each teleporter, accounting for the cost of moving to that teleporter’s position: [101, 54, 16, 10, 14, 10, 107, 43] Step 2: Sort the cost array in ascending order\nSorting the array ensures that we use the cheapest teleporters first Step 3: Compute the maximum number of teleporters we can use\nInitialize a variable used to 0 to keep track of how many teleporters we’ve used Iterate over the sorted cost array, adding the cost of each teleporter to a running total total If the current total is less than or equal to c, we can use the current teleporter, so increment used If the current total is greater than c, we’ve run out of coins and can’t use any more teleporters, so exit the loop Return the final value of used Step 4: Return the maximum number of teleporters we can use\nThe maximum number of teleporters we can use is the value of used computed in Step 3: 2 Solution:\ndef max_teleporters(n, c, a): # First, we add the index of each teleporter to its cost, so that we can easily # calculate the cost to reach each teleporter from the starting position (0). for i in range(n): a[i] += i + 1 # We then sort the list of teleporter costs in ascending order, so that we can # use the cheapest teleporters first. a.sort() # We iterate over the sorted list of teleporter costs, using as many teleporters # as possible while still having at least one coin remaining. We keep track of the # number of teleporters used in the \"used\" variable. used = 0 for cost in a: if cost \u003c= c: used += 1 c -= cost else: break # Finally, we return the number of teleporters used. return used Explanation from Codeforces:\nIt’s easy to see that it’s optimal to only move right or to use a portal once we are at it. We can notice that when we teleport back, the problem is independent of the previous choices.\nWe still are at point 0 and have some portals left. Thus, we can just find out the individual cost of each portal, sort portals by individual costs, and take them from smallest to largest by cost as long as we can.\nThe cost of portal 𝑖 is $𝑖+𝑎𝑖_$ (since we pay \u0026𝑎_𝑖\u0026 to use it and need 𝑖 moves to get to it).\nG2. Teleporters (Hard Version) https://codeforces.com/contest/1791/problem/G2\nbinary, search, greedy, sortings *1900\nExplanation from Codeforces:\nPlease also refer to the tutorial for the easy version.\nIf we are not at the first taken portal, the problem is still independent for each portal, but this time the cost of a portal is $𝑚𝑖𝑛(𝑎_𝑖+𝑖,𝑎_𝑖+𝑛+1−𝑖)$ (since we can come to a portal either from point 0 or point $𝑛+1$).\nSo, we again sort the portals by their costs. But this time, we need to make sure that the first taken portal is taken from point 0, so we will iterate over all portals and check the maximum amount of portals we can take if we use it as the first one.\nWe can check this using prefix sums over the minimum cost array and binary searching, checking if the amount of considered portals taken doesn’t exceed the number of coins we initially have (we also have to deal with the case when the portal we are considering is included both times as the initial portal and in the minimum cost prefix).\n","description":"Codeforces Round #849 (Div. 4) / 1791","title":"Round #849/1791 (Div. 4)","uri":"/en/tracks/algorithms-101/codeforces/contests/849-div-4-1791/"},{"content":" Contest problems A. TubeTube Feed Mushroom Filippov is having lunch and wants to watch a video on TubeTube. He has a specific amount of time for lunch, and he wants to make the best use of it by watching the most entertaining video that fits into his lunch break.\nGiven a list of videos, each with its duration and entertainment value, your task is to help Mushroom Filippov choose the best video to watch. He can only watch one video, and the video must not exceed his lunch break time.\nIf there are multiple videos that fit into his lunch time, choose the one with the highest entertainment value.\nLogic:\nIterate through the list of videos and checking if the video duration is less than or equal to the lunch time.\nIf it is, you then check if the entertainment value of that video is higher than the current highest entertainment value. If it is, update the highest entertainment value and remember the index of that video. After going through all the videos, you will have the index of the most entertaining video that fits into the lunch break time. 1822A Solution:\ndef solve(): n, t = map(int, input().split()) durations = list(map(int, input().split())) values = list(map(int, input().split())) max_value = 0 # max entertainment value max_index = -1 # Iterate through the videos for i in range(n): # If the video can be watched within the lunch break if i + durations[i] \u003c= t: if values[i] \u003e max_value: max_value = values[i] max_index = i + 1 print(max_index) for _ in range(int(input())): solve() ","description":"Codeforces Round #867 (Div. 3) / 1822","title":"Round #867/1822 (Div. 3)","uri":"/en/tracks/algorithms-101/codeforces/contests/867-div-3-1822/"},{"content":"About Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-premises.\nDocumentation User Guide A highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing, and health checking.\nCan create and manage your public DNS records.\nWhat is the difference between Route 53 and DNS?\nYour DNS is the service that translates your domain name into an IP address. AWS Route 53 is a smart DNS system that can dynamically change your origin address based on load, and even perform load balancing before traffic even reaches your servers.\nAlternatives Cloudflare DNS. Google Cloud DNS. Azure DNS. GoDaddy Premium DNS. DNSMadeEasy. ClouDNS. UltraDNS. NS1. Routing Policies Simple routing policy – route internet traffic to a single resource that performs a given function for your domain. You can’t create multiple records that have the same name and type, but you can specify multiple values in the same record, such as multiple IP addresses.\nFailover routing policy – use when you want to configure active-passive failover.\nGeolocation routing policy – use when you want to route internet traffic to your resources based on the location of your users.\nGeoproximity routing policy – use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.\nYou can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. The effect of changing the bias for your resources depends on a number of factors, including the following: The number of resources that you have. How close the resources are to one another. The number of users that you have near the border area between geographic regions. Latency routing policy – use when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency.\nIP-based routing policy – use when you want to route traffic based on your users’ locations, and know where the IP address or traffic is coming from.\nMultivalue answer routing policy – use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.\nWeighted routing policy – use to route traffic to multiple resources in proportions that you specify.\nWhen you register a domain or transfer domain registration to Route 53, it configures the domain to renew automatically. The automatic renewal period is typically one year, although the registries for some top-level domains (TLDs) have longer renewal periods.\nWhen you register a domain with Route 53, it creates a hosted zone that has the same name as the domain, assigns four name servers to the hosted zone, and updates the domain to use those name servers.\nDigest Route 53 is AWS DNS service Map domain names to EC2 instances, Load Balancers and 53 buckets Routing Policies Simple - Traffic routed to a single resource Weighted - Traffic routed to a resource = weight assigned to the resource/sum of all the weights Latency - serves requests from the AWS region with low latency Geographical - routes the traffic based on the location of the request origin Failover - routes traffic to primary when primary healthy; secondary when primary is unhealthy Multivalue Answer - routs randomly to multiple healthy resources VPC - private network on AWS platform Subnet, NAT Instance, NAT Gatewav, Internet Gatewav, NACLs, Route Table VPC Wizard Single public subnet Public and Private subnet (NAT) Public and private subnet and AWS managed VPN access Private subnet only and AWS managed VPN access VPC Peering - helps transfer of data VPC Flow logs - helps capture information about incoming/outgoing traffic Direct Connect - dedicated connection from on premises network to VPC Price Pay only for what you use.\nCurrent price\nUse Cases Domain Registration / transfer Manage network traffic globally Set up private DNS ","description":"Amazon Route 53 - A reliable and cost-effective way to route end users to Internet applications","title":"Route 53","uri":"/en/tracks/aws-certified-developer-associate/route53/"},{"content":"About The Relative Strength Index (RSI) is a momentum oscillator that measures the speed and change of price movements. Developed by J. Welles Wilder, the RSI is a very popular indicator that is used primarily to identify overbought and oversold conditions in a market.\nCalculating The RSI is calculated using the following steps:\nCalculate the average gain and the average loss over the specified period (usually 14 periods). Compute the relative strength (RS) which is the ratio of average gain to average loss. The RSI is then calculated using the formula: RSI = 100 - (100 / (1 + RS)) Formula RSI = 100 - (100 / (1 + RS))\nWhere:\nRS: (Relative Strength) = Average Gain / Average Loss Pros and Cons Pros:\nRSI is a versatile indicator that can be used in trending or ranging markets. It helps identify potential reversals, overbought/oversold conditions, and divergence. Cons:\nDuring strong trends, the RSI may remain overbought or oversold for extended periods. False signals can occur, especially during choppy market conditions. Example of signals Minute Open High Low Close 1 $10.0 $11.0 $9.5 $10.0 2 $10.1 $12.1 $10.0 $12.0 3 $12.2 $15.2 $12.0 $15.0 4 $15.1 $15.1 $13.9 $14.0 5 $14.1 $16.1 $14.0 $16.0 6 $16.1 $16.1 $14.9 $15.0 7 $15.1 $17.1 $15.0 $17.0 8 $17.1 $17.1 $15.9 $16.0 9 $16.1 $18.1 $16.0 $18.0 10 $18.1 $18.1 $16.9 $17.0 11 $17.1 $19.1 $17.0 $19.0 12 $19.1 $19.1 $17.9 $18.0 13 $18.1 $20.1 $18.0 $20.0 14 $20.1 $21.1 $20.0 $21.0 15 $21.1 $21.1 $19.9 $20.0 16 $20.1 $22.1 $20.0 $21.0 17 $21.1 $23.1 $21.0 $22.0 18 $22.1 $24.1 $22.0 $23.0 19 $23.1 $25.1 $23.0 $24.0 20 $24.1 $26.1 $24.0 $25.0 21 $25.1 $27.1 $25.0 $26.0 For the following examples, let’s assume the RSI settings are set to the standard 14 periods.\nTrue Positive:\nIn the 6th minute, the price drops from $16 to $15. The RSI dips below the 30 level, indicating an oversold condition. In the next minute, the price increases to $17, marking a successful buy signal.\nFalse Positive:\nIn the 21th minute, the price jumps from $25 to $26. The RSI exceeds 70, implying an overbought condition. However, the price continues to increase in the next minutes, rendering this a false sell signal.\nUse in Real Trading In real trading, RSI is often used in conjunction with other indicators to increase the robustness of the signals.\nFor instance, traders might look for price action patterns (like support/resistance breaks) or confirmations from other indicators before acting on an RSI signal.\nPython Implementation Click here to view this notebook in full screen ","description":"A comprehensive guide to the Relative Strength Index (RSI) trading indicator.","title":"RSI - Relative Strength Index","uri":"/en/posts/trading-indicators/rsi/"},{"content":"A Segment Tree is a data structure used for efficiently processing queries over intervals or ranges. It is commonly used for solving problems that involve finding information about a set of elements and their sub-intervals.\nSince a Segment Tree is a binary tree, a simple linear array can be used to represent the Segment Tree. Before building the Segment Tree, one must figure what needs to be stored in the Segment Tree’s node?.\nEach leaf node represents a single element, and each internal node represents the union of its children’s ranges. Queries are performed by traversing the tree from the root to the leaves, and aggregating information about the ranges that intersect with the query interval.\nExample 1, consider a set of numbers and the task of finding the minimum value in a range of these numbers. The Segment Tree can be used to solve this problem by storing the minimum value of each range in the tree’s internal nodes, and answering queries by searching for the smallest value in the portion of the tree that covers the query interval.\nExample 2, if the question is to find the sum of all the elements in an array from L indices to R, then at each node (except leaf nodes) the sum of its children nodes is stored.\nThe Segment Tree can be constructed in O(n log n) time, where n is the number of elements in the original set, and it can answer queries in O(log n) time. This makes it an efficient data structure for processing queries over large datasets.\nTo study the topic “segment tree” you need to know the following concepts:\narrays loops conditional operators bitwise operations A Segment Tree is a dynamic data structure used to perform operations on and update intervals. It supports two operations: Element update (update) on a given range and request (query) on the sum of elements in a given range.\nLet’s perform the following task: we have an array and we want to find the sum of the elements in a given range.\nFor this task, we can use a segment tree. It is constructed as a binary tree, where each node represents an interval and the value of the node is the sum of the elements in that interval.\nFundamentals:\nDefinition of a sum element in a segment tree: A sum element in a segment tree is the sum of all elements in the range it represents.\nConstructing a segment tree: A segment tree can be constructed from an array of numbers. Each node in the tree represents a range of elements in the array and stores the sum of the elements in that range.\nImplementation of operations: The implementation of various operations in a segment tree essentially depends on its structure. However, there are several operations that are often used in various tasks:\nUpdate value in the array: This operation allows you to change the value of an element in an array. It is usually implemented using a recursive tree traversal.\nQuery Value: This operation allows you to query the value of an element in an array. It is also usually implemented using recursive tree traversal.\nQuery for a sum: This operation allows you to query the sum of values in an array on a given interval. It is usually implemented by recursive tree traversal and sum counting\nBuilding a spanning tree Since the tree is binary, each vertex will have up to two descendants.\nGraphically it looks as follows (for an array of 8 elements):\nAt the topmost vertex the segment from the beginning of the array to the last element is fixed.\nOn the left is the left half of the parent ([0 1 2 3]). On the right is the right half ()[4 5 6 7]). And so on up to the last node with a segment of one element.\nTake the array a = [1, 3, -2, 8, -7]. We use it to build a tree of segments to write the sums of these segments in each node.\nThe structure of such a tree is as follows:\n💡 The tree contains less than 2n vertices. 2*n-1\nThe number of vertices in the worst case is estimated by the sum $n + \\frac{n}{2} + \\frac{n}{4} + \\frac{n}{8} + \\ldots + 1 \u003c 2n$\nLet us display such a tree as an array:\nThere are 9 vertices in such a tree. The array will consist of 9 elements. tree[0] = A[0:4] tree[1] = A[0:2] tree[2] = A[3:4] tree[3] = A[0:1] tree[4] = A[2:2] tree[5] = A[3:3] tree[6] = A[4:4] tree[7] = A[0:0] tree[8] = A[1:1] This tree covers all vertices.\nWith this structure, you can store different data in the vertex values, such as the sum of the segment, the smallest/the largest number, or other aggregate data on the segments.\nImplementing a segment tree in Python Initializing the tree\na = [1, 3, -2, 8, -7]\nSince the most recent nodes are segments of length == 1. So we start the process of creation with them, gradually rising to the level above.\n💡 The tree contains less than 2n vertices. 💡 Bottom vertex - the length of the segment is equal to 1.\ndef build_tree(array): n = len(array) tree = [0] * 2 * n # The tree contains less than **2n** vertices. for i in range(n): tree[n + i] = a[i] # the lowest tops of the tree # add parent nodes for i in reversed(range(n)): tree[i] = tree[2 * i] + tree[2 * i + 1] print(i, tree) \u003e\u003e array = [1, 3, -2, 8, -7] \u003e\u003e build_tree(array) 4 [0, 0, 0, 0, 1, 1, 3, -2, 8, -7] 3 [0, 0, 0, 1, 1, 1, 3, -2, 8, -7] 2 [0, 0, 2, 1, 1, 1, 3, -2, 8, -7] 1 [0, 3, 2, 1, 1, 1, 3, -2, 8, -7] 0 [3, 3, 2, 1, 1, 1, 3, -2, 8, -7] Calculating the sum on the segment:\nThe function gets the indexes of the original array.\nWhen we created the tree from the source array, we placed each individual element on the new index [n + i].\n💡 So when the function takes an index, we first find the bottommost element in the tree. It is located in the new array by the index [length_of_source_array + index]\n# calculate the sum on the segment def query_tree(l, r): global tree, n sum = 0 l += n # current item index r += n while l \u003c= r: if l % 2 == 1: # if the index is odd sum += tree[l] l += 1 if r % 2 == 0: sum += tree[r] r -= 1 l //= 2 # floor division. 8 // 3 = 2 r //= 2 return sum \u003e\u003e a = [1, 3, -2, 8, -7] \u003e\u003e n = len(a) \u003e\u003e tree = build_tree(a) \u003e\u003e query_tree(0, 4) # sum([1, 3, -2, 8, -7]) 3 \u003e\u003e query_tree(1, 3) # sum([3, -2, 8]) 9 \u003e\u003e query_tree(4, 4) -7 We get the SegmentTree class:\nSum function or any other function can be turned on at the time of tree generation\nclass SegmentTree: def __init__(self, a): self.n = len(a) self.tree = [0] * 2 * self.n for i in range(self.n): self.tree[self.n + i] = a[i] for i in range(self.n - 1, 0, -1): self.tree[i] = self.tree[2*i] + self.tree[2*i+1] def calculate_sum(self, l, r): sum = 0 l += self.n r += self.n while l \u003c= r: if l % 2 == 1: sum += self.tree[l] l += 1 if r % 2 == 0: sum += self.tree[r] r -= 1 l //= 2 r //= 2 return sum def find_value(self, l, r): l += self.n r += self.n while l \u003c r: if r % 2 == 0: r -= 1 else: r -= 1 l += 1 return l - self.n Segment Tree template class SegmentTree: def __init__(self, data, default=0, func=max): self._default = default self._func = func self._len = len(data) self._size = _size = 1 \u003c\u003c (self._len - 1).bit_length() self.data = [default] * (2 * _size) self.data[_size:_size + self._len] = data for i in reversed(range(_size)): self.data[i] = func(self.data[i + i], self.data[i + i + 1]) def __delitem__(self, idx): self[idx] = self._default def __getitem__(self, idx): return self.data[idx + self._size] def __setitem__(self, idx, value): idx += self._size self.data[idx] = value idx \u003e\u003e= 1 while idx: self.data[idx] = self._func(self.data[2 * idx], self.data[2 * idx + 1]) idx \u003e\u003e= 1 def __len__(self): return self._len def query(self, start, stop): \"\"\"func of data[start, stop)\"\"\" start += self._size stop += self._size if start==stop: return self._default res_left = res_right = self._default while start \u003c stop: if start \u0026 1: res_left = self._func(res_left, self.data[start]) start += 1 if stop \u0026 1: stop -= 1 res_right = self._func(self.data[stop], res_right) start \u003e\u003e= 1 stop \u003e\u003e= 1 return self._func(res_left, res_right) def __repr__(self): return \"SegmentTree({0})\".format(self.data) The build_tree method builds a segment tree, and query allows you to perform query operations.\nLinks https://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/ ","description":"","title":"Segment Tree","uri":"/en/tracks/algorithms-101/data-structures/segment-tree/"},{"content":"“mongodb” free tier vs “documentdb”\nProject structure - src - app.py - mongo.py - .env - requirements.txt - serverless.yml Sources\nAdd AIM user Setup specific user for serverless deployment\nusername: serverless-deployer\naws aim documentation Set policy Create:\nServerLessDeployerPolicyGroup ServerLessDeployerPolicy Policy:\n{ \"Statement\": [ { \"Action\": [ \"apigateway:*\", \"cloudformation:CancelUpdateStack\", \"cloudformation:ContinueUpdateRollback\", \"cloudformation:CreateChangeSet\", \"cloudformation:CreateStack\", \"cloudformation:CreateUploadBucket\", \"cloudformation:DeleteStack\", \"cloudformation:Describe*\", \"cloudformation:EstimateTemplateCost\", \"cloudformation:ExecuteChangeSet\", \"cloudformation:Get*\", \"cloudformation:List*\", \"cloudformation:UpdateStack\", \"cloudformation:UpdateTerminationProtection\", \"cloudformation:ValidateTemplate\", \"dynamodb:CreateTable\", \"dynamodb:DeleteTable\", \"dynamodb:DescribeTable\", \"dynamodb:DescribeTimeToLive\", \"dynamodb:UpdateTimeToLive\", \"ec2:AttachInternetGateway\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CreateInternetGateway\", \"ec2:CreateNetworkAcl\", \"ec2:CreateNetworkAclEntry\", \"ec2:CreateRouteTable\", \"ec2:CreateSecurityGroup\", \"ec2:CreateSubnet\", \"ec2:CreateTags\", \"ec2:CreateVpc\", \"ec2:DeleteInternetGateway\", \"ec2:DeleteNetworkAcl\", \"ec2:DeleteNetworkAclEntry\", \"ec2:DeleteRouteTable\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteSubnet\", \"ec2:DeleteVpc\", \"ec2:Describe*\", \"ec2:DetachInternetGateway\", \"ec2:ModifyVpcAttribute\", \"events:DeleteRule\", \"events:DescribeRule\", \"events:ListRuleNamesByTarget\", \"events:ListRules\", \"events:ListTargetsByRule\", \"events:PutRule\", \"events:PutTargets\", \"events:RemoveTargets\", \"iam:AttachRolePolicy\", \"iam:CreateRole\", \"iam:DeleteRole\", \"iam:DeleteRolePolicy\", \"iam:DetachRolePolicy\", \"iam:GetRole\", \"iam:PassRole\", \"iam:PutRolePolicy\", \"iot:CreateTopicRule\", \"iot:DeleteTopicRule\", \"iot:DisableTopicRule\", \"iot:EnableTopicRule\", \"iot:ReplaceTopicRule\", \"kinesis:CreateStream\", \"kinesis:DeleteStream\", \"kinesis:DescribeStream\", \"lambda:*\", \"logs:CreateLogDelivery\", \"logs:CreateLogGroup\", \"logs:DeleteLogGroup\", \"logs:DescribeLogGroups\", \"logs:DescribeLogStreams\", \"logs:FilterLogEvents\", \"logs:GetLogEvents\", \"logs:PutSubscriptionFilter\", \"s3:CreateBucket\", \"s3:DeleteBucket\", \"s3:DeleteBucketPolicy\", \"s3:DeleteObject\", \"s3:DeleteObjectVersion\", \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:ListAllMyBuckets\", \"s3:ListBucket\", \"s3:PutBucketNotification\", \"s3:PutBucketPolicy\", \"s3:PutBucketTagging\", \"s3:PutBucketWebsite\", \"s3:PutEncryptionConfiguration\", \"s3:PutObject\", \"sns:CreateTopic\", \"sns:DeleteTopic\", \"sns:GetSubscriptionAttributes\", \"sns:GetTopicAttributes\", \"sns:ListSubscriptions\", \"sns:ListSubscriptionsByTopic\", \"sns:ListTopics\", \"sns:SetSubscriptionAttributes\", \"sns:SetTopicAttributes\", \"sns:Subscribe\", \"sns:Unsubscribe\", \"states:CreateStateMachine\", \"states:DeleteStateMachine\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ], \"Version\": \"2012-10-17\" } Create user copy the API Key \u0026 Secret\nNeed during setup aws cli/serverless\nCreate serverless.yml In the root folder create:\norg: romankurnovskii app: app-name service: app-service-name frameworkVersion: '3' useDotenv: true custom: wsgi: app: src/app.app packRequirements: false provider: name: aws deploymentMethod: direct region: eu-west-1 runtime: python3.9 architecture: arm64 versionFunctions: false memorySize: 128 functions: api: handler: wsgi_handler.handler events: - httpApi: '*' environment: MONGO_CONNECTION_STRING: ${env:MONGO_CONNECTION_STRING} MONGO_COLLECTION_DB_NAME: ${env:MONGO_COLLECTION_DB_NAME} package: patterns: - '!.dynamodb/**' - '!.git/**' - '!.vscode/**' - '!.env' - '!node_modules/**' - '!tmp/**' - '!venv/**' - '!__pycache__/**' plugins: - serverless-wsgi - serverless-python-requirements Create Flask app Prerequisites python -m venv ./venv source ./venv/bin/activate App src/app.py\nfrom flask import Flask, ObjectId, request, jsonify, make_response from flask_cors import CORS import json from src.mongo import my_db users_collection = my_db.users app = Flask(__name__) cors = CORS(app) @app.route(\"/\", methods=['GET']) def get_user(user_id): user_id = request.args.get('id') user = users_collection.find_one({\"_id\": ObjectId(user_id)}) if not user: return jsonify({'error': 'data not found'}), 404 return jsonify({'user': user}) @app.route('/', methods=['PUT']) def create_record(): record = json.loads(request.data) user_id = record.get('user_id', None) users_collection.update_one({\"_id\": ObjectId(user_id)}, record) @app.route(\"/\") def hello(): return jsonify(message='Hello!') @app.errorhandler(404) def resource_not_found(e): return make_response(jsonify(error='Not found!'), 404) def internal_server_error(e): return 'error', 500 app.register_error_handler(500, internal_server_error) src/mongo.py\nimport os from pymongo import MongoClient MONGO_CONNECTION_STRING = os.environ.get( \"MONGO_CONNECTION_STRING\", default=\"mongodb://localhost:27017/\" ) MONGO_COLLECTION_DB_NAME = os.environ.get( \"MONGO_COLLECTION_DB_NAME\", default=\"test-mydb\" ) db_client = MongoClient(MONGO_CONNECTION_STRING) my_db = db_client[MONGO_COLLECTION_DB_NAME] .env\nMONGO_CONNECTION_STRING=mongodb+srv://login:password@cluster0.XXXXX.mongodb.net/mydb?retryWrites=true\u0026w=majority MONGO_COLLECTION_DB_NAME=mydb src/requirements.txt\ncertifi==2022.6.15 charset-normalizer==2.1.1 click==7.1.2 dnspython==2.2.1 ecdsa==0.18.0 Flask==1.1.4 Flask-Cors==3.0.10 idna==3.3 importlib-metadata==4.12.0 itsdangerous==1.1.0 Jinja2==2.11.3 jmespath==1.0.1 MarkupSafe==2.0.1 pyasn1==0.4.8 pymongo==4.2.0 python-dateutil==2.8.2 python-dotenv==0.20.0 requests==2.28.1 rsa==4.9 six==1.16.0 urllib3==1.26.12 Werkzeug==1.0.1 zipp==3.8.1 Deployment serverless login install dependencies with:\nnpm install and\npip install -r requirements.txt and then perform deployment with:\nserverless deploy After running deploy, you should see output similar to:\nDeploying app-service-name to stage dev (eu-west-1) ✔ Service deployed to stack app-service-name (182s) Local development Thanks to capabilities of serverless-wsgi, it is also possible to run your application locally, however, in order to do that, you will need to first install werkzeug dependency, as well as all other dependencies listed in requirements.txt. It is recommended to use a dedicated virtual environment for that purpose. You can install all needed dependencies with the following commands:\nAlready in requirements.txt:\npip install werkzeug pip install -r requirements.txt At this point, you can run your application locally with the following command:\nserverless wsgi serve For additional local development capabilities of serverless-wsgi plugin, please refer to corresponding GitHub repository.\n","description":"Create AWS serverless application on Flask + API Gateway + Lambda + MongoDB","title":"Serverless: Flask+API Gateway+Lambda+MongoDB","uri":"/en/posts/serverless-flask-lambda-api-gateway-mongodb/"},{"content":"Lab Sessionizing Clickstream Data with Amazon Kinesis Data Analytics Creating an Amazon Kinesis Data Analytics Application 1. In the AWS Management Console search bar, enter Kinesis, and click the Kinesis result under Services:\nYou will be taken to the Amazon Kinesis dashboard.\nIn this lab, a Kinesis Data Stream has been pre-created for you. Under Data Streams you will see Total data streams is one:\n2. In the left-hand menu, click Analytics applications and under that click SQL applications:\n3. To start creating a Kinesis Data Analytics application, under Data Analytics, click Create SQL application (legacy):\nYou will be taken to the Create legacy SQL application form.\n4. In the Application configuration section, and enter lab-application in the Application name textbox:\n5. At the bottom of the page, click Create legacy SQL application:\nYou will be taken to a page displaying details of your application and you will see a notification that your application has been created:\nYou will come back to this page later in the lab to connect the pre-created Kinesis Data Stream as a data source for your Kinesis Data Analytics application.\n6. To navigate to the Kinesis Data Streams list page, in the left-hand side menu, click Data streams:\nYou will see one data stream listed called lab-stream.\n7. To view the details of the pre-created data stream, in the list, click lab-stream:\nYou will be taken to the Stream details page and you will see a series of tabs with Monitoring selected.\n8. To see the configuration details of the data stream, click Configuration:\nTake a moment to look at the details on this page, there are several Kinesis Data Stream configuration options that you should be aware of:\nData Stream capacity: The number of shards in the Data Stream. Each shard has a maximum read and write capacity. To increase the total capacity of a data stream you can add shards. Encryption: Kinesis Data Streams can be encrypted using an AWS managed or customer-managed, KMS key. Data retention: A Kinesis Data Stream can retain data for a configurable amount of time between 24 and 168 hours. Enhanced (shard-level) metrics: More detailed CloudWatch metrics can be enabled for a Data Stream, these enhanced metrics have an extra cost. In this lab, you will be working with a small amount of sample data, so there is one shard configured.\nLeave these options without changing them.\nConnecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n2. To see available instances, click Instances in the left-hand menu:\nThe instances list page will open, and you will see an instance named cloudacademylabs:\nIf you don’t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n3. Right-click the cloudacademylabs instance, and click Connect:\nThe Connect to your instance form will load.\n4. In the form, ensure the EC2 Instance Connect tab is selected:\nYou will see the instance’s Instance ID and Public IP address displayed.\n5. In the User name textbox, enter ec2-user:\nNote: Ensure there is no space after ec2-user or connect will fail.\n6. To open a browser-based shell, click Connect:\nIf you see an error it’s likely that the environment hasn’t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\nA browser-based shell will open in a new window ready for you to use.\nKeep this window open, you will use it in later lab steps.\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\nSimulating a Real-Time Clickstream 1. To create a template JSON file for a click event, enter the following command into the shell:\necho '{ \"user_id\": \"$USER_ID\", \"event_timestamp\": \"$EVENT_TIMESTAMP\", \"event_name\": \"$EVENT_NAME\", \"event_type\": \"click\", \"device_type\": \"desktop\" }' \u003e click.json There are two parts to this command, the first uses the built-in Bash command echo to print a JSON template. The second part uses a feature of the Bash shell called redirection, it redirects the output of the echo command to a file (creating it if doesn’t exist) called click.json.\nThe template contains five fields, the event_type, and device_type fields are hardcoded, in a non-lab environment, you may encounter streams that come from different types of devices and streams that contain more than one type of event (clickstream events alongside sales or transaction data for example). The other fields will be populated dynamically.\n2. To put records into Kinesis and simulate a clickstream, enter the following command:\nUSER_IDS=(user1 user2 user3) EVENTS=(checkout search category detail navigate) for i in $(seq 1 3000); do echo \"Iteration: ${i}\" export USER_ID=\"${USER_IDS[RANDOM%${#USER_IDS[@]}]}\"; export EVENT_NAME=\"${EVENTS[RANDOM%${#EVENTS[@]}]}\"; export EVENT_TIMESTAMP=$(($(date +%s) * 1000)) JSON=$(cat click.json | envsubst) echo $JSON aws kinesis put-record --stream-name lab-stream --data \"${JSON}\" --partition-key 1 --region us-west-2 session_interval=15 click_interval=2 if ! (($i%60)); then echo \"Sleeping for ${session_interval} seconds\" \u0026\u0026 sleep ${session_interval} else echo \"Sleeping for ${click_interval} second(s)\" \u0026\u0026 sleep ${click_interval} fi done You will see the templated JSON and also the JSON response from Kinesis for each record put into the Data Stream:\nThis command simulates a real-time click-stream with the following characteristics:\nCreates three thousand events Events have a two-second interval between them After every sixty events (two minutes) there is a fifteen-second interval, later you will assume a gap of ten seconds or more is a session boundary The command has a number of parts:\nSetup of sample user ids and event types at the beginning A loop that will execute three thousand times and a sleep statement Statements that randomly select a user id and an event type, and assign them along with the current timestamp to variables A statement that uses the envsubst command to substitute defined environment variables in the JSON template A statement invoking the AWS command-line interface tool, putting the templated JSON record into the Kinesis Data Stream A condition at the end of the loop that either sleeps for a few seconds or, periodically for longer, simulating the end of a session Leave the command running.\nNavigate to Kinesis Data Analytics in the AWS Management Console. 4. In the list of applications, to expand the application, click lab-application:\n5. To connect your Data Analytics application to the pre-created Data Stream, click ConfigureunderSource streamform:\nThe Configure source for lab-application form will load.\n6. Under Source, ensure Kinesis data stream is selected:\n7. In the Kinesis data stream, click Browse to select the radio button for lab-streamand clickChoose:\n8. Under Access permissions, select Choose from IAM roles that Kinesis Data Analytics can assume:\n9. In the IAM role list, select the role beginning with cloudacademy-lab-data-analytics:\nIf you don’t see the above role listed click the refresh button:\n10. To start discovering the schema of the records you added to the Data Stream, click Discover schema:\nAfter a moment or two, you will see a notification that the discovery was successful and below, some of the records will be displayed:\n11. To finish connecting your Data Analytics application to your Data Stream, click Save changes:\nYou will be redirected to the page for your Kinesis Data Analytics application. Leave this page open in a browser tab.\nSessionizing the Clickstream Data using Amazon Kinesis Data Analytics 1. Return to the page for your Kinesis Data Analytics application in the AWS Management Console.\n2. To start your application and expand the Steps to configure your application, click Configure SQL:\n3. In the SQL code editor, replace the existing contents with the following SQL commands\nCREATE OR REPLACE STREAM \"INTERMEDIATE_SQL_STREAM\" ( \"event_timestamp\" TIMESTAMP, \"user_id\" VARCHAR(7), \"device_type\" VARCHAR(10), \"session_timestamp\" TIMESTAMP ); CREATE OR REPLACE PUMP \"STREAM_PUMP1\" AS INSERT INTO \"INTERMEDIATE_SQL_STREAM\" SELECT STREAM TO_TIMESTAMP(\"event_timestamp\") as \"event_timestamp\", \"user_id\", \"device_type\", CASE WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) \u003e (10 * 1000) THEN TO_TIMESTAMP(\"event_timestamp\") WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) IS NULL THEN TO_TIMESTAMP(\"event_timestamp\") ELSE NULL END AS \"session_timestamp\" FROM \"SOURCE_SQL_STREAM_001\"; These statements do the following:\nDefines an intermediate stream to insert data into called INTERMEDIATE_SQL_STREAM Creates a PUMP that selects data from the source stream The SELECT statement uses the LAG function to determine if there is a ten-second interval between the last event and the current event The LAG function statements are used with PARTITION statements to restrict the LAG function by the user You should know that Kinesis Data Analytics natively assumes Unix timestamps include milliseconds. The stream you simulated is providing timestamps with milliseconds. This is why the CASE WHEN statement that checks for a ten-second interval includes (10 * 1000), it’s multiplying ten by one thousand to get ten seconds in milliseconds.\nTip: you can increase the height of the SQL editor text-box by dragging the grey bar at the bottom.\n4. To execute the SQL statements, click Save and run application:\nThe query will take up to a couple of minutes to execute and start returning results.\nOccasionally you may see an error caused by the fifteen-second interval, if you do, re-run the query by clicking Save and run application again.\nTake a look at the results. Notice that only some records have a value for session_timestamp. This is because the CASE WHEN statement in the query supplies a value of null when:\nThe interval between event timestamps is less than ten seconds There is no preceding event Also notice that below the SQL Code editor, there are two streams, the INTERMEDIATE_SQL_STREAM, and an error_stream. The error stream is where any errors that occur during the execution of the SQL will be delivered to.\n5. In the SQL editor window, under the current SQL statements, add the following:\nCREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\" ( \"user_id\" CHAR(7), \"session_id\" VARCHAR(50), \"session_time\" VARCHAR(20), \"latest_time\" VARCHAR(20) ); CREATE OR REPLACE PUMP \"STREAM_PUMP2\" AS INSERT INTO \"DESTINATION_SQL_STREAM\" SELECT STREAM \"user_id\", \"user_id\"||'_'||\"device_type\"||'_'||TIMESTAMP_TO_CHAR('HH:mm:ss', LAST_VALUE(\"session_timestamp\") IGNORE NULLS OVER (PARTITION BY \"user_id\" RANGE INTERVAL '24' HOUR PRECEDING)) AS \"session_id\", TIMESTAMP_TO_CHAR('HH:mm:ss', \"session_timestamp\") AS \"session_time\", TIMESTAMP_TO_CHAR('HH:mm:ss', \"event_timestamp\") AS \"latest_time\" FROM \"INTERMEDIATE_SQL_STREAM\" WHERE \"user_id\" = 'user1'; These SQL statements do the following:\nCreates a stream called DESTINATION_SQL_STREAM Creates a PUMP that selects from the INTERMEDIATE_SQL_STREAM Constructs a session_id by combining the user, device type and time Restricts the query to user1 using a WHERE clause Something else to note about these statements is that the session and event timestamps are being converted to times.\n6. To run the updated query, click Save and run application.\nYou will see results similar to:\nYour times will be different.\nNotice that the session_time values are more than ten seconds apart. And that the seconds’ interval of the latest_time column between the rows that have a session time, is ten seconds or less.\n7. To see only the rows for new sessions, replace the last line of the query with the following:\nWHERE \"session_timestamp\" IS NOT NULL; This change to the WHERE clause of the last SQL statement removes the restriction of the query to user1, and removes rows where the value of session_timestamp is null.\n8. Click Save and run application to re-run your query.\nYou will see results similar to the following:\nYour results will be different.\nThe results now contain only session boundary rows for each of the users.\nLeave this browser tab open with the query running in Kinesis Data Analytics.\nCreating an AWS Lambda function to Store Sessions in an Amazon DynamoDB Table 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\n2. To start creating your function, click Create function:\n3. Under Create function, ensure Author from scratch is selected:\n4. Under Basic information, in the Function name text-box, enter lab-function:\n5. In the Runtime drop-down, select the latest Python 3.x version available.\n6. To expand the role selection form, click Change default execution role.\n7. Under Execution role, select the Use an existing role radio button:\n8. To assign an execution role, in the Existing role drop-down, select the role called cloudacademy-lab-lambda:\n9. To create your function, click Create function:\nYou will be taken a page where you can configure your function, and you will see a notification that your function has been successfully created:\n10. Scroll down to the Code source section and in the code editor double-click the lambda_function.py file.\n11. To update your Lambda function’s implementation, replace the code in the editor window with the following:\nfrom __future__ import print_function import boto3 import base64 from json import loads dynamodb_client = boto3.client('dynamodb') table_name = \"CloudAcademyLabs\" def lambda_handler(event, context): payload = event['records'] output = [] success = 0 failure = 0 for record in payload: try: payload = base64.b64decode(record['data']) data_item = loads(payload) ddb_item = { 'session_id': { 'S': data_item['session_id'] }, 'session_time': { 'S': data_item['session_time'] }, 'user_id': { 'S': data_item['user_id'] } } dynamodb_client.put_item(TableName=table_name, Item=ddb_item) success += 1 output.append({'recordId': record['recordId'], 'result': 'Ok'}) except Exception: failure += 1 output.append({'recordId': record['recordId'], 'result': 'DeliveryFailed'}) print('Successfully delivered {0} records, failed to deliver {1} records'.format(success, failure)) return {'records': output} This python code processes a record from Kinesis Data Analytics and puts it into a DynamoDB table.\nThe implementation is based on one provided by AWS. The only change is the statements that construct the ddb_item. They have been modified to match the data being supplied by your Kinesis Data Analytics application.\n12. To deploy your function, at the top, click Deploy:\nYou will see a notification that your function has been deployed:\n13. To configure a timeout for your function, click the Configuration tab, and click Edit:\n14. Under Timeout, enter 1 in the min text-box, and 0 in the sec text-box:\nYou are updating the timeout because the default of three seconds is too low when processing data from Kinesis Data Analytics, and may lead to failures caused by the function timing out. AWS recommends setting a higher timeout to avoid such failures.\n15. To save your function’s updated timeout, click Save:\nYou will see a notification that your change to the timeout has been saved:\nConfiguring Amazon Kinesis Data Analytics to Use Your AWS Lambda Function as a Destination 1. Navigate to Kinesis Data Analytics in the AWS Management Console.\n2. In the list of applications, to expand the application, click lab-application:\n3. To begin configuring your Lambda as a destination, expand the Steps to configure your application and click Add destination:\nThe Configure destination form will load.\n4. Under Destination select AWS Lambda function:\n5. Under AWS Lambda function, click Browse and check radio box for lab-functionfollowed by clickingChoose:\nThis is the Lambda function you created in the previous lab step.\n6. Under Access permissions, ensure Choose from IAM roles that Kinesis Data Analytics can assume is selected:\n7. In the IAM role drop-down, select the role called cloudacademy-lab-lambda:\nThis is a role that has been pre-created for this lab and allows Kinesis Data Analytics to invoke your Lambda function.\n8. In the In-application stream section, under Connect in-application stream, select Choose an existing in-application stream:\n9. In the In-application stream name drop-down, select DESTINATION_SQL_STREAM:\n10. To finish connecting your Kinesis Data Analytics application to your Lambda function, click Save changes:\nYour Kinesis Data Analytics application is being updated. Please be aware that it can take up to three minutes to complete.\nOnce complete the details page for Kinesis Data Analytics application will load.\n11. In the AWS Management Console search bar, enter DynamoDB, and click the DynamoDB result under Services:\n12. In the left-hand menu, click Tables:\n13. In the list of tables, click CloudAcademyLabs:\nThis table was pre-created as a part of this lab.\n14. To see items in the DynamoDB table, click the **Explore Table **Items button:\nYou will see the items in the table listed similar to:\nThese items have been inserted into the DyanmoDB table by your Lambda function, it’s being invoked by your Kinesis Data Analytics application.\n","description":"Sessionizing Clickstream Data with Amazon Kinesis Data Analytics","title":"Sessionizing Clickstream Data with Amazon Kinesis Data Analytics","uri":"/en/tracks/aws-certified-developer-associate/kinesis/sessionizing-clickstream-data-kinesis-data-analytics/"},{"content":"Today, I embarked on a journey to create an accessible, affordable, and easy-to-use VPN solution that anyone can set up on their devices using AWS.\nFinding a reasonably priced solution was critical. I opted for Amazon’s t3a.nano ARM Ubuntu image, which costs ~$0.0047/hour ~ $3.8/month.\nVarious Amazon EC2 instances and their prices are listed on Amazon’s On-Demand Pricing page.\nHere is the list of all Ubuntu AMI images to identify the appropriate image for the project. The selected image combined with the CloudFormation template, which was inspired by the AWS CloudFormation General Reference, provides a seamless and consistent user experience.\nNow it takes 2-3 minutes to create OpenVPN server using this repo.\n","description":"Setup OpenVPN on AWS EC2","title":"Setup OpenVPN Server on AWS EC2 Ubuntu","uri":"/en/stories/002-openvpn-aws-ec2-setup/"},{"content":"TLDR Code sources\nCreate Projects Creating three example projects:\nnode.js express server that returns json on request another node.js express server static html page Place projects data in dir ‘projects’.\nStructure:\n└── project ├── Dockerfile ├── nginx.conf ├── projects | ├── 1 | | ├── app.js | | └── package.json | ├── 2 | | ├── app.js | | └── package.json | └── 3 | └── index.html └── start.sh Nginx setup Server needs to “understand” the initial subdomain and where to forward.\nFor this use reverse proxy\nnginx.conf:\nworker_processes 1; events { worker_connections 1024; } http { sendfile on; server { listen 80; server_name mydomain1.localhost; location / { proxy_pass http://localhost:3000; } } server { listen 80; server_name mydomain2.localhost; location / { proxy_pass http://localhost:4000; } } server { listen 80; server_name mydomain3.localhost; root /var/www/domains/mydomain_with_static_files; location / { try_files $uri $uri/ =404; } } } Start apps To start js applications need to run node command.\nstart.sh:\n#!/bin/sh # Start the first app node /opt/projects/1/app.js \u0026 # Start the second app node /opt/projects/2/app.js \u0026 # Start nginx in the foreground nginx -g 'daemon off;' Docker file Next, we’ll create a Dockerfile to define the ‘centos’ Docker container where we can test our setup. Here’s what the Dockerfile looks like:\nFROM centos:latest # fixes RUN cd /etc/yum.repos.d/ RUN sed -i 's/mirrorlist/#mirrorlist/g' /etc/yum.repos.d/CentOS-* RUN sed -i 's|#baseurl=http://mirror.centos.org|baseurl=http://vault.centos.org|g' /etc/yum.repos.d/CentOS-* RUN yum update -y \u0026\u0026 yum install -y curl vim git RUN curl -sL https://rpm.nodesource.com/setup_14.x | bash - RUN yum install -y nodejs # Install Nginx RUN yum install -y epel-release RUN yum install -y nginx RUN mkdir -p /opt/projects/1 ADD ./projects/1 /opt/projects/1 WORKDIR /opt/projects/1 RUN npm install RUN mkdir -p /opt/projects/2 ADD ./projects/2 /opt/projects/2 WORKDIR /opt/projects/2 RUN npm install RUN mkdir -p /var/www/domains/mydomain_with_static_files ADD ./projects/3 /var/www/domains/mydomain_with_static_files COPY start.sh /opt/projects/start.sh RUN chmod +x /opt/projects/start.sh EXPOSE 80 COPY nginx.conf /etc/nginx/nginx.conf RUN ls -la /etc/nginx/ # CMD [\"nginx\", \"-g\", \"daemon off;\"] CMD [\"/opt/projects/start.sh\"] Test run Build docker image and run docker build -t myserver . docker run -p 80:80 myserver Open in browser on host: project 1: http://mydomain1.localhost project 2: http://mydomain2.localhost project 3: http://mydomain3.localhost ","description":"Setup subdomains on VPS CentOS. Example with Docker image.","title":"Setup subdomains on VPS CentOS","uri":"/en/posts/vps-docker-subdomains-setup/"},{"content":" Create ","description":"Generate short description from article","title":"Short description from article","uri":"/en/tracks/disser/utils/text_2_short/"},{"content":"About The Simple Moving Average (SMA) is a technical indicator that calculates the average price over a specific number of periods, and it moves along as new data is added, making it a “moving average”.\nCalculating Formula SMA = (Sum of price data for the last N periods) / N\nFor example, a 5-period SMA would sum up the last 5 closing prices and divide it by 5 to find the average. It is called a ‘moving’ average because as new prices become available, the oldest prices are dropped and the average recalculates.\nMinute Open High Low Close SMA 1 $10.0 $11.0 $9.5 $10.0 - 2 $10.1 $12.1 $10.0 $12.0 - 3 $12.2 $15.2 $12.0 $15.0 - 4 $15.1 $15.1 $13.9 $14.0 - 5 $14.1 $16.1 $14.0 $16.0 $13.4 Using our given market data and specifically the closing prices, here’s how the 5-minute SMA would be calculated after Minute 5:\nMinute 1-5 close prices: $10.0, $12.0, $15.0, $14.0, $16.0 SMA = (10.0 + 12.0 + 15.0 + 14.0 + 16.0) / 5 = $13.4 The 5-minute SMA after the 5th minute would be $13.4.\nPros and Cons Pros:\nThe SMA is simple and easy to calculate and understand. It smooths out price fluctuations and helps to filter out the “noise” of the market. It’s useful for identifying trend directions over the specified period. Cons:\nThe SMA is a lagging indicator, meaning it’s based on past prices and tends to be slow to respond to recent price changes. Because it equally weighs all data points, it might not accurately reflect recent changes in the market. It might give false signals in volatile markets because it doesn’t adapt to market changes as quickly as some other indicators. Example of signals Traders often use two SMAs: a short-term one and a long-term one. When the short-term SMA crosses above the long-term SMA, it’s considered a bullish (buy) signal. When it crosses below, it’s a bearish (sell) signal.\nTrue Positive:\nIn an uptrending market, the short-term SMA might cross above the long-term SMA, correctly indicating a continuing upward trend and a good time to buy.\nFalse Positive:\nLet’s say the market is in a sideways trend (prices fluctuate within a narrow range). A brief price spike could cause the short-term SMA to cross above the long-term SMA, generating a buy signal. However, this could be misleading as the overall trend hasn’t changed.\nUse in Real Trading In real trading, SMA can be paired with other indicators for better results.\nFor instance, a trader might use SMA in conjunction with the Relative Strength Index (RSI). The RSI could help confirm whether the market is overbought or oversold when the SMAs cross.\nPython Implementation Click here to view this notebook in full screen ","description":"SMA Trading indicator","title":"SMA - Simple Moving Average","uri":"/en/posts/trading-indicators/sma/"},{"content":"pandoc mardown -\u003e pdf\nCreate pdf file from .md in multiple folders\nprepare:\nbrew install basictex # search for cyrillic fonts fc-list | grep к\\ brew tap homebrew/cask-fonts brew install --cask font-m-plus brew tap homebrew/cask-fonts brew install --cask font-m-plus brew install --cask font-m-plus-1 brew install --cask font-m-plus-1-code ```sh pandoc --pdf-engine xelatex \\ --variable mainfont=\"M+ 1p\" --variable sansfont=\"M+ 1p\" --variable monofont=\"M+ 1m\" \\ -V geometry:\"top=1cm, bottom=2cm, left=1cm, right=1cm\" \\ --file-scope \\ --highlight-style=tango \\ -s \\ --toc-depth=1 \\ --variable=toc-title:\" \" \\ --top-level-division=chapter \\ --standalone \\ --self-contained \\ --from=markdown \\ $(find . -name '*.ru.md') \\ -o book.pdf Convert video -\u003e audio with ffmpeg in current directory\n#!/bin/bash # Check if ffmpeg is installed command -v ffmpeg \u003e/dev/null 2\u003e\u00261 || { echo \u003e\u00262 \"ffmpeg is required but not installed. Aborting.\"; exit 1; } # Get a list of all video files in the current directory video_files=(*.{mp4,mkv,flv,avi}) # Check if there are any video files in the current directory if [ ${#video_files[@]} -eq 0 ] then echo \"No video files found in the current directory.\" exit 1 fi # Loop through all video files and convert them to audio files for video_file in \"${video_files[@]}\" do # Get the file name without the extension file_name=\"${video_file%.*}\" # Convert the video file to an audio file in the current directory ffmpeg -i $video_file -vn -acodec libmp3lame -ab 128k $file_name.mp3 echo \"Conversion of $video_file completed. The audio file is located in the current directory.\" done echo \"All conversions completed.\" ","description":"Some code snippets","title":"Some code snippets","uri":"/en/posts/other-snippets/"},{"content":"About AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services.\nDocumentation User Guide Step Functions is a serverless function orches­trator that makes it easy to sequence Lambda functions \u0026 multiple AWS services into busine­ss-­cri­tical applic­ations.\nAlternatives AWS lambda Airflow Google Cloud Workflows Microsoft Flow Price Pay only for what you use\nCurrent price\nFree Tier: 4,000 state transitions per month\nUse Cases Step Functions is an easy-to-use function orchestra that makes it possible to string Lambda functions and multiple AWS services into business-critical applications.\nStep Functions manages the operations and underlying infrastructure for you to ensure your application is available at any scale.\nWith Step Functions, you are able to easily coordinate complex processes composed of different tasks.\nWithout using this service you have to coordinate each Lambda Function yourself and manage every kind of error in all steps of this complex process.\nAWS Step Functions is a useful service for breaking down complex processes into smaller and easier tasks\nAutomate Extract, Transform, and Load (ETL) process Orchestrate microservices Workflow configuration AWS service integrations Component reuse Built-in error handling Type: Orches­tration, Workflows\nStep Function Standard Workflows are optimized for long-running processes.\nExpress Workflows are better for event-driven workloads.\nPractice Introduction to AWS Step Functions\nQuestions Q1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\nWhich of the following Step Function workflows can be used to meet requirements?\nAsynchronous Express Workflow Standard Workflow Synchronous Express Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n3\n","description":"Amazon Step Functions","title":"Step Functions","uri":"/en/tracks/aws-certified-developer-associate/step-functions/"},{"content":"About The Stochastic Oscillator is a momentum indicator that shows the location of the close relative to the high-low range over a set number of periods. It was developed by George Lane in the 1950s.\nCalculating Formula The Stochastic Oscillator is calculated using the following formula:\n%K = 100[(C - L14) / (H14 - L14)] where:\nC = the most recent closing price L14 = the lowest price traded of the 14 previous trading sessions H14 = the highest price traded during the same 14-day period %K = the current value of the stochastic indicator The “%D” line is then a 3-day simple moving average of %K.\nPros and Cons Pros:\nThe Stochastic Oscillator can provide insights into potential overbought and oversold conditions. It can also be used to identify divergences, short-term overbought and oversold conditions, and generate trading signals. Cons:\nThe Stochastic Oscillator can stay in overbought or oversold territory for a long time, leading to many false signals in trending markets. As a lagging indicator, it might send a late signal, causing the trader to miss a big part of the trend. Example of signals Buy Signal: A buy signal might be identified when the Stochastic Oscillator crosses above the %D line (bullish divergence). Sell Signal: Conversely, a sell signal might be identified when the Stochastic Oscillator crosses below the %D line (bearish divergence). Use in Real Trading The Stochastic Oscillator is typically used with other oscillators such as the Relative Strength Index (RSI) and the Commodity Channel Index (CCI) to confirm trading signals.\nPython Implementation Click here to view this notebook in full screen ","description":"Stochastic Oscillator Trading Indicator","title":"Stochastic Oscillator - Momentum Indicator","uri":"/en/posts/trading-indicators/stochastic_oscillator/"},{"content":" https://www.buymeacoffee.com/romankurnovskii https://rom.gumroad.com/ https://www.patreon.com/user?u=79828420 ","description":"","title":"Support me","uri":"/en/p/supportme/"},{"content":"Common options z compress with gzip c create an archive u append files which are newer than the corresponding copy ibn the archive f filename of the archive v verbose, display what is inflated or deflated a unlike of z, determine compression based on file extension Create tar named archive.tar containing directory tar cf archive.tar /path/files Concatenate files into a single tar tar -cf archive.tar /path/files Extract the contents from archive.tar tar xf archive.tar Create a gzip compressed tar file name archive.tar.gz tar czf archive.tar.gz /path/files Extract a gzip compressed tar file tar xzf archive.tar.gz Create a tar file with bzip2 compression tar cjf archive.tar.bz2 /path/files Extract a bzip2 compressed tar file tar xjf archive.tar.bz2 List content of tar file tar -tvf archive.tar ","description":"tar command Cheat Sheet","title":"Tar command Cheat Sheet","uri":"/en/posts/cheat-sheet-command-tar/"},{"content":"most popular docker images ## lists the images docker pull imagename ## Pull an image or a repository from a registry docker ps -a ## See a list of all containers, even the ones not running docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \"imagename\" mapping port 4000 to 80 docker rmi ## removes the image docker rm ## removes the container docker stop ## stops the container docker volume ls ## lists the volumes docker kill ## kills the container docker logs ## see logs docker inspect ## shows all the info of a container docker docker cp ## Copy files/folders between a container and the local filesystem docker pull imagename ## Pull an image or a repository from a registry docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \"imagename\" mapping port 4000 to 80 docker run -d -p 4000:80 imagename ## Same thing, but in detached mode docker exec -it [container-id] bash ## Enter a running container docker ps ## See a list of all running containers docker stop \u003chash\u003e ## Gracefully stop the specified container docker ps -a ## See a list of all containers, even the ones not running docker kill \u003chash\u003e ## Force shutdown of the specified container docker rm \u003chash\u003e ## Remove the specified container from this machine docker rm -f \u003chash\u003e ## Remove force specified container from this machine docker rm $(docker ps -a -q) ## Remove all containers from this machine docker images -a ## Show all images on this machine docker rmi \u003cimagename\u003e ## Remove the specified image from this machine docker rmi $(docker images -q) ## Remove all images from this machine docker top \u003ccontainer-id\u003e ## Display the running processes of a container docker logs \u003ccontainer-id\u003e -f ## Live tail a container's logs docker login ## Log in this CLI session using your Docker credentials docker tag \u003cimage\u003e username/repository:tag ## Tag \u003cimage\u003e for upload to registry docker push username/repository:tag ## Upload tagged image to registry docker run username/repository:tag ## Run image from a registry docker system prune ## Remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes. (Docker 17.06.1-ce and superior) docker system prune -a ## Remove all unused containers, networks, images not just dangling ones (Docker 17.06.1-ce and superior) docker volume prune ## Remove all unused local volumes docker network prune ## Remove all unused networks docker compose docker-compose up # Create and start containers docker-compose up -d # Create and start containers in detached mode docker-compose down # Stop and remove containers, networks, images, and volumes docker-compose logs # View output from containers docker-compose restart # Restart all service docker-compose pull # Pull all image service docker-compose build # Build all image service docker-compose config # Validate and view the Compose file docker-compose scale \u003cservice_name\u003e=\u003creplica\u003e # Scale special service(s) docker-compose top # Display the running processes docker-compose run -rm -p 2022:22 web bash # Start web service and runs bash as its command, remove old container. docker services docker service create \u003coptions\u003e \u003cimage\u003e \u003ccommand\u003e # Create new service docker service inspect --pretty \u003cservice_name\u003e # Display detailed information Service(s) docker service ls # List Services docker service ps # List the tasks of Services docker service scale \u003cservice_name\u003e=\u003creplica\u003e # Scale special service(s) docker service update \u003coptions\u003e \u003cservice_name\u003e # Update Service options docker stack docker stack ls # List all running applications on this Docker host docker stack deploy -c \u003ccomposefile\u003e \u003cappname\u003e # Run the specified Compose file docker stack services \u003cappname\u003e # List the services associated with an app docker stack ps \u003cappname\u003e # List the running containers associated with an app docker stack rm \u003cappname\u003e # Tear down an application docker machine docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux) docker-machine create -d hyperv --hyperv-virtual-switch \"myswitch\" myvm1 # Win10 docker-machine env myvm1 # View basic information about your node docker-machine ssh myvm1 \"docker node ls\" # List the nodes in your swarm docker-machine ssh myvm1 \"docker node inspect \u003cnode ID\u003e\" # Inspect a node docker-machine ssh myvm1 \"docker swarm join-token -q worker\" # View join token docker-machine ssh myvm1 # Open an SSH session with the VM; type \"exit\" to end docker-machine ssh myvm2 \"docker swarm leave\" # Make the worker leave the swarm docker-machine ssh myvm1 \"docker swarm leave -f\" # Make master leave, kill swarm docker-machine start myvm1 # Start a VM that is currently not running docker-machine stop $(docker-machine ls -q) # Stop all running VMs docker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images docker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir docker-machine ssh myvm1 \"docker stack deploy -c \u003cfile\u003e \u003capp\u003e\" # Deploy an app Options for popular commands docker build Docs Build an image from a Dockerfile.\ndocker build [DOCKERFILE PATH] Example\nBuild an image tagged my-org/my-image where the Dockerfile can be found at /tmp/Dockerfile.\ndocker build -t my-org:my-image -f /tmp/Dockerfile --file -f Path where to find the Dockerfile --force-rm Always remove intermediate containers --no-cache Do not use cache when building the image --rm Remove intermediate containers after a successful build (this is true) by default --tag -t Name and optionally a tag in the ‘name:tag’ format docker run Docs\nCreates and starts a container in one operation. Could be used to execute a single command as well as start a long-running container.\nExample\ndocker run -it ubuntu:latest /bin/bash This will start a ubuntu container with the entrypoint /bin/bash. Note that if you do not have the ubuntu image downloaded it will download it before running it.\n-it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) --rm Automatically remove the container when it exit. Otherwise it will be stored and visible running docker ps -a. --detach -d Run container in background and print container ID --volume -v Bind mount a volume. Useful for accessing folders on your local disk inside your docker container, like configuration files or storage that should be persisted (database, logs etc.). docker exec Docs\nExecute a command inside a running container.\ndocker exec [CONTAINER ID] Example\ndocker exec [CONTAINER ID] touch /tmp/exec_works --detach -d Detached mode: run command in the background -it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) docker images Docs\nList all downloaded/created images.\ndocker images -q Only show numeric IDs docker inspect Docs\nShows all the info of a container.\ndocker inspect [CONTAINER ID] docker logs Docs\nGets logs from container.\ndocker logs [CONTAINER ID] --details Log extra details --follow -f Follow log output. Do not stop when end of file is reached, but rather wait for additional data to be appended to the input. --timestamps -t Show timestamps docker ps Docs\nShows information about all running containers.\ndocker ps --all -a Show all containers (default shows just running) --filter -f Filter output based on conditions provided, docker ps -f=\"name=\"example\" --quiet -q Only display numeric IDs docker rmi Docs\nRemove one or more images.\ndocker rmi [IMAGE ID] --force -f Force removal of the image Snippets A collection of useful tips and tricks for Docker.\nDelete all containers NOTE: This will remove ALL your containers.\ndocker container prune OR, if you’re using an older docker client:\ndocker rm $(docker ps -a -q) Delete all untagged containers docker image prune OR, if you’re using an older docker client:\ndocker rmi $(docker images | grep '^\u003cnone\u003e' | awk '{print $3}') Remove all images docker rmi --force $(docker images -q) Remove all docker images with none tag docker rmi --force $(docker images --filter \"dangling=true\" -q) See all space Docker take up docker system df Get IP address of running container docker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d '\"' -f 4 Kill all running containers docker kill $(docker ps -q) Resources docs.docker.com docker-cheat-sheet docker-cheat-sheet https://sourabhbajaj.com/mac-setup/Docker/ ","description":"Most Popular Docker Commands","title":"Top Docker Commands","uri":"/en/posts/docker-commands/"},{"content":"Uploading a File to Amazon S3 Introduction When you upload a folder from your local system or another machine, Amazon S3 uploads all the files and subfolders from the specified local folder to your bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. In this lab step, you will upload a file to your bucket. The process is similar to uploading a single file, multiple files, or a folder with files in it.\nIn order to complete this lab step, you have to upload the cloudacademy-logo.png file from your local file storage into an S3 folder you created earlier.\nDownload the Cloud Academy logo from the following location: https://s3-us-west-2.amazonaws.com/clouda-labs/scripts/s3/cloudacademy-logo.png (If the image is not downloaded for you, simply right-click the image and select Save image as to download it to your local file system.)\nInstructions Click on the cloudfolder folder. You are placed within the empty folder in your S3 bucket: Note: Click the folder name itself, not the checkbox for the folder name. If you select the folder checkbox then upload a file, it will be placed above the folder (not inside it).\nClick the Upload button.\nClick Add Files:\nA file picker will appear.\nBrowse to and select the local copy of cloudacademy-logo.png file that you downloaded earlier: The logo is added to the list of files that are ready to upload. You have several options at this point:\nAdd more files Upload However, there is another method that some users prefer to add files for upload.\nCheck the file and click on Remove:\nThis time, rather than browsing to a file, drag and drop the logo file onto the wizard. The wizard adds it to the list of files to upload.\nScroll to the bottom of the page and click Upload to upload the file:\nYou will see a blue notification that the file is uploading and then a green notification that the upload has been completed successfully.\nThe file is placed in the folder in your bucket:\n","description":"Uploading a File to Amazon S3","title":"Upload a file to S3","uri":"/en/tracks/aws-certified-developer-associate/s3/upload-file-to-s3/"},{"content":"Today, I tackled a LeetCode programming problem that pushed me to rediscover a forgotten concept and devise a creative solution to share my learning experience with others.\nThe problem centered around a backtracking algorithm. Although I had previously explored this topic, I couldn’t quite remember the exact steps and logic involved. Determined to refresh my memory, I set out not only to solve the problem but also to find a way to make it easier for myself and others to visualize and comprehend the algorithm in the future.\nI started by rewriting the algorithm from scratch, carefully examining each step and ensuring I understood the logic behind it. Once satisfied with my progress, I decided to create a video using Visual Studio Code’s debugger to capture the entire problem-solving process. I focused on problem 131: Palindrome Partitioning and meticulously documented every step from beginning to end.\nArmed with the video, I wanted to make it available on my website for others who might benefit from it. However, my website was built using the Hugo static site generator, and I soon realized that embedding the video wouldn’t be as straightforward as I initially thought. I needed a shortcode to make it work.\nA shortcode is a compact piece of code that enables users to embed various types of content, such as videos, into their website without having to write complex HTML or JavaScript. It would allow me to seamlessly integrate the video into my website, making it accessible to everyone.\n{{- $src := .Get \"src\" -}} {{- $title := .Get \"title\" -}} \u003cvideo width=\"100%\" controls\u003e \u003csource src=\"{{ $src }}\" type=\"video/mp4\"\u003e Your browser does not support the video tag. \u003c/video\u003e {{ if $title }} \u003cfigcaption\u003e{{ $title }}\u003c/figcaption\u003e {{ end }} This shortcode takes in two parameters, src and title. The src parameter specifies the video file’s URL, while the title parameter is optional and displays a caption below the video. To use the shortcode, simply include the following code in your Hugo markdown file:\n{{ \u003c video src=\"path/to/your/video.mp4\" title=\"Problem 131: Palindrome Partitioning\" \u003e}} By creating and using this shortcode, I successfully embedded my video into my Hugo site, making it available for others to learn from.\nThis experience has reaffirmed the importance of perseverance and innovation in overcoming challenges. Whether it’s relearning an algorithm or finding creative ways to share knowledge, facing obstacles head-on is an essential part of personal and professional development.\nTo try copy and use this shortcode from repo.\nLeetcode problem solution and debug video you can check on this page.\n","description":"Visualize Algorithms - Rediscovering Backtracking","title":"Visualize Algorithms - Rediscovering Backtracking","uri":"/en/stories/001-rediscovering-backtracking-algo/"},{"content":"About AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\nDocumentation User Guide X-Ray allows software engineers to view the state of a system at a glance, identify potential bottlenecks, and make informed operational decisions to improve performance and reliability. X-Ray inspects application code using a combination of machine and customer-provided data to identify potential bottlenecks and analyze performance and performance trends for each test scenario.\nTerminology AWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application\nX-Ray Trace Hierarchy: Trace \u003e Segment \u003e Sub Segment\nTrace\nAn X-Ray trace is a set of data points that share the same trace ID.\nSegments\nA segment is a JSON representation of a request that your application serves.\nA trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases.\nSubsegments\nSubsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.\nAnnotations\nAn X-Ray annotation is system-defined, or user-defined data associated with a segment A segment can contain multiple annotations. Annotations are used to describe the request, the response, and other information about the segment Can be used for adding system or user-defined data to segments and subsegments that you want to index for search. Sampling\nX-Ray traces are sampled at a rate that you specify. The rate is specified in the sampling_rate field of the sampling object in the config object.\nMetadata\nX-Ray traces contain metadata that is useful for understanding the trace.\nMetadata (Key / value pairs) is not indexed and cannot be used for searching Digest Trace request across microservices/AWS services\nAnalyze, Troubleshoot errors, Solve performance issues Gather tracing information From applications/components/AWS Services Tools to view, filter and gain insights (Ex: Service Map) How does Tracing work?\nUnique trace ID assigned to every client request X-Amzn-Trace-Id:Root=1-5759e988-bd862e3fe Each service in request chain sends traces to X-Ray with trace ID X-Ray gathers all the information and provides visualization How do you reduce performance impact due to tracing? Sampling - Only a sub set of requests are sampled (Configurable) How can AWS Services and your applications send tracing info? Step 1 : Update Application Code Using X-Ray SDK Step 2: Use X-Ray agents (EASY to use in some services! Ex: AWS Lambda) Segments and Sub-segments can include an annotations object containing one or more fields that X-Ray indexes for use with Filter Expressions. It is indexed. Use up to 50 annotations per trace.\nTotal sampled request per second = Reservoir size + ((incoming requests per second - reservoir size) * fixed rate)\nDefault sampling X-ray SDK first request each second and 5% of any additional requests\nTracing header can be added in http request header\nAnnotations vs Segments vs Subsegments vs metadata\nX-ray daemon listens for traffic on UDP port 2000\nX-ray SDK provides interceptors to add your code to trace incoming HTTP requests.\nX-ray in EC2: You need the X-Ray daemon to be running on your EC2 instances in order to send data to X-Ray. User data script could be used to install the X-Ray daemon in EC2 instance.\nX-ray in ECS: In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.\nX-ray in elastic beanstalk: Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-service architecture.\nA segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about - downstream calls that your application made to fulfill the original request.\nAdd annotations to subsegment document if you want to trace downstream calls.\nSegments and subsegment can include a metadata object containing one or more fields with values of any type, including objects and arrays.\nTracing header is added in the HTTP request header. A tracing header (X-Amzn-Trace-ld) can originate from the X-Ray SDK, an AWS service, or the - client request.\nUse the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in - order to develop the custom debug tool\nPrice Current price\nUse Cases Type: Developer Tools\nAlternatives Google Stackdriver Azure Monitor Elastic Observability Datadog Splunk AWS X-Ray supports applications running on:\nAmazon Elastic Compute Cloud (Amazon EC2) Amazon EC2 Container Service (Amazon ECS) AWS Lambda WS Elastic Beanstalk Practice Questions Q1 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\nWhich AWS service will be your main tool?\nAWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks\n1\n","description":"Analyze and debug production, distributed applications","title":"X-Ray","uri":"/en/tracks/aws-certified-developer-associate/xray/"}]