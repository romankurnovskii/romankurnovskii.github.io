<!doctype html><html lang=en dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Git snippets | Roman Kurnovskii</title><meta name=generator content="Hugo "><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css rel=stylesheet crossorigin=anonymous><script src=https://unpkg.com/flowbite@1.5.1/dist/flowbite.js></script>
<script src=https://unpkg.com/axios@0.27.2/dist/axios.min.js></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/js/all.min.js integrity=" sha512-8pHNiqTlsrRjVD4A/3va++W1sMbUHwWxxRPWNyVlql3T+Hgfd81Qc6FC5WMXDC+tSauxxzp1tgiAvSKFu1qIlA==" defer crossorigin></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" media=print onload='this.media="all",this.onload=null' crossorigin><link rel=stylesheet href=https://romankurnovskii.com/css/yalla.min.a362c6373d2043f5a1f057d2c07cc54db39ec3a049f53ea9e320794760d753cdaa2173104ca7f281516241c6e989ab5a.css><script defer src=https://romankurnovskii.com/js/yalla.min.e11e7612701aa90cb2ebcc925db8682fa8bf6a533ef79fb3658949f1f677290795ee80541ca370d10d65dd5fd04e40b8.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href=#ZgotmplZ as=style onload='this.onload=null,this.rel="stylesheet"'><link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@9.1.3/dist/mermaid.min.js crossorigin></script>
<link rel=preconnect href=https://www.google-analytics.com crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-26F2C5ZR4Y"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-26F2C5ZR4Y")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js?v2","ym"),ym(87734724,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/87734724 style=position:absolute;left:-9999px alt></div></noscript><link rel=icon type=image/png sizes=32x32 href=https://romankurnovskii.com/images/icon_huc02d7296c9eb9353758cb2467c0d17b0_10773_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://romankurnovskii.com/images/icon_huc02d7296c9eb9353758cb2467c0d17b0_10773_180x180_fill_box_center_3.png><meta name=description content="Git snippets"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Notes","item":"https://romankurnovskii.com/en/posts/"},{"@type":"ListItem","position":2,"name":"Git snippets","item":"https://romankurnovskii.com/en/posts/git-snippets/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://romankurnovskii.com/en/posts/git-snippets/"},"headline":"Git snippets | Roman Kurnovskii","image":"https://picsum.photos/700/238","datePublished":"2023-01-01T00:00:00+00:00","dateModified":"2023-01-01T21:26:52+02:00","wordCount":301,"publisher":{"@type":"Person","name":"Roman Kurnovskii","logo":{"@type":"ImageObject","url":"https://romankurnovskii.com/images/icon.png"}},"description":"Git snippets"}</script><meta property="og:title" content="Git snippets | Roman Kurnovskii"><meta property="og:type" content="article"><meta property="og:image" content="https://romankurnovskii.com/images/icon.png"><meta property="og:url" content="https://romankurnovskii.com/en/posts/git-snippets/"><meta property="og:description" content="Git snippets"><meta property="og:locale" content="en"><meta property="og:site_name" content="Roman Kurnovskii"><meta property="article:published_time" content="2023-01-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-01T21:26:52+02:00"><meta property="article:section" content="posts"><meta property="article:tag" content="git"><meta property="og:see_also" content="https://romankurnovskii.com/en/posts/js-snippets/"><meta property="og:see_also" content="https://romankurnovskii.com/en/posts/python-snippets/"><meta property="og:see_also" content="https://romankurnovskii.com/en/posts/docker-commands/"><meta property="og:see_also" content="https://romankurnovskii.com/en/posts/cheat-sheet-command-tar/"><script>var callback=function(){alert("A callback was triggered")}</script><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");const isDark2=JSON.parse(localStorage.getItem("DarkMode")||"false");isDark2?storageColorScheme="Dark":storageColorScheme="Light",((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/en/ class="me-6 text-primary-text text-xl font-bold">Roman Kurnovskii</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><ul id=menu><li class=parent><a href=/en/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Notes</a><ul class=child><li class=parent><a href=/en/categories/aws/>AWS <span class=expand>»</span></a><ul class=child><li class=parent><a href=/en/docs/aws-certified-developer-associate/>AWS Developer Associate</a></li></ul></li><li class=parent><a href=/en/categories/os/>OS <span class=expand>»</span></a><ul class=child><li class=parent><a href=/en/categories/docker/>Docker</a></li><li class=parent><a href=/en/categories/linux/>Linux</a></li><li class=parent><a href=/en/categories/macos/>Mac</a></li></ul></li><li class=parent><a href=/en/categories/programming/>Programming <span class=expand>»</span></a><ul class=child><li class=parent><a href=/en/categories/hugo/>Hugo</a></li><li class=parent><a href=/en/categories/javascript/>JavaScript</a></li><li class=parent><a href=/en/categories/python/>Python</a></li></ul></li><li><a href=/en/categories/cheatsheet/>Cheat Sheet</a></li><li><a href=/en/p/links/>Links</a></li></ul></li><li class=parent><a href=/en/categories/roadmaps/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Roadmaps</a><ul class=child><li><a href=/en/docs/algorithms-101/>Algorithms101</a></li><li><a href=/en/docs/aws-certified-developer-associate/>AWS Developer Associate</a></li><li><a href=/ru/docs/disser/>Диссертация [RU]</a></li></ul></li><li class=parent><a href=/en/apps/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Apps</a></li></ul></div><div class=flex><div class="relative pt-4 md:pt-0"><form id=search class="flex items-center"><label for=search-input class=sr-only>Search</label><div class="relative w-full"><input type=text type=search id=search-input class="bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full pl-10 p-2.5 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500" placeholder=Search required></div></form></div><div class="relative pt-4 ps-2 md:pt-0"><button class="p-1 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700" onclick='location.href="/en/search/"'>
<img width=13 src=https://img.icons8.com/ios-glyphs/30/null/search-more.png></button></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>document.addEventListener("DOMContentLoaded",()=>{switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-2xl lg:px-2 xl:px-4"><div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-1"><div class="bg-secondary-bg rounded px-6 py-4 col-span-2 lg:col-span-6"><article class=prose><h1>Git snippets</h1><div class="text-tertiary-text text-sm not-prose mt-2 flex flex-row flex-wrap items-center"><div class=me-4><i class="fa-regular fa-calendar-days"></i>
<span>Updated: 2023-01-01</span></div><div class=me-4><i class="fa-regular fa-clock"></i>
<span>2 min read</span></div><div class=me-4><i class="fa-regular fa-folder me-1"></i>
<a href=https://romankurnovskii.com/en/categories/git/ class=hover:text-yalla>Git</a>
<span>,</span>
<a href=https://romankurnovskii.com/en/categories/cheatsheet/ class=hover:text-yalla>cheatsheet</a></div><div class=me-4><i class="fa-regular fa-rectangle-list"></i>
<a href=https://romankurnovskii.com/en/series/cheatsheet/ class=hover:text-yalla>cheatsheet</a></div><div id=page__views__div class="me-4 my-2"><i class="fa-regular fa-eye me-1"></i>
<span id=page__views></span></div></div><center><a href=https://romankurnovskii.com/en/posts/git-snippets/><img src=https://picsum.photos/700/238 alt=[git] width=auto loading=lazy></a></center><div class="block lg:hidden"><h3 class=text-lg>On This Page</h3><div class=break-words><nav id=TableOfContents><ul><li><a href=#free-space-in-git-repo>Free space in git repo</a></li><li><a href=#common-git-commands>Common git commands</a></li><li><a href=#resources>Resources</a></li></ul></nav></div></div><h2 id=free-space-in-git-repo>Free space in git repo</h2><p><a href=https://rtyley.github.io/bfg-repo-cleaner/>Download BFG</a></p><p>Remove <em>history</em> files bigger than 100Kb:</p><pre><code>cd repo
java -jar bfg-1.14.0.jar --strip-blobs-bigger-than 100K .
git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive
</code></pre><p><strong>Removing an entire commit:</strong></p><p>Replace &ldquo;SHA&rdquo; with the reference you want to get rid of. The &ldquo;^&rdquo; in that command is literal.</p><pre><code>git rebase -p --onto SHA^ SHA
</code></pre><p>We want to remove commits 2 & 4 from the repo. (Higher the the number newer the commit; 0 is the oldest commit and 4 is the latest commit)</p><pre><code>commit 0 : b3d92c5
commit 1 : 2c6a45b
commit 2 : &lt;any_hash&gt;
commit 3 : 77b9b82
commit 4 : &lt;any_hash&gt;
</code></pre><p><strong>Note:</strong> You need to have admin rights over the repo since you are using <code>--hard</code> and <code>-f</code>.</p><ol><li><code>git checkout b3d92c5</code> Checkout the last usable commit.</li><li><code>git checkout -b repair</code> Create a new branch to work on.</li><li><code>git cherry-pick 77b9b82</code> Run through commit 3.</li><li><code>git cherry-pick 2c6a45b</code> Run through commit 1.</li><li><code>git checkout master</code> Checkout master.</li><li><code>git reset --hard b3d92c5</code> Reset master to last usable commit.</li><li><code>git merge repair</code> Merge our new branch onto master.</li><li><code>git push -f origin master</code> Push master to the remote repo.</li></ol><p>If didn&rsquo;t publish changes, to remove the latest commit, do:</p><pre><code>git rebase -i HEAD~&lt;number of commits to go back&gt;
git rebase -i &lt;CommitId&gt;~1
git reset --hard HEAD^
git reset --hard commitId
git rebase -i HEAD~5
</code></pre><p>If already published to-be-deleted commit:</p><p><code>git revert HEAD</code></p><p><strong>Cleanups:</strong></p><pre><code>git stash clear
git reflog expire --expire-unreachable=now --all
git fsck --full
git fsck --unreachable		# Will show you the list of what will be deleted
git gc --prune=now			# Cleanup unnecessary files and optimize the local repository
</code></pre><h2 id=common-git-commands>Common git commands</h2><pre><code>git rev-list --all --count # count commits
git clean -fd # To remove all untracked (non-git) files and folders!
</code></pre><h2 id=resources>Resources</h2><ul><li><a href=https://sethrobertson.github.io/GitFixUm/fixup.html>https://sethrobertson.github.io/GitFixUm/fixup.html</a></li><li><a href=https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-clone.html>https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-clone.html</a></li><li><a href=https://passingcuriosity.com/2017/truncating-git-history/>https://passingcuriosity.com/2017/truncating-git-history/</a></li><li><a href="https://www.npmjs.com/package/clear-git-branch?activeTab=explore">https://www.npmjs.com/package/clear-git-branch?activeTab=explore</a></li></ul></article><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script><div class=my-4><a href=https://romankurnovskii.com/en/tags/git/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-yalla">#git</a></div><style>.resp-sharing-button__link,.resp-sharing-button__icon{display:inline-block}.resp-sharing-button__link{text-decoration:none;color:#fff!important;padding:.2em;margin:{ { $buttonMargin } }}.resp-sharing-button{border-radius:5px;transition:25ms ease-out;padding:.5em .75em;font-family:Helvetica Neue,Helvetica,Arial,sans-serif; { { if (isset .Site.Params.ShareButtons "fontsize") } } font-size: { { .Site.Params.ShareButtons.FontSize } } { { end } }}.resp-sharing-button__icon svg{width:1em;height:1em;margin-right:.4em;vertical-align:middle}.resp-sharing-button--small svg{margin:0;vertical-align:middle}.resp-sharing-button__icon{stroke:#fff;fill:none}.resp-sharing-button__icon--solid,.resp-sharing-button__icon--solidcircle{fill:#fff;stroke:none}.resp-sharing-button--twitter{background-color:#55acee}.resp-sharing-button--twitter:hover{background-color:#2795e9}.resp-sharing-button--pinterest{background-color:#bd081c}.resp-sharing-button--pinterest:hover{background-color:#8c0615}.resp-sharing-button--facebook{background-color:#3b5998}.resp-sharing-button--facebook:hover{background-color:#2d4373}.resp-sharing-button--tumblr{background-color:#35465c}.resp-sharing-button--tumblr:hover{background-color:#222d3c}.resp-sharing-button--reddit{background-color:#ff4500}.resp-sharing-button--reddit:hover{background-color:#3a80c1}.resp-sharing-button--google{background-color:#dd4b39}.resp-sharing-button--google:hover{background-color:#c23321}.resp-sharing-button--linkedin{background-color:#0077b5}.resp-sharing-button--linkedin:hover{background-color:#046293}.resp-sharing-button--email{background-color:#777}.resp-sharing-button--email:hover{background-color:#5e5e5e}.resp-sharing-button--xing{background-color:#1a7576}.resp-sharing-button--xing:hover{background-color:#114c4c}.resp-sharing-button--whatsapp{background-color:#25d366}.resp-sharing-button--whatsapp:hover{background-color:#1da851}.resp-sharing-button--hackernews{background-color:#f60}.resp-sharing-button--hackernews:hover,.resp-sharing-button--hackernews:focus{background-color:#fb6200}.resp-sharing-button--vk{background-color:#507299}.resp-sharing-button--vk:hover{background-color:#43648c}.resp-sharing-button--facebook{background-color:#3b5998;border-color:#3b5998}.resp-sharing-button--facebook:hover,.resp-sharing-button--facebook:active{background-color:#2d4373;border-color:#2d4373}.resp-sharing-button--twitter{background-color:#55acee;border-color:#55acee}.resp-sharing-button--twitter:hover,.resp-sharing-button--twitter:active{background-color:#2795e9;border-color:#2795e9}.resp-sharing-button--tumblr{background-color:#35465c;border-color:#35465c}.resp-sharing-button--tumblr:hover,.resp-sharing-button--tumblr:active{background-color:#222d3c;border-color:#222d3c}.resp-sharing-button--email{background-color:#777;border-color:#777}.resp-sharing-button--email:hover,.resp-sharing-button--email:active{background-color:#5e5e5e;border-color:#5e5e5e}.resp-sharing-button--pinterest{background-color:#bd081c;border-color:#bd081c}.resp-sharing-button--pinterest:hover,.resp-sharing-button--pinterest:active{background-color:#8c0615;border-color:#8c0615}.resp-sharing-button--linkedin{background-color:#0077b5;border-color:#0077b5}.resp-sharing-button--linkedin:hover,.resp-sharing-button--linkedin:active{background-color:#046293;border-color:#046293}.resp-sharing-button--reddit{background-color:#ff4500;border-color:#ff4500}.resp-sharing-button--reddit:hover,.resp-sharing-button--reddit:active{background-color:#ff5700;border-color:#ff5700}.resp-sharing-button--xing{background-color:#1a7576;border-color:#1a7576}.resp-sharing-button--xing:hover .resp-sharing-button--xing:active{background-color:#114c4c;border-color:#114c4c}.resp-sharing-button--whatsapp{background-color:#25d366;border-color:#25d366}.resp-sharing-button--whatsapp:hover,.resp-sharing-button--whatsapp:active{background-color:#1da851;border-color:#1da851}.resp-sharing-button--hackernews{background-color:#f60;border-color:#f60}.resp-sharing-button--hackernews:hover .resp-sharing-button--hackernews:active{background-color:#fb6200;border-color:#fb6200}.resp-sharing-button--vk{background-color:#507299;border-color:#507299}.resp-sharing-button--vk:hover .resp-sharing-button--vk:active{background-color:#43648c;border-color:#43648c}.resp-sharing-button--telegram{background-color:#54a9eb}.resp-sharing-button--telegram:hover{background-color:#4b97d1}</style><div id=share_buttons class="mx-2 mt-1 border-t px-2 pt-4 md:flex-row md:justify-between"><a class=resp-sharing-button__link href="mailto:?subject=Git%20snippets&body=https://romankurnovskii.com/en/posts/git-snippets/" target=_self rel=noopener aria-label title=E-Mail><div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9.0 6v12c0 1.1.9 2 2 2h20c1.1.0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17.0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1.0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08.0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://twitter.com/intent/tweet/?text=Git%20snippets&url=https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label title=Twitter><div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5.0-4.55 2.04-4.55 4.54.0.36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3.0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35.0 12.92-6.92 12.92-12.93.0-.2.0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://facebook.com/sharer/sharer.php?u=https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label title=Facebook><div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://www.linkedin.com/shareArticle?mini=true&url=https://romankurnovskii.com/en/posts/git-snippets/&title=Git%20snippets&summary=Git%20snippets&https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6.0 2.5 1 2.6 2.5.0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/></svg></div></div></a><a class=resp-sharing-button__link href="http://vk.com/share.php?title=Git%20snippets&url=https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--vk resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 21 21"><path d="M21.547 7h-3.29a.743.743.0 00-.655.392s-1.312 2.416-1.734 3.23C14.734 12.813 14 12.126 14 11.11V7.603A1.104 1.104.0 0012.896 6.5h-2.474a1.982 1.982.0 00-1.75.813s1.255-.204 1.255 1.49c0 .42.022 1.626.04 2.64a.73.73.0 01-1.272.503A21.54 21.54.0 016.197 7.403.693.693.0 005.567 7h-2.99a.508.508.0 00-.48.685C3.005 10.175 6.918 18 11.38 18h1.878a.742.742.0 00.742-.742v-1.135a.73.73.0 011.23-.53l2.247 2.112a1.09 1.09.0 00.746.295h2.953c1.424.0 1.424-.988.647-1.753-.546-.538-2.518-2.617-2.518-2.617a1.02 1.02.0 01-.078-1.323c.637-.84 1.68-2.212 2.122-2.8.603-.804 1.697-2.507.197-2.507z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://news.ycombinator.com/submitlink?u=https://romankurnovskii.com/en/posts/git-snippets/&t=Git%20snippets" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 140 140"><path fill-rule="evenodd" d="M60.94 82.314 17 0h20.08l25.85 52.093c.397.927.86 1.888 1.39 2.883.53.994.995 2.02 1.393 3.08.265.4.463.764.596 1.095.13.334.262.63.395.898.662 1.325 1.26 2.618 1.79 3.877.53 1.26.993 2.42 1.39 3.48 1.06-2.254 2.22-4.673 3.48-7.258s2.552-5.27 3.877-8.052L103.49.0h18.69L77.84 83.308v53.087h-16.9v-54.08z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://reddit.com/submit/?url=https://romankurnovskii.com/en/posts/git-snippets/&resubmit=true&title=Git%20snippets" target=_blank rel=noopener aria-label title=Reddit><div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M24 11.5c0-1.65-1.35-3-3-3-.96.0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65.0 3-1.35 3-3s-1.35-3-3-3c-1.38.0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65.0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66.0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64.0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4.0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6.0.23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1.0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1.0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/></svg></div></div></a><a class=resp-sharing-button__link href="whatsapp://send?text=Git%20snippets%20https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3.0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7.0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1.0-5.2 4.2-9.4 9.4-9.4 2.5.0 4.9 1 6.7 2.8s2.8 4.2 2.8 6.7c-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2s0-.5.1-.6.3-.3.4-.5c.2-.1.3-.3.4-.5s0-.4.0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg></div></div></a><a class=resp-sharing-button__link href="https://telegram.me/share/url?text=Git%20snippets&url=https://romankurnovskii.com/en/posts/git-snippets/" target=_blank rel=noopener aria-label><div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden=true class="resp-sharing-button__icon resp-sharing-button__icon--solid"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M.707 8.475C.275 8.64.0 9.508.0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102.0 001.75.527l2.96-2.41a.405.405.0 01.494-.013l5.34 3.87a1.1 1.1.0 001.046.135 1.1 1.1.0 00.682-.803l3.91-18.795A1.102 1.102.0 0022.5.075L.706 8.475z"/></svg></div></div></a></div><div class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"><div><span class="text-primary-text block font-bold">Previous</span>
<a href=https://romankurnovskii.com/en/posts/code-style/ class=block>Code style notes</a></div><div class="mt-4 md:mt-0 md:text-right"></div></div><div id=open_comments_block></div><script src=/opencomments.js renderdivid=open_comments_block server=https://eyt4njm3se.execute-api.eu-west-1.amazonaws.com/comments awstag defer></script></div><div class=col-span-2><div class="bg-secondary-bg prose max-w-none rounded mt-0 p-6"><h3>Series of Posts</h3>- <a href=https://romankurnovskii.com/en/posts/git-snippets/ class=no-underline>Git snippets</a><br>- <a href=https://romankurnovskii.com/en/posts/js-snippets/ class=no-underline>JavaScript code snippets</a><br>- <a href=https://romankurnovskii.com/en/posts/python-snippets/ class=no-underline>Python Cheat Sheet</a><br>- <a href=https://romankurnovskii.com/en/posts/docker-commands/ class=no-underline>Top Docker Commands</a><br>- <a href=https://romankurnovskii.com/en/posts/cheat-sheet-command-tar/ class=no-underline>Tar command Cheat Sheet</a><br></div><div class="bg-primary-bg prose sticky top-7 z-10 hidden
px-6 lg:block"><h3 class=text-lg>On This Page</h3></div><div class="sticky-toc hidden px-0 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#free-space-in-git-repo>Free space in git repo</a></li><li><a href=#common-git-commands>Common git commands</a></li><li><a href=#resources>Resources</a></li></ul></nav><div class="feedback-links mt-4 text-sm"><hr><div><a href=https://github.com/romankurnovskii/romankurnovskii.github.io/edit/main/content/posts/git-snippets.en.md title="Edit this page" target=_blank><i class="fas fa-edit me-1"></i>
<span>Edit this page</span></a></div><div><a href="https://github.com/romankurnovskii/romankurnovskii.github.io/issues/new/?body=File:%20[/content/posts/git-snippets.en.md/]%28/https:/romankurnovskii.com/en/posts/git-snippets/%29" title="Request issue" target=_blank><i class="fas fa-check me-1" aria-hidden=true></i>
<span>Request page change</span></a></div></div></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2022 <a href=https://romankurnovskii.com>Roman Kurnovskii</a> personal page
&#183;
<a href=https://romankurnovskii.com/en/index.xml target=_blank>RSS</a>
&#183;
<a href=https://gohugo.io>Hugo</a> fe1896c 2023-01-01</p></p></div><script src=/js/search.js?v5 languagemode=en></script><div id=search-result tabindex=-1 class="overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs" hidden><div class="relative p-4 w-full max-w-xs h-full md:h-auto"><div class="relative bg-white rounded-lg shadow dark:bg-gray-700"><div class=p-6><h3>Search results</h3><div id=search-results class=prose></div></div></div></div></div><script>window.store={"https://romankurnovskii.com/en/docs/algorithms-101/":{title:"Algorithms 101",tags:[],content:`Smart / OKR What is SMART and OKR
SMART Specific:
Goal: prepare for contests. Pass 50-\u0026gt;75-\u0026gt;100% of contest problems in time Improve python skills. Improve understanding of common algorithms and data structures. Measurable: How will we know that change has occurred?
solve top 100 questions that cover common algorithms and data structures. Achievable:
participate in LeetCode contest, solve 50%+ problems in time. Relevant: Is it possible to achieve this objective?
achievable with practice. improve skills in solving business problems more efficiently, quickly, understandable. Time-Bound: When will this objective be accomplished?
1-2 hours a day, 5-6 days a week, ~5 problems a week first contest after 20% problems pass. 20 weeks from start. Summarize results on 28 Feb 2023 OKR + roadmap pass 20 problems: (4 weeks, 12 Nov 2022) Ability to define algorithm/idea of solving problem. participate in contest, solve minimum 1-2 problems in time. next participate in contest/solve contest tasks every week: solve minimum 1-2 problems. fix results, correct next goal keys if I go ahead. pass 40 problems: (10 Dec 2022) participate in contest, solve minimum 2 problems in time. after 50 problems have a rest one week. (24 Dec 2022) pass 70 problems: (28 Jan 2023) solve next 10 medium problems without hints effectively. 80-100 problems: (28 Feb 2023) have understanding in which topics I have gaps. emphasize problem solving on these topics in addition to the tasks on the list. Sum up results (28 Feb 2023) Solving plan open task read first thoughts spend 1510 minutes on coding/drawing/understanding algo finished or not, read hints spend 10 minutes on fixing if needed read solution, discussions if there is a new algo, read theory the rest of first hour, practice code from scratch with comments/code snippets repeat 7-9 until tests pass Prepare environment vscode to observe any change in python use nodemon npm package
1npm i -g nodemon run python file:
1nodemon --exec python p.py Template 1from typing import List 2 3class Solution: 4 def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: 5 return 1 6 7nums = [1,2,3] 8target = 5 9 10s = Solution() 11res = s.twoSum(nums, target) 12 13print(res) Problems list leetcode Top 100 Interview Questions
List of problems cis mutable. Will take first not solved until all first top 100 problems are solved. It can take + ~5-15 problems
Problems order
More info:
Top MAANG interview questions 2022 Tutorial subscriptions https://www.enjoyalgorithms.com/data-structures-and-algorithms-course/ https://www.scaler.com/topics/data-structures/ https://leetcode.com/subscribe/ https://algo.monster/subscribe https://www.algoexpert.io/purchase#algoexpert `,url:"https://romankurnovskii.com/en/docs/algorithms-101/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/create-s3-bucket/":{title:"Create S3 Bucket",tags:["s3"],content:`Creating an Amazon S3 Bucket Introduction You can create an Amazon S3 bucket using the AWS Management Console. As with many other AWS services, you can use the AWS API or CLI (command-line interface) as well.
In this lab step, you will create a new Amazon S3 bucket.
Instructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: You will be placed in the S3 console.
From the S3 console, click the orange Create Bucket button: Enter a unique Bucket name on the Name and region screen of the wizard: Region: US West (Oregon) (This should be set for you. If not, please select this region.) **Important!**Bucket names must be globally unique, regardless of the AWS region in which you create the bucket. Buckets must also be DNS-compliant.
The rules for DNS-compliant bucket names are:
Bucket names must be at least 3 and no more than 63 characters long. Bucket names can contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. Bucket names must not be formatted as an IP address (for example, 192.168.1.1). The following examples are valid bucket names: calabs-bucket-1, cloudacademybucket , cloudacademy.bucket , calabs.1 or ca-labs-bucket.
Troubleshooting Tip: If you receive an error because your bucket name is not unique, append a unique number to the bucket name in order to guarantee its uniqueness:
For example, change \u0026ldquo;calabs-bucket\u0026rdquo; to \u0026ldquo;calabs-bucket-1\u0026rdquo; (or a unique number/character string) and try again. Leave the Block public access (bucket settings) at the default values: No changes are needed. This is where you can set public access permissions.
5. Click on Create bucket:
A page with a table listing buckets will load and you will see a green notification that your bucket was created successfully.
In the Buckets table, click the name of your bucket in the Name column: A page will load with a row of tabs at the top.
To see details and options for your bucket, click on the Properties: This page allows you to configure your Amazon S3 bucket in many different ways. No changes are needed in this lab at this time.
Feel free to look at the other tabs and see the configuration options that are available.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/create-s3-bucket/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/":{title:"AWS Certified Developer (DVA-C01 -\u003e DVA-C02)",tags:["AWS"],content:`TL;DR Passed exam in one month. Created an app with questions and progress that helped me a lot Note The AWS Certified Developer - Associate exam is changing February 28, 2023. The last date to take the current exam is February 27, 2023.
To keep the docs up to date I will add new and latest information.
DVA-C01 vs DVA-C02 new domain: Domain 3: Deployment focus will be on testing and deploying your code into different environments including development, test, and production environments. You’ll need to know how CloudFormation, the AWS Cloud Development Kit (CDK), and AWS SAM are used to deploy applications. Domains 4 and 5 (“Refactoring” along with “Monitoring and Troubleshooting”) from the DVA-C01 exam guide have been consolidated into Domain 4 (“Troubleshooting and Optimization”) in the DVA-C02 exam guide Plus 2% questions in Development with AWS Services domain Test questions for DVA-C02 - here and here
Criteria In order to pass the exam, you must score more than 720/1000 (unspecified) points. Criterion will be a minimum threshold of 75/100%, unless conditions change in preparation.
Study Plan Find out what the exam requirements are Have a list of topics that will be on the exam Practice each service for comprehension Read extra theory that will not be covered during practice Go through the test general questions Repeat 3-5 repeat until the result of failed block greater than 80 points Entrypoint:
AWS Certified Developer Exam Information Prepare The AWS website has:
Exam Preparation Guide DVA-C01 From 27 Feb 2023 Exam Preparation Guide DVA-C02 To pass the exam, you need to know certain services from the 4 domains: Development with AWS Services, Security, Deployment, Refactoring, Monitoring and Troubleshooting
List of services on the exam Version 2.1 DVA-C01 Version 1.0 DVA-C02
Analytics:
Amazon Athena (new in DVA-C02) Amazon OpenSearch Service (Amazon Elasticsearch Service) Amazon Kinesis Application Integration:
AWS AppSync (new in DVA-C02) Amazon EventBridge (Amazon CloudWatch Events) Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Compute:
Amazon EC2 AWS Elastic Beanstalk AWS Lambda AWS Serverless Application Model (AWS SAM) (new in DVA-C02) Containers:
AWS Copilot (new in DVA-C02) Amazon Elastic Container Registry (Amazon ECR) Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Services (Amazon EKS) Database:
Amazon Aurora (new in DVA-C02) Amazon DynamoDB Amazon ElastiCache Amazon MemoryDB for Redis (new in DVA-C02) Amazon RDS Developer Tools:
AWS Amplify (new in DVA-C02) AWS Cloud9 (new in DVA-C02) AWS CloudShell (new in DVA-C02) AWS CodeArtifact AWS CodeBuild AWS CodeCommit AWS CodeDeploy Amazon CodeGuru AWS CodePipeline AWS CodeStar AWS Fault Injection Simulator AWS X-Ray Management and Governance:
AWS AppConfig (new in DVA-C02) AWS Cloud Development Kit (AWS CDK) (new in DVA-C02) AWS CloudFormation AWS CloudTrail (new in DVA-C02) Amazon CloudWatch Amazon CloudWatch Logs (new in DVA-C02) AWS Command Line Interface (AWS CLI) (new in DVA-C02) AWS Systems Manager (new in DVA-C02) Networking and Content Delivery:
Amazon API Gateway Amazon CloudFront Elastic Load Balancing Amazon Route 53 (new in DVA-C02) Amazon VPC (new in DVA-C02) Security, Identity, and Compliance:
AWS Certificate Manager (ACM) (new in DVA-C02) AWS Certificate Manager Private Certificate Authority (new in DVA-C02) Amazon Cognito AWS Identity and Access Management (IAM) AWS Key Management Service (AWS KMS) AWS Secrets Manager (new in DVA-C02) AWS Security Token Service (AWS STS) (new in DVA-C02) AWS WAF (new in DVA-C02) Storage:
Amazon Elastic Block Store (Amazon EBS) (new in DVA-C02) Amazon Elastic File System (Amazon EFS) (new in DVA-C02) Amazon S3 Amazon S3 Glacier (new in DVA-C02) Training plan Opened a training plan for any tutorial to understand where to start learning. Have chosen cloudacademy service (but for example FreeCodeCamp has a free course with content).
Another option is to use free AWS Workshops
AWS Developer - Associate (DVA-C01) Certification Preparation
Don\u0026rsquo;t see coverage of the following services, so I add them to the block when related topics are covered:
Analytics:
Amazon Elasticsearch Service (Amazon ES) -\u0026gt; OpenSearch Service Developer Tools:
AWS CodeArtifact AWS Fault Injection Simulator My roadmap The following is my roadmap for the study. There may be adjustments.
AWS Identity and Access Management (IAM) Amazon EC2 AWS Elastic Beanstalk AWS Lambda Amazon S3 Amazon DynamoDB Amazon ElastiCache Amazon RDS Amazon API Gateway Amazon CloudFront Elastic Load Balancing (ELB) Amazon Kinesis Amazon OpenSearch Service (Amazon Elasticsearch Service) Amazon CloudWatch AWS CloudFormation AWS CodeCommit AWS CodeDeploy AWS CodeBuild AWS CodePipeline Amazon CodeGuru AWS CodeStar AWS CodeArtifact AWS X-Ray AWS Fault Injection Simulator Amazon Elastic Container Registry (Amazon ECR) Amazon Elastic Container Service (Amazon ECS) AWS Fargate Amazon Elastic Kubernetes Services (Amazon EKS) Amazon Cognito Route 53 AWS Key Management Service (AWS KMS) Amazon EventBridge (Amazon CloudWatch Events) Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Resources AWS Certified Developer A brief overview of the official documentation Exam Preparation Guide Sample Exam Questions https://github.com/itsmostafa/certified-aws-developer-associate-notes https://github.com/arnaudj/mooc-aws-certified-developer-associate-2020-notes FreeCodeCamp Youtube - AWS Certified Developer - Associate 2020 How-To Labs from AWS AWS Ramp-Up guides: Downloadable AWS Ramp-Up Guides offer a variety of resources to help you build your skills and knowledge of the AWS Cloud. Coursera\u0026rsquo;s AWS Courses(Free to enroll via audit): AWS also provides various specializations in partnership with coursera AWS Architecture center: Provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more. This expert guidance was contributed by cloud architecture experts from AWS, including AWS Solutions Architects, Professional Services Consultants, and Partners. AWS Whitepapers: Expand your knowledge of the cloud with AWS technical content authored by AWS and the AWS community, including technical whitepapers, technical guides, reference material, and reference architecture diagrams. Back to Basics: Back to Basics\u0026rsquo; is a video series that explains, examines, and decomposes basic cloud architecture pattern best practices. AWS Heroes Content Library: AWS Hero authored content including blogs, videos, slide presentations, podcasts, and more. https://amazon.qwiklabs.com/catalog AWS Workshops: This website lists workshops created by the teams at Amazon Web Services (AWS). Workshops are hands-on events designed to teach or introduce practical skills, techniques, or concepts which you can use to solve business problems. https://wellarchitectedlabs.com/ https://testseries.edugorilla.com/tests/1359/aws-certified-developer-associate Community posts https://dev.to/romankurnovskii/aws-certified-developer-associate-prepare-2np https://www.reddit.com/user/romankurnovskii/comments/x8rgig/what_is_the_topics_order_to_cover_to_get_prepared/?utm_source=share\u0026amp;utm_medium=web2x\u0026amp;context=3 https://twitter.com/romankurnovskii/status/1567746601136832512 `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudfront/configuring-static-website-s3-and-cloudfront/":{title:"Configuring a Static Website With S3 And CloudFront",tags:["aws","elasticache"],content:`Practice Lab link Creating an Amazon S3 Bucket for a Static Website 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:
You will be placed in the Amazon S3 console.
2. To start creating a new Amazon S3 bucket, in the top-right, click Create bucket:
The Amazon S3 bucket creation form will load.
3. Under General configuration, enter the following:
Bucket name: Enter _calabs-bucket-\u0026lt;UniqueNumber\u0026gt; _(Append a unique number to the end of calabs-bucket-) Region: Ensure US West (Oregon) us-west-2 is selected You have added a unique number to the bucket name because Amazon S3 bucket names must be unique regardless of the AWS region in which the bucket is created.
A bucket name must also be DNS compliant. Here are some of the rules it must adhere to:
They must be at least 3 and no more than 63 characters long. They may contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number. They cannot be formatted as an IP address (for example, 192.168.1.1). The following are examples of valid bucket names:
calabs-bucket-1 cloudacademybucket cloudacademy.bucket calabs.1 ca-labs-bucket Make a note of the name of your bucket, you will use it later.
4. Make sure to select ACLs Enabled:
5. In the Block Public Access section, un-check the Block all public access check-box:
6. To acknowledge that you want to make this bucket publicly accessible, check I acknowledge that the current settings might result in this bucket and the objects within becoming public:
7. To finish creating your Amazon S3 bucket, scroll to the bottom of the form and click Create bucket:
Note: If you see an error because your bucket name is not unique, append a different unique number to the bucket name. For example, change \u0026ldquo;calabs-bucket\u0026rdquo; to \u0026ldquo;calabs-bucket-1\u0026rdquo; (or a unique number/character string) and click Create bucket again.
The Buckets list page will load and you will see a notification that your bucket has been created:
Next, you will enable static website hosting for your bucket.
8. In the list, click the name of your bucket:
You will see an overview of your Amazon S3 bucket, and a row of tabs with Objects selected.
9. In the row of tabs under Bucket overview, click Properties:
The Properties tab allows you to enable and disable various Amazon S3 bucket features, including:
Bucket Versioning: Old versions can be kept when objects are updated Default encryption: A bucket can be configured to encrypt all objects by default Server access logging: Web-server style access logs can be enabled Requester pays: When enabled, the entity downloading data from this bucket will pay data transfer costs incurred 10. Scroll to the bottom of the Properties page and in the Static website hosting section, on the right, click Edit:
The Edit static website hosting form will load.
11. In the Static website hosting field, select Enable:
The form will expand to reveal more configuration options.
12. Enter the following, leaving all other fields at their defaults:
Index document: Enter index.html Error document: Enter error/index.html 13. To finish enabling static website hosting, scroll to the bottom, and click Save changes:
The bucket overview page will load and you\u0026rsquo;ll see a notification that you have successfully enabled static website hosting:
Your S3 bucket is ready to host content.
Next, you will create a bucket policy that will apply to all objects uploaded to your bucket.
14. In the row of tabs, click Permissions:
15. Scroll down to the Bucket policy section, and on the right, click Edit.
The Edit bucket policy form will load.
Amazon S3 bucket policies are defined in JavaScript Object Notation, commonly referred to as JSON.
16. In the Policy editor, copy and paste the following and replace YOUR_BUCKET_NAME with the name of your S3 bucket:
1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 3 \u0026#34;Statement\u0026#34;: [ 4 { 5 \u0026#34;Sid\u0026#34;: \u0026#34;AddPerm\u0026#34;, 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, 8 \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, 9 \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::YOUR_BUCKET_NAME/*\u0026#34; 10 } 11 ] 12} This policy will allow public access to all objects in your S3 bucket.
This is a permissive policy. In a non-lab environment, security concerns may require you to implement a more restrictive policy. To learn more about bucket policies, visit the AWS Policies and Permissions in Amazon S3 documentation.
17. To save your bucket policy, at the bottom of the page, click Save changes.
The bucket overview page will load and you will see a notification that the policy has been edited.
Next, you will download a basic website from a public GitHub repository and load it into your S3 bucket.
18. To download a zip file containing a basic website, click here.
This is an example website provided by CloudAcademy that is suitable for static website hosting.
19. Extract the zip to your local file system.
Exact instructions will vary depending on your operating system and browser. In most browsers, you can click the downloaded file and a file extraction utility will open.
20. In the row of tabs, click Objects.
21. To begin uploading the website to your Amazon S3 bucket, scroll down and click Upload:
The Upload form will load.
22. In the Files and folders section, click Add files:
A file picker will open.
23. Using the file picker, select all files and folders from inside the zip file you downloaded and extracted earlier.
If your extraction utility extracted the files to a folder called static-website-example-master, ensure you upload all the files and folders inside it but not the static-website-example-master folder itself. To be able to access the website, the index.html file must be at the top-level of your Amazon S3 bucket.
You may find it easier to drag and drop the files and folders onto the Upload page instead of using the file picker.
You may also see a browser confirmation dialog asking you to confirm you want to upload the files.
Once selected, the files and folders from the zip file will appear in the Files and folders section.
24. Scroll to the bottom of the page and click Upload.
You will see a blue notification displaying the progress of the upload, and when complete you will see a green Upload succeeded notification.
25. To exit the Upload form, on the right, click Close.
The bucket overview page will load.
Your Objects section should look similar to:
Next, you will test that your website is accessible.
26. To retrieve the endpoint for your bucket, click the Properties tab, scroll to the bottom, and click the copy icon next to the Bucket website endpoint:
27. Paste the endpoint into the address bar of a new browser tab.
You will see a website load that looks like this:
This website is being served by your Amazon S3 bucket.
Creating an Amazon CloudFront Distribution for the Static Website 1. In the AWS Management Console search bar, enter CloudFront, and click the CloudFront result under Services:
The Amazon CloudFront console will load.
2. To start creating a distribution, click Create a CloudFront Distribution:
3. Under Origin, in the Origin Domain text-box, enter the Amazon S3 static website hosting endpoint that you created earlier:
4. Scroll down to the** Settings** settings, in the Default Root Object text-box, enter index.html:
You are setting this field because Amazon CloudFront doesn\u0026rsquo;t always transparently relay requests to the origin. If you did not set a default root object on the distribution you would see an AccessDenied error when you access the CloudFront distribution\u0026rsquo;s domain later in this lab step.
To learn more, see the How CloudFront Works if You Don\u0026rsquo;t Define a Root Object section of the AWS developer guide for Specifying a Default Root Object.
5. Leave all other settings at their default values, scroll to the bottom, and click Create Distribution.
The CloudFront distribution list page will load and you will see your distribution listed.
You will see the Last Modified of your distribution is Deploying:
It can take up to 15 minutes to deploy a new Amazon CloudFront distribution. Once complete, the Status will change to Enabled.
There are two main types of origin that Amazon CloudFront supports, Amazon S3 buckets, and custom origins. A custom origin could be a website being served by an EC2 instance, or it could be a web server outside of AWS. To learn more while your CloudFront distribution is deploying, visit the AWS Using Amazon S3 Origins, MediaPackage Channels, and Custom Origins for Web Distributions page. Once your deployment is complete, continue with the instructions.
6. To view details of your distribution, click the random alphanumeric ID:
Note: Your ID will be different.
7. Copy the value of the Distribution Domain Name field:
8. Paste the domain name into the address bar of a new browser tab.
You will see the website that you uploaded to your Amazon S3 bucket display:
You are accessing the website through your Amazon CloudFront distribution.
Note: The instructions below are optional. Perform them if there is enough time left in the lab.
9. On the website, click through and visit the different pages a few times to generate traffic.
If you have a different web browser available, try accessing the site in the other browser.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudfront/configuring-static-website-s3-and-cloudfront/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/create-folder-s3/":{title:"Create a folder inside S3 Bucket",tags:["s3"],content:`Creating a Folder inside an Amazon S3 Bucket Introduction The AWS S3 console allows you to create folders for grouping objects. This can be a very helpful organizational tool. However, in Amazon S3, buckets and objects are the primary resources. A folder simply becomes a prefix for object key names that are virtually archived into it.
Instructions Return to the Buckets menu by clicking here, and click on the calabs-bucket you created earlier. (Reminder: Your bucket name will differ slightly.)
Click Create folder:
In the Folder name textbox, enter cloudfolder:
Scroll to the bottom and click Create folder:
The folder is created inside your S3 bucket:
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/create-folder-s3/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/upload-file-to-s3/":{title:"Upload a file to S3",tags:["s3"],content:`Uploading a File to Amazon S3 Introduction When you upload a folder from your local system or another machine, Amazon S3 uploads all the files and subfolders from the specified local folder to your bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. In this lab step, you will upload a file to your bucket. The process is similar to uploading a single file, multiple files, or a folder with files in it.
In order to complete this lab step, you have to upload the cloudacademy-logo.png file from your local file storage into an S3 folder you created earlier.
Download the Cloud Academy logo from the following location: https://s3-us-west-2.amazonaws.com/clouda-labs/scripts/s3/cloudacademy-logo.png (If the image is not downloaded for you, simply right-click the image and select Save image as to download it to your local file system.)
Instructions Click on the cloudfolder folder. You are placed within the empty folder in your S3 bucket: Note: Click the folder name itself, not the checkbox for the folder name. If you select the folder checkbox then upload a file, it will be placed above the folder (not inside it).
Click the Upload button.
Click Add Files:
A file picker will appear.
Browse to and select the local copy of cloudacademy-logo.png file that you downloaded earlier: The logo is added to the list of files that are ready to upload. You have several options at this point:
Add more files Upload However, there is another method that some users prefer to add files for upload.
Check the file and click on Remove:
This time, rather than browsing to a file, drag and drop the logo file onto the wizard. The wizard adds it to the list of files to upload.
Scroll to the bottom of the page and click Upload to upload the file:
You will see a blue notification that the file is uploading and then a green notification that the upload has been completed successfully.
The file is placed in the folder in your bucket:
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/upload-file-to-s3/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/grant-access-s3/":{title:"Grant public access to S3 Object",tags:["s3"],content:`Granting Public Access to an Amazon S3 Object Introduction All uploaded files are private by default and can only be viewed, edited, or downloaded by you. In order to illustrate this point, complete the instructions below.
Note: The terms \u0026ldquo;file\u0026rdquo; and \u0026ldquo;object\u0026rdquo; are often used interchangeably when discussing Amazon S3. Technically, Amazon S3 is an object-store. It is not a block storage device and does not contain a file system as your local computer does. However, files such as images, movies, and sound clips are often uploaded from your file system to Amazon S3.
Instructions 1. Click on the object you just uploaded to the S3 bucket.
Take a look at the Object overview section:
Under Object URL, right-click the link and open the URL in a new browser tab: You will see an XML (eXtensible Markup Language) response telling you that access is denied for this object:
Note: The response may appear differently depending upon your web browser.
Leave the browser tab open. You will return to it shortly.
To allow public access to objects, you need to disable the default safety guards that prevent them from being made publicly accessible.
To return to the bucket view, at the top of the page, click the name of your bucket in the bread crumb trail:
Click the Permissions tab and click Edit in the Block public access section:
Uncheck all of the options to allow all kinds of public access:
You should carefully consider anytime you allow public access to S3 buckets. AWS has implemented these security features to help prevent data breaches. For this lab, there is no sensitive data and you do want to allow public access.
Poorly managed Amazon S3 permissions have been a contributing factor to many unauthorized data access events. AWS is making sure you understand the implications of allowing public access to an Amazon S3 bucket.
At the bottom of the page, click Save changes: A confirmation dialog box will appear.
Enter confirm in the confirmation dialog box and click Confirm: You will see a green notification that the public access settings have been edited.
Turning off Block all public access does not automatically make objects in an Amazon S3 bucket public. There are several ways of of explicitly granting public access including:
Bucket policies IAM policies Access control lists Pre-signed URLs In this lab, you will use a bucket policy to grant public access to your Amazon S3 bucket.
Scroll down to the Bucket policy section and click Edit: The Edit bucket policy page will load. Here you can specify a JSON (JavaScript Object Notation) policy to control access to your Amazon S3 bucket.
Replace the contents of the Policy editor with the following: 1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 3 \u0026#34;Statement\u0026#34;: [ 4 { 5 \u0026#34;Action\u0026#34;: [ 6 \u0026#34;s3:GetObject\u0026#34; 7 ], 8 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 9 \u0026#34;Resource\u0026#34;: \u0026#34;BUCKET_ARN/*\u0026#34;, 10 \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; 11 } 12 ] 13} This is a permissive policy that allows GetObject access to anyone. More restrictive policies are possible such as
Restricting access to specific principals Allow cross AWS account access Using conditions to restrict access to a specific IP address Notice the Resource is currently \u0026ldquo;BUCKET_ARN/*\u0026quot;, which is causing an error. We need to replace this with the ARN of the bucket we created:
Click the copy icon under **Bucket ARN **and replace BUCKET_ARN in the value of the Resource key with the ARN you just copied : Note: Ensure that you preserve the /* at the end of the value. This means that the policy will apply to all objects inside the bucket recursively. Public access won\u0026rsquo;t be granted if this is not present.
At the bottom of the page, click Save changes: You will see a green notification that the bucket policy was edited.
Return to the browser tab where access was denied and fresh the browser tab. You will see the response change from “Access Denied” to the logo: `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/grant-access-s3/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/how-to-change-metadata-s3/":{title:"Change metadata of S3 Object",tags:["s3"],content:`Changing the Metadata of an Amazon S3 Object Introduction Each object in Amazon S3 has a set of key/value pairs representing its metadata. There are two types of metadata: \u0026ldquo;System metadata\u0026rdquo; (for example, Content-Type and Content-Length) and custom \u0026ldquo;User metadata\u0026rdquo;. User metadata is stored with the object and returned with it.
As an example, you might have your own taxonomy for various images, such as “logo”, “screenshot”, “diagram”, \u0026ldquo;flowchart\u0026rdquo; and so on. In this lab step, you will change the Content-Type of your image to \u0026ldquo;text/plain\u0026rdquo;. You will also create custom user metadata.
Note: With the new Amazon S3 UI you can set the metadata as part of the upload process, or add it later.
Instructions Return to the cloudfolder/ and delete the cloudacademy-logo.png from your Amazon S3 bucket by checking the checkbox and clicking Delete: The Delete objects form page will load. Because a deleted object is not retrievable, AWS asks you to confirm that you want to delete the object before deletion.
In the textbox at the bottom of the page, enter permanently delete and click Delete objects:
To return to the bucket object view, at the top-right, click Close.
Click Upload, then Add files and browse to the logo file (or drag-and-drop it into the Upload wizard) in order to upload it again.
Near the bottom of the page, expand the Properties section:
Scroll down to the Metadata section and click Add metadata:
A row of form elements will appear.
Enter the following: Type: Select System defined Key: Select Content-Type Value: Enter text/plain The drop-down list contains the System metadata that you can set.
In this lab, you have set the content type to text/plain as an example to see how to add metadata to an object when uploading to Amazon S3.
Next you will add custom user metadata. User metadata must be prefaced with \u0026ldquo;x-amz-meta-\u0026rdquo;. The remaining instructions will add a custom user type for imagetype, and imagestatus.
Click Add metadata again to add another row. Enter the following to define custom metadata:
Type: Select User defined Key: Enter imagetype after x-amz-meta Value: Enter logo You have added two metadata key-value pairs to the object you are going to upload. One system metadata and one user-defined.
At the bottom of the page, click Upload:
To exit the upload form, at the top-right, click Close.
In the Objects table click the cloudacademy-logo.png object:
Scroll down to the Metadata section and observe the key-value pairs you added:
This is also where you can add, remove, and edit metadata after you have uploaded objects to Amazon S3.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/how-to-change-metadata-s3/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/delete-from-s3/":{title:"Delete S3 Bucket",tags:["s3"],content:`Deleting an Amazon S3 Bucket Introduction You can delete an Amazon S3 bucket using the S3 console. You will delete all objects within the bucket as well.
Instructions In the AWS Management Console search bar, enter S3, and click the S3 result under Services: From the top level of the S3 console, notice the Delete button is not actionable.
2. Check the name of your bucket to select it:
With the bucket selected, click Empty: The Empty bucket form page will load.
It\u0026rsquo;s not possible to delete a bucket that contains objects.
To confirm that you want to delete all objects in this bucket, in the textbox at the bottom, enter permanently delete and click Empty:
To exit the empty bucket page, at the top-right, click Exit:
You will be returned to the Buckets page.
To delete your bucket, select it in the list, and click Delete
To confirm that you want to delete the bucket, in the textbox, enter the name of your bucket:
Click Delete bucket to delete the bucket.
Warning: Make sure to delete all the files/folders inside the bucket before deleting it, otherwise AWS won\u0026rsquo;t allow you to delete the S3 bucket.
Important! Notice the message from AWS: \u0026ldquo;Amazon S3 buckets are unique. If you delete this bucket, you may lose the bucket name to another AWS user.\u0026rdquo;
If retaining the bucket name is important to you, consider using the Empty bucket feature and not actually deleting the bucket.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/delete-from-s3/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/1-two-sum/":{title:"1. Two Sum",tags:["Array","Hash Table"],content:`LeetCode problem
Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.
You may assume that each input would have exactly one solution, and you may not use the same element twice.
You can return the answer in any order.
Example 1:
Input: nums = [2,7,11,15], target = 9 Output: [0,1] Explanation: Because nums[0] + nums[1] == 9, we return [0, 1]. Topics: array, hash table
If we fix one of the numbers, say x, we have to scan the entire array to find the next number y which is value - x where value is the input parameter. Can we change our array somehow so that this search becomes faster?
The second train of thought is, without changing the array, can we use additional space somehow? Like maybe a hash map to speed up the search?
First accepted 1class Solution: 2 def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: 3 nums_set = set(nums) # to search in a O(1) 4 while nums: 5 n1 = nums.pop() # get last 6 n2 = target - n1 7 8 if n1 == n2: 9 if n2 in nums: 10 return [len(nums), nums.index(n2)] 11 elif n2 in nums_set: 12 return [len(nums), nums.index(n2)] faster than 64.51% Memory Usage: 15.5 MB, less than 9.15% Time complexity:
set(nums): O(n) while nums: O(n) nums.pop: O(1) nums.index: O(n) Better solution remember indexes of \u0026ldquo;passed\u0026rdquo; n's from nums
1class Solution: 2 def twoSum(self, nums: List[int], target: int) -\u0026gt; List[int]: 3 hashmap = {} 4 for idx, n1 in enumerate(nums): 5 n2 = target - n1 6 if n2 in hashmap: 7 return [idx, hashmap[n2]] 8 hashmap[n1] = idx faster than 53.00% Memory Usage: 15.1 MB, less than 52.14% `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/1-two-sum/"},"https://romankurnovskii.com/en/docs/algorithms-101/plan/":{title:"Plan",tags:[],content:`Q1 # Problem Difficulty Topics 1 1. Two Sum Easy Array, Hash Table 2 13. Roman to Integer Easy Hash Table, Math, String 3 14. Longest Common Prefix Easy String 4 20. Valid Parentheses Easy String, Stack 5 21. Merge Two Sorted Lists Easy Linked List, Recursion 6 26. Remove Duplicates from Sorted Array Easy Array, Two pointers 7 66. Plus One Easy Array, Math 8 69. Sqrt(x) Easy Math, Binary Search, 9 70. Climbing Stairs Easy Math, Dynamic Programming, Memoization 10 88. Merge Sorted Array Easy Array, Two pointers, Sorting 11 94. Binary Tree Inorder Traversal Easy Stack, Tree, Depth-First Search, Binary Tree 12 2. Add Two Numbers Medium Linked List, Math, Recursion 13 3. Longest Substring Without Repeating Characters Medium Hash Table, String, Sliding Window 14 5. Longest Palindromic Substring Medium String, Dynamic Programming 15 7. Reverse Integer Medium Math 16 11. Container With Most Water Medium Array, Two pointers, Greedy 17 15. 3Sum Medium Array, Two pointers, Sorting 18 17. Letter Combinations of a Phone Number Medium Hash Table, String, Backtracking 19 19. Remove Nth Node From End of List Medium Linked List, Two pointers 20 22. Generate Parentheses Medium String, Dynamic Programming, Backtracking Intermediate results Appeared intuitive understanding of algorithms. In most cases, one hour is not enough to solve the problem. If you start sketching an intuitive algorithm, then in the process comes an understanding and an improved solution.
Update plan by solution:
After reading, if there is no exact solution: Assume/analyze/draw the proposed algorithm View solutions with explanations Compare with your own / analyze Code Q2 # Problem Difficulty Topics 21 28. Find the Index of the First Occurrence in a String Medium String, Two pointers, String Matching 22 29. Divide Two Integers Medium Math, Bit Manipulation 23 33. Search in Rotated Sorted Array Medium Array, Binary Search, 24 34. Find First and Last Position of Element in Sorted Array Medium Array, Binary Search, 25 36. Valid Sudoku Medium Array, Hash Table,, Matrix 26 38. Count and Say Medium String 27 46. Permutations Medium Array, Backtracking 28 48. Rotate Image Medium Array, Math, Matrix 29 49. Group Anagrams Medium Array, Hash Table,, String, Sorting 30 50. Pow(x, n) Medium Math, Recursion 31 53. Maximum Subarray Medium Array, Divide and Conquer, Dynamic Programming Intermediate results Revise training tactics. Prepare list of top coding patterns.
Practice on each coding pattern.
Q3 # Problem Difficulty Topics 32 55. Jump Game Medium Array, Greedy, Dynamic Programming 33 56. Merge Intervals Medium Array, Sorting, 34 62. Unique Paths Medium Math, Dynamic Programming, Combinatorics] 35 36 37 38 39 40 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/plan/"},"https://romankurnovskii.com/en/docs/algorithms-101/leetcode-tools/":{title:"Tools",tags:[],content:" ",url:"https://romankurnovskii.com/en/docs/algorithms-101/leetcode-tools/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/13-roman-to-integer/":{title:"13. Roman to Integer",tags:["Hash Table","Math","String"],content:`LeetCode problem
Roman numerals are represented by seven different symbols: I, V, X, L, C, D and M.
Symbol Value I 1 V 5 X 10 L 50 C 100 D 500 M 1000 For example, 2 is written as II in Roman numeral, just two ones added together. 12 is written as XII, which is simply X + II. The number 27 is written as XXVII, which is XX + V + II.
Roman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used:
I can be placed before V (5) and X (10) to make 4 and 9. X can be placed before L (50) and C (100) to make 40 and 90. C can be placed before D (500) and M (1000) to make 400 and 900.
Given a roman numeral, convert it to an integer.
Example 1:
Input: s = \u0026quot;III\u0026quot; Output: 3 Explanation: III = 3. Example 2:
Input: s = \u0026quot;MCMXCIV\u0026quot; Output: 1994 Explanation: M = 1000, CM = 900, XC = 90 and IV = 4. First accepted 1class Solution: 2 def romanToInt(self, s: str) -\u0026gt; int: 3 dict = {\u0026#39;I\u0026#39;:1,\u0026#39;V\u0026#39;:5,\u0026#39;X\u0026#39;:10,\u0026#39;L\u0026#39;:50,\u0026#39;C\u0026#39;:100,\u0026#39;D\u0026#39;:500,\u0026#39;M\u0026#39;:1000} 4 n_sum = 0 5 prev = 0 6 for c in reversed(s): 7 n = dict[c] 8 n = -n if n in (1,10,100) and prev in (n*5, n*10) else n 9 n_sum += n 10 prev = abs(n) 11 return n_sum `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/13-roman-to-integer/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/":{title:"Problems",tags:[],content:`Python template
Tips I learned Don\u0026rsquo;t code before \u0026ldquo;design\u0026rdquo; and understand the solution algorithm First 20 problems. Spent 5 min for thinking. Next look for 2-3 different approaches. Understand. Decide. Write. Resources https://walkccc.me/LeetCode/problems/ https://books.halfrost.com/leetcode/ https://grandyang.com/leetcode/ `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/14-longest-common-prefix/":{title:"14. Longest Common Prefix",tags:["String"],content:`LeetCode problem
Write a function to find the longest common prefix string amongst an array of strings.
If there is no common prefix, return an empty string \u0026quot;\u0026quot;.
Example 1:
Input: strs = [\u0026quot;flower\u0026quot;,\u0026quot;flow\u0026quot;,\u0026quot;flight\u0026quot;] Output: \u0026quot;fl\u0026quot; Example 2:
Input: strs = [\u0026quot;dog\u0026quot;,\u0026quot;racecar\u0026quot;,\u0026quot;car\u0026quot;] Output: \u0026quot;\u0026quot; Explanation: There is no common prefix among the input strings. First accepted Idea:
1class Solution: 2 def longestCommonPrefix(self, strs: List[str]) -\u0026gt; str: 3 strs.sort() 4 l = strs[0] 5 r = strs[-1] 6 if l == r: 7 return l 8 res = \u0026#34;\u0026#34; 9 for i in range(0, len(l)): 10 if l[i] == r[i]: 11 res += l[i] 12 else: 13 return res 14 return res `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/14-longest-common-prefix/"},"https://romankurnovskii.com/en/docs/algorithms-101/algorithms/":{title:"Algorithm Patterns",tags:[],content:`Intro Big-O Cheat Sheet Binary Search Binary search template 1 def find_target(nums, target): 2 left = 0 3 right = len(nums) - 1 4 5 while left \u0026lt;= right: 6 mid = (left + right) // 2 7 if nums[mid] == target: 8 return mid 9 10 if nums[mid] \u0026lt; target: 11 left = mid + 1 12 else: 13 right = mid - 1 14 15 return -1 Dynamic programming (DP) Breadth First Search (BFS) BFS on Tree:
1def bfs(root): 2 queue = deque([root]) 3 while len(queue) \u0026gt; 0: 4 node = queue.popleft() 5 for child in node.children: 6 if is_goal(child): 7 return FOUND(child) 8 queue.append(child) 9 return NOT_FOUND BFS on Graph:
1def bfs(root): 2 queue = deque([root]) 3 visited = set([root]) 4 while len(queue) \u0026gt; 0: 5 node = queue.popleft() 6 for neighbor in get_neighbors(node): 7 if neighbor in visited: 8 continue 9 queue.append(neighbor) 10 visited.add(neighbor) BFS on a Matrix:
1num_rows, num_cols = len(grid), len(grid[0]) 2def get_neighbors(coord): 3 row, col = coord 4 delta_row = [-1, 0, 1, 0] 5 delta_col = [0, 1, 0, -1] 6 res = [] 7 for i in range(len(delta_row)): 8 neighbor_row = row + delta_row[i] 9 neighbor_col = col + delta_col[i] 10 if 0 \u0026lt;= neighbor_row \u0026lt; num_rows and 0 \u0026lt;= neighbor_col \u0026lt; num_cols: 11 res.append((neighbor_row, neighbor_col)) 12 return res 13 14from collections import deque 15 16def bfs(starting_node): 17 queue = deque([starting_node]) 18 visited = set([starting_node]) 19 while len(queue) \u0026gt; 0: 20 node = queue.popleft() 21 for neighbor in get_neighbors(node): 22 if neighbor in visited: 23 continue 24 # Do stuff with the node if required 25 # ... 26 queue.append(neighbor) 27 visited.add(neighbor) Depth-first search (DFS) DFS on Tree:
1def dfs(root, target): 2 if root is None: 3 return None 4 if root.val == target: 5 return root 6 left = dfs(root.left, target) 7 if left is not None: 8 return left 9 return dfs(root.right, target) DFS on Graph:
1def dfs(root, visited): 2 for neighbor in get_neighbors(root): 3 if neighbor in visited: 4 continue 5 visited.add(neighbor) 6 dfs(neighbor, visited) Sliding Window Usage: Use when need to handle the input data in specific window size.
Example: Sliding window technique to find the largest sum of 4 consecutive numbers. Template:
1while j \u0026lt; size: 2 # Calculation\u0026#39;s happen\u0026#39;s here 3 # ... 4 5 if condition \u0026lt; k: 6 j+=1 7 elif condition == k: # ans \u0026lt;-- calculation 8 j+=1 9 elif condition \u0026gt; k: 10 while condition \u0026gt; k: 11 i+=1 # remove calculation for i 12 j+=1 13return ans Examples Problem: Find the largest sum of K consecutive entries, given an array of size N
Add the first K components together and save the result in the currentSum variable. Because this is the first sum, it is also the current maximum; thus, save it in the variable maximumSum. As the window size is ww, we move the window one place to the right and compute the sum of the items in the window. Update the maximum if the currentSum is greater than the maximumSum, and repeat step 2. 1def maxSum(arr, k): 2	n = len(arr) # length of the array 3 4	# length of array must be greater 5 # window size 6	if n \u0026lt; k: 7	print(\u0026#34;Invalid\u0026#34;) 8	return -1 9 10	# sum of first k elements 11	window_sum = sum(arr[:k]) 12	max_sum = window_sum 13 14	# remove the first element of previous 15	# window and add the last element of 16	# the current window to calculate the 17 # the sums of remaining windows by 18	for i in range(n - k): 19	window_sum = window_sum - arr[i] + arr[i + k] 20	max_sum = max(window_sum, max_sum) 21 22	return max_sum 23 24 25arr = [16, 12, 9, 19, 11, 8] 26k = 3 27print(maxSum(arr, k)) Problem: Find duplicates within a range ‘k’ in an array
Input: nums = [5, 6, 8, 2, 4, 6, 9] k = 2 Ouput: False 1def getDuplicates(nums, k): 2 d = {} 3 count = 0 4 for i in range(len(nums)): 5 if nums[i] in d and i - d[nums[i]] \u0026lt;= k: 6 return True 7 else: 8 d[nums[i]] = i 9 10 return False Problem/solution examples Article on LeetCode Practice questions Two Pointers A classic way of writing a two-pointer sliding window. The right pointer keeps moving to the right until it cannot move to the right (the specific conditions depend on the topic). When the right pointer reaches the far right, start to move the left pointer to release the left boundary of the window.
Usage: Use two pointers to iterate the input data. Generally, both pointers move in the opposite direction at a constant interval.
Practice questions Two pointers intro Backtracking Based on Depth-first search (DFS)
Backtracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point of time.
Backtracking algorithm is derived from the Recursion algorithm, with the option to revert if a recursive solution fails, i.e. in case a solution fails, the program traces back to the moment where it failed and builds on another solution. So basically it tries out all the possible solutions and finds the correct one.
Backtracking == DFS on a tree
Usage:
Finding all permutations, combinations, subsets and solving sudoku are classic combinatorial problems.
Howto:
Backtracking is drawing tree When drawing the tree, bear in mind: how do we know if we have reached a solution? how do we branch (generate possible children)? Template:
Pattern template:
1def backtrack(some_len_data): 2 result = [] 3 4 def dfs(left, right): 5 if isCondition: 6 result.append(\u0026#34;WhatNeeded\u0026#34;) 7 if left \u0026gt; 0: 8 dfs(left-1, right) 9 if left \u0026lt; right: 10 dfs(left, right-1) 11 12 dfs(some_len_data) 13 return result Full with comments:
1 def backtrack(A): 2 # Set up a list of list to hold all possible solutions 3 result = [] 4 if A == None or len(A) == 0): 5 return result 6 7 # As we need to recursively generate every solution, 8 # a variable is needed to store single solution. 9 solution = [] 10 11 # The process of constructing the solution corresponds exactly to 12 # doing a Depth-First-Search of the backtrack tree. Viewing backtracking 13 # as a Depth-First-Search on a tree yields a natural recursive implementation 14 # of the algorithm. 15 dfs(result, solution, A, 0) 16 return result 17 18 def dfs(result, solution, A, pos): 19 # For recursion, the first thing we need to think about is how to terminate it, i.e., base case. 20 # Method isASolution() could be implemented as a private method which returns boolean, 21 # or for simple test we can directly use the condition replace it, e.g., generate all subsets 22 # we can write the condition: if (A.length == pos) 23 if isASolution(A, pos): 24 # Because we use the variable - \u0026#39;solution\u0026#39; to hold partial solution, 25 # when this search is terminated, the variable must hold a complete 26 # solution. 27 # Same like isASolution, processSolution method should print, count or however 28 # process the complete solution once it is constructed. 29 processSolution(result, solution) 30 return 31 32 i = pos 33 while i \u0026lt; len(A): 34 # Before Depth-First-Search, we should decide whether we need to prune searching 35 # which means it\u0026#39;s unnecessary to continue search along a wrong partial solution 36 if (!isValid(A[i])): 37 continue; # stop searching along this path 38 39 # Add the ith element of the array into the partial solution. 40 # Before searching, we need to record the latest element we are using 41 # to build the solution tree. Method makeMove should be able to add A[i] 42 # to the solution, i.e., solution.add(A[i]) 43 makeMove(solution, A[i]) 44 45 # Right now, we are searching all possible solutions based on ith element. 46 # Warning: remember as we\u0026#39;ve already added ith element, the last parameter 47 # must be \`i + 1\`. 48 dfs(result, solution, A, i + 1) 49 50 # After searching based on ith element, take back the move and search another 51 # possible partial solution. 52 unmakeMove(solution, A[i]) 53 i += 1 Resources https://algo.monster/problems/backtracking Trie 1class Node: 2 def __init__(self, value): 3 self.value = value 4 self.children = {} 5 6 def insert(self, s, idx): 7 # idx: index of the current character in s 8 if idx != len(s): 9 self.children.setdefault(s[idx], Node(s[idx])) 10 self.children.get(s[idx]).insert(s, idx + 1) Resources https://www.geeksforgeeks.org/learn-data-structures-and-algorithms-dsa-tutorial/ https://algo.monster/templates https://interviewnoodle.com/grokking-leetcode-a-smarter-way-to-prepare-for-coding-interviews-e86d5c9fe4e1 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/algorithms/"},"https://romankurnovskii.com/en/docs/algorithms-101/helpers/":{title:"Help functions",tags:[],content:`Create Linked List Definition for singly-linked list:
1class ListNode: 2 def __init__(self, val=0, next=None): 3 self.val = val 4 self.next = next 1values = [2, 4, 3] 2 3def createLinkedNode(values): 4 head = ListNode(values[0]) # start node, head of the linkedlist 5 current = head # current node in the linked list where we change/add next node 6 for i in values[1:]: 7 node = ListNode(i) 8 current.next = node 9 current = current.next # now current node is last created 10 return head 11 12linled_list = createLinkedNode(values) Object structure:
`,url:"https://romankurnovskii.com/en/docs/algorithms-101/helpers/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/20-valid-parentheses/":{title:"20. Valid Parentheses",tags:["String","Stack"],content:`LeetCode problem
Given a string s containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.
An input string is valid if:
Open brackets must be closed by the same type of brackets. Open brackets must be closed in the correct order. Every close bracket has a corresponding open bracket of the same type. Example 1:
Input: s = \u0026quot;()[]{}\u0026quot; Output: true Example 2:
Input: s = \u0026quot;()[]{}\u0026quot; Output: true First accepted Idea:
Loop through string If current \u0026ldquo;closes\u0026rdquo; the last in stack, then remove last from stack Else: add current to stack 1class Solution: 2 def isValid(self, s: str) -\u0026gt; bool: 3 stack = [] 4 par_dict = {\u0026#39;(\u0026#39;: \u0026#39;)\u0026#39;, \u0026#39;{\u0026#39;: \u0026#39;}\u0026#39;, \u0026#39;[\u0026#39;: \u0026#39;]\u0026#39;} 5 last_value = None 6 for i in s: 7 second_value = par_dict.get(last_value, None) 8 if i == second_value: 9 stack.pop() 10 last_value = stack[-1] if stack else None 11 else: 12 stack.append(i) 13 last_value = i 14 return not stack Better solution 1class Solution: 2 def isValid(self, s: str) -\u0026gt; bool: 3 par_dict = {\u0026#39;(\u0026#39;: \u0026#39;)\u0026#39;, \u0026#39;{\u0026#39;: \u0026#39;}\u0026#39;, \u0026#39;[\u0026#39;: \u0026#39;]\u0026#39;} 4 stack = [] 5 for char in s: 6 if char in par_dict: # If it\u0026#39;s an opening bracket, add it to the stack 7 stack.append(char) 8 elif stack: # If there\u0026#39;s something in the stack 9 if char == par_dict[stack[-1]]: 10 # If it\u0026#39;s a closing bracket for the last opened bracket, remove it from the stack. 11 stack.pop() 12 else: # It\u0026#39;s not a closing bracket for the last opened bracket. Invalid string. 13 return False 14 else: # Not an opening bracket or closing bracket. Invalid string. 15 return False 16 return stack == [] `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/20-valid-parentheses/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/21-merge-two-sorted-lists/":{title:"21. Merge Two Sorted Lists",tags:["Linked List","Recursion"],content:`LeetCode problem
You are given the heads of two sorted linked lists list1 and list2.
Merge the two lists in a one sorted list. The list should be made by splicing together the nodes of the first two lists.
Return the head of the merged linked list.
Example 1:
Input: list1 = [1,2,4], list2 = [1,3,4] Output: [1,1,2,3,4,4] Example 2:
Input: list1 = [], list2 = [0] Output: [0] First accepted Idea:
Get smallest head. Loop and update its next.
1# Definition for singly-linked list. 2# class ListNode: 3# def __init__(self, val=0, next=None): 4# self.val = val 5# self.next = next 6class Solution: 7 def mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: 8 res = ListNode() 9 current = res 10 while l1 and l2: 11 if l1.val \u0026lt;= l2.val: 12 node = ListNode(l1.val) 13 l1 = l1.next 14 else: 15 node = ListNode(l2.val) 16 l2 = l2.next 17 current.next = node 18 current = current.next 19 20 if l1: 21 current.next = l1 22 if l2: 23 current.next = l2 24 25 return res.next Better solution Recursion
1def mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: 2	if l1 and l2: 3	if l1.val \u0026gt; l2.val: 4	l1, l2 = l2, l1 #swap smaller and larger: make l1 the one with the smaller first value 5	l1.next = self.mergeTwoLists(l1.next, l2) # move forward in the list which starts with the smaller value 6	return l1 or l2 # return whichever of the two lists remains at the end Loop
1def mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: 2 res = ListNode() 3 current = res 4 while l1 and l2: 5 if l1.val \u0026lt;= l2.val: 6 current.next = l1 7 l1 = l1.next 8 else: 9 current.next = l2 10 l2 = l2.next 11 current = current.next 12 1class Solution: 2 def mergeTwoLists(self, a, b): 3 if a and b: 4 if a.val \u0026gt; b.val: 5 a, b = b, a 6 a.next = self.mergeTwoLists(a.next, b) 7 return a or b First make sure that a is the \u0026ldquo;better\u0026rdquo; one (meaning b is None or has larger/equal value). Then merge the remainders behind a.
1def mergeTwoLists(self, a, b): 2 if not a or b and a.val \u0026gt; b.val: 3 a, b = b, a 4 if a: 5 a.next = self.mergeTwoLists(a.next, b) 6 return a `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/21-merge-two-sorted-lists/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/26-remove-duplicates-from-sorted-array/":{title:"26. Remove Duplicates from Sorted Array",tags:["Array","Two pointers"],content:`LeetCode problem
Given an integer array nums sorted in non-decreasing order, remove the duplicates in-place such that each unique element appears only once. The relative order of the elements should be kept the same.
Since it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.
Return k after placing the final result in the first k slots of nums.
Do not allocate extra space for another array. You must do this by modifying the input array in-place with O(1) extra memory.
Custom Judge:
The judge will test your solution with the following code:
int[] nums = [...]; // Input array int[] expectedNums = [...]; // The expected answer with correct length int k = removeDuplicates(nums); // Calls your implementation assert k == expectedNums.length; for (int i = 0; i \u0026lt; k; i++) { assert nums[i] == expectedNums[i]; } If all assertions pass, then your solution will be accepted.
Example 1:
Input: nums = [1,1,2] Output: 2, nums = [1,2,_] Explanation: Your function should return k = 2, with the first two elements of nums being 1 and 2 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). Example 2:
Input: nums = [0,0,1,1,1,2,2,3,3,4] Output: 5, nums = [0,1,2,3,4,_,_,_,_,_] Explanation: Your function should return k = 5, with the first five elements of nums being 0, 1, 2, 3, and 4 respectively. It does not matter what you leave beyond the returned k (hence they are underscores). First accepted Idea:
1class Solution: 2 def removeDuplicates(self, nums: List[int]) -\u0026gt; int: 3 k = 1 4 if len(nums) == 1: 5 return k 6 7 p1 = 0 8 p2 = 1 9 10 while p2 \u0026lt; len(nums): 11 if nums[p1] == nums[p2]: 12 p2 += 1 13 else: 14 p1 += 1 15 nums[p1] = nums[p2] 16 p2 += 1 17 k += 1 18 19 return k Better solution 1class Solution: 2 def removeDuplicates(self, nums: List[int]) -\u0026gt; int: 3 if len(nums) == 1: 4 return 1 5 6 k = 1 7 i = 1 8 for n in nums: 9 if nums[i-1] != n: 10 nums[i] = n 11 i += 1 12 k += 1 13 14 return k `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/26-remove-duplicates-from-sorted-array/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/66-plus-one/":{title:"66. Plus One",tags:["Array","Math"],content:`LeetCode problem
You are given a large integer represented as an integer array digits, where each digits[i] is the ith digit of the integer. The digits are ordered from most significant to least significant in left-to-right order. The large integer does not contain any leading 0\u0026rsquo;s.
Increment the large integer by one and return the resulting array of digits.
Example 1:
Input: digits = [1,2,3] Output: [1,2,4] Explanation: The array represents the integer 123. Incrementing by one gives 123 + 1 = 124. Thus, the result should be [1,2,4]. Example 2:
Input: digits = [4,3,2,1] Output: [4,3,2,2] Explanation: The array represents the integer 4321. Incrementing by one gives 4321 + 1 = 4322. Thus, the result should be [4,3,2,2]. First accepted 1class Solution: 2 def plusOne(self, digits: List[int]) -\u0026gt; List[int]: 3 i = len(digits) - 1 4 5 while i \u0026gt;= 0 and digits[i] == 9: 6 digits[i] = 0 7 i -= 1 8 9 if i \u0026lt; 0: 10 return [1] + digits 11 digits[i] = digits[i] + 1 12 13 return digits `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/66-plus-one/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/69-sqrtx/":{title:"69. Sqrt(x)",tags:["Math","Binary Search"],content:`LeetCode problem
Given a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.
You must not use any built-in exponent function or operator.
For example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.
Example 1:
Input: x = 4 Output: 2 Explanation: The square root of 4 is 2, so we return 2. Example 2:
Input: x = 8 Output: 2 Explanation: The square root of 8 is 2.82842..., and since we round it down to the nearest integer, 2 is returned. First accepted 1class Solution: 2 def mySqrt(self, x: int, div=2) -\u0026gt; int: 3 s = x // div 4 s1 = (s + div) // 2 5 if s1 * s1 \u0026gt; x: 6 s1 = self.mySqrt(x, s1) 7 return s1 8 else: 9 return s1 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/69-sqrtx/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/70-climbing-stairs/":{title:"70. Climbing Stairs",tags:["Math","Dynamic Programming","Memoization"],content:`LeetCode problem
You are climbing a staircase. It takes n steps to reach the top.
Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?
Example 1:
Input: n = 2 Output: 2 Explanation: There are two ways to climb to the top. 1. 1 step + 1 step 2. 2 steps Example 2:
Input: n = 3 Output: 3 Explanation: There are three ways to climb to the top. 1. 1 step + 1 step + 1 step 2. 1 step + 2 steps 3. 2 steps + 1 step First accepted Idea:
Tried to calculate by hand. There is a sequence Fibonacci here
1class Solution: 2 def climbStairs(self, n: int) -\u0026gt; int: 3 if n == 1: 4 return 1 5 if n == 2: 6 return 2 7 8 prev1 = 1 9 prev2 = 2 10 current = 2 11 while n \u0026gt; 2: 12 current = prev1 + prev2 13 prev1 = prev2 14 prev2 = current 15 n -= 1 16 return current `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/70-climbing-stairs/"},"https://romankurnovskii.com/en/docs/disser/utils/text_2_short/":{title:"Short description from article",tags:[],content:" Create ",url:"https://romankurnovskii.com/en/docs/disser/utils/text_2_short/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/88-merge-sorted-array/":{title:"88. Merge Sorted Array",tags:["Array","Two Pointers","Sorting"],content:`LeetCode problem
You are given two integer arrays nums1 and nums2, sorted in non-decreasing order, and two integers m and n, representing the number of elements in nums1 and nums2 respectively.
Merge nums1 and nums2 into a single array sorted in non-decreasing order.
The final sorted array should not be returned by the function, but instead be stored inside the array nums1. To accommodate this, nums1 has a length of m + n, where the first m elements denote the elements that should be merged, and the last n elements are set to 0 and should be ignored. nums2 has a length of n.
Example 1:
Input: nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] Explanation: The arrays we are merging are [1,2,3] and [2,5,6]. The result of the merge is [1,2,2,3,5,6] with the underlined elements coming from nums1. Example 2:
Input: nums1 = [1], m = 1, nums2 = [], n = 0 Output: [1] Explanation: The arrays we are merging are [1] and []. The result of the merge is [1]. Example 3:
Input: nums1 = [0], m = 0, nums2 = [1], n = 1 Output: [1] Explanation: The arrays we are merging are [] and [1]. The result of the merge is [1]. Note that because m = 0, there are no elements in nums1. The 0 is only there to ensure the merge result can fit in nums1. First accepted 1class Solution: 2 def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -\u0026gt; None: 3 \u0026#34;\u0026#34;\u0026#34; 4 Do not return anything, modify nums1 in-place instead. 5 \u0026#34;\u0026#34;\u0026#34; 6 i = len(nums1) - n 7 for j in nums2: 8 nums1[i] = j 9 i += 1 10 nums1.sort() `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/88-merge-sorted-array/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/iam/":{title:"IAM",tags:["aws","iam"],content:`About IAM - AWS Identity and Access Management
AWS IAM AWS IAM User Guide AWS Identity and Access Management (IAM) allows to securely control user access to AWS services and resources.
Designed for organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon RDS, and AWS Management Console.
With IAM, you can centrally manage users, security credentials such as access keys, and permissions that control user access to AWS resources.
There are three ways IAM authenticates a principal:
User Name/Password Access Key Access Key/Session Token Digest IAM consists of the following: Users Groups Roles Policy Documents IAM is Global. It doesn\u0026rsquo;t apply to any specific region. There is no charge to use IAM. IAM is compliant with Payment Card Industry (PCI) Data Security Standard (DSS) The \u0026ldquo;root account\u0026rdquo; has complete Admin access. Don\u0026rsquo;t use \u0026ldquo;root account\u0026rdquo; for everyday use. Instead, create users. A new user will have NO permissions by default. Grant least privilege needed for their job. New user will be assigned with password, Access Key ID \u0026amp; Secret Access Keys. The password will be used to login to AWS management console. Access Key ID \u0026amp; Secret Access Key will be used to login via the APIs and CLI Always setup MFA on your root account. Use Groups to assign permissions to IAM users Use Roles to Delegate permissions. Role is more secure than creating individual user. Roles gives temporary credentials for access; whereas User has long term credentials. Create and customize password rotation policies Policies can be attached to users, groups and roles. Use AWS defined policies, assign permissions wherever possible. Policy is defined in JSON format and contains version, statements, - effect, action, resource, principal, and condition. STS Security Token Service provides temporary security credentials to the trusted users. STS is global and there is no charge to use it. Digest: https://tutorialsdojo.com/aws-identity-and-access-management-iam/ IAM best practices - Question might ask you to identify best practices among the given choices. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html Difference between when to use Role and User. IAM Policy Simulator - service for testing and troubleshooting IAM Policies. Details Practice Go to IAM page
Creating IAM groups On the User Groups page, click Create group
Specify the name of the group. Mine is: DevOps. Add permission to view EC2: AmazonEC2ReadOnlyAccess. create The group was created
Creating IAM users On the Users page, click Create user Type in user name (login) Permissions Add user to the group Tags Skip section or put tags. It is useful and popular to set tags for resources in companies with a lot of connected AWS resources
Login/Password At the last step, download the .csv file with login, keys and password. You will need the password later to log in as this user. On this page there is a link to log in. We will use it in the next step Logging in as a new user Checking privileges. This user has access to view EC2 instances. Let\u0026rsquo;s check whether or not the S3 garbage cans have access.
Let\u0026rsquo;s try to create an S3 bucket After trying to create a recycle bucket, we get a window indicating no permissions Questions Q1 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.
What are the best practice guidelines for use of the root account?
Never use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1
Q2 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.
Which of the following services do you need to call from AWS STS service after you authenticate with your on-premise?
AssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1
Q3 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.
Which of the following APIs will be leveraged to provide temporary security credentials?
AssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html
(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA
3
Q4 A leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC.
The access should allow the users to create and configure various AWS resources, such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user’s organization, including the master account email and organization limitations.
Which of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?
Attach the PowerUserAccess AWS managed policy to the IAM users. Attach the AdministratorAccess AWS managed policy to the IAM users. Create a new IAM role and attach the SystemAdministrator AWS managed policy to it. Assign the IAM Role to the IAM users. Create a new IAM role and attach the AdministratorAccess AWS managed policy to it. Assign the IAM Role to the IAM users. Explanation AWS managed policies for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function.
These policies consolidate permissions for many services into a single policy that’s easier to work with than having permissions scattered across many policies.
For Developer Power Users, you can use the AWS managed policy name: PowerUserAccess if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development.
The first statement of this policy uses the NotAction element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role.
This is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user’s organization, including the master account email and organization limitations.
1
Q5 A company has 100 AWS accounts that are consolidated using AWS Organizations. The accountants from the finance department log in as IAM users in the TD-Finance AWS account. The finance team members need to read the consolidated billing information in the TD-Master AWS master account that pays the charges of all the member (linked) accounts. The required IAM access to the AWS billing services has already been provisioned in the master account.
The Security Officer should ensure that the finance team must not be able to view any other resources in the master account.
Which of the following grants the finance team the necessary permissions for the above requirement?
Set up an IAM group for the finance users in the TD-Finance account then attach a ViewBilling permission and AWS managed ReadOnlyAccess IAM policy to the group. Set up individual IAM users for the finance users in the TD-Master account then attach the AWS managed ReadOnlyAccess IAM policy to the group with cross-account access. Set up an AWS IAM role in the TD-Finance account with the ViewBilling permission then grant the finance users in the TD-Master account the permission to assume that role. Set up an IAM role in the TD-Master account with the ViewBilling permission then grant the finance users in the TD-Finance account the permission to assume the role. Explanation You can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Internet Services Pvt. Ltd (AISPL) accounts. Every organization in AWS Organizations has a master (payer) account that pays the charges of all the member (linked) accounts.
ModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyAccount – Allow or deny IAM users permission to modify Account Settings. ModifyBilling – Allow or deny IAM users permission to modify billing settings. ModifyPaymentMethods – Allow or deny IAM users permission to modify payment methods. ViewAccount – Allow or deny IAM users permission to view account settings. ViewBilling – Allow or deny IAM users permission to view billing pages in the console. ViewPaymentMethods – Allow or deny IAM users permission to view payment methods. ViewUsage – Allow or deny IAM users permission to view AWS usage reports. Use policies to grant permissions to perform an operation in AWS. When you use an action in a policy, you usually allow or deny access to the API operation or CLI command with the same name. However, in some cases, a single action controls access to more than one operation.
4
Resources Security best practices in IAM IAM Hands-On Lab IAM Workshops Security workshop tutorialsdojo digest Community posts https://dev.to/romankurnovskii/aws-iam-cheet-sheet-3if4 `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/iam/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/94-binary-tree-inorder-traversal/":{title:"94. Binary Tree Inorder Traversal",tags:["Stack","Tree","Depth-First Search","Binary Tree"],content:`##TODO
implement: create binary tree from list of values LeetCode problem
Given the root of a binary tree, return the inorder traversal of its nodes' values.
Example 1:
Input: root = [1,null,2,3] Output: [1,3,2] Example 2:
Input: root = [] Output: [] Example 3:
Input: root = [1] Output: [1] Thoughts Don\u0026rsquo;t understand what needed. Why:
1-null-2-3 becomes 1-3-2 [1,2,5,7,8,9,10] becomes [7,2,8,1,9,5,10] In 1-null-2-3 1 becomes the first because we loop to its left node which is null, then come back and first value here is 1.
First accepted 1# Definition for a binary tree node. 2# class TreeNode: 3# def __init__(self, val=0, left=None, right=None): 4# self.val = val 5# self.left = left 6# self.right = right 7class Solution: 8 def inorderTraversal(self, root: Optional[TreeNode]) -\u0026gt; List[int]: 9 10 # add all left, then add right 11 def get_child(head): 12 if head: 13 get_child(head.left) 14 result.append(head.val) 15 get_child(head.right) 16 17 18 result = [] 19 get_child(root) 20 return result Better solution Morris Traversal
Resources LeetCode explanation `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/94-binary-tree-inorder-traversal/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/2-add-two-numbers/":{title:"2. Add Two Numbers",tags:["Linked List","Math","Recursion"],content:` Before task: Create function for test data - code snippet You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.
You may assume the two numbers do not contain any leading zero, except the number 0 itself.
Example 1:
Input: l1 = [2,4,3], l2 = [5,6,4] Output: [7,0,8] Explanation: 342 + 465 = 807. First accepted Idea:
Loop through lists add each value to the list reverse list calculate sum create linked list from reversed sum 1class Solution: 2 def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: 3 4 def createLinkedNode(values): 5 head = ListNode(values[0]) 6 current = head 7 for i in values[1:]: 8 node = ListNode(i) 9 current.next = node 10 current = current.next 11 return head 12 13 14 res = None 15 16 vals_l1 = [] 17 cur = l1 18 while cur: 19 vals_l1.append(cur.val) 20 cur = cur.next 21 22 vals_l2 = [] 23 cur = l2 24 while cur: 25 vals_l2.append(cur.val) 26 cur = cur.next 27 28 s_l1 = \u0026#39;\u0026#39; 29 for i in reversed(vals_l1): 30 s_l1 += str(i) 31 32 s_l2 = \u0026#39;\u0026#39; 33 for i in reversed(vals_l2): 34 s_l2 += str(i) 35 36 ll_sum = int(s_l1) + int(s_l2) 37 values = [] 38 for val in reversed(str(ll_sum)): 39 values.append(int(val)) 40 41 res = createLinkedNode(values) 42 return res Better solution Idea:
Just like how you would sum two numbers on a piece of paper.
1class Solution: 2 def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -\u0026gt; Optional[ListNode]: 3 dummyHead = ListNode(0) 4 curr = dummyHead 5 carry = 0 6 while l1 != None or l2 != None or carry != 0: 7 l1Val = l1.val if l1 else 0 8 l2Val = l2.val if l2 else 0 9 columnSum = l1Val + l2Val + carry 10 carry = columnSum // 10 11 newNode = ListNode(columnSum % 10) 12 curr.next = newNode 13 curr = newNode 14 l1 = l1.next if l1 else None 15 l2 = l2.next if l2 else None 16 return dummyHead.next `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/2-add-two-numbers/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/3-longest-substring-without-repeating-characters/":{title:"3. Longest Substring Without Repeating Characters",tags:["Hash Table","String","Sliding Window"],content:`LeetCode problem
Given a string s, find the length of the longest substring without repeating characters.
Example 1:
Input: s = \u0026quot;abcabcbb\u0026quot; Output: 3 Explanation: The answer is \u0026quot;abc\u0026quot;, with the length of 3. First accepted Idea:
Loop through string Calculate max count of elements in substring If get double element, then go back until get this element and do step 2. Proceed the main loop 1class Solution: 2 def lengthOfLongestSubstring(self, s: str) -\u0026gt; int: 3 uniqs = set() 4 len_max = 0 5 len_current = 0 6 idx = 0 7 for i in s: 8 if i in uniqs: 9 len_max = max(len_max, len_current) 10 len_current = 1 11 uniqs = set(i) 12 for j in reversed(s[:idx]): 13 if j == i: 14 break 15 else: 16 len_current += 1 17 uniqs.add(j) 18 19 else: 20 uniqs.add(i) 21 len_current += 1 22 len_max = max(len_max, len_current) 23 idx += 1 24 return len_max Better solution Sliding Window - template
Window Sliding Technique is a computational technique which aims to reduce the use of nested loop and replace it with a single loop, thereby reducing the time complexity. The Sliding window technique can reduce the time complexity to O(n).
Tips for identifying this kind of problem where we could use the sliding window technique:
The problem will be based on an array, string, or list data structure. You need to find the subrange in this array or string that should provide the longest, shortest, or target values. A classic problem: to find the largest/smallest sum of given k (for example, three) consecutive numbers in an array.
1class Solution: 2 def lengthOfLongestSubstring(self, s: str) -\u0026gt; int: 3 n = len(s) 4 ans = 0 5 # mp stores the current index of a character 6 mp = {} 7 8 i = 0 9 # try to extend the range [i, j] 10 for j in range(n): 11 if s[j] in mp: 12 i = max(mp[s[j]], i) 13 14 ans = max(ans, j - i + 1) 15 mp[s[j]] = j + 1 16 17 return ans 1class Solution: 2 def lengthOfLongestSubstring(self, s: str) -\u0026gt; int: 3 chars = [None] * 128 4 left = right = 0 5 res = 0 6 while right \u0026lt; len(s): 7 r = s[right] 8 9 index = chars[ord(r)] 10 if index is not None and left \u0026lt;= index \u0026lt; right: 11 left = index + 1 12 13 res = max(res, right - left + 1) 14 15 chars[ord(r)] = right 16 right += 1 17 return res 1class Solution(): 2 def lengthOfLongestSubstring(self, s): 3 max_len = 0 4 substr = \u0026#39;\u0026#39; 5 for char in s: 6 if char not in substr: 7 substr += char 8 max_len = max(max_len, len(substr)) 9 else: 10 start = substr.index(char) + 1 11 substr = substr[start:] + char 12 return max_len Resources https://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2694302/JS-or-98-or-Sliding-window-or-With-exlanation https://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2133524/JavaC%2B%2B-A-reall-Detailed-Explanation `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/3-longest-substring-without-repeating-characters/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/5-longest-palindromic-substring/":{title:"5. Longest Palindromic Substring",tags:["String","Dynamic Programming"],content:`LeetCode problem
Given a string s, return the longest palindromic substring in s.
A string is called a palindrome string if the reverse of that string is the same as the original string.
Example 1:
Input: s = \u0026quot;babad\u0026quot; Output: \u0026quot;bab\u0026quot; Explanation: \u0026quot;aba\u0026quot; is also a valid answer. Example 2:
Input: s = \u0026quot;cbbd\u0026quot; Output: \u0026quot;bb\u0026quot; First accepted Hints How can we reuse a previously computed palindrome to compute a larger palindrome?
How can we reuse a previously computed palindrome to compute a larger palindrome?
Complexity based hint: If we use brute-force and check whether for every start and end position a substring is a palindrome we have O(n^2) start - end pairs and O(n) palindromic checks. Can we reduce the time for palindromic checks to O(1) by reusing some previous computation.
Idea:
We start at index = 0 and iterate through all values until n. At each index we call the function getPalindrome that will check the values to the left and right of the provided indices. It will continue to do so until the longest palindrome within the given range is found.
Link to diagram
1class Solution: 2 def longestPalindrome(self, s: str) -\u0026gt; str: 3 def getPalindrome(left, right): 4 while(left \u0026gt;= 0 and 5 right \u0026lt; len(s) and 6 s[left] == s[right]): 7 left -= 1 8 right += 1 9 10 return left+1, right-1 11 12 pal_left = 0 13 pal_right = 0 14 len_max = 1 15 16 for i in range(len(s)): 17 left, right = getPalindrome(i, i) 18 pal_len= right - left + 1 19 if pal_len \u0026gt; len_max: 20 pal_left = left 21 pal_right = right 22 len_max = pal_len 23 24 left, right = getPalindrome(i, i+1) 25 pal_len = right - left + 1 26 if pal_len \u0026gt; len_max: 27 pal_left = left 28 pal_right = right 29 len_max = pal_len 30 31 return s[pal_left:pal_right+1] Better solution Manacher\u0026rsquo;s algorithm There is an O(n) algorithm called Manacher\u0026rsquo;s algorithm.
1class Solution: 2 def longestPalindrome(self, s: str) -\u0026gt; str: 3 # @ and $ signs are sentinels appended to each end to avoid bounds checking 4 t = \u0026#39;#\u0026#39;.join(\u0026#39;@\u0026#39; + s + \u0026#39;$\u0026#39;) 5 n = len(t) 6 # t[i - maxExtends[i]..i) == 7 # t[i + 1..i + maxExtends[i]] 8 maxExtends = [0] * n 9 center = 0 10 11 for i in range(1, n - 1): 12 rightBoundary = center + maxExtends[center] 13 mirrorIndex = center - (i - center) 14 maxExtends[i] = rightBoundary \u0026gt; i and \\ 15 min(rightBoundary - i, maxExtends[mirrorIndex]) 16 17 # Attempt to expand palindrome centered at i 18 while t[i + 1 + maxExtends[i]] == t[i - 1 - maxExtends[i]]: 19 maxExtends[i] += 1 20 21 # If palindrome centered at i expand past rightBoundary, 22 # adjust center based on expanded palindrome. 23 if i + maxExtends[i] \u0026gt; rightBoundary: 24 center = i 25 26 # Find the maxExtend and bestCenter 27 maxExtend, bestCenter = max((extend, i) 28 for i, extend in enumerate(maxExtends)) 29 return s[(bestCenter - maxExtend) // 2:(bestCenter + maxExtend) // 2] Resources Manacher\u0026rsquo;s algorithm Errichto:Leetcode problem Longest Palindromic Substring (two solutions) https://redquark.org/leetcode/0005-longest-palindromic-substring/ RU
Разбор задачи с интервью. Литкод 5. Longest Palindromic Substring Алгоритмика: Алгоритм Манакера Википедия:Алгоритм Манакера `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/5-longest-palindromic-substring/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/7-reverse-integer/":{title:"7. Reverse Integer",tags:["Math"],content:`LeetCode problem
Given a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-231, 231 - 1], then return 0.
Assume the environment does not allow you to store 64-bit integers (signed or unsigned).
Example 1:
Input: x = 123 Output: 321
Example 2:
Input: x = -123 Output: -321
Example 3:
Input: x = 120 Output: 21
First accepted Idea:
Convert number to int Remove minus if exist (or convert module of number) reverse 1class Solution: 2 def reverse(self, x: int) -\u0026gt; int: 3 reversed_int = [] 4 str_int = str(x) 5 if x \u0026lt; 0: 6 str_int = str_int[1:] 7 for i in reversed(range(len(str_int))): 8 reversed_int.append(str_int[i]) 9 res = int(\u0026#39;\u0026#39;.join(reversed_int)) 10 if x \u0026lt; 0: 11 res = -res 12 return res if (res \u0026gt;= -2147483648 and res \u0026lt;= 2147483647) else 0 Better solution 1class Solution: 2 def reverse(self, x: int) -\u0026gt; int: 3 s = str(abs(x)) 4 rev = int(s[::-1]) 5 6 if rev \u0026gt; 2147483647: 7 return 0 8 9 return rev if x \u0026gt; 0 else (rev * -1) `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/7-reverse-integer/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/11-container-with-most-water.en./":{title:"11. Container With Most Water",tags:["Array","Two Pointers","Greedy"],content:`LeetCode problem
You are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).
Find two lines that together with the x-axis form a container, such that the container contains the most water.
Return the maximum amount of water a container can store.
Notice that you may not slant the container.
Example 1:
Input: height = [1,8,6,2,5,4,8,3,7] Output: 49 Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49. Example 2:
Input: height = [1,1] Output: 1 First accepted Idea:
Two Pointers Max water area is limited by the height of the shorter line Get most left pointer and most right Loop until left==right 1class Solution: 2 def maxArea(self, height: List[int]) -\u0026gt; int: 3 p1 = 0 # indexes 4 p2 = len(height) - 1 5 6 max_water = 0 7 while p1 \u0026lt; p2: 8 area = (p2 - p1) * min(height[p1], height[p2]) 9 max_water = max(area, max_water) 10 11 if height[p1] \u0026lt;= height[p2]: 12 p1 += 1 13 else: 14 p2 -= 1 15 return max_water `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/11-container-with-most-water.en./"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/15-3sum/":{title:"15. 3Sum",tags:["Array","Two Pointers","Sorting"],content:`LeetCode problem
Given an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.
Notice that the solution set must not contain duplicate triplets.
Example 1:
Input: nums = [-1,0,1,2,-1,-4] Output: [[-1,-1,2],[-1,0,1]] Explanation: nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0. nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0. nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0. The distinct triplets are [-1,0,1] and [-1,-1,2]. Notice that the order of the output and the order of the triplets does not matter. Example 2:
Input: nums = [0,1,1] Output: [] Explanation: The only possible triplet does not sum up to 0. First accepted Idea:
1class Solution: 2 def threeSum(self, nums: List[int]) -\u0026gt; List[List[int]]: 3 nums.sort() 4 5 x = 0 # index 6 ll = len(nums) 7 8 res = [] 9 while x \u0026lt; ll - 2: 10 if x == 0 or nums[x] != nums[x-1]: 11 y = x + 1 12 z = ll - 1 13 14 while y \u0026lt; z: 15 s = nums[x] + nums[y] + nums[z] 16 17 if s == 0: 18 res.append([nums[x], nums[y], nums[z]]) 19 while y \u0026lt; z and nums[y] == nums[y+1]: 20 y += 1 21 while z \u0026gt; y and nums[z] == nums[z-1]: 22 z -= 1 23 y += 1 24 z -= 1 25 elif s \u0026gt; 0: 26 z -= 1 27 else: 28 y += 1 29 x += 1 30 31 return res `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/15-3sum/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/17-letter-combinations-of-a-phone-number/":{title:"17. Letter Combinations of a Phone Number",tags:["Hash Table","String","Backtracking"],content:`LeetCode problem
Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.
A mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.
Example 1:
Input: digits = \u0026quot;23\u0026quot; Output: [\u0026quot;ad\u0026quot;,\u0026quot;ae\u0026quot;,\u0026quot;af\u0026quot;,\u0026quot;bd\u0026quot;,\u0026quot;be\u0026quot;,\u0026quot;bf\u0026quot;,\u0026quot;cd\u0026quot;,\u0026quot;ce\u0026quot;,\u0026quot;cf\u0026quot;] Example 2:
Input: digits = \u0026quot;\u0026quot; Output: [] First accepted 1class Solution: 2 def letterCombinations(self, digits: str) -\u0026gt; List[str]: 3 if not digits: 4 return [] 5 6 letters = [\u0026#39;\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;abc\u0026#39;, \u0026#39;def\u0026#39;, 7 \u0026#39;ghi\u0026#39;, \u0026#39;jkl\u0026#39;, \u0026#39;mno\u0026#39;, 8 \u0026#39;pqrs\u0026#39;, \u0026#39;tuv\u0026#39;, \u0026#39;wxyz\u0026#39;] 9 10 result = [\u0026#39;\u0026#39;] 11 12 for d in digits: 13 d = int(d) 14 tmp = [] 15 for letter in letters[d]: 16 for word in result: 17 word += letter 18 tmp.append(word) 19 result = tmp 20 21 return result `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/17-letter-combinations-of-a-phone-number/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/19-remove-nth-node-from-end-of-list/":{title:"19. Remove Nth Node From End of List",tags:["Medium","Linked List","Two Pointers"],content:`LeetCode problem
Given the \u0026lsquo;head\u0026rsquo; of a linked list, remove the \u0026rsquo;nth\u0026rsquo; node from the end of the list and return its head.
Example 1:
Input: head = [1,2,3,4,5], n = 2 Output: [1,2,3,5] Example 2:
Input: head = [1], n = 1 Output: [] First accepted Create Linked List
Idea:
Two pointers. Second pointer starts from nth position. Run while second pointer exist. First version:
1# Definition for singly-linked list. 2# class ListNode: 3# def __init__(self, val=0, next=None): 4# self.val = val 5# self.next = next 6class Solution: 7 def removeNthFromEnd(self, head, n: int): 8 p1 = head 9 p2 = head 10 11 for _ in range(n): 12 p1 = p1.next 13 14 if not p1: 15 return head.next # in case: head=[1], n=1 -\u0026gt; return [] 16 17 while p1.next: 18 p1 = p1.next 19 p2 = p2.next 20 21 p2.next = p2.next.next 22 23 return head `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/19-remove-nth-node-from-end-of-list/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/22-generate-parentheses/":{title:"22. Generate Parentheses",tags:["String","Dynamic Programming","Backtracking"],content:`LeetCode problem
Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.
Example 1:
Input: n = 3 Output: [\u0026quot;((()))\u0026quot;,\u0026quot;(()())\u0026quot;,\u0026quot;(())()\u0026quot;,\u0026quot;()(())\u0026quot;,\u0026quot;()()()\u0026quot;] Example 2:
Input: n = 1 Output: [\u0026quot;()\u0026quot;] Prerequirements Backtracking pattern
First accepted Idea:
1class Solution: 2 def generateParenthesis(self, n): 3 ans = [] 4 5 def dfs(l: int, r: int, s: str) -\u0026gt; None: 6 if l == 0 and r == 0: 7 ans.append(s) 8 if l \u0026gt; 0: 9 dfs(l - 1, r, s + \u0026#39;(\u0026#39;) 10 if l \u0026lt; r: 11 dfs(l, r - 1, s + \u0026#39;)\u0026#39;) 12 13 dfs(n, n, \u0026#39;\u0026#39;) 14 return ans `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/22-generate-parentheses/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ec2/":{title:"EC2",tags:["aws","ec2"],content:`About AWS EC2 AWS EC2 User Guide Amazon Elastic Compute Cloud (EC2) - one of the most popular AWS services.
Allows:
to run different types of cloud instances and pay-per-use models. to control computing resources at the operating system level working in an Amazon computing environment. Digest EC2 \u0026amp; EBS EC2 (Elastic Compute Cloud) Instance EBS (Elastic Block Store) - Persistent storage volume AMI (Amazon Machine Image) - Packages OS and additional installations in a reusable template Instance and Instance Types: General Purpose (t-type and m-type), Compute Optimized(c-type), GPU Graphics, GPU Compute, Memory Optimized(r, × and z-type), and Storage Optimized(d, h and i-type) Purchasing Options: On Demand, Reserved, Scheduled, Spot, Dedicated Instance and Dedicated Host Spot: Partial hours are not billed if terminated by AWS EC2 Secure login information for your instances using key pairs Placement group: Cluster and Spread For root:
General purpose SSD (balances price \u0026amp; performance) Provisioned OPS SD (Highest performance for mission critical low-latency or high throughput workloads) Magnetic HDD (previous generation) For other:
Throughput Provisioned HDD (low cost for frequently accessed, throughput intensive workloads) Cold HDD (lowest cost for less frequently workloads) Instance Store - temporary storage volume in which data is deleted when you STOP or TERMINATE your instance Price Pricing models:
On Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time. Reserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper. Spot - Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use. Current price
Practice TL;DR Choose a region close to you Go to EC2 service Click on \u0026ldquo;Instances\u0026rdquo; in the menu and click on \u0026ldquo;Launch instances\u0026rdquo; Choose image: Amazon Linux 2 Choose instance type: t2.micro Make sure \u0026ldquo;Delete on Termination\u0026rdquo; is checked in the storage section Under the \u0026ldquo;User data\u0026rdquo; field the following: 1yum update -y 2yum install -y httpd 3systemctl start httpd 4systemctl enable httpd 5echo \u0026#34;\u0026lt;h1\u0026gt;Hello from web!\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html Add tags with the following keys and values: key \u0026ldquo;Type\u0026rdquo; and the value \u0026ldquo;web\u0026rdquo; key \u0026ldquo;Name\u0026rdquo; and the value \u0026ldquo;web-1\u0026rdquo; In the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere Click on \u0026ldquo;Review\u0026rdquo; and then click on \u0026ldquo;Launch\u0026rdquo; after reviewing. If you don\u0026rsquo;t have a key pair, create one and download it. Now HTTP traffic (port 80) should be accepted from anywhere Create an EC2 Instance Go to EC2 page -\u0026gt; Launch Instance
EC2 image Choose the image we want Create keys Let\u0026rsquo;s create a key to use to connect to the instance externally
Enter any name you want. Leave all other parameters by default
After the key is created it will start automatic downloading. You need it to connect to EC2 from your local terminal
Network Settings Under Network Settings I leave Allow SSH traffic from
Create Click Launch Instance
The Instance has been created and is available for connection
Connecting to EC2 from the terminal Connect to EC2 from a local terminal
Let\u0026rsquo;s move previously created and downloaded mykey key to home folder of current user and give permissions to file CHMOD 400
1cd ~ 2cd Downloads/ 3mv mykey.pem $HOME 4cd .. 5chmod 400 mykey.pem To connect, we need a public iPv4 address. Find it on the instance page
Connect with the command ssh.
1ssh -i mykey.pem ec2-user@52.24.109.78 Questions Q1 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.
Which approach meets these requirements?
Store the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager
Secrets Manager offers secret rotation
4
Q2 An organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.
In this scenario, which of the following can be used to fulfill this requirement?
Directly attach multiple Instance Store volumes in an EC2 instance to deliver maximum IOPS performance. Launch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Launch an Amazon EFS file system and mount it to a Nitro-based Amazon EC2 instance and set the performance mode to Max I/O. Launch any type of Amazon EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS. Explanation An Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible.
The AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.
Amazon EBS is a persistent block storage volume. It can persist independently from the life of an instance. Since the scenario requires you to have an EBS volume with up to 64,000 IOPS, you have to launch a Nitro-based EC2 instance.
Amazon EBS volume types
2
Q3 A Database Specialist manages an EBS-Optimized Amazon RDS for MySQL DB instance with Provisioned IOPS storage. The users recently raised a database IO latency issue during peak hours when it was always under a heavy workload. Upon review, the Specialist noticed that the RDS DB instance was barely using the maximum IOPS configured but was fully utilizing the maximum bandwidth for the required throughput. CloudWatch metrics showed that CPU and Memory utilization were at optimum levels.
Which action should the Database Specialist take to fix the performance issue?
Change the underlying EBS storage type of the instance to General Purpose (SSD). Modify the DB instance to an EBS-Optimized instance class with higher maximum bandwidth. Disable EBS optimization on the MySQL DB instance to allow higher maximum bandwidth. Modify the DB instance to increase the size and corresponding Provisioned IOPS allocated to the storage. Explanation Amazon RDS volumes are built using Amazon EBS volumes, except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads. RDS currently supports both magnetic and SSD-based storage volume types. There are two supported Amazon EBS SSD-based storage types, Provisioned IOPS (called io1) and General Purpose (called gp2).
Provisioned IOPS storage is a storage type that delivers predictable performance and consistently low latency. If your workload is I/O constrained, using Provisioned IOPS SSD storage can increase the number of I/O requests that the system can process concurrently.
Provisioned IOPS SSD storage provides a way to reserve I/O capacity by specifying IOPS. However, as with any other system capacity attribute, its maximum throughput under load is constrained by the resource that is consumed first. That resource might be network bandwidth, CPU, memory, or database internal resources.
EBS–optimized instances deliver dedicated bandwidth to Amazon EBS. When attached to an EBS–optimized instance, Provisioned IOPS SSD (io1) volumes are designed to achieve their provisioned performance, 99.9% of the time. Choose an EBS–optimized instance that provides more dedicated Amazon EBS throughput than your application needs; otherwise, the connection between Amazon EBS and Amazon EC2 can become a performance bottleneck.
2
Q4 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.
How can the application find this information?
Query the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1
Q5 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.
Which actions should you take to make this function perform correctly? (2 answers)
Restart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html
2, 3
Resources EC2 Linux Hands-On Lab EB FAQ EC2 Digest EB Digest Community posts https://dev.to/romankurnovskii/aws-ec2-cheat-sheet-2mhp `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ec2/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/28-find-the-index-of-the-first-occurrence-in-a-string.en./":{title:"28. Find the Index of the First Occurrence in a String",tags:["String","Two pointers","String Matching"],content:`LeetCode problem
Given two strings needle and haystack, return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.
Example 1:
Input: haystack = \u0026ldquo;sadbutsad\u0026rdquo;, needle = \u0026ldquo;sad\u0026rdquo; Output: 0 Explanation: \u0026ldquo;sad\u0026rdquo; occurs at index 0 and 6. The first occurrence is at index 0, so we return 0.
Example 2:
Input: haystack = \u0026ldquo;leetcode\u0026rdquo;, needle = \u0026ldquo;leeto\u0026rdquo; Output: -1 Explanation: \u0026ldquo;leeto\u0026rdquo; did not occur in \u0026ldquo;leetcode\u0026rdquo;, so we return -1.
Code 1class Solution: 2 def strStr(self, haystack: str, needle: str) -\u0026gt; int: 3 return haystack.find(needle) 1class Solution: 2 def strStr(self, haystack: str, needle: str) -\u0026gt; int: 3 start = 0 4 end = len(needle) 5 6 while end \u0026lt;= len(haystack): 7 if haystack[start:end] == needle: 8 return start 9 start += 1 10 end += 1 11 12 return -1 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/28-find-the-index-of-the-first-occurrence-in-a-string.en./"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/29-divide-two-integers/":{title:"29. Divide Two Integers",tags:["Math","Bit Manipulation"],content:`LeetCode problem
Given two integers dividend and divisor, divide two integers without using multiplication, division, and mod operator.
The integer division should truncate toward zero, which means losing its fractional part. For example, 8.345 would be truncated to 8, and -2.7335 would be truncated to -2.
Return the quotient after dividing dividend by divisor.
Note: Assume we are dealing with an environment that could only store integers within the 32-bit signed integer range: [−231, 231 − 1]. For this problem, if the quotient is strictly greater than 231 - 1, then return 231 - 1, and if the quotient is strictly less than -231, then return -231.
Example 1:
Input: dividend = 10, divisor = 3 Output: 3 Explanation: 10/3 = 3.33333.. which is truncated to 3.
Example 2:
Input: dividend = 7, divisor = -3 Output: -2 Explanation: 7/-3 = -2.33333.. which is truncated to -2.
Code Idea:
Remove decimals from both divisor and divident Remember the result sign (positive or \u0026lt; 0) Subtract divisor from divident until result is less or equal to zero. Works but is too slow in case small number divisor (1) and greater number dividend (-2147483648):
1class Solution: 2 def divide(self, dividend: int, divisor: int) -\u0026gt; int: 3 res = 0 4 5 dd = abs(dividend) 6 ds = abs(divisor) 7 8 sign = -1 if (dividend \u0026gt; 0 and divisor \u0026lt; 9 0) or (dividend \u0026lt; 0 and divisor \u0026gt; 0) else 1 10 11 while dd \u0026gt;= ds: 12 dd -= ds 13 res += 1 14 15 return sign * res Improve idea:
Sum divisor after \u0026ldquo;success\u0026rdquo; subtract until result of subtract is \u0026gt; 0 Subtract divisor back until we can subtract it from dividend 1class Solution: 2 def divide(self, dividend: int, divisor: int) -\u0026gt; int: 3 res = 0 4 5 dd = abs(dividend) 6 ds = abs(divisor) 7 8 sign = -1 if (dividend \u0026gt; 0 and divisor \u0026lt; 9 0) or (dividend \u0026lt; 0 and divisor \u0026gt; 0) else 1 10 11 if divisor == -1 and dividend == -2147483648: 12 return 2147483647 13 elif divisor == 1: 14 return sign * dd 15 16 while dd \u0026gt;= ds: 17 tmp = ds 18 multiples = 1 # count of subtracts 19 while dd \u0026gt;= tmp: ## sum divisor 20 dd -= tmp 21 res += multiples # hense sum count of subtracts 22 23 tmp += tmp 24 multiples += multiples 25 else: 26 if dd \u0026gt;= ds: 27 dd -= ds 28 res += 1 29 30 return sign * res Better idea Idea: Bit manipulation
1class Solution: 2 def divide(self, dividend, divisor): 3 positive = (dividend \u0026lt; 0) is (divisor \u0026lt; 0) 4 dividend, divisor = abs(dividend), abs(divisor) 5 res = 0 6 while dividend \u0026gt;= divisor: 7 curr_divisor, num_divisors = divisor, 1 8 while dividend \u0026gt;= curr_divisor: 9 dividend -= curr_divisor 10 res += num_divisors 11 12 curr_divisor = curr_divisor \u0026lt;\u0026lt; 1 13 num_divisors = num_divisors \u0026lt;\u0026lt; 1 14 15 if not positive: 16 res = -res 17 18 return min(max(-2147483648, res), 2147483647) Explanation:
https://leetcode.com/problems/divide-two-integers/discuss/715094/Python-fast-code-with-detailed-explanation Another:
Time: $O(\\log^2 n)$ Space: $O(1)$
1class Solution: 2 def divide(self, dividend: int, divisor: int) -\u0026gt; int: 3 if dividend == -2**31 and divisor == -1: 4 return 2**31 - 1 5 6 sign = -1 if (dividend \u0026gt; 0) ^ (divisor \u0026gt; 0) else 1 7 ans = 0 8 dvd = abs(dividend) 9 dvs = abs(divisor) 10 11 while dvd \u0026gt;= dvs: 12 k = 1 13 while k * 2 * dvs \u0026lt;= dvd: 14 k \u0026lt;\u0026lt;= 1 15 dvd -= k * dvs 16 ans += k 17 18 return sign * ans `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/29-divide-two-integers/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/33-search-in-rotated-sorted-array/":{title:"33. Search in Rotated Sorted Array",tags:["Math","Bit Manipulation"],content:`LeetCode problem
There is an integer array nums sorted in ascending order (with distinct values).
Prior to being passed to your function, nums is possibly rotated at an unknown pivot index k (1 \u0026lt;= k \u0026lt; nums.length) such that the resulting array is [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]] (0-indexed). For example, [0,1,2,4,5,6,7] might be rotated at pivot index 3 and become [4,5,6,7,0,1,2].
Given the array nums after the possible rotation and an integer target, return the index of target if it is in nums, or -1 if it is not in nums.
You must write an algorithm with O(log n) runtime complexity.
Example 1:
Input: nums = [4,5,6,7,0,1,2], target = 0 Output: 4
Example 2:
Input: nums = [4,5,6,7,0,1,2], target = 3 Output: -1
Example 3:
Input: nums = [1], target = 0 Output: -1
Code Idea:
Values in the right part of the array are always lower than in the left part.
Use binary search Define where to move (left or right) 1class Solution: 2 def search(self, nums: List[int], target: int) -\u0026gt; int: 3 left = 0 4 right = len(nums) - 1 5 6 while left \u0026lt;= right: 7 mid = (left + right) // 2 8 if nums[mid] == target: 9 return mid 10 11 if nums[left] \u0026lt;= nums[mid]: 12 if nums[left] \u0026lt;= target \u0026lt; nums[mid]: 13 right = mid - 1 14 else: 15 left = mid + 1 16 else: 17 if nums[mid] \u0026lt; target \u0026lt;= nums[right]: 18 left = mid + 1 19 else: 20 right = mid - 1 21 22 return -1 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/33-search-in-rotated-sorted-array/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/34-find-first-and-last-position-of-element-in-sorted-array.en-copy/":{title:"34. Find First and Last Position of Element in Sorted Array",tags:["Array","Bit Manipulation"],content:`LeetCode problem
Given an array of integers nums sorted in non-decreasing order, find the starting and ending position of a given target value.
If target is not found in the array, return [-1, -1].
You must write an algorithm with O(log n) runtime complexity.
Example 1:
Input: nums = [5,7,7,8,8,10], target = 8 Output: [3,4]
Example 2:
Input: nums = [5,7,7,8,8,10], target = 6 Output: [-1,-1]
Example 3:
Input: nums = [], target = 0 Output: [-1,-1]
Code Idea:
Find target index (target_index) using Binary Search If not exist then return [-1, -1] If exist then goto step 2 We got the middle index. For now this is the most left and most right index. Divide nums into two arrays: left_nums and right_nums: left_nums = nums[0:target_index] right_nums = nums[target_index:] Find the most left target in left_nums. (Set right border in subarray) Find the most right target in right_nums. (Set left border in subarray) 1class Solution: 2 def searchRange(self, nums: List[int], target: int) -\u0026gt; List[int]: 3 4 def find_target(): 5 left = 0 6 right = len(nums) - 1 7 8 while left \u0026lt;= right: 9 mid = (left + right) // 2 10 if nums[mid] == target: 11 return mid 12 13 if nums[mid] \u0026lt; target: 14 left = mid + 1 15 else: 16 right = mid - 1 17 18 return -1 19 20 def find_most_left(right_idx): 21 l = 0 22 r = right_idx 23 24 while l \u0026lt;= r: 25 m = (l + r) // 2 26 if nums[m] \u0026lt; target: 27 l = m + 1 28 else: 29 r = m - 1 30 return l 31 32 def find_most_right(left_idx): 33 l = left_idx 34 r = len(nums) - 1 35 36 while l \u0026lt;= r: 37 m = (l + r) // 2 38 if nums[m] == target: # ex: [8, 8, 8, 9, 10] 39 l = l + 1 40 else: # ex: [8, 8, 8, 9, 10] 41 r = m - 1 42 return l - 1 43 44 target_idx = find_target() 45 if target_idx == -1: 46 return [-1,-1] 47 48 left = find_most_left(target_idx) 49 right = find_most_right(target_idx) 50 51 return [left, right] Code Ver2 Use prebuilt Python functions:
bisect_left bisect_right 1class Solution: 2 def searchRange(self, nums: List[int], target: int) -\u0026gt; List[int]: 3 l = bisect_left(nums, target) 4 if l == len(nums) or nums[l] != target: 5 return -1, -1 6 r = bisect_right(nums, target) - 1 7 return l, r `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/34-find-first-and-last-position-of-element-in-sorted-array.en-copy/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/36-valid-sudoku/":{title:"36. Valid Sudoku",tags:["Array","Hash Table","Matrix"],content:`LeetCode problem
Determine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:
Each row must contain the digits 1-9 without repetition. Each column must contain the digits 1-9 without repetition. Each of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.
Note:
A Sudoku board (partially filled) could be valid but is not necessarily solvable. Only the filled cells need to be validated according to the mentioned rules. Example 1:
Input: board = [[\u0026quot;5\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;7\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;5\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;3\u0026quot;] ,[\u0026quot;4\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;1\u0026quot;] ,[\u0026quot;7\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;5\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;7\u0026quot;,\u0026quot;9\u0026quot;]] Output: true Example 2:
Input: board = [[\u0026quot;8\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;7\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;5\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;3\u0026quot;] ,[\u0026quot;4\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;3\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;1\u0026quot;] ,[\u0026quot;7\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;6\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;2\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;4\u0026quot;,\u0026quot;1\u0026quot;,\u0026quot;9\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;5\u0026quot;] ,[\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;8\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;.\u0026quot;,\u0026quot;7\u0026quot;,\u0026quot;9\u0026quot;]] Output: false Explanation: Same as Example 1, except with the 5 in the top left corner being modified to 8. Since there are two 8's in the top left 3x3 sub-box, it is invalid. Code Idea:
1class Solution: 2 def isValidSudoku(self, board: List[List[str]]) -\u0026gt; bool: 3 exist = set() 4 for i in range(9): 5 for j in range(9): 6 x = board[i][j] 7 if x != \u0026#39;.\u0026#39;: 8 uniqs = ( 9 (i, x), 10 (x, j), 11 (int(i/3), int(j/3), x) ) # devide 3 because of third check in 3x3 block 12 for z in uniqs: 13 if z in exist: 14 return False 15 exist.add(z) 16 return True `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/36-valid-sudoku/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/38-count-and-say/":{title:"38. Count and Say",tags:["String"],content:`LeetCode problem
The count-and-say sequence is a sequence of digit strings defined by the recursive formula:
countAndSay(1) = \u0026quot;1\u0026quot; countAndSay(n) is the way you would \u0026ldquo;say\u0026rdquo; the digit string from countAndSay(n-1), which is then converted into a different digit string. To determine how you \u0026ldquo;say\u0026rdquo; a digit string, split it into the minimal number of substrings such that each substring contains exactly one unique digit. Then for each substring, say the number of digits, then say the digit. Finally, concatenate every said digit.
For example, the saying and conversion for digit string \u0026quot;3322251\u0026quot;: Given a positive integer n, return the nth term of the count-and-say sequence.
Example 1:
Input: n = 1 Output: \u0026quot;1\u0026quot; Explanation: This is the base case. Example 2:
Input: n = 4 Output: \u0026quot;1211\u0026quot; Explanation: countAndSay(1) = \u0026quot;1\u0026quot; countAndSay(2) = say \u0026quot;1\u0026quot; = one 1 = \u0026quot;11\u0026quot; countAndSay(3) = say \u0026quot;11\u0026quot; = two 1's = \u0026quot;21\u0026quot; countAndSay(4) = say \u0026quot;21\u0026quot; = one 2 + one 1 = \u0026quot;12\u0026quot; + \u0026quot;11\u0026quot; = \u0026quot;1211\u0026quot; Idea:
1class Solution: 2 def countAndSay(self, n: int) -\u0026gt; str: 3 res = \u0026#39;1\u0026#39; 4 while n \u0026gt; 1: 5 l = len(res) 6 new_str = \u0026#39;\u0026#39; 7 i = 0 8 while i \u0026lt; l: 9 count = 1 10 while i \u0026lt; l - 1 and res[i] == res[i+1]: 11 count += 1 12 i += 1 13 new_str += str(count) + res[i] 14 i += 1 15 res = new_str 16 n -= 1 17 18 return res `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/38-count-and-say/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/46-permutations/":{title:"46. Permutations",tags:["Array","Backtracking"],content:`LeetCode problem
Given an array nums of distinct integers, return all the possible permutations. You can return the answer in any order.
Example 1:
Input: nums = [1,2,3] Output: [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] Example 2:
Input: nums = [0,1] Output: [[0,1],[1,0]] Example 3:
Input: nums = [1] Output: [[1]] Idea:
Draw a decigion tree Fix when branch is ready to return Implementation:
Recursive: Go through every value in nums Pop value call perm() with updated nums from each call(step) append \u0026lsquo;poped\u0026rsquo; value from step 2 1class Solution: 2 def permute(self, nums: List[int]) -\u0026gt; List[List[int]]: 3 4 result_permutation = [] 5 6 if len(nums) == 1: # base case 7 return [nums[:]] 8 9 for _ in nums: 10 tmp_removed = nums.pop(0) # remove current element before next step 11 12 permutations = self.permute(nums) 13 14 for perm in permutations: 15 perm.append(tmp_removed) 16 17 nums.append(tmp_removed) 18 result_permutation.extend(permutations) 19 20 return result_permutation Resources https://www.youtube.com/watch?v=s7AvT7cGdSo https://walkccc.me/LeetCode/problems/0046/ `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/46-permutations/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/48-rotate-image/":{title:"48. Rotate Image",tags:["Array","Math","Matrix"],content:`LeetCode problem
You are given an n x n 2D matrix representing an image, rotate the image by 90 degrees (clockwise).
You have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation.
Example 1:
Input: matrix = [[1,2,3],[4,5,6],[7,8,9]] Output: [[7,4,1],[8,5,2],[9,6,3]] Example 2:
Input: matrix = [[5,1,9,11],[2,4,8,10],[13,3,6,7],[15,14,12,16]] Output: [[15,13,2,5],[14,3,4,1],[12,6,8,9],[16,7,10,11]] Idea:
1class Solution: 2 def rotate(self, matrix: List[List[int]]) -\u0026gt; None: 3 \u0026#34;\u0026#34;\u0026#34; 4 Do not return anything, modify matrix in-place instead. 5 \u0026#34;\u0026#34;\u0026#34; 6 l = 0 7 r = len(matrix) - 1 8 9 while l \u0026lt; r: 10 for i in range(r-l): # for not only \u0026#34;corners\u0026#34; 11 t = l 12 b = r 13 top_left = matrix[t][l + i] 14 matrix[t][l + i] = matrix[b - i][l] # top left=bottom left 15 matrix[b - i][l] = matrix[b][r - i] # bottom left=bottom right 16 matrix[b][r - i] = matrix[t+i][r] # bottom right=top right 17 matrix[t + i][r] = top_left # top right=top left 18 19 l += 1 20 r -= 1 Approach 2: Reverse:
1class Solution: 2 def rotate(self, matrix: List[List[int]]) -\u0026gt; None: 3 matrix.reverse() 4 5 for i in range(len(matrix)): 6 for j in range(i + 1, len(matrix)): 7 matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j] Resources https://www.youtube.com/watch?v=fMSJSS7eO1w https://walkccc.me/LeetCode/problems/0048/ `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/48-rotate-image/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/49-group-anagrams/":{title:"49. Group Anagrams",tags:["Array","Hash table","String","Sorting"],content:`LeetCode problem
Given an array of strings strs, group the anagrams together. You can return the answer in any order.
An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.
Example 1:
Input: strs = [\u0026quot;eat\u0026quot;,\u0026quot;tea\u0026quot;,\u0026quot;tan\u0026quot;,\u0026quot;ate\u0026quot;,\u0026quot;nat\u0026quot;,\u0026quot;bat\u0026quot;] Output: [[\u0026quot;bat\u0026quot;],[\u0026quot;nat\u0026quot;,\u0026quot;tan\u0026quot;],[\u0026quot;ate\u0026quot;,\u0026quot;eat\u0026quot;,\u0026quot;tea\u0026quot;]] Example 2:
Input: strs = [\u0026quot;\u0026quot;] Output: [[\u0026quot;\u0026quot;]] Example 3:
Input: strs = [\u0026quot;a\u0026quot;] Output: [[\u0026quot;a\u0026quot;]] Idea:
1class Solution: 2 def groupAnagrams(self, strs: List[str]) -\u0026gt; List[List[str]]: 3 dd = {} 4 for s in strs: 5 s_sort = \u0026#34;\u0026#34;.join(sorted(s)) 6 values = dd.get(s_sort, []) 7 values.append(s) 8 dd[s_sort] = values 9 return dd.values() Approach 2:
Intuition:
Two strings are anagrams if and only if their character counts (respective number of occurrences of each character) are the same.
Algorithm:
We can transform each string s into a character count, count\\text{count}count, consisting of 26 non-negative integers representing the number of a\u0026rsquo;s, b\u0026rsquo;s, z\u0026rsquo;s, etc. We use these counts as the basis for our hash map.
In python, the representation will be a tuple of the counts. For example, abbccc will be (1, 2, 3, 0, 0, ..., 0), where again there are 26 entries total.
1class Solution: 2 def groupAnagrams(strs): 3 ans = collections.defaultdict(list) 4 for s in strs: 5 count = [0] * 26 6 for c in s: 7 count[ord(c) - ord(\u0026#39;a\u0026#39;)] += 1 8 ans[tuple(count)].append(s) 9 return ans.values() Resources LeetCode expl `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/49-group-anagrams/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/50/":{title:"50. Pow(x, n)",tags:["Math","Recursion"],content:`LeetCode problem
Implement pow(x, n), which calculates x raised to the power n (i.e., x^n).
Example 1:
Input: x = 2.00000, n = 10 Output: 1024.00000 Example 2:
Input: x = 2.10000, n = 3 Output: 9.26100 Example 3:
Input: x = 2.00000, n = -2 Output: 0.25000 Explanation: 2-2 = 1/22 = 1/4 = 0.25 Approach 1:
1class Solution: 2 def myPow(self, x: float, n: int) -\u0026gt; float: 3 return x ** n Approach 2:
Recursive
1class Solution: 2 def myPow(self, x, n): 3 if not n: 4 return 1 5 if n \u0026lt; 0: 6 return 1 / self.myPow(x, -n) 7 if n % 2: 8 return x * self.myPow(x, n-1) 9 return self.myPow(x * x, n/2) Approach 3:
1class Solution: 2 def myPow(self, x, n): 3 if n \u0026lt; 0: 4 x = 1 / x 5 n = -n 6 pow = 1 7 while n: 8 if n \u0026amp; 1: 9 pow *= x 10 x *= x 11 n \u0026gt;\u0026gt;= 1 12 return pow `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/50/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticbeanstalk/":{title:"Elastic Beanstalk",tags:["aws","Elastic Beanstalk"],content:`About AWS Elastic Beanstalk AWS Elastic Beanstalk User Guide AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.
Deploying new application versions to existing resources in AWS Elastic Beanstalk happens much faster (typically under a minute) and once again is mostly dependent on the size of the new application version.
Digest When you want to use new run time capabilities with elastic bean stalk, it is better to use blue-green deployment Security group will not be removed when removing the stack with elastic bean stalk For long running tasks - Use Elastic Beanstalk worker environment to process the tasks asynchronously Launch configuration is used for modifying instance type, key pair, elastic block storage and other settings that can be configured only when launching the instance Rolling with Additional Batch and Immutable both involve provisioning new servers to ensure capacity is not reduced. All At Once means the application will be offline for the duration of the update. Performing a Rolling Update without an additional batch of servers means a reduction in capacity. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html For Blue green deployment - Use Elastic beanstalk swap URL feature or route 53 with weighted routing policies You create your own Elastic Beanstalk platform using Packer, which is an open-source tool for creating machine images for many platforms, including AMIs for use with Amazon Elastic Compute Cloud (Amazon EC2). Price There is no additional charge for AWS Elastic Beanstalk. Only the AWS resources required to store and run applications are charged.
Concepts AWS doc Applications An application is a collection of different elements, such as environments, environment configurations, and application versions.
You can have multiple application versions held within an application.
Application Version An application version is a very specific reference to a section of deployable code. The application version will point typically to simple storage service (S3) where the deployable code may reside.
Environment Configurations An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.
Environment An environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage the application is deployed as a solution and becomes operational within your environment.
The “environment” is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.
Environment Tier Reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment.
Configuration Template This is the template that provides the baseline for creating a new, unique, environment configuration.
Platform Culmination of components in which you can build your application upon using Elastic Beanstalk. These are comprised of the OS of the instance, the programming language, the server type (web or application), and components of Elastic Beanstalk
Deployment policies All at once – deploys the new version to all instances simultaneously and will be out of service for a short time. Rolling – deploys the new version in batches. Rolling with additional batch – deploys the new version in batches, but first launch a new batch of instances. Immutable – deploys the new version to a new set of instances. Traffic splitting – deploys the new version to a new set of instances and temporarily split incoming client traffic. Practice Controlled deployment with AWS Elastic Beanstalk Lab Controlled deployment with AWS Elastic Beanstalk
In this lab, we will deploy several application version updates in a load-balanced, auto-scaling environment.
The first update is deployed using a simple deployment. The second update is deployed using a \`blue-green\u0026rsquo; deployment, where a separate environment is created to run the new version of the application, and the DNS switch switches incoming traffic to the new environment.
The final deployment architecture will look like this
Loading the application In this review, I\u0026rsquo;m using the code that Cloudacademy provided me, but I have a ready-made launch script that you can download from Elastic Beanstalk: download
Create Go to Elastic Beanstalk page and click Create Application.
Set Name Specify a name for the new application Choose platform Under Platform choose the desired platform of the application. In our case - Node.js. Download source code Under Source code origin specify the version of the application and download the archive with the application. Example
Application Configuration Change the preset Configuration to Custom configuration:
Click Edit under Rolling updates and deployments
In the default configuration, updates are distributed to all instances at the same time. This leads to application downtime, which is unacceptable for production environments.
We will set Rolling and Batch size to 30%
Network Back in the main application form, click Edit in the Network configuration.
On the Modify network form, configure the following values, then Save. VPC: Select VPC with CIDR block 10.0.0.0/16. This will not be the default VPC. Load balancer settings: Load balancer subnets: Select subnets with CIDR blocks **10.0.100.0/24 **(us-west-2a)and 10.0.101.0/24 (us-west-2b). These are public subnets. The application load balancer requires at least two subnets in different availability zones Instance settings: * Instance subnets: Select a subnet with CIDR block 10.0.1.0/24. This is a private subnet.
Confirmation. Press Create app.
The app creation process takes from 5 minutes.
Then go to Dasboard This concludes the loading phase of the app in Elastic Beanstalk. Next, let\u0026rsquo;s break down how to switch the downloading of the new version of the application to the clients.
Downloading version 2 of the app Downloading version 2.0 Press Upload and deploy and download the updated code. For example, you can change the text in the same source code for comparison.
Specify new version and publication settings Version comparison Now we can compare both versions by following the links. In my case the applications look like this
Changing the url of the apps Now let\u0026rsquo;s swap the apps around. So that a user who previously went to one address will now see the 2nd version of the app.
Under Actions, click on Swap environment URLs and then select the app you want to swap
Removing Elastic Beanstalk resources Elastic Beanstalk runs EC2 instances as well as other services to deploy applications. But you can remove all services from a single window.
go to the Applications section Select an application.f Click on Actions -\u0026gt; Terminate environment Translated with www.DeepL.com/Translator (free version) Questions Q1 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.
What is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?
Use the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)
3
Q2 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.
Which of the following options would meet this requirement?
Use an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html
An Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.
2
Q3 An online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.
In Elastic Beanstalk, where does it store the application files and server log files?
Application files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk. Application files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs. Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs. Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs. Explanation AWS Elastic Beanstalk stores your application files and optionally, server log files in Amazon S3. If you are using the AWS Management Console, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account and the files you upload will be automatically copied from your local client to Amazon S3.
Optionally, you may configure Elastic Beanstalk to copy your server log files every hour to Amazon S3. You do this by editing the environment configuration settings.
With CloudWatch Logs, you can monitor and archive your Elastic Beanstalk application, system, and custom log files from Amazon EC2 instances of your environments. You can also configure alarms that make it easier for you to react to specific log stream events that your metric filters extract.
The CloudWatch Logs agent installed on each Amazon EC2 instance in your environment publishes metric data points to the CloudWatch service for each log group you configure.
Each log group applies its own filter patterns to determine what log stream events to send to CloudWatch as data points. Log streams that belong to the same log group share the same retention, monitoring, and access control settings. You can configure Elastic Beanstalk to automatically stream logs to the CloudWatch service.
The option that says: Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs is incorrect because the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to CloudTrail as this service is primarily used for auditing API calls.
4
Q4 A former colleague reached out to you for consultation. He uploads a Django project in Elastic Beanstalk through CLI using instructions he read in a blog post, but for some reason he could not create the environment he needs for his project. He encounters an error message saying “The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist.”
What are the possible causes of this issue? (Select TWO.)
He selected the wrong platform for the Django code. Elastic Beanstalk CLI did not create one because your IAM role has no permission to create roles. Instance profile container for the role needs to be manually replaced every time a new environment is launched. You have not associated an Elastic Beanstalk role to your CLI. IAM role already exists but has insufficient permissions that Elastic Beanstalk needs. Explanation AWS EB CLI cannot create the instance profile for your beanstalk environment if your IAM role has no access to creating roles.
This error is also thrown when the instance profile has insufficient or outdates policies that beanstalk needs to function. More details on this can be seen on the references provided.
2, 5
Resources https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html Tutorials and samples Community posts https://dev.to/romankurnovskii/todo-aws-aws-elastic-beanstalk-cheat-sheet-1718 https://dev.to/romankurnovskii/aws-elastic-beanstalk-top-questions-certified-developer-exam-478g `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticbeanstalk/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/53/":{title:"53. Maximum Subarray",tags:["Array","Divide and Conquer","Dynamic Programming"],content:`LeetCode problem
Given an integer array nums, find the subarray which has the largest sum and return its sum.
Example 1:
Input: nums = [-2,1,-3,4,-1,2,1,-5,4] Output: 6 Explanation: [4,-1,2,1] has the largest sum = 6. Example 2:
Input: nums = [1] Output: 1 Example 3:
Input: nums = [5,4,-1,7,8] Output: 23 Approach 1:
1class Solution: 2 def maxSubArray(self, nums: List[int]) -\u0026gt; int: 3 4 max_ = nums[0] 5 max2 = nums[0] 6 7 if len(nums) == 1: 8 return max_ 9 10 for i in range(1, len(nums)): 11 max_ = max(nums[i], nums[i] + max_) 12 max2 = max(max_, max2) 13 14 return max2 15 `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/53/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/55/":{title:"55. Jump Game",tags:["Array","Greedy","Dynamic Programming"],content:`LeetCode problem
You are given an integer array nums. You are initially positioned at the array\u0026rsquo;s first index, and each element in the array represents your maximum jump length at that position.
Return true if you can reach the last index, or false otherwise.
Example 1:
Input: nums = [2,3,1,1,4] Output: true Explanation: Jump 1 step from index 0 to 1, then 3 steps to the last index. Example 2:
Input: nums = [3,2,1,0,4] Output: false Explanation: You will always arrive at index 3 no matter what. Its maximum jump length is 0, which makes it impossible to reach the last index. Approach 1:
Idea: go forward on each step and mark next cell if can achieve it.
1class Solution: 2 def canJump(self, nums: List[int]) -\u0026gt; bool: 3 last_i = len(nums) 4 if last_i == 1: 5 return True 6 nn = [0] * last_i 7 nn[0] = nums[0] 8 for i in range(last_i): 9 el = nums[i] 10 if el or nn[i+1]: 11 for j in range(el): 12 nn[i+j+1] = el 13 if nn[last_i - 1]: 14 return True 15 else: 16 return False 17 return False Approach 2:
Going forwards. m tells the maximum index we can reach so far.
1class Solution: 2 def canJump(self, nums): 3 m = 0 4 for i, n in enumerate(nums): 5 if i \u0026gt; m: 6 return False 7 m = max(m, i + n) 8 return True 9 10class Solution: 11 def canJump(self, nums: List[int]) -\u0026gt; bool: 12 i = 0 13 m = 0 14 while i \u0026lt; len(nums) and i \u0026lt;= m: 15 m = max(m, i + nums[i]) 16 i += 1 17 return i == len(nums) `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/55/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/56/":{title:"56. Merge Intervals",tags:["Array","Sorting"],content:`LeetCode problem
Given an array of intervals where intervals[i] = [starti, endi], merge all overlapping intervals, and return an array of the non-overlapping intervals that cover all the intervals in the input.
Example 1:
Input: intervals = [[1,3],[2,6],[8,10],[15,18]] Output: [[1,6],[8,10],[15,18]] Explanation: Since intervals [1,3] and [2,6] overlap, merge them into [1,6]. Example 2:
Input: intervals = [[1,4],[4,5]] Output: [[1,5]] Explanation: Intervals [1,4] and [4,5] are considered overlapping. Approach 1:
1class Solution: 2 def merge(self, intervals: List[List[int]]) -\u0026gt; List[List[int]]: 3 intervals.sort() 4 res = [intervals[0]] 5 for ir in range(1, len(intervals)): 6 if intervals[ir][0] \u0026gt;= res[-1][0] and intervals[ir][0] \u0026lt;= res[-1][1]: # [1,3],[2,6] 7 res[-1][0] = min(intervals[ir][0], res[-1][0]) 8 res[-1][1] = max(intervals[ir][1], res[-1][1]) 9 elif res[-1][0] \u0026gt;= intervals[ir][0] and res[-1][0] \u0026lt;= intervals[ir][1]: # [1,3],[0,4] 10 res[-1][0] = min(intervals[ir][0], res[-1][0]) 11 res[-1][1] = max(intervals[ir][1], res[-1][1]) 12 else: 13 res.append(intervals[ir]) 14 return res Approach 2:
1class Solution: 2 def merge(self, intervals: List[List[int]]) -\u0026gt; List[List[int]]: 3 ans = [] 4 for interval in sorted(intervals): 5 if not ans or ans[-1][1] \u0026lt; interval[0]: 6 ans.append(interval) 7 else: 8 ans[-1][1] = max(ans[-1][1], interval[1]) 9 return ans `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/56/"},"https://romankurnovskii.com/en/docs/algorithms-101/problems/62/":{title:"62. Unique Paths",tags:["Math","Dynamic Programming","Combinatorics"],content:`LeetCode problem
There is a robot on an m x n grid. The robot is initially located at the top-left corner (i.e., grid[0][0]). The robot tries to move to the bottom-right corner (i.e., grid[m - 1][n - 1]). The robot can only move either down or right at any point in time.
Given the two integers m and n, return the number of possible unique paths that the robot can take to reach the bottom-right corner.
The test cases are generated so that the answer will be less than or equal to 2 * 10^9.
Example 1:
Input: m = 3, n = 7 Output: 28 Example 2:
Input: m = 3, n = 2 Output: 3 Explanation: From the top-left corner, there are a total of 3 ways to reach the bottom-right corner: 1. Right -\u0026gt; Down -\u0026gt; Down 2. Down -\u0026gt; Down -\u0026gt; Right 3. Down -\u0026gt; Right -\u0026gt; Down Approach 1:
LeetCode Submission
1class Solution: 2 def uniquePaths(self, m: int, n: int) -\u0026gt; int: 3 if m == 1 or n == 1: 4 return 1 5 matrix = [ [1 for j in range(n)] for i in range(m)] 6 7 for i in range(1, m): 8 for j in range(1, n): 9 max_above = 0 10 max_left = 1 11 if i \u0026gt; 0: 12 max_above = matrix[i-1][j] 13 if j \u0026gt; 0: 14 max_left = matrix[i][j-1] 15 matrix[i][j] = max_above + max_left 16 17 m = matrix[i][j] 18 return m 19 20class Solution: 21 def uniquePaths(self, m: int, n: int) -\u0026gt; int: 22 matrix = [[1] * n for _ in range(m)] 23 24 for i in range(1, m): 25 for j in range(1, n): 26 matrix[i][j] = matrix[i - 1][j] + matrix[i][j - 1] 27 28 return matrix[-1][-1] `,url:"https://romankurnovskii.com/en/docs/algorithms-101/problems/62/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/lambda/":{title:"Lambda",tags:["aws","lambda"],content:`About AWS Lambda AWS Lambda User Guide AWS Lambda is a serverless computing service that runs program code in response to certain events and is responsible for automatically allocating the necessary computing resources.
AWS Lambda automatically runs program code in response to various events, such as HTTP requests through Amazon API Gateway, changing objects in Amazon Simple Storage Service garbage cans (Amazon S3), updating tables in Amazon DynamoDB or changing states in AWS Step Functions.
Supports for Java, Go, PowerShell, Node.js, C#, Python and Ruby. It also provides a Runtime API which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function\u0026rsquo;s handler method when the function is invoked. You can include a runtime in your function\u0026rsquo;s deployment package in the form of an executable file named bootstrap
When you publish a version, AWS Lambda makes a snapshot copy of the Lambda function code (and configuration) in the $LATEST version. A published version is immutable.
Lambda execution role is an IAM role that grants the function permission to access AWS services and resources. Under Attach permissions policies, choose the AWS managed policies AWSLambdaBasicExecutionRole and AWSXRayDaemonWriteAccess.
AWS managed policies for Lambda features
Digest Types of lambda invocation RequestResponse. Event. Dryrun. Lambda execution context is a temporary runtime environment that initializes any external dependencies of our Lambda function code, such as database connections or HTTP endpoints Lambda Environment variables are variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration. Lambda concurrent executions = (invocations per second) x (average execution duration in seconds). Concurrency limit of lambda execution, Default 1000 Reserved - 900 unreserved 100. Will get throttled if it exceeds concurrency limit AWS_PROXY in API gateway is primarily used for Lambda proxy integration. A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. Lambda authorizer can be used for custom authorization scheme. 2 types: Token based. Request parameter based Lambda authorizer. Lambda deployment configuration: HalfAtATime Canary Linear. AWS Lambda compute platform deployments cannot use an in-place deployment type Increasing memory in lambda will increase CPU in lambda Lambda Versioning: By default, each AWS Lambda function has a single current version of the code. Clients of Lambda function can call a specific version or at the latest implementation Lambda Alias: You can create one or more aliases for our AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency Lambda Layer - Layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package Amazon DynamoDB is integrated with AWS Lambda so that you can trigger pieces of code that automatically respond to events in DynamoDB Streams. AWSLambdaDynamoDBExecutionRole is required to enable Lambda to work with DynamoDB API Gateway - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. Integrating Cloud Watch Events with lambda can be used for scheduling events If there is an incompatible output returned from a Lambda proxy integration backend, it will return 502 To resolve lambda throttled exception when using Cognito events, perform retry on sync. Lambda Event hook running order: start -\u0026gt; BeforeAllowTraffic -\u0026gt; AllowTraffic -\u0026gt; After AllowTraffic -\u0026gt; End AWS Lambda runs function code securely within a VPC b default. To enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS Lambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC Lambda Asynchronous invocation can be triggered by Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config. Lambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html Lambda provides 500 MB of additional disk space as a workspace. Lambda logs all stout for a lambda function to CloudWatch Logs. Any additional logging calls used in the function will also be sent to CloudWatch Logs. To connect to a VPC, lambda function execution role must have the following permissions: ec2:Create Networkinterface, ec2:DescribeNetworkinterfaces, ec2:Delete Networkinterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy When lambda execution is hit by concurrency limit, you need to request AWS to increase concurrency limit For stream-based services like Dynamo b streams, that don\u0026rsquo;t invoke Lambda functions directly, the event source mapping configuration should be made on the Lambda side. A deployment package is a ZIP archive that contains your function code and dependencies. You can unload the package directly to lambda. Or you can use an Amazon S3 bucket and then upload it to lambda. If the deployment package is larger than 50 MB. you must use Amazon 53 Lambda can incur a first run penalty also called cold starts. Cold starts can cause slower than expected behavior on infrequently run functions or functions with high concurrency demands Price Price
Price x86
0.000016667 USD per gigabyte-second 0,20 USD per 1 million requests Arm price
0,0000133334 USD for each gigabyte-second 0,20 USD for 1 million queries Practice In the AWS Management Console search bar, type Lambda and select Lambda under \u0026ldquo;Services\u0026rdquo;:
https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#
On page Functions click Create a function
Author from scratch is selected and enter the following values in the bottom form:
Function name: *MyCustomFunc Runtime: Node.js 16.X I select this section because I use the cloudacademy account. This role gives you permission to create functions
Permissions: Change default execution role. Execution Role: Select Use an existing role. Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole Create function I\u0026rsquo;m writing a function to view the log, I\u0026rsquo;ll add a print to the terminal. And I\u0026rsquo;ll also add processing of the message I receive (In the next step in the testing section)
The function takes as an object event which contains an array of Records. On the 1st (0) position the object Sns (name of the service SNS Notifications).
In the object itself there will be 2 values:
cook_secs - cooking time (microwave) req_secs - cooking time (prepare) 1console.log(\u0026#39;Loading function\u0026#39;); 2exports.handler = function(event, context) { 3 console.log(JSON.stringify(event, null, 2)); 4 const message = JSON.parse(event.Records[0].Sns.Message); 5 if (message.cook_secs \u0026lt; message.req_secs) { 6 if (message.pre) { 7 context.succeed(\u0026#34;User ended \u0026#34; + message.pre + \u0026#34; preset early\u0026#34;); 8 } 9 else { 10 context.succeed(\u0026#34;User ended custom cook time early\u0026#34;); 11 } 12 } 13 context.succeed(); 14}; Deploy Test This functionality allows you to test how the function reacts to certain events. Let\u0026rsquo;s try to add an event from SNS Notifications.
Let\u0026rsquo;s choose from the list
We get a template in which we make some changes, adjust the field Message - the one that we will process in our function.
Field Message - string, so our object will need to be wrapped in quotes
To make the handler understand that we put quotation marks inside quotation marks, we must put a special symbol \\ before the quotation mark.
Finally we update one line and save it - Create
Now we click the Test button.
Since cook_secs in our event was less than req_secs, the function printed the first condition, and below in Function Logs we see the message that we print when we initialize the Loading function
Questions Q1 When working with a published version of the AWS Lambda function, you should note that the _____.
Use the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager
C
Q2 A developer is building a streamlined development process for Lambda functions related to S3 storage.The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.
Which of the following AWS solutions offers is best suited for this objective?
AWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model
2
Q3 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events
Which combination of actions should the developer take to satisfy these requirements? (Select TWO.)
Use Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.
1, 5
Q4 A developer is designing a web application that allows the users to post comments and receive a real-time feedback.
Which architectures meet these requirements? (Select TWO.)
Create an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.
AWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.
The WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.
1, 2
Q5 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.
Which approach best meets these requirements?
Use EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html
3
Resources Community posts `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/lambda/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/":{title:"S3",tags:["aws","s3"],content:`About Amazon S3 (Simple Storage Service) provides object storage through a web service interface.
Amazon S3 Amazon S3 User Guide Price Pay only for what you use. There is no minimum charge.
Price
S3 | EFS | EBS Amazon S3 is an object storage designed for storing large numbers of user files and backups. Good for storing backups and other static data Can be publicly accessible Web interface Object Storage Scalable Slower than EBS and EFS Amazon EBS (Amazon Elastic Block Store) is block storage for Amazon EC2 compute instances - it is similar to hard drives attached to your computers or laptops, but in a virtualized environment. Is meant to be EC2 drive Accessible only via the given EC2 Machine File System interface Block Storage Hardly scalable Faster than S3 and EFS Amazon EFS (Amazon Elastic File System) provides scalable network file storage for Amazon EC2 cloud computing service users. Good for applications and shareable workloads Accessible via several EC2 machines and AWS services Web and file system interface Object storage Scalable Faster than S3, slower than EBS Features Amazon S3 allows people to store objects (files) in “buckets” (directories) Buckets must have a globally unique name Naming convention: No uppercase No underscore 3-63 characters long Not an IP Must start with lowercase letter or number Objects Objects (files) have a Key. The key is the FULL path: \u0026lt;my_bucket\u0026gt;/my_file.txt \u0026lt;my_bucket\u0026gt;/my_folder/another_folder/my_file.txt There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise) Just keys with very long names that contain slashes (“/“) Object Values are the content of the body: Max Size is 5TB If uploading more than 5GB, must use “multi-part upload” Metadata (list of text key / value pairs - system or user metadata) Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle Version ID (if versioning Versioning It is enabled at the bucket level Same key overwrite will increment the “version”: 1, 2, 3 It is best practice to version your buckets Protect against unintended deletes (ability to restore a version) Easy roll back to previous versions Any file that is not version prior to enabling versioning will have the version “null” Encryption for Objects There are 4 methods of encrypt objects in S3 SSE-S3: encrypts S3 objects Encryption using keys handled \u0026amp; managed by AWS S3 Object is encrypted server side AES-256 encryption type Must set header: “x-amz-server-side-encryption”:”AES256” SSE-KMS: encryption using keys handled \u0026amp; managed by KMS KMS Advantages: user control + audit trail Object is encrypted server side Maintain control of the rotation policy for the encryption keys Must set header: “x-amz-server-side-encryption”:”aws:kms” SSE-C: server-side encryption using data keys fully managed by the customer outside of AWS Amazon S3 does not store the encryption key you provide HTTPS must be used Encryption key must be provided in HTTP headers, for every HTTP request made Client Side Encryption Client library such as the amazon S3 Encryption Client Clients must encrypt data themselves before sending to S3 Clients must decrypt data themselves when retrieving from S3 Customer fully manages the keys and encryption cycle Encryption in transit (SSL) exposes: HTTP endpoint: non encrypted HTTPS endpoint: encryption in flight You’re free to use the endpoint your ant, but HTTPS is recommended HTTPS is mandatory for SSE-C Encryption in flight is also called SSL / TLS Security By default, all S3 objects are private
A user who does not have AWS credentials or permission to access an S3 object can be granted temporary access by using a presigned URL. A presigned URL is generated by an AWS user who has access to the object. The generated URL is then given to the unauthorized user
User based IAM policies - which API calls should be allowed for a specific user from IAM console Resource based Bucket policies - bucket wide rules from the S3 console - allows cross account Object Access Control List (ACL) - finer grain Bucket Access Control List (ACL) - less common Networking Support VPC endpoints (for instances in VPC without www internet) Logging and Audit: S3 access logs can be stored in other S3 buckets API calls can be logged in AWS CloudTrail User Security: MFA (multi factor authentication) can be required in versioned buckets to delete objects Signed URLs: URLS that are valid only for a limited time (ex: premium video services for logged in users) Bucket Policies JSON based policies Resources: buckets and objects Actions: Set of API to Allow or Deny Effect: Allow / Deny Principal: The account or user to apply the policy to Use S3 bucket for policy to: Grant public access to the bucket Force objects to be encrypted at upload Grant access to another account (Cross Account) Websites S3 can host static website sand have them accessible on the world wide web The website URL will be: \u0026lt;bucket-name\u0026gt;.s3-website.\u0026lt;AWS-region\u0026gt;.amzonaws.com OR \u0026lt;bucket-name\u0026gt;.s3-website.\u0026lt;AWS-region\u0026gt;.amazonaws.com If you get a 403 (forbidden) error, make sure the bucket policy allows public reads! CORS If you request data from another S3 bucket, you need to enable CORS Cross Origin Resource Sharing allows you to limit the number of websites that can request your files in S3 (and limit your costs) This is a popular exam question Consistency Model Read after write consistency for PUTS of new objects As soon as an object is written, we can retrieve itex: (PUT 200 -\u0026gt; GET 200) This is true, except if we did a GET before to see if the object existedex: (GET 404 -\u0026gt; PUT 200 -\u0026gt; GET 404) - eventually consistent Eventual Consistency for DELETES and PUTS of existing objects If we read an object after updating, we might get the older versionex: (PUT 200 -\u0026gt; PUT 200 -\u0026gt; GET 200 (might be older version)) If we delete an object, we might still be able to retrieve it for a short timeex: (DELETE 200 -\u0026gt; GET 200) Performance Faster upload of large objects (\u0026gt;5GB), use multipart upload Parallelizes PUTs for greater throughput Maximize your network bandwidth Decrease time to retry in case a part fails Use CloudFront to ache S3 objects around the world (improves reads) S3 Transfer Acceleration (uses edge locations) - just need to change the endpoint you write to, not the code If using SSE-KMS encryption, you may be limited to your AWS limits for KMS usage (~100s - 1000s downloads / uploads per second) Questions Q1 Developer wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.
Which methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)
Control access based on the time of day Control access based on IP Address Control access based on Active Directory group Control access based on CIDR block Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html CIDRs - A set of Classless Inter-Domain Routings
https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html
1, 2, 4
Q2 To ensure that an encryption key was not corrupted in transit, Elastic Transcoder uses a(n) ____ digest of the decryption key as a checksum.
BLAKE2 SHA-1 SHA-2 MD5 Explanation https://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html
MD5 digest (or checksum)
4
Q3 Dan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.
What are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)
Use the encrypted SSL/TLS endpoint. Enable encryption in the bucket policy. Encrypt it on the client-side before uploading. Set the server-side encryption option on upload. Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html
1, 3
Q4 A company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.
What is causing the duplicate entries?
The S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket. The Lambda function did not run correctly, and Lambda retried the invocation with a delay. Amazon S3 is delivering the same event multiple times. The application stopped intermittently and then resumed, splitting the logs into multiple smaller files. Explanation 1
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/s3/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/dynamodb/":{title:"DynamoDB",tags:["aws","dynamodb"],content:`About Fast and flexible NoSQL database service for performance with millisecond latency at any scale
Amazon DynamoDB documentation Developer Guide Pricing Amazon DynamoDB pricing
When you use the DynamoDB service, you are charged for reading, writing, and storing data in DynamoDB tables, as well as any additional features you enable. DynamoDB supports two resource provisioning modes that correspond to specific billing schemes for processing read and write operations on your tables: on-demand and with preparation. Click the following links to learn more about the billing options for each provisioning mode
Use Cases Type: Key-value
Ecommerce Websites, gaming websites etc.
Digest Global tables are useful for having multiple copies of tables in different region. All DynamoDB tables are encrypted at rest using an AWS owned CMK by default. Items - in DynamoDB is similar in many ways to rows, records, or tuples in other database systems. Each DynamoDB table contains zero or more items. An item is a collection of attributes that is uniquely identifiable for each record in that table. Attributes - Each item is composed of one or more attributes. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems. Each item in the table has a unique identifier, a primary key, or a partition key that distinguishes the item from all of the others in the table. The primary key consists of one attribute. Primary key Partition key Partition key and sort key (range attribute) A primary key can either be a sinale-attribute partition key or a composite partition-sort key. Both partition and sort keys attributes must be defined as type string, number, or binary. Global secondary index - a partition key and a sort key that can be different from those on the base table; query at table level across all partitions; eventual consistency: Different partition key and sort key from base table Only eventually consistent Can be created after table is created Using a random prefix for the GSI partition key enables to have high cardinality for the partition key Local secondary index - same partition key as the base table, but a different sort key: query on a single partition; eventual or strong consistency: Same partition key, different sort key from base table Eventual and strongly consistent Should be created when creating a table Calculate RCU (read capacity unit) \u0026amp; WCU (write capacity unit): 1 RCU = 2 eventual consistent read of 4 KB, 1 strongly consistent read of 4 KB 1 WCU = 1 write per second for data for an item as large as 1 KB. DynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables. The data about these events appears in the stream in near real time and in the order that the events occurred. Queries or scan on GSI consume RCU on index not on table Consistency: Auto scaling Storing session state could be on elastic cache or dynamodb Provisioned throughput - ProvisionedThroughputExceededException Reserved capacity, On-demand, Burst. Adaptive On-demand backups, point-in-time recovery Best practices when using Scan in dynamodb - Use parallel scan to control the amount of data returned per request use the Limit parameter. This can help prevent situations where one worker consumes all the provisioned throuahput at the expense of all other workers DynamoDB does not support item locking, and conditional writes are perfect for implementing optimistic concurrency. DynamoDB vs Aurora Amazon DynamoDB Amazon Aurora Was developed by Amazon in 2012 Was developed by Amazon in 2015. It is hosted, scalable database service by Amazon with data stored in Amazon cloud It is MySQL and PostgreSQL compatible cloud service by Amazon It does not provide concept of Referential Integrity. Hence, no Foreign Keys It provides concept of Referential Integrity. Hence, no Foreign Keys Eventual Consistency and Immediate Consistency are used to ensure consistency in distributed system Immediate Consistency is used to ensure consistency in distributed system Its Primary database models are Document store and Key-value store Its Primary database model is Relational DBMS It does not support Server-side scripting It supports Server-side scripting It supports sharding as partitioning method Partitioning can be done with horizontal partitioning It does not support SQL query language It supports SQL query language It supports replication methods It supports only one replication method – Master-slave replication It does not offer API for user-defined Map/Reduce methods. But maybe implemented via Amazon Elastic MapReduce It does not offer API for user-defined Map/Reduce methods DynamoDB supports different consistency models when performing reads:
Eventually, consistent reads may not always reflect the latest data if there was recently write activity on the table. Since the data in this scenario rarely changes, eventually consistent reads, which are cheaper than strongly consistent reads, can be tolerated. Practice Introduction to DynamoDB
Resources AWS Database Blog
Questions Q1 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.
Which architectures meet these requirements? (Select TWO.)
Create an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.
AWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.
The WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.
1, 2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/dynamodb/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticache/":{title:"ElastiCache",tags:["aws","elasticache"],content:`About Documentation User Guide Amazon Elasticache is a fully managed Redis or Memcached in-memory data store.
It\u0026rsquo;s great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.
You can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don\u0026rsquo;t require durability like session stores, gaming leaderboards, streaming, and analytics.
Compatible with Redis and Memcached
Price Current price
Use Cases Type: In-memory
Use Case Benefit Web session store In cases with load-balanced web servers, store web session information in Redis so if a server is lost, the session info is not lost, and another web server can pick it up Database caching Use Memcached in front of AWS RDS to cache popular queries to offload work from RDS and return results faster to users Leaderboards Use Redis to provide a live leaderboard for millions of users of your mobile app Streaming data dashboards Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays Caching Engines Memcached Redis Simple, no-frills You need encryption You need to elasticity (scale out and in) You need HIPAA compliance You need to run multiple CPU cores and threads Support for clustering You need to cache objects (e.g. database queries) You need complex data types You need HA (replication Backup and restore features Pub/Sub capability Multi-AZ with Auto-Failover Non persistent. No backups Multi-node for partitioning of data (sharding) Memcached ElastiCache manages Memcached nodes as a pool that can grow and shrink (similar to an EC2 Auto Scaling group); individual nodes are expendable and non-persistent.
Memcached provides a simple caching solution that best supports object caching and lets you scale out horizontally. Ideal for offloading a DB\u0026rsquo;s contents into a cache.
Redis ElastiCache manages Redis more as a relational database, i.e. Redis clusters are managed as persistent, stateful entities that include using multi-AZ redundancy for handling failover (similar to RDS).
Redis supports complex data structures, hence would be ideal in cases where sorting and ranking datasets in memory are important (e.g. such as in leaderboards for games).
Caching Strategies Lazy Loading The data that is read from the DB is stored in the cache. The data can become stale The data becomes stale because there are no updates to the cache when data is changed in the database Only cache data when it is requested. Cache miss penalty on initial request. Chance to produce stale data; can be mitigated by setting a TTL. Shorter TTL = less stale data.
Write-Through The data is added/updated into the cache everytime the data is written to the DB (no stale data) Because the data in the cache is updated every time it\u0026rsquo;s written to the database, the data in the cache is always current. Every database write will write to the cache as well. Data is never stale however there will be alot more operations to perform; and these resources are wasted if most of the data is never used.
Session Store Stores temporary session data in cache (with TTL) - Time to Live. Data expires after the given time Practice Configuring a Lambda function to access Amazon ElastiCache in an Amazon VPC
Questions Q1 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?
The metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to Elasticache is required for test or development purposes, it can be done through a VPN.
2
Q2 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.
What is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?
Use the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)
3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticache/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/rds/":{title:"RDS",tags:["aws","rds"],content:`About Relational Database Service Managed DB service that uses SQL as query language Amazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.
Documentation User Guide Supports engines:
Amazon Aurora with MySQL compatibility: 5432 Amazon Aurora with PostgreSQL compatibility: 5432 MySQL: 3306 MariaDB: 3306 PostgreSQL: 5432 Oracle: 1521 SQL Server: 1433 Engine modes:
Used in CreateDBCluster
global parallelquery serverless multimaster Backups Backups are enabled by default in RDS Automated backups
Daily full backup (during maintenance window) Backups of transaction logs (every 5 minutes) 7 days retention (can increase upto 35) DB Snapshots
Manually triggered by the user Can retain backup as long as you want Auto scaling When RDS detects you\u0026rsquo;re running out of space, it scales automatically Digest To verify slowly running queries enable slow query log. TDE (Transparent data encryption) supports encryption on Microsoft SQL server AWS Systems Manager Parameter Store provides secure, hierarchical storage for confiquration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values AWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases. Price Current price
Use Cases Type: Relational
This type services: Aurora, Redshift, RDS
Ecommerce websites, Traditional sites etc.
Amazon Relational Database Service (Amazon RDS) on [AWS Outposts](AWS Outposts) allows you to deploy fully managed database instances in your on-premises environment
Questions Q1 Explain RDS Multi Availability Zone
Explanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically Q2 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.
Which approach meets these requirements?
Store the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Explanation AWS Secrets Manager
Secrets Manager offers secret rotation
4
Q3 Explain RDS Multi Availability Zone
Explanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/rds/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/api-gateway/":{title:"API Gateway",tags:["aws","API Gateway"],content:`About Documentation User Guide API Gateway provides the opportunity to create and expand your own REST and WebSocket APIs at any size.
API endpoints can be cached to accommodate for frequent similar requests.
Use Cases Build a network for micros­ervices archit­ectures.
Amazon CloudWatch metrics - Collects near-real-time metrics Examples: 4XXError (client-side errors), 5XXError(server-side errors), CacheHitCount Amazon CloudWatch Logs - Debug issues related to request execution AWS CloudTrail - Record of actions taken by a user, role, or an AWS service in API Gateway AWS X-Ray - Trace your request across different AWS Services Digests Concepts REST API, HTTP API, WebSocket API
Deployment - point-in-time snapshot of your API Gateway API
Endpoint - https://api-id.execute-api.region-id.amazonaws.com
Edge-optimized Private Regional Stage - A logical reference to a lifecycle state of your API. Route - URL path, Latency based routing, Integration - Lambda, HTTP, Private VPC, CORS Import/Export - Open API AM User should have permission to enable logging Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale.
Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. The act like environment variables and can be used in your API setup and mapping templates.
With deployment stages in API Gateway you can manage multiple release stages for each API, such as: alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.
When you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through our API, you must manually add these permissions.
Integration timeout for AWS, Lambda, Lambda proxy, HTTP, HTTP proxy - 50 ms to 29 seconds
You can enable API caching to cache your endpoint\u0026rsquo;s responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API
AWS Gateway Integration types:
AWS_ Proxy - lambda proxy integration HTTP - http custom integration HTTP_PROXY - http proxy Practice Creating a RESTful API Using Amazon API Gateway Questions Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.
A) HTTP proxy B) mock C) AWS service proxy D) Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html
Amazon API Gateway supports mock integrations for API methods.
B
Q2 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.
Which architectures meet these requirements? (Select TWO.)
Create an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.
AWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.
The WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.
1, 2
Q3 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers
Which AWS service should be used to refactor this architecture?
AWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4
Q4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.
How should Veronika convert the query string parameters to arguments for the Lambda function?
Enable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template
4
Q5 A developer is designing a full-stack serverless application. Files for the website are stored in an Amazon S3 bucket. AWS Lambda functions that use Amazon API Gateway endpoints return results from an Amazon DynamoDB table. The developer must create a solution that securely provides registration and authentication for the application while minimizing the amount of configuration.
Which solution meets these requirements?
Create an Amazon Cognito user pool and an app client. Configure the app client to use the user pool and provide the hosted web UI provided for sign-up and sign-in. Configure an Amazon Cognito identity pool. Map the users with IAM roles that are configured to access the S3 bucket that stores the website. Configure and launch an Amazon EC2 instance to set up an identity provider with an Amazon Cognito user pool. Configure the user pool to provide the hosted web UI for sign-up and sign-in. Create an IAM policy that allows access to the website that is stored in the S3 bucket. Attach the policy to an IAM group. Add IAM users to the group. Explanation 2
Q6 A company has moved a legacy on-premises application to AWS by performing a lift and shift. The application exposes a REST API that can be used to retrieve billing information. The application is running on a single Amazon EC2 instance. The application code cannot support concurrent invocations. Many clients access the API, and the company adds new clients all the time.
A developer is concerned that the application might become overwhelmed by too many requests. The developer needs to limit the number of requests to the API for all current and future clients. The developer must not change the API, the application, or the client code.
What should the developer do to meet these requirements?
Place the API behind an Amazon API Gateway API. Set the server-side throttling limits. Place the API behind a Network Load Balancer. Set the target group throttling limits. Place the API behind an Application Load Balancer. Set the target group throttling limits. Place the API behind an Amazon API Gateway API. Set the per-client throttling limits. Explanation 4
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/api-gateway/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudfront/":{title:"CloudFront",tags:["aws","CloudFront"],content:`About Securely deliver data, videos, applic­ations, and APIs to customers globally with low latency, and high transfer speeds
CloudFront is a distributed content delivery network (CDN) that enables easy delivery of web content to end users from a pool of web servers around the globe
Documentation User Guide CloudFront is a global service:
Ingress to upload objects. Egress to distribute content. Terminology Edge Location: The location where content is cached to be accessed by users. These are READ/WRITE. CDN: A collection of Edge Locations that can distribute content around the world. Origin: The origin of all files the CDN will distribute. E.g. an S3 bucket hosting some images, or hosting a static website an EC2 instance running a website with dynamic content an ELB pointing to several EC2 instances a DNS endpoint using Route53 any origin server, even non-AWS Distribution: The name of the CDN. Web Distribution: Used for delivering content over HTTP/HTTPS. Can be either an S3 bucket or a web server (EC2/non-AWS). Cannot serve multimedia content. RTMP Distribution: Uses RTMP for media streaming and flash multimedia content. Probably what Netflix uses. Price Current price
There is an option for reserved capacity over 12 months or longer (starts at 10TB of data transfer in a single region).
Pay do not pay Data Transfer Out to Internet Data transfer between AWS regions and CloudFront. Data Transfer Out to Origin Regional edge cache. Number of HTTP/HTTPS Requests AWS ACM SSL/TLS certificates. Invalidation Requests Shared CloudFront certificates. Dedicated IP Custom SSL Field level encryption requests Use Cases Type: Content delivery networks
Practice Configuring a Static Website With S3 And CloudFront
Questions Q1 A company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.
Which API command can a developer use to ensure cache storage consistency with minimal duplication?
A) CreateReusableDelegationSet with Route 53 B) CreateStackSet with CloudFormation C) CreateGlobalReplicationGroup with ElastiCache D) CreateCachePolicy with CloudFront Explanation https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html
D
Q2 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.
Which architectures meet these requirements? (Select TWO.)
Create an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.
AWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.
The WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.
1, 2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudfront/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticloadbalancing/":{title:"Elastic Load Balancing",tags:["aws","ELB"],content:`About Elastic Load Balancing (ELB) automa­tically distri­butes incoming applic­ation traffic across multiple targets, such as EC2\u0026rsquo;s, contai­ners, IP addresses, \u0026amp; Lambda functions.
Elastic Load Balancing is a best practice to assign incoming traffic to a single target, such as an EC2 Instance, and then distribute the rest of the traffic across the target\u0026rsquo;s resources. An elastic load balancer distributes traffic across an arbitrary number of targets.
Documentation User Guide Types of Load Balancers Type Description Application Load Balancer (ALB) Operates at the Application Layer (OSI L7), handling HTTP/HTTPS traffic. Allows routing requests to specific web servers. Network Load Balancer (NLB) Operates at the Network Layer (OSI L4), handling TCP traffic. Recommended for performance. Classic Load Balancer (CLB) Operates at OSI L7 and OSI L4, however has limited functions. Requests are forwarded by the load balancer without “looking into” any of these requests. They just get forwarded to the backend section. Not recommended for use except for apps built in the EC2-Classic network. Compare ALB vs NLB vs CLB Basic load balancing features ALB NLB CLB Balance load between targets Yes Yes Yes Perform health checks on targets Yes Yes Yes Highly available Yes Yes Yes Elastic Yes Yes Yes TLS Termination Yes Yes Yes Performance Good Very high Good Send logs and metrics to CloudWatch Yes Yes Yes Layer 4 (TCP) No Yes Yes Layer 7 (HTTP) Yes No Yes Running costs Low Low Low Advanced load balancing features ALB NLB CLB Advanced routing options Yes N/A No Can send fixed response without backend Yes No No Supports user authentication Yes No No Can serve multiple domains over HTTPS Yes Yes No Preserve source IP No Yes No Can be used in EC2-Classic No No Yes Supports application-defined sticky session cookies No N/A Yes Supports Docker containers Yes Yes Yes Supports targets outside AWS Yes Yes No Supports websockets Yes N/A No Can route to many ports on a given target Yes Yes No Scaling Vertical Scaling
Increasing the size of the instances (ie- increase in RAM and vCPUs ) Ex: from t2.micro to t3.2xlarge (doesn\u0026rsquo;t have to be the same instance family) In vertical scaling, you scale up/down Vertical scaling usually happens in databases, to handle high workloads as your application grows *Horizontal Scaling
Increasing the no. of instances In horizontal scaling, you scale out/in Ex: ASG scaling out EC2 instances to match increased workload for your web application Cross Zone Load Balancing ALB
Enabled by default. Cannot disable it Not charges for data transfer between AZs (inter AZ data transfer) CLB
Enabled by default. Can disable it Not charged for data transfer between AZs NLB
Disabled by default. Can enable it Charged for data transfer between AZs Digest ELB(Elastic Load Balancing) distributes application or network traffic across multiple targets, such as EC2 instances, containers(ECS), and IP addresses, in multiple AZs. Cross Zone Load Balancing – when enabled, each load balancer node distributes traffic across the registered targets in all enabled AZs. 3 Types of Load balancers - Application, Network, Classic Deleting ELB won\u0026rsquo;t delete the instances registered to it. Termination protection will be disabled by default; enable it to prevent accidental delete. 504 error means the gateway has timed out and the application is not responding within the idle timeout period Look for the X-Forwarded-For header, if you need the end user IPv4 address ASG (Auto Scaling Group) ensures you\u0026rsquo;ve the correct number of EC2 instances available. Specify minimum, maximum and desired number of instances. Lifecycle hook - perform custom actions when instances launch or terminate Cool down period - ensure not to launch additional instances before previous scaling activities complete Launch configuration - Instance configuration template the ASG uses to launch EC2 instances Price Current price
Use Cases Type: Scale your network design
AWS discourages the use of Classic Load Balancer in favor of its newer load balancers
Application Load Balancer is typically used for web applications.
Network Load Balancer would be used for anything that ALBs don’t cover. A typical use case would be a near real-time data streaming service (video, stock quotes, etc.) Another typical case is that you would need to use an NLB if your application uses non-HTTP protocols
Practice Create Classic Load Balancer
Questions Q1 Which load balancer would you use for services which use HTTP or HTTPS traffic?
Explanation Application Load Balancer (ALB). Q2 What are possible target groups for ALB (Application Load Balancer)?
Explanation EC2 tasks ECS instances Lambda functions Private IP Addresses Q3 Your would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.
Which solution below would be best to implement for this?
Enable X-Forwarded For on the web servers and use a Classic Load Balancer Configure proxy servers to forward the traffic to the correct instances Use an Application Load Balancer with path-based routing rules to forward the traffic to the correct instances Use an Application Load Balancer with host-based routing rules to forward the traffic to the correct instances Explanation Application Load Balancer with host-based routing rules
https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/
4
Q4 What is the primary reason why you should be using an elastic load balancer for a website with high activity?
ELBs help you scale servers easily without manual intervention ELBs can distribute traffic equally to your backend targets to handle the incoming traffic load ELBs help tighten security through the use of security groups ELBs boost your website’s overall performance Explanation Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.
Elastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant. They are Application Load Balancer, Network Load Balancer, Classic Load Balancer, and Gateway Load Balancer.
2
Q5 After a year of development, the company’s 100-node high-performance computing (HPC) application is now ready to be deployed to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Horizontal Pod Autoscaler (HPA). The application must be capable of receiving millions of UDP traffic per second from the public Internet while maintaining low latency.
Which of the following is the most operationally efficient solution that should be implemented to meet the above requirements?
Launch a Gateway Load Balancer and integrate an Elastic Fabric Adapter (EFA) to each Kubernetes pod deployed in the Amazon EKS cluster Integrate the AWS Load Balancer Controller add-on to the EKS cluster. Launch a Network Load Balancer to load balance network traffic to individual Kubernetes pods. Install the AWS Load Balancer Controller add-on to the EKS cluster and launch an Application Load Balancer to distribute the incoming traffic to the Kubernetes pods. Set up the kube-proxy Amazon EKS add-on to the cluster and configure the Source Network Address Translation (SNAT) of the Kubernetes pods by setting the AWS_VPC_K8S_CNI_EXTERNALSNAT configuration to true. Explanation Network Load Balancer operates at the connection level (Layer 4), routing connections to targets – Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.
Ideal for load balancing of both TCP and UDP traffic, Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone.
It is integrated with other popular AWS services such as Auto Scaling, Amazon EC2 Container Service (ECS), Amazon CloudFormation, and AWS Certificate Manager (ACM).
Network Load Balancer preserves the client-side source IP address, allowing the back-end EC2 instances to see the IP address of the client. This can then be used by applications for further processing.
Network traffic is load balanced at L4 of the OSI model. To load balance application traffic at L7, you deploy a Kubernetes ingress, which provisions an AWS Application Load Balancer. An AWS Network Load Balancer can load balance network traffic to pods deployed to Amazon EC2 IP and instance targets or to AWS Fargate IP targets.
The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources:
– An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress.
– An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer.
2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticloadbalancing/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kinesis/":{title:"Kinesis",tags:["aws","kinesis"],content:`About Kinesis makes it easy to collect, process, \u0026amp; analyze real-time, streaming data, so one can get timely insights.
Documentation User Guide Amazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.
Real-time-based Fully managed Scalable Capabilities Kinesis Video Streams Capture, process, and store video streams Kinesis Data Streams Capture, process, and store data streams Kinesis Data Firehose Load data streams into AWS data stores
The easiest way to capture, transform, and load data streams into AWS data stores for near real-time analytics
Kinesis Data Analytics Analyze data streams with SQL or Apache Flink
Digest Kinesis data stream. Hot shard vs cold shard. Merging shards will decrease streams capacity. Kinesis adapter is the recommended way to consume streams from DynamoDB. Incoming write bandwidth and outgoing read bandwidth are used to calculate initial number of shards for kinesis stream. A single Kinesis Shard can handle 1MB per second write. 2MB per second read. It can also handle 1000 writes per second, and 5 read transactions a second Price Current price
Use Cases Type: Analytics
Same type services: Athena, EMR, Redshift, Kinesis, Elasti­cSearch Service, Quicksight
Netflix uses Amazon Kinesis to monitor the communications between all of its applications so it can detect and fix issues quickly, ensuring high service uptime and availability to its customers.
Practice Questions Q1 You built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.
What steps will you take to keep a strong performance?
Remove the hot shards Merge the hot shards Split the hot shards Increase the shard capacity Explanation https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html
Split the hot shards
3
Q2 Jasmin needs to perform ad-hoc business analytics queries on well-structured data. Data comes in constantly at a high velocity. Jasmin\u0026rsquo;s team can understand SQL.
What AWS service(s) should Jasmin look to first?
EMR using Hive EMR running Apache Spark Kinesis Firehose + RDS Kinesis Firehose + RedShift Explanation RedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol
https://aws.amazon.com/kinesis/data-firehose/features/
4
Resources FAQ OpenGuide HowTo Introduction to Amazon Kinesis AWS Webcast - Introduction to Amazon Kinesis `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kinesis/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/opensearch-service/":{title:"OpenSearch Service",tags:["aws","opensearch"],content:`About OpenSearch Service is a fully managed service that makes it easy to deploy, secure
Amazon OpenSearch Service is an open source, distributed search and analytics suite based on Elasticsearch.
Documentation User Guide You can load streaming data from the following sources using AWS Lambda event handlers:
Amazon S3 Amazon Kinesis Data Streams and Data Firehose Amazon DynamoDB Amazon CloudWatch AWS IoT Exposes three Elasticsearch logs through CloudWatch Logs:
error logs. search slow logs – These logs help fine tune the performance of any kind of search operation on Elasticsearch. index slow logs – These logs provide insights into the indexing process and can be used to fine-tune the index setup. Indexing Before you can search data, you must index it. Indexing is the method by which search engines organize data for fast retrieval. the basic unit of data is a JSON document. Within an index, Elasticsearch organizes documents into types (arbitrary data categories that you define) and identifies them using a unique ID. Price Current price
Has free tier. You pay for each hour of use of an EC2 instance and for the cumulative size of any EBS storage volumes attached to your instances. You can use Reserved Instances to reduce long term cost on your EC2 instances. Use Cases Type: Analytics
Same type services: Athena, EMR, Redshift, Kinesis, Elasti­csearch Service, Quicksight
Log Analytics Real-Time Application Monitoring Security Analytics Full Text Search Clickstream Analytics Practice Build A Log Aggregation System in AWS
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/opensearch-service/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudwatch/":{title:"CloudWatch",tags:["aws","cloudwatch"],content:`About CloudWatch offers a reliable, scalable, \u0026amp; flexible monitoring solution that can easily start.
CloudWatch can provide a dependable, scalable, \u0026amp; flexible monitoring solution that is simple to set up.
Documentation User Guide CloudWatch vs CloudTrail CloudWatch CloudTrail Performance monitoring Auditing Log events across AWS Services – think operations Log API activity across AWS services – think activities, or who to blame Higher-level comprehensive monitoring and event service More low-level, granular Log from multiple accounts Log from multiple accounts Logs stored indefinitely Logs stored to S3 or CloudWatch indefinitely Alarms history for 14 days No native alarming; can use CloudWatch alarms Digest Standard vs High resolution metrics. CloudWatch unified agent collect both system metrics and log files from Amazon EC2 Instances and on-premise servers. The agent supports both Windows Server and Linux, and enables to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server. Cloudwatch events could be used for scheduling lambda functions. RAM Utilization needs custom metrics which use-cases are covered in Standard metrics and which ones would need custom metrics. (memory utilization needs custom metrics) Basic (5 min) vs detailed (1 min) vs high resolution (1s) monitoring. Price Current price
Use Cases Type: Operate
Same type services: CloudWatch, CloudTrail, Systems Manager, Cost \u0026amp; usage report, Cost explorer, Managed Services
Practice Introduction to CloudWatch
Questions Q1 A Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.
Which of the following enables monitoring of the metric in Amazon CloudWatch?
Retrieve the log data from AWS CloudTrail using the LookupEvents API Call Retrieve the log data from CloudWatch using the GetMetricData API call Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application Install the Amazon CloudWatch logs agent on the EC2 instance that the application is running on Explanation 4
Q2 A developer must use AWS X-Ray to monitor an application that is running on an Amazon EC2 instance. Developer has prepared the application by using the X-Ray SDK.
What should the developer do to perform the monitoring?
Configure the X-Ray SDK sampling rule and target. Activate the X-Ray daemon from the EC2 console or the AWS CLI with the modify-instance-attribute command to set the XRayEnabled flag. Install the X-Ray daemon. Assign an IAM role to the EC2 instance with a policy that allows writes to X-Ray. Install the X-Ray daemon. Configure it to forward data to Amazon EventBridge (Amazon CloudWatch Events). Grant the EC2 instance permission to write to Event Bridge (CloudWatch Events). Deploy the X-Ray SDK with the application, and instrument the application code. Use the SDK logger to capture and send the events. Explanation 3
Q3 A developer is designing an AWS Lambda function to perform a maintenance activity. The developer will use Amazon EventBridge (Amazon CloudWatch Events) to invoke the function on an hourly schedule. The developer wants the function to log information at different levels of detail according to the value of a log level variable. The developer must design the function so that the log level can be set without requiring a change to the function code.
Which solution will meet these requirements?
A. Add a custom log level parameter for the Lambda function. Set the parameter by using the Lambda console. Set the log level in the Amazon CloudWatch Logs console. Set the log level in a Lambda environment variable. Add a custom log level parameter for the Lambda function. Set the parameter by using the AWS CLI. Explanation 3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudwatch/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudformation/":{title:"CloudFormation",tags:["aws","cloudformation"],content:`About CloudF­orm­ation enables the user to design \u0026amp; provision AWS infras­tru­cture deploy­ments predic­tably \u0026amp; repeat­edly
Documentation User Guide Terminology Component Description Templates The JSON or YAML text file that contains the instructions for building out the AWS environment Stacks The entire environment described by the template and created, updated, and deleted as a single unit StackSets AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation Change Sets A summary of proposed changes to your stack that will allow you to see how those changes might impact your existing resources before implementing them Template The main sections in a CloudFormation template:
Parameters - specify variables requiring user input Conditions - define entities based on a condition e.g. provision resources based on environment, or specify AMI for and EC2 instance to deploy based on region Resources - the AWS resources to create Mappings - create custom mappings e.g. RegionMap for Region : AMI Transforms - reference code located in S3 e.g. Lambda code or reusable snippets of CloudFormation code Digest A CloudFormation template will consist of a set of resources defined. These resources will be part of a single stack, once built. CloudFormation will treat all the resources as a collection of resources CloudFormation supports JSON and YAM for its template languages. All ID\u0026rsquo;s are unique to each region, account, and VPC. It is best practice to not embed such IU\u0026rsquo;s inside a CloudFormation template. Instead, define parameters, mappings and conditions to create a dynamic template that could be run across VP\u0026rsquo;s, Regions or even accounts Cloudformation stackset vs changeset vs nested stack Nested stacks - stacks created as part of other stacks. You create a nested stack within another stack by using the AWS: CloudFormation:Stack resource. For example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting the same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the resource to reference that template from within other templates Change Sets will produce a summary of changes and their impact on the resources. StackSets is used for deploying or managing template resources across accounts and/or regions. Sting, Number, List are supported data type in CFT Including lambda function as zipfile parameter in CFT is the easiest way to deploy lambda function If stack creation fails, AWS CloudFormation rolls back any changes by deleting the resources that it created. Fn:FindInMap to perform a dynamic lookup in Cloud formation template Transform section of Cloud formation specifies version of SAM model to use. Two templates, one for Intra and one for App. Price Current price
Use Cases Type: Provision
Same type services: CloudF­orm­ation, Service Catalog, OpsWorks, Market­place
Manage infrastructure with DevOps
Automate, test, and deploy infrastructure templates with continuous integration and delivery (CI/CD) automations.
Scale production stacks
Run anything from a single Amazon Elastic Compute Cloud (EC2) instance to a complex multi-region application.
Share best practices
Define an Amazon Virtual Private Cloud (VPC) subnet or provisioning services like AWS OpsWorks or Amazon Elastic Container Service (ECS) with ease.
Compare CloudFormation vs Elastic Beanstalk CloudFormation Elastic Beanstalk “Template-driven provisioning” “Web apps made easy” Deploys infrastructure using code Deploys applications on EC2 (PaaS) Can be used to deploy almost any AWS service Deploys web applications based on Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker Uses JSON or YAML template files Uses ZIP or WAR files Similar to Terraform Similar to Google App Engine Comparing CloudFormation Init and EC2 User Data CloudFormation Init and EC2 User Data
With EC2 Instance user data, developers are able to run commands and scripts during the launch of an EC2 instance. User data can be used to install necessary packages, update the ownership of files and directories, or even update or run services. Developers who are familiar with shell scripting may find user data as the easiest way to incorporate launch instructions for EC2 instances. EC2 Instance user data allows for a procedural-based approach to configuring an instance during launch.
The following snippet represents a UserData property of an EC2 instance defined within a CloudFormation template:
The shell script above begins with an update and installation of the httpd Apache service using the yum package manager. The systemctl start and enable commands start the Apache server and allow it to serve content from the EC2 instance. The cat command adds an HTML snippet to the index.html file located in the /var/www/html/ directory of the instance. Once the user data script is completed, you can view the HTML content by accessing the EC2 instance\u0026rsquo;s public URL.
It\u0026rsquo;s important to note that this user data script runs only when the EC2 instance is launched. CloudFormation Init (cfn-init) AWS provides CloudFormation helper scripts like cfn-init to help fine-tune your stack templates to better fit your needs. CloudFormation cfn-init allows developers to establish a desired state of their instance using metadata. This means that these configurations can be updated and run on the same instance over time.
The following snippet represents the AWS::CloudFormation::Init metadata type for an EC2 instance defined within a CloudFormation template:
The config section details the packages, files, and services to be configured on the EC2 instance. This eases the burden of managing a bash script since each type of configuration is held in its own dedicated section. The snippet performs the same configuration on the EC2 instance as the previous user data example. Unlike EC2 user data, the cfn-init script does not run automatically. The next lab step will cover how to utilize helper scripts in a CloudFormation stack deployment.
Key Differences Instance user data is procedural-based, while CloudFormation init can be used to achieve the desired state of an instance When you update the instance user data in CloudFormation and perform a stack update, the instance is terminated and replaced. However, when you update the CloudFormation init metadata and perform a stack update, the instance will be updated in place Instance user data is run only once during the instance launch The success or failure of a user data script does not affect a CloudFormation stack creation process. With the CloudFormation signal helper script, a successful stack creation relies on a successful instance configuration (More on this in the next lab step) Practice Initializing Amazon EC2 Instances with AWS CloudFormation Init
Questions Q1 You are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.
What steps can you take in this situation? (Choose 2 answers)
Use a template to first create Resource A with the ARN as an output value. Use a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt. Hard code the ARN value output from creating Resource A into the second template. Just create Resource B. Explanation http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html
2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudformation/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codecommit/":{title:"CodeCommit",tags:["aws","codecommit"],content:`About CodeCommit is a version control service (Git repository service) that enables the user to personally store \u0026amp; manage Git archives in the AWS cloud.
Documentation User Guide CodeCommit is integrated with Jenkins, CodeBuild and other CI tools.
Price 5 active users free per month with the
Current price
Practice Introduction to CodeCommit
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codecommit/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codedeploy/":{title:"CodeDeploy",tags:["aws","codedeploy"],content:`About CodeDeploy is a fully managed deployment service that automates software deploy­ments to a variety of compute services such as EC2, Fargate, Lambda, \u0026amp; on-pre­mises servers
Documentation User Guide CodeDeploy can also deploy a serverless Lambda function.
CodeDeploy can be connected to CodePipeline and use artifacts from there.
Platforms Need to choose the compute platform:
EC2/On-premises. AWS Lambda. Amazon ECS. AppSpec File The application specification file (AppSpec file) is a YAML-formatted, or JSON-formatted file used by CodeDeploy to manage a deployment.
The AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.
Deployment types In-place deployment (EC2 only)
Blue/green deployments:
AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.
Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.
EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.
Price Current price
Use Cases Type: Developer Tools
Practice Continuous Integration and Deployment with AWS Code Services Questions Q1 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?
You will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html
Can delete only if unused.
1
Q2 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?
All details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html
If you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.
3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codedeploy/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codebuild/":{title:"CodeBuild",tags:["aws","codebuild"],content:`About CodeBuild is a fully managed service that assembles source code, runs unit tests, \u0026amp; also generates artefacts ready to deploy.
Documentation User Guide CodeBuild is a code creation service that also produces code artefacts upon request.
CodeBuild is an alternative to other build tools such as Jenkins.
CodeBuild is integrated with KMS for encryption of build artifacts, IAM for build permissions, VPC for network security, and CloudTrail for logging API calls.
CodeBuild is a fully managed build service to compile source code, run unit tests and produce artifacts that are ready for deployment. Not the best fit for serverless template deployment or serverless application initialization.
buildspec.yml Build instructions can be defined in the code (buildspec.yml).
CodeBuild Local Build In case you need to do deep troubleshooting beyond analyzing log files.
Can run CodeBuild locally on your computer using Docker.
Leverages the CodeBuild agent.
Price Current price
You pay based on the time it takes to complete the builds.
Lab cicd-aws-code-services Chapters:
Logging in to the Amazon Web Services Console Creating an AWS CodeCommit Repository Committing Code to Your AWS CodeCommit Repository Building and Testing with AWS CodeBuild Deploying with AWS CodeDeploy Automating Your Deployment with AWS CodePipeline Following the Continuous Deployment Pipeline Recovering Automatically from a Failed Deployment Using AWS CodeDeploy Blue/Green Deployments in Your Pipeline Questions Q1 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.
What is the easiest way to develop these simple applications?
Install AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model
https://aws.amazon.com/serverless/sam/
1
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codebuild/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codepipeline/":{title:"CodePipeline",tags:["aws","codepipeline"],content:`About AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.
Documentation User Guide CodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.
You can easily integrate AWS CodePipeline with third-party services such as GitHub or with your own custom plugin. With AWS CodePipeline, you only pay for what you use.
Alternatives Bamboo. CircleCI. Jenkins. Travis CI. GitLab. TeamCity. Azure DevOps Server. Google Cloud Build. Terminology Pipelines
A workflow that describes how software changes go through the release process.
Artifacts
Files or changes that will be worked on by the actions and stages in the pipeline. Each pipeline stage can create “artifacts”. Artifacts are passed, stored in Amazon S3, and then passed on to the next stage. Stages
Pipelines are broken up into stages, e.g., build stage, deployment stage. Each stage can have sequential actions and or parallel actions. Stage examples would be build, test, deploy, load test etc. Manual approval can be defined at any stage. Actions
Stages contain at least one action, these actions take some action on artifacts and will have artifacts as either an input, and output, or both.
Transitions
The progressing from one stage to another inside of a pipeline.
Price Current price
Questions Q1 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.
Which actions should you take to make this function perform correctly? (2 answers)
Restart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html
2, 3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codepipeline/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeguru/":{title:"CodeGuru",tags:["aws","codeguru"],content:`About Amazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application’s most expensive lines of code.
CodeGuru is a developer tool powered by machine learning that provides intell­igent recomm­end­ations for improving code quality \u0026amp; identi­fying an applic­ation’s most expensive lines of code.
Documentation User Guide Amazon CodeGuru is comprised of two services: CoduGuru Reviewer and CodeGuru Profiler. Reviewer is what listens for pull requests in a repository and reviews code changes.
Amazon CodeGuru Reviewer is triggered by pull requests to Code Commit, then makes suggestions as comments in the pull request wherever it sees fit.
Amazon CodeGuru Reviewer generates suggestions in its reviews as comments in pull requests.
Type detection Security Detection Secrets Detection Code Quality Benefits Amazon CodeGuru Reviewer
Catch code problems before they hit production Fix security vulnerabilities Proactively improve code quality with continuous monitoring CodeGuru Profiler
Troubleshoot performance issues Discover anomalies and common issues in your application performance Catch your most expensive line of code today Price Current price
Practice Automating Code Reviews with Amazon CodeGuru
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeguru/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codestar/":{title:"CodeStar",tags:["aws","codestar"],content:`About AWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place.
Documentation User Guide CodeStar enables to quickly develop, build, \u0026amp; deploy applic­ations on AWS.
With AWS CodeStar, you can create, manage, and scale automated code reviews with a single click. You can also monitor the performance and scalability of your code review process with the built-in metrics dashboard.
Each AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software.
Alternatives Alternatives to AWS CodeStar:
Jenkins. Azure DevOps Projects. GitHub. GitLab. CircleCI. CloudBees CI. Plesk. Copado CI/CD. Price There is no additional charge for using AWS CodeStar. You only pay for the AWS resources that you provision for developing and running your application (for example, Amazon EC2 instances).
Current price
Practice Develop and Deploy an Application with AWS CodeStar
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codestar/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeartifact/":{title:"CodeArtifact",tags:["aws","codeartifact"],content:`About AWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.
Documentation User Guide CodeAr­tifact is a secure storage, publishing, and sharing of software code packages used in a development process organisation\u0026rsquo;s software development. CodeAr­tifact makes it easy for small organisations to store, publish, and share software packages.
CodeArtifact can be configured to automatically fetch software packages and dependencies from public artifact repositories.
CodeArtifact works with commonly used package managers and build tools like Maven, Gradle, npm, yarn, twine, pip, and NuGet making it easy to integrate into existing development workflows.
Price Pay only for what you use – the size of the artifacts stored, the number of requests made, and the amount of data transferred out of an AWS Region. CodeArtifact includes a monthly free tier for storage and requests
Current price
Use Cases Type: Developer Tools
Alternatives JFrog Artifactory Docker hub Sonatype Nexus Platform Helm Azure DevOps Services Github Usage 1$ aws codeartifact list-domains --region us-east-1 Practice Getting started using the console
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeartifact/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/xray/":{title:"X-Ray",tags:["aws","xray"],content:`About AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.
Documentation User Guide X-Ray allows software engineers to view the state of a system at a glance, identify potential bottlenecks, and make informed operational decisions to improve performance and reliability. X-Ray inspects application code using a combination of machine and customer-provided data to identify potential bottlenecks and analyze performance and performance trends for each test scenario.
Terminology AWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application
X-Ray Trace Hierarchy: Trace \u0026gt; Segment \u0026gt; Sub Segment
Trace
An X-Ray trace is a set of data points that share the same trace ID.
Segments
A segment is a JSON representation of a request that your application serves.
A trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases.
Subsegments
Subsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.
Annotations
An X-Ray annotation is system-defined, or user-defined data associated with a segment A segment can contain multiple annotations. Annotations are used to describe the request, the response, and other information about the segment Can be used for adding system or user-defined data to segments and subsegments that you want to index for search. Sampling
X-Ray traces are sampled at a rate that you specify. The rate is specified in the sampling_rate field of the sampling object in the config object.
Metadata
X-Ray traces contain metadata that is useful for understanding the trace.
Metadata (Key / value pairs) is not indexed and cannot be used for searching Digest Trace request across microservices/AWS services
Analyze, Troubleshoot errors, Solve performance issues Gather tracing information From applications/components/AWS Services Tools to view, filter and gain insights (Ex: Service Map) How does Tracing work?
Unique trace ID assigned to every client request X-Amzn-Trace-Id:Root=1-5759e988-bd862e3fe Each service in request chain sends traces to X-Ray with trace ID X-Ray gathers all the information and provides visualization How do you reduce performance impact due to tracing? Sampling - Only a sub set of requests are sampled (Configurable) How can AWS Services and your applications send tracing info? Step 1 : Update Application Code Using X-Ray SDK Step 2: Use X-Ray agents (EASY to use in some services! Ex: AWS Lambda) Segments and Sub-segments can include an annotations object containing one or more fields that X-Ray indexes for use with Filter Expressions. It is indexed. Use up to 50 annotations per trace.
Total sampled request per second = Reservoir size + ((incoming requests per second - reservoir size) * fixed rate)
Default sampling X-ray SDK first request each second and 5% of any additional requests
Tracing header can be added in http request header
Annotations vs Segments vs Subsegments vs metadata
X-ray daemon listens for traffic on UDP port 2000
X-ray SDK provides interceptors to add your code to trace incoming HTTP requests.
X-ray in EC2: You need the X-Ray daemon to be running on your EC2 instances in order to send data to X-Ray. User data script could be used to install the X-Ray daemon in EC2 instance.
X-ray in ECS: In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.
X-ray in elastic beanstalk: Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code
AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-service architecture.
A segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about - downstream calls that your application made to fulfill the original request.
Add annotations to subsegment document if you want to trace downstream calls.
Segments and subsegment can include a metadata object containing one or more fields with values of any type, including objects and arrays.
Tracing header is added in the HTTP request header. A tracing header (X-Amzn-Trace-ld) can originate from the X-Ray SDK, an AWS service, or the - client request.
Use the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in - order to develop the custom debug tool
Price Current price
Use Cases Type: Developer Tools
Alternatives Google Stackdriver Azure Monitor Elastic Observability Datadog Splunk AWS X-Ray supports applications running on:
Amazon Elastic Compute Cloud (Amazon EC2) Amazon EC2 Container Service (Amazon ECS) AWS Lambda WS Elastic Beanstalk Practice Questions Q1 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.
Which AWS service will be your main tool?
AWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks
1
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/xray/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/fis/":{title:"Fault Injection Simulator",tags:["aws","Fault Injection Simulator","FIS"],content:`About AWS Fault Injection Simulator (FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application’s performance, observability, and resiliency.
Documentation User Guide Price Current price
With AWS FIS, you pay only for what you use. There are no upfront costs or minimum fees. You are charged based on the duration that an action is active. The AWS FIS price is $0.10 per action-minute.
Terminology and Concepts Everything starts with an experiment template. The experiment template defines the targets that participate in the experiment. Supported targets are:
EC2 Instances EKS node groups RDS clusters \u0026amp; instances IAM roles The actions define the injected faults. You can run actions in parallel or sequence.
Some action examples:
AWS API level errors for the EC2 service Stop/reboot/terminate EC2 instances Run SSM commands on EC2 instances to stress CPU or memory, add network latency, or kill a process Reboot RDS instance Failover RDS cluster Drain ECS container instance Terminate EKS node group instance Use Cases Periodic Game Days Continuous Delivery Pipeline Integration Practice Test instance stop and start using
Questions Q1 What is Chaos Engineering?
Explanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.
Chaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.
It starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/fis/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ecr/":{title:"Elastic Container Registry",tags:["aws","Elastic Container Registry","ecr"],content:`About Amazon Elastic Container Registry (Amazon ECR) - Fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere
Documentation User Guide Hosted private Docker registry
Alternatives Docker Hub JFrog Artifactory Azure Container Registry Harbor Google Container Registry Red Hat Quay JFrog Container Registry Price Current price
Use Cases Store, encrypt, and manage container images
Manage software vulnerabilities Streamline your deployment workloads Manage image lifecycle policies Type: Containers
Same type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate
Practice This commands returns the command to execute to be able to login to ECR:
Login get-login-password:aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com Create a repository: 1aws ecr create-repository \\ 2 --repository-name hello-repository \\ 3 --image-scanning-configuration scanOnPush=true \\ 4 --region region Tag image docker tag hello-world:latest aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Push docker push aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository Pull docker pull aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository:latest Delete an image 1aws ecr batch-delete-image \\ 2 --repository-name hello-repository \\ 3 --image-ids imageTag=latest \\ 4 --region region Delete a repository 1aws ecr delete-repository \\ 2 --repository-name hello-repository \\ 3 --force \\ 4 --region region Labs:
Use AWS Fargate for Serverless Deployment of Container Applications Quick start: Publishing to Amazon ECR Public using the AWS CLI Notes:
If you get a 503 Service Temporarily Unavailable error, try again after 30 seconds to let the load balancer finish adding the task to the target group. `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ecr/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ecs/":{title:"Elastic Container Service",tags:["aws","Elastic Container Service","ecs"],content:`About Documentation User Guide Highly secure, reliable, \u0026amp; scalable way to run contai­ners
Alternatives Google Container Engine (GKE) Azure Container Service IBM Bluemix Container Service Jelastic Multi-Cloud PaaS Terminology Amazon ECS Term	Definition Cluster Logical Grouping of EC2 Instances Container Instance	EC2 instance running the ECS agent Task Definition Blueprint that describes how a docker container should launch Task A running container using settings in a Task Definition Service Defines long running tasks – can control task count with Auto Scaling and attach an ELB Digest Microservices are built in multiple programming languages Containers simplify deployment of microservices: Step 1 : Create a self contained Docker image Application Runtime (JDK or Python), Application code and Dependencies Step 2 : Run it as a container any where Local machine OR Corporate data center OR Cloud Use On-Demand instances or Spot instances Launch type: EC2 or Fargate Data volumes attached to containers Deployment type: Rolling update Blue/green deployment (powered by AWS CodeDeploy) Task Placement Strategies: binpack - Leave least amount of unused CPU or memory. Minimizes number of container instances in use random - Random task placement spread - Based on specified values: Host (instanceId) (OR) Availability Zone(attribute:ecs.availability-zone) (Alowed) Combine strategies and prioritize How do you manage 100s of containers? ECS - Fully managed service for container orchestration Step 1 : Create a Cluster (Group of one or more EC2 instances) Step 2: Deploy your microservice containers AWS Fargate: Serverless ECS. DON\u0026rsquo;T worry about EC2 instances. Cloud Neutral: Kubernetes AWS - AWS Elastic Kubernetes Service (EKS) Load balancing: Performed using Application Load Balancers Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.myapp.com/microservice-a and www.myapp.com/microservice-b) Price Current price
Use Cases Type: Containers
Same type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate
Best practice:
10 Microservices =\u0026gt; 10 Task Definitions =\u0026gt; 10 Task IAM Roles with individual permissions needed by each microservice ECS vs EKS Amazon also provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.
Amazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Questions Q1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.
memory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_monitoring.html
1, 2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/ecs/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/fargate/":{title:"Fargate",tags:["aws","Fargate"],content:`About Serverless version of ECS.
Serverless compute for contai­ners.
AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.
Deploy and manage your applications, not infrastructure. Fargate removes the operational overhead of scaling, patching, securing, and managing servers.
Compatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).
Documentation User Guide Alternatives Google Kubernetes Engine (GKE) Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Azure Container Instances Cloud Foundry Oracle Cloud Infrastructure Container Engine for Kubernetes Price Current price
Use Cases Web apps, APIs, and microservices Run and scale container workloads Support AI and ML training applications Type: Containers
Same type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate
Questions Q1 How AWS Fargate different from AWS ECS?
Explanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don\u0026rsquo;t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/fargate/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/eks/":{title:"Elastic Kubernetes Service",tags:["aws","EKS"],content:`About Kubernetes (K8) Docker Container/Cluster management
Run highly secure, reliable, and scalable containers
Documentation User Guide Alternatives Red Hat OpenShift Container Platform Azure Kubernetes Service (AKS) Rancher Google Kubernetes Engine (GKE) Oracle Cloud Infrastructure Container Engine for Kubernetes Mirantis Kubernetes Engine (formerly Docker Enterprise) Kubernetes Cloud Foundry Price Current price
Use Cases Build and run web applications Deploy across hybrid environments Model machine learning (ML) workflows ECS vs EKS Amazon provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.
Amazon ECS Amazon EKS Managed, highly available, highly scalable container platform Managed, highly available, highly scalable container platform AWS-specific platform that supports Docker Containers Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments Considered simpler and easier to use Considered more feature-rich and complex with a steep learning curve Leverages AWS services like Route 53, ALB, and CloudWatch A hosted Kubernetes platform that handles many things internally “Tasks” are instances of containers that are run on underlying compute but more of less isolated “Pods” are containers collocated with one another and can have shared access to each other Limited extensibility Extensible via a wide variety of third-party and community add-ons. Practice Building a Cloud Native Application
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/eks/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cognito/":{title:"Cognito",tags:["aws","Cognito"],content:`About Amazon Cognito - Simple and Secure User Sign-Up, Sign-In, and Access Control
Documentation User Guide Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.
Users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.
Alternatives Auth0 Microsoft Azure Active Directory OneLogin Google Cloud Identity Platform IBM Security Verify Keycloak Terminology Credentials: The temporary security credentials, which include an access key ID, a secret access key, and a security token.
AssumedRoleUser: The ARN and the assumed role ID, which are identifiers for the temporary security credentials that you can programatically refer to.
Price Pay only for what you use. First 50,000 (monthly active users (MAUs) - Free.
Current price
Use Cases Type: Identity \u0026amp; access management
Same type services: Identity \u0026amp; Access Management (IAM), Single Sign-On, Cognito, Directory Service, Resource Access Manager, Organisations
Workflow The process of authenticating a user with Cognito is as follows:
The user signs in with a Web ID provider (Google, Facebook, Amazon, etc.) The Web ID provider returns a JWT token to the user The user application makes an STS API call: sts assume-role-with-web-identity STS returns an API response with the temporary credentials The user application now has AWS access e.g. for S3, DynamoDB, etc. Practice Manage Authentication with Amazon Cognito
Questions Q1 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.
What action will best solve this issue?
Use AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html
3
Q2 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events
Which combination of actions should the developer take to satisfy these requirements? (Select TWO.)
Use Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.
1, 5
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cognito/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/route53/":{title:"Route 53",tags:["aws","Route 53"],content:`About Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-premises.
Documentation User Guide A highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing, and health checking.
Can create and manage your public DNS records.
What is the difference between Route 53 and DNS?
Your DNS is the service that translates your domain name into an IP address. AWS Route 53 is a smart DNS system that can dynamically change your origin address based on load, and even perform load balancing before traffic even reaches your servers.
Alternatives Cloudflare DNS. Google Cloud DNS. Azure DNS. GoDaddy Premium DNS. DNSMadeEasy. ClouDNS. UltraDNS. NS1. Routing Policies Simple routing policy – route internet traffic to a single resource that performs a given function for your domain. You can’t create multiple records that have the same name and type, but you can specify multiple values in the same record, such as multiple IP addresses.
Failover routing policy – use when you want to configure active-passive failover.
Geolocation routing policy – use when you want to route internet traffic to your resources based on the location of your users.
Geoproximity routing policy – use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.
You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource. The effect of changing the bias for your resources depends on a number of factors, including the following: The number of resources that you have. How close the resources are to one another. The number of users that you have near the border area between geographic regions. Latency routing policy – use when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency.
IP-based routing policy – use when you want to route traffic based on your users’ locations, and know where the IP address or traffic is coming from.
Multivalue answer routing policy – use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.
Weighted routing policy – use to route traffic to multiple resources in proportions that you specify.
When you register a domain or transfer domain registration to Route 53, it configures the domain to renew automatically. The automatic renewal period is typically one year, although the registries for some top-level domains (TLDs) have longer renewal periods.
When you register a domain with Route 53, it creates a hosted zone that has the same name as the domain, assigns four name servers to the hosted zone, and updates the domain to use those name servers.
Digest Route 53 is AWS DNS service Map domain names to EC2 instances, Load Balancers and 53 buckets Routing Policies Simple - Traffic routed to a single resource Weighted - Traffic routed to a resource = weight assigned to the resource/sum of all the weights Latency - serves requests from the AWS region with low latency Geographical - routes the traffic based on the location of the request origin Failover - routes traffic to primary when primary healthy; secondary when primary is unhealthy Multivalue Answer - routs randomly to multiple healthy resources VPC - private network on AWS platform Subnet, NAT Instance, NAT Gatewav, Internet Gatewav, NACLs, Route Table VPC Wizard Single public subnet Public and Private subnet (NAT) Public and private subnet and AWS managed VPN access Private subnet only and AWS managed VPN access VPC Peering - helps transfer of data VPC Flow logs - helps capture information about incoming/outgoing traffic Direct Connect - dedicated connection from on premises network to VPC Price Pay only for what you use.
Current price
Use Cases Domain Registration / transfer Manage network traffic globally Set up private DNS `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/route53/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kms/":{title:"Key Management Service",tags:["AWS","Amazon Key Management Service"],content:`About AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.
Documentation User Guide AWS KMS provides management of encryption keys for use with other AWS services (e.g. EBS, S3, RDS, etc.).
Key management on AWS is a broad range of activities from creating \u0026amp; storing public \u0026amp; private keys to creating, managing, and authorising access to AWS services with digital keys. This guide explains the key management solution on AWS that is easiest to use, most secure, and provides the most flexibility for you to create and manage your keys the way you need them.
Alternatives HashiCorp Vaultc Azure Key Vault Google Cloud Key Management Service OpenSSH Akeyless Vault Platform Virtru Concepts Key types:
Customer managed keys AWS managed keys AWS owned keys In May 2022, AWS KMS changed the rotation schedule for AWS managed keys from every three years (approximately 1,095 days) to every year (approximately 365 days)
Type of KMS key Can view KMS key metadata Can manage KMS key Used only for my AWS account Automatic rotation Pricing Customer managed key Yes Yes Yes Optional. Every year (approximately 365 days) Monthly fee (pro-rated hourly)
Per-use fee AWS managed key Yes No Yes Required. Every year (approximately 365 days) No monthly fee
Per-use fee (some AWS services pay this fee for you) AWS owned key No No No Varies Varies Customer Managed Keys (CMK)
The primary resources in AWS KMS are customer master keys (CMKs). Typically, you use CMKs to protect data encryption keys (or data keys) which are then used to encrypt or decrypt larger amounts of data outside of the service. CMKs never leave AWS KMS unencrypted, but data keys can. AWS KMS does not store, manage, or track your data keys.
There is one AWS-managed CMK for each service that is integrated with AWS KMS. When you create an encrypted resource in these services, you can choose to protect that resource under the AWS-managed CMK for that service. This CMK is unique to your AWS account and the AWS region in which it is used, and it protects the data keys used by the AWS services to protect your data.
Data keys
Data keys are used to encrypt large data objects within an application outside AWS KMS. Key rotation and Backing Keys
When you create a customer master key (CMK) in AWS KMS, the service creates a key ID for the CMK and key material, referred to as a backing key, that is tied to the key ID of the CMK. If you choose to enable key rotation for a given CMK, AWS KMS will create a new version of the backing key for each rotation. It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. CMK is simply a logical resource that does not change regardless of whether or of how many times the underlying backing keys have been rotated.
A KMS key consists of
Alias Creation date Description Key state Key material (either customer provided or AWS provided) A KMS key can:
encrypt data up to 4KB in size generate, encrypt, and decrypt Data Encryption Keys (DEKs) never be exported from KMS (CloudHSM allows this). Aliases
Use an alias as a friendly name for a KMS key. For example, you can refer to a KMS key as test-key instead of 1234abcd-12ab-34cd-56ef-1234567890ab.
Custom key stores
A custom key store is an AWS KMS resource associated with FIPS 140-2 Level 3 hardware security modules (HSMs) in a AWS CloudHSM cluster that you own and manage.
Key policy
When you create a KMS keys, you determine who can use and manage that KMS keys.
Digest KMS encrypts small pieces of data (usually data keys) MAX - 4 KB Use Envelope Encryption for larger objects (CMK never leaves KMS) Generate a data key (plain-text and encrypted) from KMS (GenerateDataKey) Use data key to perform encryption/decryption on the object (within the service or client-side) You can assign an encryption context with cryptographic operations If encryption context is different, decryption will NOT succeed Request quotas for KMS Cryptographic operations: 5,500 to 50,000 per second (varies with Region) You might get a ThrottlingException if you exceed the limit Lower your request rate to AWS KMS or Retry with Exponential Backoff Usage of KMS CMKs can be tracked in CloudTrail Key policies control access to CMKs (incl. cross account access) Use AWS Encryption SDK to interact with KMS(Provides Data Key Caching) Price Current price
KMS vs Cloud HSM Generate, store, use and replace your keys(symmetric \u0026amp; asymmetric) KMS: Multi-tenant Key Management Service KMS integrates with all storage and database services in AWS Define key usage permissions (including cross account access) Automatically rotate master keys once a year Schedule key deletion to verify if the key is used Mandatory minimum wait period of 7 days (max-30 days) CloudHSM: Dedicated single-tenant HSM for regulatory compliance AWS KMS is a Multi-tenant service AWS CANNOT access your encryption master keys in CloudHSM (Recommendation) Be ultra safe with your keys. Use two or more HSMs in separate AZs. AWS KMS can use CloudHSM cluster as \u0026ldquo;custom key store\u0026rdquo; to store the keys: AWS Services can continue to talk to KMS for data encryption (AND) KMS does the necessary integration with CloudHSM cluster Use Cases Type: Data protection
Same type services: Macie, Key Management Service (KMS), CloudHSM, Certif­icate Manager, Secrets Manager
CloudHSM: (Web servers) Offload SSL processing, Certificate Authority etc
Practice How to encrypt S3 Objects Using SSE-KMS
Questions Q1 Key rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?
KMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key. KMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys. Key rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed. Key rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key. Explanation When you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key\u0026rsquo;s older cryptographic material so it can be used to decrypt data that it has encrypted.
Q2 Alan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.
Which statements are true with regard to this scenario? (Choose 2 answers)
Create Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts. Can share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys. Make encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts. You can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used. Explanation https://docs.aws.amazon.com/kms/latest/developerguide/overview.html
1, 2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kms/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/eventbridge/":{title:"EventBridge",tags:["AWS","Amazon EventBridge"],content:`About EventB­ridge is a serverless event bus that makes it easy to connect applic­ations together using data from apps, integrated SaaS apps, \u0026amp; AWS services.
Documentation User Guide EventB­ridge is a low-cost alternative to building a new backend infrastructure for every new app. With Serverless EventB­ridge, you can connect your existing apps with a few lines of code. You don’t have to build a new backend for every new app you want to connect to.
You can use existing infrastructure as a provider of event data, and connect your apps using Serverless EventB­ridge.
Alternatives Azure Service Bus TIBCO Cloud Integration (including BusinessWorks and Scribe) IBM App Connect Google Cloud Pub/Sub Apache Camel Peregrine Connect Software AG webMethods IBM Cloud Pak for Integration Price Current price
Use Cases Type: Applic­ation integr­ation
Same type services: SNS, SQS, AppSync, EventBridge
Re-architect for speed Extend functionality via SaaS integrations Monitoring and Auditing Customize SaaS with AI/ML EventBridge vs Amazon SNS In comparison with Amazon SNS, EventBridge:
Integrates with more AWS services than SNS Supports registering message schemas Has sophisticated third-party integrations available Supports transforming event messages before sending them You should choose to use Amazon EventBridge over Amazon SNS when the system you are building is expected to:
Support significant asynchronous functionality Grow significantly in terms of both usage and complexity Have changing requirements over time Have components built by different teams that interact Need support for disparate event sources and targets Amazon EventBridge vs CloudWatch Events Amazon EventBridge extends CloudWatch Events - Build event-driven architectures Original goal with CloudWatch Events was to help with monitoring usecases specific to AWS services. React to events from Your Applications, AWS services and Partner Services Example: EC2 status change, change in your application or SaaS partner application Event Targets can be a Lambda function, an SNS Topic, an SQS queues etc Rules map events to targets (Make sure that IAM Roles have permissions) Event buses receive the events: Default event bus (for AWS services) Custom event bus (custom applications) Partner event bus (partner applications) Over time, Amazon EventBridge will replace Amazon CloudWatch Events Practice Processing File Uploads Asynchronously with Amazon EventBridge
Questions Q1 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.
Which approach best meets these requirements?
Use EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use a Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html
3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/eventbridge/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sns/":{title:"Simple Notification Service",tags:["AWS","SNS"],content:`About Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication.
Documentation User Guide SNS is a fully managed messaging service for both system­-to­-system \u0026amp; app-to­-person (A2P) commun­ica­tion.
Amazon SNS can also send notifications via SMS text message, email, SQS queues or to any HTTP endpoint.
Amazon SNS notifications can also trigger Lambda functions.
Amazon SNS is inexpensive and based on a pay-as-you-go model with no upfront costs.
Alternatives Airship Apple Push Notification Beamer Drift Expo Firebase FCM MagicBell OneSignal Push Terminology SNS Topics
A topic is an “access point” for allowing recipients to dynamically subscribe for identical copies of the same notification.
An SNS topic is a named communication channel.
SNS Subscribers and Endpoints
When subscribing to an SNS topic the following endpoint types are supported:
HTTP/HTTPS Email/Email-JSON Amazon Kinesis Data Firehose Amazon SQS AWS Lambda Platform application endpoint (mobile push) SMS Topic types:
Standard Topics FIFO Topics Price Current price
Use Cases Type: Applic­ation integr­ation
Same type services: SNS, SQS, AppSync, EventB­ridge
Example: Send email notification when CloudWatch alarm is triggered
Practice Process Amazon SNS Notifications with AWS Lambda
Questions Q1 You’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.
Which options are supported notification methods? (Choose 3 answers)
HTTP or HTTPS POST notifications Email using SMTP or plain text Kinesis Stream Invoking of a Lambda function Explanation https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html
1, 2, 4
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sns/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sqs/":{title:"Simple Queue Service",tags:["AWS","SQS"],content:`About Fully managed message queues for microservices, distributed systems, and serverless applications
Documentation User Guide SQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.
SQS is a fully managed message queuing service that enables to decouple \u0026amp; scale micros­erv­ices, distri­buted systems, \u0026amp; serverless applic­ations.
Messages are published to an Amazon SNS topic and then pushed to Amazon SQS subscriber queues. This eliminates the need for periodic polling and allows for messages to be processed in parallel asynchronously by the subscribers.
Alternatives Apache Kafka Google Cloud Pub/Sub RabbitMQ IBM MQ TIBCO Enterprise Message Service AWS IoT Core Amazon CloudWatch Azure Service Bus Configuration Visibility timeout: Other consumers will not receive a message being processed for the configured time period (default 30 seconds, min - 0, max - 12 hours) Consumer processing a message can call ChangeMessageVisibility to increase visibility timeout of a message (before visibility timeout) DelaySeconds: Time period before a new message is visible on the queue Delay Queue = Create Queue + Delay Seconds default - 0, max - 15 minutes Can be set at Queue creation or updated using SetQueueAttributes Use message timers to configure a message specific DelaySeconds value Message retention period: Maximum period a message can be on the queue Default - 4 days, Min - 60 seconds, Max - 14 days MaxReceiveCount: Maximum number of failures in processing a message Digest Long polling: Long polling will reduce the overhead of the CPU and not require excessive dolls. Visibility timeout: a period of time during which Amazon SOS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is O seconds. The maximum is 12 hours Delay queue: Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period Know FIFO vs Standard Queue SQS is a fully managed message queuing service that enables you to decouple and scale micro-services, distributed systems, and Serverless applications SNS: To receive subset of messages, subscriber can apply filter policy to topic subscription. If use case has subscribed to email notification, go with SNS as opposed to SQS. By default only the queue owner is allowed to use the queue. Configure SQS Queue Access Policy to provide access to other AWS accounts Price Current price
Use Cases Type: Applic­ation integr­ation
Same type services: SNS, SQS, AppSync, EventB­ridge
Practice Fan-Out Orders using Amazon SNS and SQS
Questions Q1 Which endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?
SNS E-Mail SQS Kinesis Explanation https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html
3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sqs/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/step-functions/":{title:"Step Functions",tags:["AWS","Step Functions"],content:`About AWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services.
Documentation User Guide Step Functions is a serverless function orches­trator that makes it easy to sequence Lambda functions \u0026amp; multiple AWS services into busine­ss-­cri­tical applic­ations.
Alternatives AWS lambda Airflow Google Cloud Workflows Microsoft Flow Price Pay only for what you use
Current price
Free Tier: 4,000 state transitions per month
Use Cases Step Functions is an easy-to-use function orchestra that makes it possible to string Lambda functions and multiple AWS services into business-critical applications.
Step Functions manages the operations and underlying infrastructure for you to ensure your application is available at any scale.
With Step Functions, you are able to easily coordinate complex processes composed of different tasks.
Without using this service you have to coordinate each Lambda Function yourself and manage every kind of error in all steps of this complex process.
AWS Step Functions is a useful service for breaking down complex processes into smaller and easier tasks
Automate Extract, Transform, and Load (ETL) process Orchestrate microservices Workflow configuration AWS service integrations Component reuse Built-in error handling Type: Orches­tration, Workflows
Step Function Standard Workflows are optimized for long-running processes.
Express Workflows are better for event-driven workloads.
Practice Introduction to AWS Step Functions
Questions Q1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.
Which of the following Step Function workflows can be used to meet requirements?
Asynchronous Express Workflow Standard Workflow Synchronous Express Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/
3
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/step-functions/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/questions/":{title:"Questions",tags:[],content:`On this page you can find 50 random questions.
To get prepared for exam you can use cloud-exam-prepare.com
Q1 - Q10 Q1 You are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)____ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.
HTTP proxy mock AWS service proxy Lambda function Explanation http://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html
Amazon API Gateway supports mock integrations for API methods.
2
Q2 You are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.
What steps can you take in this situation? (Choose 2 answers)
Use a template to first create Resource A with the ARN as an output value. Use a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt. Hard code the ARN value output from creating Resource A into the second template. Just create Resource B. Explanation http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html
2
Q3 A company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.
Which API command can a developer use to ensure cache storage consistency with minimal duplication?
CreateReusableDelegationSet with Route 53 CreateStackSet with CloudFormation CreateGlobalReplicationGroup with ElastiCache CreateCachePolicy with CloudFront Explanation https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html
4
Q4 You are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.
What is the easiest way to develop these simple applications?
Install AWS SAM CLI and run “sam init [options]” with the templates’ data. Use AWS step function visual workflow and insert your templates in the states Save your template in the Serverless Application Repository and use AWS SAM Explanation AWS SAM - AWS Serverless Application Model
https://aws.amazon.com/serverless/sam/
1
Q5 What will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?
You will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups. Nothing will happen, as the custom deployment configuration was unused. All deployment groups associated with the custom deployment configuration will also be deleted. All deployments associated with the custom deployment configuration will be terminated. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html
Can delete only if unused.
1
Q6 What happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?
All details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks. The instances used in the deployment group will change. All details associated with that deployment group will also be deleted from AWS CodeDeploy. The instances that were participating in the deployment group will run once again. Explanation https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html
If you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.
3
Q7 You are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.
Which actions should you take to make this function perform correctly? (2 answers)
Restart all Amazon EC2 instances that are running a Windows operating system. Provide the IAM user credentials to integrate AWS CodePipeline. Fill out the required fields for your proxy host. Modify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system. Explanation https://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html
2, 3
Q8 You are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.
What action will best solve this issue?
Use AWS Lambda to send the time-based one-time password by SMS Increase the complexity of the password Create and assign a role with a policy that enables Cognito to send SMS messages to users Create and assign a role with a policy that enables Cognito to send Email messages to users Explanation https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html
3
Q9 A developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events
Which combination of actions should the developer take to satisfy these requirements? (Select TWO.)
Use Amazon Cognito to provide the sign-up and sign-in functionality Use AWS IAM to provide the sign-up and sign-in functionality Configure an AWS Config rule to make the API call triggered by the post-authentication event Invoke an Amazon API Gateway method to make the API call triggered by the post-authentication event Execute an AWS Lambda function to make the API call triggered by the post-authentication event Explanation Amazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.
1, 5
Q10 A developer is designing a web application that allows the users to post comments and receive in a real-time feedback.
Which architectures meet these requirements? (Select TWO.)
Create an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store. Create a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store Create an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets. Create a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store. Enable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store Explanation AWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.
AWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.
The WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.
1, 2
Q11 - Q20 1 You are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and ____ for your Amazon ECS clusters and the CPU and ____ metrics for your Amazon ECS services.
memory reservation and utilization; concurrent connections memory utilization; memory reservation and utilization concurrent connections; memory reservation and utilization memory reservation and utilization; memory utilization Explanation 4
2 What is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?
The metrics reported by CloudWatch are more difficult to report. Security concerns and network latency over the public internet. The ElastiCache cluster becomes more prone to failures. The performance of the ElastiCache cluster is no longer controllable. Explanation Elasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to ElastiCache is required for test or development purposes, it can be done through a VPN.
2
3 You are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.
What is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?
Use the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the aws cloudformation deploy command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation [AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)
3
4 Emily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.
Which of the following options would meet this requirement?
Use an Amazon Elastic File System (EFS) volume to serve the static image files. Configure the AWS ElasticBeanstalk proxy server to serve the static image files. Use an Amazon S3 bucket to serve the static image files. Use an Amazon Elastic Block Store (EBS) volume to serve the static image files. Explanation https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html
An Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.
2
5 A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers
Which AWS service should be used to refactor this architecture?
AWS X-Ray Amazon SQS AWS Lambda Amazon API Gateway Explanation 4
6 Which load balancer would you use for services which use HTTP or HTTPS traffic?
Explanation Application Load Balancer (ALB). 7 What are possible target groups for ALB (Application Load Balancer)?
Explanation EC2 tasks ECS instances Lambda functions Private IP Addresses 8 Your would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.
Which solution below would be best to implement for this?
Enable X-Forwarded For on the web servers and use a Classic Load Balancer Configure proxy servers to forward the traffic to the correct instances Use Classic Load Balancer with path-based routing rules to forward the traffic to the correct instances Use Application Load Balancer with host-based routing rules to forward the traffic to the correct instances Explanation Application Load Balancer with host-based routing rules
https://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/
4
9 A company uses Amazon DynamoDB for managing and tracking orders. DynamoDB table is partitioned based on the order date. The company receives a huge increase in orders during a sales event, causing DynamoDB writes to throttle, and the consumed throughput is below the provisioned throughput.
According to AWS best practices, how can this issue be resolved with MINIMAL costs?
Create a new Dynamo DB table for every order date Add a random number suffix to the partition key values Add a global secondary index to the DynamoDB table Increase the read and write capacity units of the DynamoDB table Explanation A randomizing strategy can greatly improve write throughput. But it’s difficult to read a specific item because you don’t know which suffix value was used when writing the item.
Choosing the Right DynamoDB Partition Key
Using write sharding to distribute workloads evenly
2
10 A food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.
Which approach best meets these requirements?
Use EventBridge with Kinesis Data Streams to send messages. Use a Step Function to send SQS messages. Use Lambda function to send SNS messages. Use AWS Batch and SNS to send messages. Explanation https://docs.aws.amazon.com/sns/latest/dg/welcome.html
3
Q21 - Q30 1 How AWS Fargate different from AWS ECS?
Explanation In AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances. While in AWS Fargate, you don\u0026rsquo;t provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.
2 What is Chaos Engineering?
Explanation Chaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.
Chaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.
It starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.
3 A client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.
What are the best practice guidelines for use of the root account?
Never use the root account. Use the root account only to create administrator accounts. Use the root account to create your first IAM user and then lock away the root account. Use the root account to create all other accounts, and share the root account with one backup administrator. Explanation lock-away-credentials 1
4 Veronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.
How should Veronika convert the query string parameters to arguments for the Lambda function?
Enable request validation Include the Amazon Resource Name (ARN) of the Lambda function Change the integration type Create a mapping template Explanation API Gateway mapping template
4
5 Your organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.
Which of the following services do you need to call from AWS STS service after you authenticate with your on-premise?
AssumeRoleWithSAML GetFederationToken AssumeRoleWithWebIdentity GetCallerIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html 1
6 Alice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.
Which of the following APIs will be leveraged to provide temporary security credentials?
AssumeRoleWithSAML GetFederationToken GetSessionToken AssumeRoleWithWebIdentity Explanation https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html
(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA
3
7 You built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.
What steps will you take to keep a strong performance?
Remove the hot shards Merge the hot shards Split the hot shards Increase the shard capacity Explanation https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html
Split the hot shards
3
8 Jasmin needs to perform ad-hoc business analytics queries on well-structured dat1. Data comes in constantly at a high velocity. Jasmin\u0026rsquo;s team can understand SQL.
What AWS service(s) should Jasmin look to first?
EMR using Hive EMR running Apache Spark Kinesis Firehose + RDS Kinesis Firehose + RedShift Explanation RedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol
https://aws.amazon.com/kinesis/data-firehose/features/
4
9 Key rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?
KMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key. KMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys. Key rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed. Key rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key. Explanation When you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key\u0026rsquo;s older cryptographic material so it can be used to decrypt data that it has encrypted.
10 Alan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.
Which statements are true with regard to this scenario? (Choose 2 answers)
Create Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts. Can share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys. Make encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts. You can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used. Explanation https://docs.aws.amazon.com/kms/latest/developerguide/overview.html
1, 2
Q31 - Q40 1 When working with a published version of the AWS Lambda function, you should note that the _____.
Use the AWS Management Console to create and configure the cluster. Create a cron job to schedule the cluster deployment using the _aws cloudformation deploy_ command Create a configuration file with the .config extension and place it into the .ebextensions folder in the application package. Build an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster. Explanation AWS Secrets Manager
3
2 A Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.
Which of the following enables monitoring of the metric in Amazon CloudWatch?
Retrieve the log data from AWS CloudTrail using the LookupEvents API Call Retrieve the log data from CloudWatch using the GetMetricData API call Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application Install the Amazon CloudWatch logs agent on the EC2 instance that the application is running on Explanation 4
3 A developer is building a streamlined development process for Lambda functions related to S3 storage. The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.
Which of the following AWS solutions offers is best suited for this objective?
AWS Software Development Kits (SDKs) AWS Serverless Application Model (SAM) templates AWS Systems Manager AWS Step Functions Explanation Serverless Application Model
2
4 Explain RDS Multi Availability Zone
Explanation RDS multi AZ used mainly for disaster recovery purposes There is an RDS master instance and in another AZ an RDS standby instance The data is synced synchronously between them The user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically 5 Developer wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.
Which methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)
Control access based on the time of day Control access based on IP Address Control access based on Active Directory group Control access based on CIDR block Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html CIDRs - A set of Classless Inter-Domain Routings
https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html
1, 2, 4
6 To ensure that an encryption key was not corrupted in transit, Elastic Transcoder uses a(n) ____ digest of the decryption key as a checksum.
BLAKE2 SHA-1 SHA-2 MD5 Explanation https://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html
MD5 digest (or checksum)
4
7 Dan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.
What are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)
Use the encrypted SSL/TLS endpoint. Enable encryption in the bucket policy. Encrypt it on the client-side before uploading. Set the server-side encryption option on upload. Explanation https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html
1, 3
8 In a move toward using microservices, a company’s Management team has asked all Development teams to build their services so that API requests depend only on that services data store. One team is building a Payments service that has its own database. The service floods data that originates in the Accounts database. Both are using Amazon DynamoDB.
What approach will result in the simplest, decoupled, and reliable method to get near-real-time updates from the Accounts database?
Use Amazon Glue to perform frequent updates from the Accounts database to the Payments database Use Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database. Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database. Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database. Explanation 3
9 You’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.
Which options are supported notification methods? (Choose 3 answers)
HTTP or HTTPS POST notifications Email using SMTP or plain text Kinesis Stream Invoking of a Lambda function Explanation https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html
1, 2, 4
10 Which endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?
SNS E-Mail SQS Kinesis Explanation https://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html
3
Q41 - Q50 1 A developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.
Which of the following Step Function workflows can be used to meet requirements?
Asynchronous Express Workflow Synchronous Express Workflow Standard Workflow Standard Express Workflow Explanation https://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/
2
2 You joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.
Which AWS service will be your main tool?
AWS X-Ray AWS IAM AWS CloudTrail AWS Athena Explanation AWS X-Ray provides graphs of system performance and identifies bottlenecks
1
3 A team of Developers must migrate an application running inside an AWS Elastic Beastalk environment from a Classic Load Balancer to an Application Load Balancer.
Which steps should be taken to accomplish the task using the AWS Management Console?
1 Select a new load balancer type before running the deployment. Update the application code in the existing deployment. Deploy the new version of the application code to the environment. 2 Create a new environment with the same configurations except for the load balancer type. Deploy the same application versions as used in the original environment. Run the Swap-environment-cnames action. 3 Clone the existing environment, changing the associated load balancer type. Deploy the same application version as used in the original environment. Run the Swap-environment-cnames action. 4 Edit the environment definitions in the existing deployment. Change the associated load balancer type according to the requirements. Rebuild the environment with the new load balancer type. Explanation 4
4 A developer is deploying an application that will store files in an Amazon S3 bucket. The files must be encrypted at rest. The developer wants to automatically replicate the files to an S3 bucket in a different AWS Region for disaster recovery.
How can the developer accomplish this task with the LEAST amount of configuration?
Encrypt the files by using server-side encryption with S3 managed encryption keys (SSE-S3). Enable S3 bucket replication. Encrypt the files by using server-side encryption (SSE) with an AWS Key Management Service (AWS KMS) customer master key (CMK). Enable S3 bucket replication. Use the s3 sync command to sync the files to the S3 bucket in the other Region. Configure an S3 Lifecycle configuration to automatically transfer files to the S3 bucket in the other Region. Explanation 2
5 A serverless application is using AWS Step Functions to process data and save it to a database. The application needs to validate some data with an external service before saving the dat1. The application will call the external service from an AWS Lambda function, and the external service will take a few hours to validate the dat1. The external service will respond to a webhook when the validation is complete.
A developer needs to pause the Step Functions workflow and wait for the response from the external service.
What should the developer do to meet this requirement?
Use the .waitForTaskToken option in the Lambda function task state. Pass the token in the body. Use the .waitForTaskToken option in the Lambda function task state. Pass the invocation request. Call the Lambda function in synchronous mode. Wait for the external service to complete the processing. Call the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing. Explanation 4
6 A company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.
What is causing the duplicate entries?
The S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket. The Lambda function did not run correctly, and Lambda retried the invocation with a delay. Amazon S3 is delivering the same event multiple times. The application stopped intermittently and then resumed, splitting the logs into multiple smaller files. Explanation 1
7 An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function.
How can the developer inspect the timing of the DynamoDB API calls?
Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs Limit Lambda to no more than five concurrent invocations. Monitor from the Lambda console. Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service. Explanation 4
8 A developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance. How can the application find this information?
Query the instance metadata from http://169.254.169.254/latest/meta-data/. Query the instance user data from http://169.254.169.254/latest/user-data/. Query the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/. Check the hosts file of the operating system. Explanation 1
9 A developer is creating a serverless application that uses an AWS Lambda function The developer will use AWS CloudFormation to deploy the application. The application will write logs to Amazon CloudWatch Logs. The developer has created a log group in a CloudFormation template for the application to use. The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime.
Which solution will meet this requirement?
Use the AWS::Include transform in CloudFormation to provide the log group\u0026rsquo;s name to the application. Pass the log group\u0026rsquo;s name to the application in the user data section of the CloudFormation template Use the CloudFormation template\u0026rsquo;s Mappings section to specify the log group\u0026rsquo;s name for the application. Pass the log group\u0026rsquo;s Amazon Resource Name (ARN) as an environment variable to the Lambda function. Explanation 4
10 A developer needs to use the AWS CLI on an on-premises development server temporarily to access AWS services while performing maintenance. The developer needs to authenticate to AWS with their identity for several hours.
What is the MOST secure way to call AWS CLI commands with the developer\u0026rsquo;s IAM identity?
Specify the developer\u0026rsquo;s IAM access key ID and secret access key as parameters for each CLI command. Run the AWS configure CLI command. Provide the developer\u0026rsquo;s IAM access key ID and secret access key. Specify the developer\u0026rsquo;s IAM profile as a parameter for each CLI command. Run the get-session-token CLI command with the developer\u0026rsquo;s IAM user. Use the returned credentials to call the CLI. Explanation 4
Q51 - Q60 1 Explanation 2 Explanation 3 Explanation 4 Explanation 5 Explanation 6 A developer notices timeouts from the AWS CLI when the developer runs list commands.
What should the developer do to avoid these timeouts?
Use the --page-size parameter to request a smaller number of items. Use shorthand syntax to separate the list by a single space. Use the yaml-stream output for faster viewing of large datasets. Use quotation marks around strings to enclose data structure. Explanation 1
7 A company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.
Which CodeDeploy predefined configuration will meet these requirements?
CodeDeployDefault.ECSCanary10Percent15Minutes CodeDeployDefault.LambdaCanary10Percent5Minutes CodeDeployDefault.LambdaCanary10Percent15Minutes CodeDeployDefault.ECSLinear10PercentEvery1 Minutes Explanation 1
8 Explanation 9 A microservices application is deployed across multiple containers in Amazon Elastic Container Service (Amazon ECS). To improve performance, a developer wants to capture trace information between the microservices and visualize the microservices architecture.
Which solution will meet these requirements?
Build the container from the amazon/aws-xray-daemon base image. Use the AWS X-Ray SDK to instrument the application. Install the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices. Install the AWS X-Ray daemon on each of the ECS instances. Configure AWS CloudTrail data events to capture the traffic between the microservices. Explanation 3
10 A company is running an application on Amazon Elastic Container Service (Amazon ECS). When the company deploys a new version of the application, the company initially needs to expose 10% of live traffic to the new version. After a period of time, the company needs to immediately route all the remaining live traffic to the new version.
Which ECS deployment should the company use to meet these requirements?
Rolling update Blue/green with canary Blue/green with all at once Blue/green with linear Explanation 2
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/questions/"},"https://romankurnovskii.com/en/categories/algorithms/":{title:"Algorithms",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/algorithms/"},"https://romankurnovskii.com/en/authors/":{title:"Authors",tags:[],content:"",url:"https://romankurnovskii.com/en/authors/"},"https://romankurnovskii.com/en/categories/":{title:"Categories",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/"},"https://romankurnovskii.com/en/tags/combinatorics/":{title:"Combinatorics",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/combinatorics/"},"https://romankurnovskii.com/en/series/combinatorics/":{title:"Combinatorics",tags:[],content:"",url:"https://romankurnovskii.com/en/series/combinatorics/"},"https://romankurnovskii.com/en/tags/dynamic-programming/":{title:"Dynamic Programming",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/dynamic-programming/"},"https://romankurnovskii.com/en/series/dynamic-programming/":{title:"Dynamic Programming",tags:[],content:"",url:"https://romankurnovskii.com/en/series/dynamic-programming/"},"https://romankurnovskii.com/en/tags/math/":{title:"Math",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/math/"},"https://romankurnovskii.com/en/series/math/":{title:"Math",tags:[],content:"",url:"https://romankurnovskii.com/en/series/math/"},"https://romankurnovskii.com/en/categories/medium/":{title:"Medium",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/medium/"},"https://romankurnovskii.com/en/":{title:"Roman Kurnovskii",tags:[],content:"",url:"https://romankurnovskii.com/en/"},"https://romankurnovskii.com/en/series/":{title:"Series",tags:[],content:"",url:"https://romankurnovskii.com/en/series/"},"https://romankurnovskii.com/en/tags/":{title:"Tags",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/"},"https://romankurnovskii.com/en/series/cheatsheet/":{title:"cheatsheet",tags:[],content:"",url:"https://romankurnovskii.com/en/series/cheatsheet/"},"https://romankurnovskii.com/en/categories/cheatsheet/":{title:"cheatsheet",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cheatsheet/"},"https://romankurnovskii.com/en/tags/git/":{title:"git",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/git/"},"https://romankurnovskii.com/en/categories/git/":{title:"Git",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/git/"},"https://romankurnovskii.com/en/posts/git-snippets/":{title:"Git snippets",tags:["git"],content:`Free space in git repo Download BFG
Remove history files bigger than 100Kb:
cd repo java -jar bfg-1.14.0.jar --strip-blobs-bigger-than 100K . git reflog expire --expire=now --all \u0026amp;\u0026amp; git gc --prune=now --aggressive Removing an entire commit:
Replace \u0026ldquo;SHA\u0026rdquo; with the reference you want to get rid of. The \u0026ldquo;^\u0026rdquo; in that command is literal.
git rebase -p --onto SHA^ SHA We want to remove commits 2 \u0026amp; 4 from the repo. (Higher the the number newer the commit; 0 is the oldest commit and 4 is the latest commit)
commit 0 : b3d92c5 commit 1 : 2c6a45b commit 2 : \u0026lt;any_hash\u0026gt; commit 3 : 77b9b82 commit 4 : \u0026lt;any_hash\u0026gt; Note: You need to have admin rights over the repo since you are using --hard and -f.
git checkout b3d92c5 Checkout the last usable commit. git checkout -b repair Create a new branch to work on. git cherry-pick 77b9b82 Run through commit 3. git cherry-pick 2c6a45b Run through commit 1. git checkout master Checkout master. git reset --hard b3d92c5 Reset master to last usable commit. git merge repair Merge our new branch onto master. git push -f origin master Push master to the remote repo. If didn\u0026rsquo;t publish changes, to remove the latest commit, do:
git rebase -i HEAD~\u0026lt;number of commits to go back\u0026gt; git rebase -i \u0026lt;CommitId\u0026gt;~1 git reset --hard HEAD^ git reset --hard commitId git rebase -i HEAD~5 If already published to-be-deleted commit:
git revert HEAD
Cleanups:
git stash clear git reflog expire --expire-unreachable=now --all git fsck --full git fsck --unreachable	# Will show you the list of what will be deleted git gc --prune=now	# Cleanup unnecessary files and optimize the local repository Common git commands git rev-list --all --count # count commits git clean -fd # To remove all untracked (non-git) files and folders! Resources https://sethrobertson.github.io/GitFixUm/fixup.html https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-clone.html https://passingcuriosity.com/2017/truncating-git-history/ https://www.npmjs.com/package/clear-git-branch?activeTab=explore `,url:"https://romankurnovskii.com/en/posts/git-snippets/"},"https://romankurnovskii.com/en/p/links/":{title:"Links",tags:[],content:` The curated list of AI tools for marketing
Discover the newest AIs for any given task
Fast and simple way to visualize your story - free 1 Storyboard and 10 Frames https://airtrackbot.com/
Create 1,000 AI art images a day for free
Create and sell your own personalized books and journals
Turn any body of text into a few sentences with one click - Chrome extension
Block Diagram Maker `,url:"https://romankurnovskii.com/en/p/links/"},"https://romankurnovskii.com/en/p/":{title:"Ps",tags:[],content:"",url:"https://romankurnovskii.com/en/p/"},"https://romankurnovskii.com/en/tags/array/":{title:"Array",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/array/"},"https://romankurnovskii.com/en/series/array/":{title:"Array",tags:[],content:"",url:"https://romankurnovskii.com/en/series/array/"},"https://romankurnovskii.com/en/tags/greedy/":{title:"Greedy",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/greedy/"},"https://romankurnovskii.com/en/tags/sorting/":{title:"Sorting",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/sorting/"},"https://romankurnovskii.com/en/series/sorting/":{title:"Sorting",tags:[],content:"",url:"https://romankurnovskii.com/en/series/sorting/"},"https://romankurnovskii.com/en/tags/divide-and-conquer/":{title:"Divide and Conquer",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/divide-and-conquer/"},"https://romankurnovskii.com/en/tags/recursion/":{title:"Recursion",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/recursion/"},"https://romankurnovskii.com/en/tags/code/":{title:"Code",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/code/"},"https://romankurnovskii.com/en/posts/code-style/":{title:"Code style notes",tags:["Code"],content:`Release notes example Changed
feat(exports): export mergeConfig #5151 Fixed
fix(CancelledError): include config #4922 fix(general): removing multiple/trailing/leading whitespace #5022 fix(headers): decompression for responses without Content-Length header #5306 fix(webWorker): exception to sending form data in web worker #5139 Refactors
refactor(types): AxiosProgressEvent.event type to any #5308 refactor(types): add missing types for static AxiosError.from method #4956 Chores
chore(docs): remove README link to non-existent upgrade guide #5307 chore(docs): typo in issue template name #5159 Code format Python style JavaScript style `,url:"https://romankurnovskii.com/en/posts/code-style/"},"https://romankurnovskii.com/en/series/hash-table/":{title:"Hash table",tags:[],content:"",url:"https://romankurnovskii.com/en/series/hash-table/"},"https://romankurnovskii.com/en/tags/hash-table/":{title:"Hash Table",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/hash-table/"},"https://romankurnovskii.com/en/tags/string/":{title:"String",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/string/"},"https://romankurnovskii.com/en/series/string/":{title:"String",tags:[],content:"",url:"https://romankurnovskii.com/en/series/string/"},"https://romankurnovskii.com/en/tags/matrix/":{title:"Matrix",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/matrix/"},"https://romankurnovskii.com/en/tags/backtracking/":{title:"Backtracking",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/backtracking/"},"https://romankurnovskii.com/en/docs/disser/canditate-minimum/01-economic-theory/":{title:"Раздел 1. Экономическая теория.",tags:[],content:` РУДН - https://econ.rudn.ru/general_information/cathedras/ekonomicheskih_otnosheniy/kandidatskiy_minimum_2016_vesna_osen_voprosy/ ВШЭ - https://we.hse.ru/phd_programm МГИМО - https://mgimo.ru/upload/2021/04/progr_kand-ehkz-08-00-14-meo-i-ves.pdf Перечень вопросов, выносимых на кандидатский минимум по специальности 08.00.14 Мировая экономика:
Раздел 1. Экономическая теория. (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)
Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле. Адам Смит: теория абсолютного преимущества в торговле. Давид Рикардо: теория сравнительного преимущества в торговле. Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары. Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле. Меркантилизм (15-17 вв) - это экономическая политика, цель которой — накопление в стране драгоценных металлов, средство достижения цели – активный торговый баланс, то есть превышение экспорта над импортом.
Необходимость активного вмешательства государства в хозяйственную деятельность, в основном в форме протекционизма: установления высоких импортных пошлин, выдачи субсидий национальным производителям и так далее. 1. Ранний меркантилизм (конец XV — середина XVI века).
Представители: У. Стаффорд, Де Сантис, Г. Скаруффи. В этот период в учении преобладает теория денежного баланса, в рамках которой было закреплено увеличение национального благосостояния законодательным путем: устанавливался запрет на вывоз золота и серебра за границу. Деньги выполняли только функцию средства накопления.
2. Поздний меркантилизм (вторая половина XVI — начало XVII века).
Представители: Т. Ман, А. Серра, А. де Монкретьен.
Ими была создана теория торгового баланса, который обеспечивался путем активной внешней торговли. Главенствовал принцип: покупать дешевле в одной стране и продавать дороже в другой. Вывоз денежных средств за границу был разрешен. Деньгам отводились функции средства накопления и средства обращения — поздний меркантилизм трактовал деньги как капитал и признавал их товаром.
Поздний меркантилизм был прогрессивным. Он содействовал развитию торговли, судостроения, экспортной промышленности, международного разделения труда.
Основные принципы:
— регулирование внешней торговли с целью притока в страну золота и серебра; — поддержка промышленности путем импорта дешевого сырья; — протекционизм; — поощрение экспорта готовой продукции; — рост населения для поддержания низкого уровня зарплаты; — рассмотрение проблем сферы обращения в отрыве от сферы производства; — достижение экономического роста путем приумножения денежного богатства страны через государственное регулирование внешней торговли и достижение положительного сальдо торгового баланса.
Преобладал в странах Западной Европы (преимущественно Франции, Италии и Англии). В России одним из приверженцев идей меркантилизма был выдающийся государственный деятель А. Л. Ордын-Нащекин (1605—1680).
Игра с нулевой суммой — это противоположность беспроигрышным ситуациям — таким как торговое соглашение, которое значительно увеличивает торговлю между двумя странами — или проигрышным ситуациям, таким как война, например. В реальной жизни, однако, не всегда все так очевидно, и зачастую сложно измерить прибыли и убытки.
Игра с нулевой суммой — это ситуация, когда, если одна сторона проигрывает, другая сторона выигрывает, а чистое изменение богатства равно нулю.
Источники:
https://www.banki.ru/wikibank/merkantilizm/ Меркантелизм / годы /страны Адам Смит: теория абсолютного преимущества в торговле. Исследование о природе и причинах богатства народов (1776 г.) - основная работа шотландского экономиста Адама Смита.
А. Смит (1723—1790) распространил и на мирохозяйственную сферу, впервые теоретически обосновав принцип абсолютных преимуществ (или абсолютных издержек)
«Основное правило каждого благоразумного главы семьи состоит в том, чтобы не пытаться изготовить дома такие предметы, изготовление которых обойдется дороже, чем при покупке их на стороне\u0026hellip; То, что представляется разумным в образе действия любой частной семьи, вряд ли может оказаться неразумным для всего королевства. Если какая-либо чужая страна может снабдить нас каким-нибудь товаром по более дешевой цене, чем мы в состоянии изготовить его, гораздо лучше покупать его у нее на некоторую часть продукта нашего собственного промышленного труда, прилагаемого в той области, в которой мы обладаем некоторым преимуществом»
Основой развития международной торговли служит различие в абсолютных издержках. Торговля будет приносить экономический эффект, если товары будут ввозиться из страны, где издержки абсолютно меньше, а вывозиться те товары, издержки которых в данной стране ниже, чем за рубежом.
Благосостояние наций зависит не столько от количества накопленного ими золота, сколько от их способностей производить конечные товары и услуги.
Основные положения А.Смита в теории международной торговли:
правительствам не следует вмешиваться во внешнюю торговлю, поддерживая режим открытых рынков и свободы торговли; нации, так же как и частные лица, должны специализироваться на изготовлении товаров, в производстве которых у них есть абсолютные преимущества, и торговать ими в обмен на товары, абсолютным преимуществом в производстве которых обладают другие нации; концентрация усилий (ресурсов) стран на производстве товаров, по которым страны имеют абсолютное преимущество, приводит к увеличению общих объемов производства, росту обмена между странами продуктами своего труда; свободная торговля между странами обусловливает эффективное распределение мировых ресурсов, обеспечивая прибыль любой и каждой торгующей стране. Давид Рикардо: теория сравнительного преимущества в торговле. Теория сформулированна Давидом Рикардо (1772-1823) (классик политической экономии, последователь и одновременно оппонент Адама Смита) в начале XIX века.
Давид Рикардо развил теорию абсолютных преимуществ Адама Смита и показал, что торговля выгодна каждой из двух стран, даже если одна из них не обладает абсолютным преимуществом в производстве любых конкретных товаров. Специализация на производстве товара, имеющего максимальные сравнительные преимущества, выгодна, даже если нет абсолютных преимуществ.
Теория сравнительных преимуществ на примере двух стран и двух товаров
Временные затраты на производство единицы товара:
Сыр (в ед. Вина) Вино (в ед. Сыра) Франция 2 1 Испания 4 3 В данном случае во Франции затраты времени в производстве обоих товаров меньше (она обладает абсолютным преимуществом). Согласно А. Смиту, торговля между странами принесёт выгоды только Франции. Однако, с точки зрения теории сравнительных преимуществ Д. Рикардо, при определённом соотношении цен между товарами, торговля может приводить к взаимной выгоде обеих стран даже при абсолютном преимуществе только одной из них.
Рассчитаем альтернативные цены производства каждого из товаров в каждой стране:
Альтернативная цена производства единицы товара:
Сыр (в ед. Вина) Вино (в ед. Сыра) Франция 2 / 1 1 / 2 Испания 4 / 3 3 / 4 В данном случае одна единица сыра (например, килограмм) во Франции стоит 2 единицы вина (2 бутылки), а в Испании единица сыра стоит дешевле (4 / 3 единицы вина). В то же время единица вина в Испании стоит 3 / 4 единицы сыра, что дороже чем во Франции. Таким образом, если Франция будет производить вино для Испании, а Испания — сыр для Франции, то обе страны выиграют трудовые ресурсы. На каждой закупленной единице сыра Франция будет экономить 2 - 4 /3 = 2/3 единицы вина, а Испания 3/4-1/2=1/4 единицы сыра на каждой закупленной единице вина.
Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары. Теория Хекшера — Олина (теория соотношения факторов производства) - каждая страна экспортирует товары, для производства которых она обладает относительно избыточными факторами производства, и импортирует товары, для производства которых она испытывает относительный недостаток факторов производства.
Источники:
https://studfile.net/preview/9266193/page:7/
`,url:"https://romankurnovskii.com/en/docs/disser/canditate-minimum/01-economic-theory/"},"https://romankurnovskii.com/en/tags/bit-manipulation/":{title:"Bit Manipulation",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/bit-manipulation/"},"https://romankurnovskii.com/en/tags/python/":{title:"Python",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/python/"},"https://romankurnovskii.com/en/categories/python/":{title:"Python",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/python/"},"https://romankurnovskii.com/en/posts/python-bitwise-operators/":{title:"Python bitwise operators",tags:["Python"],content:"\u0026laquo; Left Shift Moves the bits of its first operand to the left by the number of places specified in its second operand.\nShifting a single bit to the left by one place doubles its value. Shifting to two places to the left by one place quadruple its value. $ a \u0026laquo; n = a * 2^n $\n1\u0026gt;\u0026gt;\u0026gt; 100 \u0026lt;\u0026lt; 1 2200 3\u0026gt;\u0026gt;\u0026gt; 100 \u0026lt;\u0026lt; 2 4400 5\u0026gt;\u0026gt;\u0026gt; 100 \u0026lt;\u0026lt; 3 6800 \u0026raquo; Right Shift The rightmost bits always get dropped. Every time you shift a bit to the right by one position, you halve its underlying value. 1\u0026gt;\u0026gt;\u0026gt; 100 \u0026gt;\u0026gt; 1 250 3\u0026gt;\u0026gt;\u0026gt; 100 \u0026gt;\u0026gt; 2 425 5\u0026gt;\u0026gt;\u0026gt; 100 \u0026gt;\u0026gt; 3 612 7\u0026gt;\u0026gt;\u0026gt; 5 \u0026gt;\u0026gt; 10 80 $ a \u0026raquo; n = [a/2^n] $\nthe right shift operator automatically floors the result. 1\u0026gt;\u0026gt;\u0026gt; 5 \u0026gt;\u0026gt; 1 # Bitwise right shift 22 3\u0026gt;\u0026gt;\u0026gt; 5 // 2 # Floor division (integer division) 42 5\u0026gt;\u0026gt;\u0026gt; 5 / 2 # Floating-point division 62.5 7\u0026gt;\u0026gt;\u0026gt; -2 \u0026gt;\u0026gt; 5 8-1 Resources https://realpython.com/python-bitwise-operators/ ",url:"https://romankurnovskii.com/en/posts/python-bitwise-operators/"},"https://romankurnovskii.com/en/tags/string-matching/":{title:"String Matching",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/string-matching/"},"https://romankurnovskii.com/en/tags/two-pointers/":{title:"Two Pointers",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/two-pointers/"},"https://romankurnovskii.com/en/tags/linked-list/":{title:"Linked List",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/linked-list/"},"https://romankurnovskii.com/en/tags/medium/":{title:"Medium",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/medium/"},"https://romankurnovskii.com/en/series/two-pointers/":{title:"Two Pointers",tags:[],content:"",url:"https://romankurnovskii.com/en/series/two-pointers/"},"https://romankurnovskii.com/en/tags/binary-tree/":{title:"Binary Tree",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/binary-tree/"},"https://romankurnovskii.com/en/tags/depth-first-search/":{title:"Depth-First Search",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/depth-first-search/"},"https://romankurnovskii.com/en/categories/easy/":{title:"Easy",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/easy/"},"https://romankurnovskii.com/en/tags/stack/":{title:"Stack",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/stack/"},"https://romankurnovskii.com/en/tags/tree/":{title:"Tree",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/tree/"},"https://romankurnovskii.com/en/tags/memoization/":{title:"Memoization",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/memoization/"},"https://romankurnovskii.com/en/tags/binary-search/":{title:"Binary Search",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/binary-search/"},"https://romankurnovskii.com/en/p/supportme/":{title:"Support me",tags:[],content:" https://www.buymeacoffee.com/romankurnovskii https://rom.gumroad.com/ https://www.patreon.com/user?u=79828420 ",url:"https://romankurnovskii.com/en/p/supportme/"},"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/chart/":{title:"chart",tags:["Hugo"],content:`Display Chart.js diagrams/blocks
Sources
`,url:"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/chart/"},"https://romankurnovskii.com/en/tags/hugo/":{title:"hugo",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/hugo/"},"https://romankurnovskii.com/en/categories/hugo-shorcodes/":{title:"Hugo shorcodes",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/hugo-shorcodes/"},"https://romankurnovskii.com/en/categories/hugo/":{title:"Hugo",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/hugo/"},"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/":{title:"Hugo shortcode examples",tags:["Hugo"],content:`Source code of examples can be found in the repo
Image - insert resizable image in post `,url:"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/"},"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/img/":{title:"img",tags:["Hugo"],content:` My image float right, tall Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
My image float right Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
My image text Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
My image float left Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\u0026rsquo;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.
`,url:"https://romankurnovskii.com/en/posts/hugo-shortcode-examples/img/"},"https://romankurnovskii.com/en/tags/sliding-window/":{title:"Sliding Window",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/sliding-window/"},"https://romankurnovskii.com/en/categories/roadmaps/":{title:"Roadmaps",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/roadmaps/"},"https://romankurnovskii.com/en/series/aws-exam-quizz/":{title:"AWS exam quizz",tags:[],content:"",url:"https://romankurnovskii.com/en/series/aws-exam-quizz/"},"https://romankurnovskii.com/en/apps/cloud-exam-quizz/":{title:"Cloud exam Quizz",tags:[],content:`Goal: Check if you are ready to pass Cloud exam
Goal: Check if you are ready to pass the Cloud exam
The application calculates progress after each answered question. Ability to answer at least one question and get a comment at the same time. No need to pass all questions before. It is convenient to spend 20 min a day Works from web/tablet/mobile Link: https://www.cloud-exam-prepare.com
`,url:"https://romankurnovskii.com/en/apps/cloud-exam-quizz/"},"https://romankurnovskii.com/en/posts/interactivebrokers-deposit/":{title:"Deposit Interactive Brokers from Israel Discount bank",tags:["interactivebrokers","invest"],content:`Web Create IB notification Login to https://www.interactivebrokers.co.uk/portal/#/ Click Deposit Click Use a new deposit method if no one exist Bank Wire -\u0026gt; Get instructions Account Number: Bank account number
Next you get Bank Wire Instructions These data you need to make a payment from Discount bank
Send money from Discount bank Login start.telebank.co.il Click: ביצוע העברה
Fill the form
Click המשך and proceed `,url:"https://romankurnovskii.com/en/posts/interactivebrokers-deposit/"},"https://romankurnovskii.com/en/tags/interactivebrokers/":{title:"interactivebrokers",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/interactivebrokers/"},"https://romankurnovskii.com/en/tags/invest/":{title:"invest",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/invest/"},"https://romankurnovskii.com/en/tags/aws/":{title:"AWS",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/aws/"},"https://romankurnovskii.com/en/categories/aws/":{title:"AWS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/aws/"},"https://romankurnovskii.com/en/tags/step-functions/":{title:"Step Functions",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/step-functions/"},"https://romankurnovskii.com/en/categories/step-functions/":{title:"Step Functions",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/step-functions/"},"https://romankurnovskii.com/en/tags/sns/":{title:"SNS",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/sns/"},"https://romankurnovskii.com/en/categories/sns/":{title:"SNS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/sns/"},"https://romankurnovskii.com/en/tags/sqs/":{title:"SQS",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/sqs/"},"https://romankurnovskii.com/en/categories/sqs/":{title:"SQS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/sqs/"},"https://romankurnovskii.com/en/tags/amazon-eventbridge/":{title:"Amazon EventBridge",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/amazon-eventbridge/"},"https://romankurnovskii.com/en/categories/amazon-eventbridge/":{title:"Amazon EventBridge",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/amazon-eventbridge/"},"https://romankurnovskii.com/en/tags/amazon-key-management-service/":{title:"Amazon Key Management Service",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/amazon-key-management-service/"},"https://romankurnovskii.com/en/categories/amazon-key-management-service/":{title:"Amazon Key Management Service",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/amazon-key-management-service/"},"https://romankurnovskii.com/en/tags/cognito/":{title:"Cognito",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cognito/"},"https://romankurnovskii.com/en/categories/cognito/":{title:"Cognito",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cognito/"},"https://romankurnovskii.com/en/tags/route-53/":{title:"Route 53",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/route-53/"},"https://romankurnovskii.com/en/categories/route-53/":{title:"Route 53",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/route-53/"},"https://romankurnovskii.com/en/tags/eks/":{title:"EKS",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/eks/"},"https://romankurnovskii.com/en/categories/eks/":{title:"EKS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/eks/"},"https://romankurnovskii.com/en/tags/fargate/":{title:"Fargate",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/fargate/"},"https://romankurnovskii.com/en/categories/fargate/":{title:"Fargate",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/fargate/"},"https://romankurnovskii.com/en/tags/ecr/":{title:"ecr",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/ecr/"},"https://romankurnovskii.com/en/categories/ecr/":{title:"ecr",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/ecr/"},"https://romankurnovskii.com/en/tags/ecs/":{title:"ecs",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/ecs/"},"https://romankurnovskii.com/en/categories/ecs/":{title:"ecs",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/ecs/"},"https://romankurnovskii.com/en/tags/elastic-container-registry/":{title:"Elastic Container Registry",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/elastic-container-registry/"},"https://romankurnovskii.com/en/categories/elastic-container-registry/":{title:"Elastic Container Registry",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/elastic-container-registry/"},"https://romankurnovskii.com/en/tags/elastic-container-service/":{title:"Elastic Container Service",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/elastic-container-service/"},"https://romankurnovskii.com/en/categories/elastic-container-service/":{title:"Elastic Container Service",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/elastic-container-service/"},"https://romankurnovskii.com/en/tags/fault-injection-simulator/":{title:"Fault Injection Simulator",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/fault-injection-simulator/"},"https://romankurnovskii.com/en/categories/fault-injection-simulator/":{title:"Fault Injection Simulator",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/fault-injection-simulator/"},"https://romankurnovskii.com/en/tags/fis/":{title:"FIS",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/fis/"},"https://romankurnovskii.com/en/tags/xray/":{title:"xray",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/xray/"},"https://romankurnovskii.com/en/categories/xray/":{title:"xray",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/xray/"},"https://romankurnovskii.com/en/tags/codeartifact/":{title:"codeartifact",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codeartifact/"},"https://romankurnovskii.com/en/categories/codeartifact/":{title:"codeartifact",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codeartifact/"},"https://romankurnovskii.com/en/tags/codeguru/":{title:"codeguru",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codeguru/"},"https://romankurnovskii.com/en/categories/codeguru/":{title:"codeguru",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codeguru/"},"https://romankurnovskii.com/en/tags/codestar/":{title:"codestar",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codestar/"},"https://romankurnovskii.com/en/categories/codestar/":{title:"codestar",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codestar/"},"https://romankurnovskii.com/en/tags/codebuild/":{title:"codebuild",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codebuild/"},"https://romankurnovskii.com/en/categories/codebuild/":{title:"codebuild",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codebuild/"},"https://romankurnovskii.com/en/tags/codecommit/":{title:"codecommit",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codecommit/"},"https://romankurnovskii.com/en/categories/codecommit/":{title:"codecommit",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codecommit/"},"https://romankurnovskii.com/en/tags/codedeploy/":{title:"codedeploy",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codedeploy/"},"https://romankurnovskii.com/en/categories/codedeploy/":{title:"codedeploy",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codedeploy/"},"https://romankurnovskii.com/en/tags/codepipeline/":{title:"codepipeline",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/codepipeline/"},"https://romankurnovskii.com/en/categories/codepipeline/":{title:"codepipeline",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/codepipeline/"},"https://romankurnovskii.com/en/tags/cloudformation/":{title:"cloudformation",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cloudformation/"},"https://romankurnovskii.com/en/categories/cloudformation/":{title:"cloudformation",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cloudformation/"},"https://romankurnovskii.com/en/tags/cloudwatch/":{title:"cloudwatch",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cloudwatch/"},"https://romankurnovskii.com/en/categories/cloudwatch/":{title:"cloudwatch",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cloudwatch/"},"https://romankurnovskii.com/en/tags/javascript/":{title:"JavaScript",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/javascript/"},"https://romankurnovskii.com/en/categories/javascript/":{title:"JavaScript",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/javascript/"},"https://romankurnovskii.com/en/posts/js-snippets/":{title:"JavaScript code snippets",tags:["js","javascript","typescript"],content:"Web / Browser get base URL 1const getBaseURL = url =\u0026gt; url.replace(/[?#].*$/, \u0026#39;\u0026#39;); 2 3getBaseURL(\u0026#39;http://url.com/page?name=Adam\u0026amp;surname=Smith\u0026#39;); 4// \u0026#39;http://url.com/page\u0026#39; 5 6const url = new URL(\u0026#34;https://example.com/login?user=someguy\u0026amp;page=news\u0026#34;); 7 8url.origin // \u0026#34;https://example.com\u0026#34; 9url.host // \u0026#34;example.com\u0026#34; 10url.protocol // \u0026#34;https:\u0026#34; 11url.pathname // \u0026#34;/login\u0026#34; 12url.searchParams.get(\u0026#39;user\u0026#39;) // \u0026#34;someuser\u0026#34; get URL parameters as object 1const getURLParameters = url =\u0026gt; 2 (url.match(/([^?=\u0026amp;]+)(=([^\u0026amp;]*))/g) || []).reduce( 3 (a, v) =\u0026gt; ( 4 (a[v.slice(0, v.indexOf(\u0026#39;=\u0026#39;))] = v.slice(v.indexOf(\u0026#39;=\u0026#39;) + 1)), a 5 ), 6 {} 7 ); 8 9getURLParameters(\u0026#39;google.com\u0026#39;); // {} 10getURLParameters(\u0026#39;http://url.com/page?name=Adam\u0026amp;surname=Smith\u0026#39;); 11// {name: \u0026#39;Adam\u0026#39;, surname: \u0026#39;Smith\u0026#39;} 12 13// One line 14Object.fromEntries(\u0026#39;http://url.com/page?name=Adam\u0026amp;surname=Smith\u0026#39;.split(\u0026#39;?\u0026#39;)[1].split(\u0026#39;\u0026amp;\u0026#39;).map(x=\u0026gt;x.split(\u0026#39;=\u0026#39;))) if DOC element contains another element 1const elementContains = (parent, child) =\u0026gt; 2 parent !== child \u0026amp;\u0026amp; parent.contains(child); 3 4elementContains( 5 document.querySelector(\u0026#39;head\u0026#39;), 6 document.querySelector(\u0026#39;title\u0026#39;) 7); 8// true 9elementContains(document.querySelector(\u0026#39;body\u0026#39;), document.querySelector(\u0026#39;body\u0026#39;)); 10// false Date 1const {locale, timeZone} = Intl.DateTimeFormat().resolvedOptions(); is Date valid 1const isDateValid = (...val) =\u0026gt; !Number.isNaN(new Date(...val).valueOf()); 2 3isDateValid(\u0026#39;December 17, 1995 03:24:00\u0026#39;); // true 4isDateValid(\u0026#39;1995-12-17T03:24:00\u0026#39;); // true 5isDateValid(\u0026#39;1995-12-17 T03:24:00\u0026#39;); // false 6isDateValid(\u0026#39;Duck\u0026#39;); // false 7isDateValid(1995, 11, 17); // true 8isDateValid(1995, 11, 17, \u0026#39;Duck\u0026#39;); // false 9isDateValid({}); // false UNIX timestamp from Date 1const getTimestamp = (date = new Date()) =\u0026gt; Math.floor(date.getTime() / 1000); 2 3getTimestamp(); // 1602162242 Login Secure Your Node.js App with JSON Web Tokens\nclient.ts 1// client.ts 2import axios, { AxiosInstance } from \u0026#39;axios\u0026#39;; 3 4export class Client { 5 private _client: AxiosInstance; 6 7 constructor(accessToken?: string, url?: string) { 8 const apiUrl = this.selectApiTarget(); 9 let headers = {}; 10 if (accessToken !== undefined) { 11 headers = { 12 \u0026#39;Authorization\u0026#39;: `Bearer ${accessToken}` 13 }; 14 } 15 this._client = axios.create({ 16 baseURL: url || apiUrl, 17 headers: headers, 18 }); 19 } 20 21 private selectApiTarget(): string { 22 let backendUrl = config.backend.url; 23 if (window.location.host.includes(\u0026#34;node.sharedtodos.com\u0026#34;)) { 24 backendUrl = config.backend.url.slice().replace(\u0026#34;api.sharedtodos.com\u0026#34;, \u0026#34;node-api.sharedtodos.com\u0026#34;); 25 } 26 return `${backendUrl}/api/v1/`; 27 } 28 29 async getLoggedInUser(): Promise\u0026lt;User\u0026gt; { 30 return await this._client.get(\u0026#39;/user/me\u0026#39;).then((response) =\u0026gt; response.data); 31 } 32 33 async forgetLoggedInUser(): Promise\u0026lt;void\u0026gt; { 34 return await this._client.delete(\u0026#39;/user/me\u0026#39;).then((response) =\u0026gt; response.data); 35 } 36 37 async getTasks(listId: number): Promise\u0026lt;Task[]\u0026gt; { 38 return await this._client.get(`boards/${listId}/tasks`).then((response) =\u0026gt; response.data); 39 } 40 41 async deleteTask(listId: number, taskId: number) { 42 return await this._client.delete(`boards/${listId}/tasks/${taskId}`).then((response) =\u0026gt; response.data); 43 } 44 45 async createTask(listId: number, title: string, description: string) { 46 const task: Task = { 47 title: title, 48 description: description, 49 }; 50 return await this._client.post(`boards/${listId}/tasks`, task); 51 } 52 async updateTask(listId: number, taskId: string, task: Task) { 53 return await this._client.put(`boards/${listId}/tasks/${taskId}`, task); 54 } 55 56 async login(email: string): Promise\u0026lt;string\u0026gt; { 57 let data = new FormData(); 58 data.append(\u0026#39;user_email\u0026#39;, email); 59 return await this._client.post(`login`, data, { 60 headers: {\u0026#39;Content-Type\u0026#39;: \u0026#39;multipart/form-data\u0026#39; } 61 }).then((response) =\u0026gt; response.data.access_token); 62 } 63} 64 65export const getClient = (accessToken?, url?): Client =\u0026gt; new Client(accessToken, url); config.ts 1// config.ts 2interface ConfigOptions { 3 backend: { url: string }; 4 auth0: any; 5 authentication: { provider: string }; 6 authorization: { embedUrl: string }; 7} 8declare global { 9 interface Window { 10 _env_: any; 11 } 12} 13 14const Config: ConfigOptions = { 15 backend: { 16 url: 17 process.env.REACT_APP_BACKEND_URL || 18 window?._env_?.BACKEND_URL || 19 \u0026#34;http://localhost:8008\u0026#34;, 20 }, 21 auth0: { 22 domain: 23 process.env.AUTH0_DOMAIN || 24 window?._env_?.AUTH0_DOMAIN || 25 \u0026#34;acalla-demoapp.us.auth0.com\u0026#34;, 26 clientId: 27 process.env.AUTH0_CLIENT_ID || 28 window?._env_?.AUTH0_CLIENT_ID || 29 \u0026#34;myClientID\u0026#34;, 30 audience: 31 process.env.AUTH0_AUDIENCE || 32 window?._env_?.AUTH0_AUDIENCE || 33 \u0026#34;https://demoapi.server.com/v1/\u0026#34;, 34 }, 35 authentication: { 36 provider: \u0026#34;auth0\u0026#34;, 37 }, 38 authorization: { 39 embedUrl: window?._env_?.AUTHZ_EMBED_URL || \u0026#34;http://localhost:3000\u0026#34;, 40 } 41}; 42 43export default Config; Cheat Sheet 1// Single-line comments start with two slashes. 2/* Multiline comments start with slash-star, 3 and end with star-slash */ 4 5// Statements can be terminated by ; 6doStuff(); 7 8// ... but they don\u0026#39;t have to be, as semicolons are automatically inserted 9// wherever there\u0026#39;s a newline, except in certain cases. 10doStuff() 11 12// Because those cases can cause unexpected results, we\u0026#39;ll keep on using 13// semicolons in this guide. 14 15/////////////////////////////////// 16// 1. Numbers, Strings and Operators 17 18// JavaScript has one number type (which is a 64-bit IEEE 754 double). 19// Doubles have a 52-bit mantissa, which is enough to store integers 20// up to about 9✕10¹⁵ precisely. 213; // = 3 221.5; // = 1.5 23 24// Some basic arithmetic works as you\u0026#39;d expect. 251 + 1; // = 2 260.1 + 0.2; // = 0.30000000000000004 278 - 1; // = 7 2810 * 2; // = 20 2935 / 5; // = 7 30 31// Including uneven division. 325 / 2; // = 2.5 33 34// And modulo division. 3510 % 2; // = 0 3630 % 4; // = 2 3718.5 % 7; // = 4.5 38 39// Bitwise operations also work; when you perform a bitwise operation your float 40// is converted to a signed int *up to* 32 bits. 411 \u0026lt;\u0026lt; 2; // = 4 42 43// Precedence is enforced with parentheses. 44(1 + 3) * 2; // = 8 45 46// There are three special not-a-real-number values: 47Infinity; // result of e.g. 1/0 48-Infinity; // result of e.g. -1/0 49NaN; // result of e.g. 0/0, stands for \u0026#39;Not a Number\u0026#39; 50 51// There\u0026#39;s also a boolean type. 52true; 53false; 54 55// Strings are created with \u0026#39; or \u0026#34;. 56\u0026#39;abc\u0026#39;; 57\u0026#34;Hello, world\u0026#34;; 58 59// Negation uses the ! symbol 60!true; // = false 61!false; // = true 62 63// Equality is === 641 === 1; // = true 652 === 1; // = false 66 67// Inequality is !== 681 !== 1; // = false 692 !== 1; // = true 70 71// More comparisons 721 \u0026lt; 10; // = true 731 \u0026gt; 10; // = false 742 \u0026lt;= 2; // = true 752 \u0026gt;= 2; // = true 76 77// Strings are concatenated with + 78\u0026#34;Hello \u0026#34; + \u0026#34;world!\u0026#34;; // = \u0026#34;Hello world!\u0026#34; 79 80// ... which works with more than just strings 81\u0026#34;1, 2, \u0026#34; + 3; // = \u0026#34;1, 2, 3\u0026#34; 82\u0026#34;Hello \u0026#34; + [\u0026#34;world\u0026#34;, \u0026#34;!\u0026#34;]; // = \u0026#34;Hello world,!\u0026#34; 83 84// and are compared with \u0026lt; and \u0026gt; 85\u0026#34;a\u0026#34; \u0026lt; \u0026#34;b\u0026#34;; // = true 86 87// Type coercion is performed for comparisons with double equals... 88\u0026#34;5\u0026#34; == 5; // = true 89null == undefined; // = true 90 91// ...unless you use === 92\u0026#34;5\u0026#34; === 5; // = false 93null === undefined; // = false 94 95// ...which can result in some weird behaviour... 9613 + !0; // 14 97\u0026#34;13\u0026#34; + !0; // \u0026#39;13true\u0026#39; 98 99// You can access characters in a string with `charAt` 100\u0026#34;This is a string\u0026#34;.charAt(0); // = \u0026#39;T\u0026#39; 101 102// ...or use `substring` to get larger pieces. 103\u0026#34;Hello world\u0026#34;.substring(0, 5); // = \u0026#34;Hello\u0026#34; 104 105// `length` is a property, so don\u0026#39;t use (). 106\u0026#34;Hello\u0026#34;.length; // = 5 107 108// There\u0026#39;s also `null` and `undefined`. 109null; // used to indicate a deliberate non-value 110undefined; // used to indicate a value is not currently present (although 111 // `undefined` is actually a value itself) 112 113// false, null, undefined, NaN, 0 and \u0026#34;\u0026#34; are falsy; everything else is truthy. 114// Note that 0 is falsy and \u0026#34;0\u0026#34; is truthy, even though 0 == \u0026#34;0\u0026#34;. 115 116/////////////////////////////////// 117// 2. Variables, Arrays and Objects 118 119// Variables are declared with the `var` keyword. JavaScript is dynamically 120// typed, so you don\u0026#39;t need to specify type. Assignment uses a single `=` 121// character. 122var someVar = 5; 123 124// If you leave the var keyword off, you won\u0026#39;t get an error... 125someOtherVar = 10; 126 127// ...but your variable will be created in the global scope, not in the scope 128// you defined it in. 129 130// Variables declared without being assigned to are set to undefined. 131var someThirdVar; // = undefined 132 133// If you want to declare a couple of variables, then you could use a comma 134// separator 135var someFourthVar = 2, someFifthVar = 4; 136 137// There\u0026#39;s shorthand for performing math operations on variables: 138someVar += 5; // equivalent to someVar = someVar + 5; someVar is 10 now 139someVar *= 10; // now someVar is 100 140 141// and an even-shorter-hand for adding or subtracting 1 142someVar++; // now someVar is 101 143someVar--; // back to 100 144 145// Arrays are ordered lists of values, of any type. 146var myArray = [\u0026#34;Hello\u0026#34;, 45, true]; 147 148// Their members can be accessed using the square-brackets subscript syntax. 149// Array indices start at zero. 150myArray[1]; // = 45 151 152// Arrays are mutable and of variable length. 153myArray.push(\u0026#34;World\u0026#34;); 154myArray.length; // = 4 155 156// Add/Modify at specific index 157myArray[3] = \u0026#34;Hello\u0026#34;; 158 159// Add and remove element from front or back end of an array 160myArray.unshift(3); // Add as the first element 161someVar = myArray.shift(); // Remove first element and return it 162myArray.push(3); // Add as the last element 163someVar = myArray.pop(); // Remove last element and return it 164 165// Join all elements of an array with semicolon 166var myArray0 = [32,false,\u0026#34;js\u0026#34;,12,56,90]; 167myArray0.join(\u0026#34;;\u0026#34;); // = \u0026#34;32;false;js;12;56;90\u0026#34; 168 169// Get subarray of elements from index 1 (include) to 4 (exclude) 170myArray0.slice(1,4); // = [false,\u0026#34;js\u0026#34;,12] 171 172// Remove 4 elements starting from index 2, and insert there strings 173// \u0026#34;hi\u0026#34;,\u0026#34;wr\u0026#34; and \u0026#34;ld\u0026#34;; return removed subarray 174myArray0.splice(2,4,\u0026#34;hi\u0026#34;,\u0026#34;wr\u0026#34;,\u0026#34;ld\u0026#34;); // = [\u0026#34;js\u0026#34;,12,56,90] 175// myArray0 === [32,false,\u0026#34;hi\u0026#34;,\u0026#34;wr\u0026#34;,\u0026#34;ld\u0026#34;] 176 177// JavaScript\u0026#39;s objects are equivalent to \u0026#34;dictionaries\u0026#34; or \u0026#34;maps\u0026#34; in other 178// languages: an unordered collection of key-value pairs. 179var myObj = {key1: \u0026#34;Hello\u0026#34;, key2: \u0026#34;World\u0026#34;}; 180 181// Keys are strings, but quotes aren\u0026#39;t required if they\u0026#39;re a valid 182// JavaScript identifier. Values can be any type. 183var myObj = {myKey: \u0026#34;myValue\u0026#34;, \u0026#34;my other key\u0026#34;: 4}; 184 185// Object attributes can also be accessed using the subscript syntax, 186myObj[\u0026#34;my other key\u0026#34;]; // = 4 187 188// ... or using the dot syntax, provided the key is a valid identifier. 189myObj.myKey; // = \u0026#34;myValue\u0026#34; 190 191// Objects are mutable; values can be changed and new keys added. 192myObj.myThirdKey = true; 193 194// If you try to access a value that\u0026#39;s not yet set, you\u0026#39;ll get undefined. 195myObj.myFourthKey; // = undefined 196 197/////////////////////////////////// 198// 3. Logic and Control Structures 199 200// The `if` structure works as you\u0026#39;d expect. 201var count = 1; 202if (count == 3){ 203 // evaluated if count is 3 204} else if (count == 4){ 205 // evaluated if count is 4 206} else { 207 // evaluated if it\u0026#39;s not either 3 or 4 208} 209 210// As does `while`. 211while (true){ 212 // An infinite loop! 213} 214 215// Do-while loops are like while loops, except they always run at least once. 216var input; 217do { 218 input = getInput(); 219} while (!isValid(input)); 220 221// The `for` loop is the same as C and Java: 222// initialization; continue condition; iteration. 223for (var i = 0; i \u0026lt; 5; i++){ 224 // will run 5 times 225} 226 227// Breaking out of labeled loops is similar to Java 228outer: 229for (var i = 0; i \u0026lt; 10; i++) { 230 for (var j = 0; j \u0026lt; 10; j++) { 231 if (i == 5 \u0026amp;\u0026amp; j ==5) { 232 break outer; 233 // breaks out of outer loop instead of only the inner one 234 } 235 } 236} 237 238// The for/in statement allows iteration over properties of an object. 239var description = \u0026#34;\u0026#34;; 240var person = {fname:\u0026#34;Paul\u0026#34;, lname:\u0026#34;Ken\u0026#34;, age:18}; 241for (var x in person){ 242 description += person[x] + \u0026#34; \u0026#34;; 243} // description = \u0026#39;Paul Ken 18 \u0026#39; 244 245// The for/of statement allows iteration over iterable objects (including the built-in String, 246// Array, e.g. the Array-like arguments or NodeList objects, TypedArray, Map and Set, 247// and user-defined iterables). 248var myPets = \u0026#34;\u0026#34;; 249var pets = [\u0026#34;cat\u0026#34;, \u0026#34;dog\u0026#34;, \u0026#34;hamster\u0026#34;, \u0026#34;hedgehog\u0026#34;]; 250for (var pet of pets){ 251 myPets += pet + \u0026#34; \u0026#34;; 252} // myPets = \u0026#39;cat dog hamster hedgehog \u0026#39; 253 254// \u0026amp;\u0026amp; is logical and, || is logical or 255if (house.size == \u0026#34;big\u0026#34; \u0026amp;\u0026amp; house.colour == \u0026#34;blue\u0026#34;){ 256 house.contains = \u0026#34;bear\u0026#34;; 257} 258if (colour == \u0026#34;red\u0026#34; || colour == \u0026#34;blue\u0026#34;){ 259 // colour is either red or blue 260} 261 262// \u0026amp;\u0026amp; and || \u0026#34;short circuit\u0026#34;, which is useful for setting default values. 263var name = otherName || \u0026#34;default\u0026#34;; 264 265// The `switch` statement checks for equality with `===`. 266// Use \u0026#39;break\u0026#39; after each case 267// or the cases after the correct one will be executed too. 268grade = \u0026#39;B\u0026#39;; 269switch (grade) { 270 case \u0026#39;A\u0026#39;: 271 console.log(\u0026#34;Great job\u0026#34;); 272 break; 273 case \u0026#39;B\u0026#39;: 274 console.log(\u0026#34;OK job\u0026#34;); 275 break; 276 case \u0026#39;C\u0026#39;: 277 console.log(\u0026#34;You can do better\u0026#34;); 278 break; 279 default: 280 console.log(\u0026#34;Oy vey\u0026#34;); 281 break; 282} 283 284 285/////////////////////////////////// 286// 4. Functions, Scope and Closures 287 288// JavaScript functions are declared with the `function` keyword. 289function myFunction(thing){ 290 return thing.toUpperCase(); 291} 292myFunction(\u0026#34;foo\u0026#34;); // = \u0026#34;FOO\u0026#34; 293 294// Note that the value to be returned must start on the same line as the 295// `return` keyword, otherwise you\u0026#39;ll always return `undefined` due to 296// automatic semicolon insertion. Watch out for this when using Allman style. 297function myFunction(){ 298 return // \u0026lt;- semicolon automatically inserted here 299 {thisIsAn: \u0026#39;object literal\u0026#39;}; 300} 301myFunction(); // = undefined 302 303// JavaScript functions are first class objects, so they can be reassigned to 304// different variable names and passed to other functions as arguments - for 305// example, when supplying an event handler: 306function myFunction(){ 307 // this code will be called in 5 seconds\u0026#39; time 308} 309setTimeout(myFunction, 5000); 310// Note: setTimeout isn\u0026#39;t part of the JS language, but is provided by browsers 311// and Node.js. 312 313// Another function provided by browsers is setInterval 314function myFunction(){ 315 // this code will be called every 5 seconds 316} 317setInterval(myFunction, 5000); 318 319// Function objects don\u0026#39;t even have to be declared with a name - you can write 320// an anonymous function definition directly into the arguments of another. 321setTimeout(function(){ 322 // this code will be called in 5 seconds\u0026#39; time 323}, 5000); 324 325// JavaScript has function scope; functions get their own scope but other blocks 326// do not. 327if (true){ 328 var i = 5; 329} 330i; // = 5 - not undefined as you\u0026#39;d expect in a block-scoped language 331 332// This has led to a common pattern of \u0026#34;immediately-executing anonymous 333// functions\u0026#34;, which prevent temporary variables from leaking into the global 334// scope. 335(function(){ 336 var temporary = 5; 337 // We can access the global scope by assigning to the \u0026#34;global object\u0026#34;, which 338 // in a web browser is always `window`. The global object may have a 339 // different name in non-browser environments such as Node.js. 340 window.permanent = 10; 341})(); 342temporary; // raises ReferenceError 343permanent; // = 10 344 345// One of JavaScript\u0026#39;s most powerful features is closures. If a function is 346// defined inside another function, the inner function has access to all the 347// outer function\u0026#39;s variables, even after the outer function exits. 348function sayHelloInFiveSeconds(name){ 349 var prompt = \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34;; 350 // Inner functions are put in the local scope by default, as if they were 351 // declared with `var`. 352 function inner(){ 353 alert(prompt); 354 } 355 setTimeout(inner, 5000); 356 // setTimeout is asynchronous, so the sayHelloInFiveSeconds function will 357 // exit immediately, and setTimeout will call inner afterwards. However, 358 // because inner is \u0026#34;closed over\u0026#34; sayHelloInFiveSeconds, inner still has 359 // access to the `prompt` variable when it is finally called. 360} 361sayHelloInFiveSeconds(\u0026#34;Adam\u0026#34;); // will open a popup with \u0026#34;Hello, Adam!\u0026#34; in 5s 362 363/////////////////////////////////// 364// 5. More about Objects; Constructors and Prototypes 365 366// Objects can contain functions. 367var myObj = { 368 myFunc: function(){ 369 return \u0026#34;Hello world!\u0026#34;; 370 } 371}; 372myObj.myFunc(); // = \u0026#34;Hello world!\u0026#34; 373 374// When functions attached to an object are called, they can access the object 375// they\u0026#39;re attached to using the `this` keyword. 376myObj = { 377 myString: \u0026#34;Hello world!\u0026#34;, 378 myFunc: function(){ 379 return this.myString; 380 } 381}; 382myObj.myFunc(); // = \u0026#34;Hello world!\u0026#34; 383 384// What this is set to has to do with how the function is called, not where 385// it\u0026#39;s defined. So, our function doesn\u0026#39;t work if it isn\u0026#39;t called in the 386// context of the object. 387var myFunc = myObj.myFunc; 388myFunc(); // = undefined 389 390// Inversely, a function can be assigned to the object and gain access to it 391// through `this`, even if it wasn\u0026#39;t attached when it was defined. 392var myOtherFunc = function(){ 393 return this.myString.toUpperCase(); 394}; 395myObj.myOtherFunc = myOtherFunc; 396myObj.myOtherFunc(); // = \u0026#34;HELLO WORLD!\u0026#34; 397 398// We can also specify a context for a function to execute in when we invoke it 399// using `call` or `apply`. 400 401var anotherFunc = function(s){ 402 return this.myString + s; 403}; 404anotherFunc.call(myObj, \u0026#34; And Hello Moon!\u0026#34;); // = \u0026#34;Hello World! And Hello Moon!\u0026#34; 405 406// The `apply` function is nearly identical, but takes an array for an argument 407// list. 408 409anotherFunc.apply(myObj, [\u0026#34; And Hello Sun!\u0026#34;]); // = \u0026#34;Hello World! And Hello Sun!\u0026#34; 410 411// This is useful when working with a function that accepts a sequence of 412// arguments and you want to pass an array. 413 414Math.min(42, 6, 27); // = 6 415Math.min([42, 6, 27]); // = NaN (uh-oh!) 416Math.min.apply(Math, [42, 6, 27]); // = 6 417 418// But, `call` and `apply` are only temporary. When we want it to stick, we can 419// use `bind`. 420 421var boundFunc = anotherFunc.bind(myObj); 422boundFunc(\u0026#34; And Hello Saturn!\u0026#34;); // = \u0026#34;Hello World! And Hello Saturn!\u0026#34; 423 424// `bind` can also be used to partially apply (curry) a function. 425 426var product = function(a, b){ return a * b; }; 427var doubler = product.bind(this, 2); 428doubler(8); // = 16 429 430// When you call a function with the `new` keyword, a new object is created, and 431// made available to the function via the `this` keyword. Functions designed to be 432// called like that are called constructors. 433 434var MyConstructor = function(){ 435 this.myNumber = 5; 436}; 437myNewObj = new MyConstructor(); // = {myNumber: 5} 438myNewObj.myNumber; // = 5 439 440// Unlike most other popular object-oriented languages, JavaScript has no 441// concept of \u0026#39;instances\u0026#39; created from \u0026#39;class\u0026#39; blueprints; instead, JavaScript 442// combines instantiation and inheritance into a single concept: a \u0026#39;prototype\u0026#39;. 443 444// Every JavaScript object has a \u0026#39;prototype\u0026#39;. When you go to access a property 445// on an object that doesn\u0026#39;t exist on the actual object, the interpreter will 446// look at its prototype. 447 448// Some JS implementations let you access an object\u0026#39;s prototype on the magic 449// property `__proto__`. While this is useful for explaining prototypes it\u0026#39;s not 450// part of the standard; we\u0026#39;ll get to standard ways of using prototypes later. 451var myObj = { 452 myString: \u0026#34;Hello world!\u0026#34; 453}; 454var myPrototype = { 455 meaningOfLife: 42, 456 myFunc: function(){ 457 return this.myString.toLowerCase(); 458 } 459}; 460 461myObj.__proto__ = myPrototype; 462myObj.meaningOfLife; // = 42 463 464// This works for functions, too. 465myObj.myFunc(); // = \u0026#34;hello world!\u0026#34; 466 467// Of course, if your property isn\u0026#39;t on your prototype, the prototype\u0026#39;s 468// prototype is searched, and so on. 469myPrototype.__proto__ = { 470 myBoolean: true 471}; 472myObj.myBoolean; // = true 473 474// There\u0026#39;s no copying involved here; each object stores a reference to its 475// prototype. This means we can alter the prototype and our changes will be 476// reflected everywhere. 477myPrototype.meaningOfLife = 43; 478myObj.meaningOfLife; // = 43 479 480// The for/in statement allows iteration over properties of an object, 481// walking up the prototype chain until it sees a null prototype. 482for (var x in myObj){ 483 console.log(myObj[x]); 484} 485///prints: 486// Hello world! 487// 43 488// [Function: myFunc] 489// true 490 491// To only consider properties attached to the object itself 492// and not its prototypes, use the `hasOwnProperty()` check. 493for (var x in myObj){ 494 if (myObj.hasOwnProperty(x)){ 495 console.log(myObj[x]); 496 } 497} 498///prints: 499// Hello world! 500 501// We mentioned that `__proto__` was non-standard, and there\u0026#39;s no standard way to 502// change the prototype of an existing object. However, there are two ways to 503// create a new object with a given prototype. 504 505// The first is Object.create, which is a recent addition to JS, and therefore 506// not available in all implementations yet. 507var myObj = Object.create(myPrototype); 508myObj.meaningOfLife; // = 43 509 510// The second way, which works anywhere, has to do with constructors. 511// Constructors have a property called prototype. This is *not* the prototype of 512// the constructor function itself; instead, it\u0026#39;s the prototype that new objects 513// are given when they\u0026#39;re created with that constructor and the new keyword. 514MyConstructor.prototype = { 515 myNumber: 5, 516 getMyNumber: function(){ 517 return this.myNumber; 518 } 519}; 520var myNewObj2 = new MyConstructor(); 521myNewObj2.getMyNumber(); // = 5 522myNewObj2.myNumber = 6; 523myNewObj2.getMyNumber(); // = 6 524 525// Built-in types like strings and numbers also have constructors that create 526// equivalent wrapper objects. 527var myNumber = 12; 528var myNumberObj = new Number(12); 529myNumber == myNumberObj; // = true 530 531// Except, they aren\u0026#39;t exactly equivalent. 532typeof myNumber; // = \u0026#39;number\u0026#39; 533typeof myNumberObj; // = \u0026#39;object\u0026#39; 534myNumber === myNumberObj; // = false 535if (0){ 536 // This code won\u0026#39;t execute, because 0 is falsy. 537} 538if (new Number(0)){ 539 // This code will execute, because wrapped numbers are objects, and objects 540 // are always truthy. 541} 542 543// However, the wrapper objects and the regular builtins share a prototype, so 544// you can actually add functionality to a string, for instance. 545String.prototype.firstCharacter = function(){ 546 return this.charAt(0); 547}; 548\u0026#34;abc\u0026#34;.firstCharacter(); // = \u0026#34;a\u0026#34; 549 550// This fact is often used in \u0026#34;polyfilling\u0026#34;, which is implementing newer 551// features of JavaScript in an older subset of JavaScript, so that they can be 552// used in older environments such as outdated browsers. 553 554// For instance, we mentioned that Object.create isn\u0026#39;t yet available in all 555// implementations, but we can still use it with this polyfill: 556if (Object.create === undefined){ // don\u0026#39;t overwrite it if it exists 557 Object.create = function(proto){ 558 // make a temporary constructor with the right prototype 559 var Constructor = function(){}; 560 Constructor.prototype = proto; 561 // then use it to create a new, appropriately-prototyped object 562 return new Constructor(); 563 }; 564} 565 566// ES6 Additions 567 568// The \u0026#34;let\u0026#34; keyword allows you to define variables in a lexical scope, 569// as opposed to a function scope like the var keyword does. 570let name = \u0026#34;Billy\u0026#34;; 571 572// Variables defined with let can be reassigned new values. 573name = \u0026#34;William\u0026#34;; 574 575// The \u0026#34;const\u0026#34; keyword allows you to define a variable in a lexical scope 576// like with let, but you cannot reassign the value once one has been assigned. 577 578const pi = 3.14; 579 580pi = 4.13; // You cannot do this. 581 582// There is a new syntax for functions in ES6 known as \u0026#34;lambda syntax\u0026#34;. 583// This allows functions to be defined in a lexical scope like with variables 584// defined by const and let. 585 586const isEven = (number) =\u0026gt; { 587 return number % 2 === 0; 588}; 589 590isEven(7); // false 591 592// The \u0026#34;equivalent\u0026#34; of this function in the traditional syntax would look like this: 593 594function isEven(number) { 595 return number % 2 === 0; 596}; 597 598// I put the word \u0026#34;equivalent\u0026#34; in double quotes because a function defined 599// using the lambda syntax cannnot be called before the definition. 600// The following is an example of invalid usage: 601 602add(1, 8); 603 604const add = (firstNumber, secondNumber) =\u0026gt; { 605 return firstNumber + secondNumber; 606}; Cheat Sheet Typescript 1// There are 3 basic types in TypeScript 2let isDone: boolean = false; 3let lines: number = 42; 4let name: string = \u0026#34;Anders\u0026#34;; 5 6// But you can omit the type annotation if the variables are derived 7// from explicit literals 8let isDone = false; 9let lines = 42; 10let name = \u0026#34;Anders\u0026#34;; 11 12// When it\u0026#39;s impossible to know, there is the \u0026#34;Any\u0026#34; type 13let notSure: any = 4; 14notSure = \u0026#34;maybe a string instead\u0026#34;; 15notSure = false; // okay, definitely a boolean 16 17// Use const keyword for constants 18const numLivesForCat = 9; 19numLivesForCat = 1; // Error 20 21// For collections, there are typed arrays and generic arrays 22let list: number[] = [1, 2, 3]; 23// Alternatively, using the generic array type 24let list: Array\u0026lt;number\u0026gt; = [1, 2, 3]; 25 26// For enumerations: 27enum Color { Red, Green, Blue }; 28let c: Color = Color.Green; 29console.log(Color[c]); // \u0026#34;Green\u0026#34; 30 31// Lastly, \u0026#34;void\u0026#34; is used in the special case of a function returning nothing 32function bigHorribleAlert(): void { 33 alert(\u0026#34;I\u0026#39;m a little annoying box!\u0026#34;); 34} 35 36// Functions are first class citizens, support the lambda \u0026#34;fat arrow\u0026#34; syntax and 37// use type inference 38 39// The following are equivalent, the same signature will be inferred by the 40// compiler, and same JavaScript will be emitted 41let f1 = function (i: number): number { return i * i; } 42// Return type inferred 43let f2 = function (i: number) { return i * i; } 44// \u0026#34;Fat arrow\u0026#34; syntax 45let f3 = (i: number): number =\u0026gt; { return i * i; } 46// \u0026#34;Fat arrow\u0026#34; syntax with return type inferred 47let f4 = (i: number) =\u0026gt; { return i * i; } 48// \u0026#34;Fat arrow\u0026#34; syntax with return type inferred, braceless means no return 49// keyword needed 50let f5 = (i: number) =\u0026gt; i * i; 51 52// Interfaces are structural, anything that has the properties is compliant with 53// the interface 54interface Person { 55 name: string; 56 // Optional properties, marked with a \u0026#34;?\u0026#34; 57 age?: number; 58 // And of course functions 59 move(): void; 60} 61 62// Object that implements the \u0026#34;Person\u0026#34; interface 63// Can be treated as a Person since it has the name and move properties 64let p: Person = { name: \u0026#34;Bobby\u0026#34;, move: () =\u0026gt; { } }; 65// Objects that have the optional property: 66let validPerson: Person = { name: \u0026#34;Bobby\u0026#34;, age: 42, move: () =\u0026gt; { } }; 67// Is not a person because age is not a number 68let invalidPerson: Person = { name: \u0026#34;Bobby\u0026#34;, age: true }; 69 70// Interfaces can also describe a function type 71interface SearchFunc { 72 (source: string, subString: string): boolean; 73} 74// Only the parameters\u0026#39; types are important, names are not important. 75let mySearch: SearchFunc; 76mySearch = function (src: string, sub: string) { 77 return src.search(sub) != -1; 78} 79 80// Classes - members are public by default 81class Point { 82 // Properties 83 x: number; 84 85 // Constructor - the public/private keywords in this context will generate 86 // the boiler plate code for the property and the initialization in the 87 // constructor. 88 // In this example, \u0026#34;y\u0026#34; will be defined just like \u0026#34;x\u0026#34; is, but with less code 89 // Default values are also supported 90 91 constructor(x: number, public y: number = 0) { 92 this.x = x; 93 } 94 95 // Functions 96 dist(): number { return Math.sqrt(this.x * this.x + this.y * this.y); } 97 98 // Static members 99 static origin = new Point(0, 0); 100} 101 102// Classes can be explicitly marked as implementing an interface. 103// Any missing properties will then cause an error at compile-time. 104class PointPerson implements Person { 105 name: string 106 move() {} 107} 108 109let p1 = new Point(10, 20); 110let p2 = new Point(25); //y will be 0 111 112// Inheritance 113class Point3D extends Point { 114 constructor(x: number, y: number, public z: number = 0) { 115 super(x, y); // Explicit call to the super class constructor is mandatory 116 } 117 118 // Overwrite 119 dist(): number { 120 let d = super.dist(); 121 return Math.sqrt(d * d + this.z * this.z); 122 } 123} 124 125// Modules, \u0026#34;.\u0026#34; can be used as separator for sub modules 126module Geometry { 127 export class Square { 128 constructor(public sideLength: number = 0) { 129 } 130 area() { 131 return Math.pow(this.sideLength, 2); 132 } 133 } 134} 135 136let s1 = new Geometry.Square(5); 137 138// Local alias for referencing a module 139import G = Geometry; 140 141let s2 = new G.Square(10); 142 143// Generics 144// Classes 145class Tuple\u0026lt;T1, T2\u0026gt; { 146 constructor(public item1: T1, public item2: T2) { 147 } 148} 149 150// Interfaces 151interface Pair\u0026lt;T\u0026gt; { 152 item1: T; 153 item2: T; 154} 155 156// And functions 157let pairToTuple = function \u0026lt;T\u0026gt;(p: Pair\u0026lt;T\u0026gt;) { 158 return new Tuple(p.item1, p.item2); 159}; 160 161let tuple = pairToTuple({ item1: \u0026#34;hello\u0026#34;, item2: \u0026#34;world\u0026#34; }); 162 163// Including references to a definition file: 164/// \u0026lt;reference path=\u0026#34;jquery.d.ts\u0026#34; /\u0026gt; 165 166// Template Strings (strings that use backticks) 167// String Interpolation with Template Strings 168let name = \u0026#39;Tyrone\u0026#39;; 169let greeting = `Hi ${name}, how are you?` 170// Multiline Strings with Template Strings 171let multiline = `This is an example 172of a multiline string`; 173 174// READONLY: New Feature in TypeScript 3.1 175interface Person { 176 readonly name: string; 177 readonly age: number; 178} 179 180var p1: Person = { name: \u0026#34;Tyrone\u0026#34;, age: 42 }; 181p1.age = 25; // Error, p1.age is read-only 182 183var p2 = { name: \u0026#34;John\u0026#34;, age: 60 }; 184var p3: Person = p2; // Ok, read-only alias for p2 185p3.age = 35; // Error, p3.age is read-only 186p2.age = 45; // Ok, but also changes p3.age because of aliasing 187 188class Car { 189 readonly make: string; 190 readonly model: string; 191 readonly year = 2018; 192 193 constructor() { 194 this.make = \u0026#34;Unknown Make\u0026#34;; // Assignment permitted in constructor 195 this.model = \u0026#34;Unknown Model\u0026#34;; // Assignment permitted in constructor 196 } 197} 198 199let numbers: Array\u0026lt;number\u0026gt; = [0, 1, 2, 3, 4]; 200let moreNumbers: ReadonlyArray\u0026lt;number\u0026gt; = numbers; 201moreNumbers[5] = 5; // Error, elements are read-only 202moreNumbers.push(5); // Error, no push method (because it mutates array) 203moreNumbers.length = 3; // Error, length is read-only 204numbers = moreNumbers; // Error, mutating methods are missing 205 206// Tagged Union Types for modelling state that can be in one of many shapes 207type State = 208 | { type: \u0026#34;loading\u0026#34; } 209 | { type: \u0026#34;success\u0026#34;, value: number } 210 | { type: \u0026#34;error\u0026#34;, message: string }; 211 212declare const state: State; 213if (state.type === \u0026#34;success\u0026#34;) { 214 console.log(state.value); 215} else if (state.type === \u0026#34;error\u0026#34;) { 216 console.error(state.message); 217} 218 219// Template Literal Types 220// Use to create complex string types 221type OrderSize = \u0026#34;regular\u0026#34; | \u0026#34;large\u0026#34;; 222type OrderItem = \u0026#34;Espresso\u0026#34; | \u0026#34;Cappuccino\u0026#34;; 223type Order = `A ${OrderSize} ${OrderItem}`; 224 225let order1: Order = \u0026#34;A regular Cappuccino\u0026#34;; 226let order2: Order = \u0026#34;A large Espresso\u0026#34;; 227let order3: Order = \u0026#34;A small Espresso\u0026#34;; // Error 228 229// Iterators and Generators 230 231// for..of statement 232// iterate over the list of values on the object being iterated 233let arrayOfAnyType = [1, \u0026#34;string\u0026#34;, false]; 234for (const val of arrayOfAnyType) { 235 console.log(val); // 1, \u0026#34;string\u0026#34;, false 236} 237 238let list = [4, 5, 6]; 239for (const i of list) { 240 console.log(i); // 4, 5, 6 241} 242 243// for..in statement 244// iterate over the list of keys on the object being iterated 245for (const i in list) { 246 console.log(i); // 0, 1, 2 247} 248 249// Type Assertion 250 251let foo = {} // Creating foo as an empty object 252foo.bar = 123 // Error: property \u0026#39;bar\u0026#39; does not exist on `{}` 253foo.baz = \u0026#39;hello world\u0026#39; // Error: property \u0026#39;baz\u0026#39; does not exist on `{}` 254 255// Because the inferred type of foo is `{}` (an object with 0 properties), you 256// are not allowed to add bar and baz to it. However with type assertion, 257// the following will pass: 258 259interface Foo { 260 bar: number; 261 baz: string; 262} 263 264let foo = {} as Foo; // Type assertion here 265foo.bar = 123; 266foo.baz = \u0026#39;hello world\u0026#39; Resources react cheatsheet https://learnxinyminutes.com/docs/typescript/ ",url:"https://romankurnovskii.com/en/posts/js-snippets/"},"https://romankurnovskii.com/en/tags/js/":{title:"js",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/js/"},"https://romankurnovskii.com/en/tags/typescript/":{title:"typescript",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/typescript/"},"https://romankurnovskii.com/en/tags/kinesis/":{title:"kinesis",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/kinesis/"},"https://romankurnovskii.com/en/categories/kinesis/":{title:"kinesis",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/kinesis/"},"https://romankurnovskii.com/en/tags/opensearch/":{title:"opensearch",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/opensearch/"},"https://romankurnovskii.com/en/categories/opensearch/":{title:"opensearch",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/opensearch/"},"https://romankurnovskii.com/en/tags/api-gateway/":{title:"API Gateway",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/api-gateway/"},"https://romankurnovskii.com/en/categories/api-gateway/":{title:"API Gateway",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/api-gateway/"},"https://romankurnovskii.com/en/tags/cloudfront/":{title:"CloudFront",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cloudfront/"},"https://romankurnovskii.com/en/categories/cloudfront/":{title:"CloudFront",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cloudfront/"},"https://romankurnovskii.com/en/tags/elb/":{title:"ELB",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/elb/"},"https://romankurnovskii.com/en/categories/elb/":{title:"ELB",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/elb/"},"https://romankurnovskii.com/en/tags/dynamodb/":{title:"dynamodb",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/dynamodb/"},"https://romankurnovskii.com/en/categories/dynamodb/":{title:"dynamodb",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/dynamodb/"},"https://romankurnovskii.com/en/tags/elasticache/":{title:"elasticache",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/elasticache/"},"https://romankurnovskii.com/en/categories/elasticache/":{title:"elasticache",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/elasticache/"},"https://romankurnovskii.com/en/tags/rds/":{title:"rds",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/rds/"},"https://romankurnovskii.com/en/categories/rds/":{title:"rds",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/rds/"},"https://romankurnovskii.com/en/tags/lambda/":{title:"lambda",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/lambda/"},"https://romankurnovskii.com/en/categories/lambda/":{title:"lambda",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/lambda/"},"https://romankurnovskii.com/en/tags/s3/":{title:"s3",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/s3/"},"https://romankurnovskii.com/en/categories/s3/":{title:"s3",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/s3/"},"https://romankurnovskii.com/en/tags/ec2/":{title:"ec2",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/ec2/"},"https://romankurnovskii.com/en/categories/ec2/":{title:"ec2",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/ec2/"},"https://romankurnovskii.com/en/tags/elastic-beanstalk/":{title:"Elastic Beanstalk",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/elastic-beanstalk/"},"https://romankurnovskii.com/en/categories/elastic-beanstalk/":{title:"Elastic Beanstalk",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/elastic-beanstalk/"},"https://romankurnovskii.com/en/tags/iam/":{title:"iam",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/iam/"},"https://romankurnovskii.com/en/categories/iam/":{title:"iam",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/iam/"},"https://romankurnovskii.com/en/tags/flask/":{title:"flask",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/flask/"},"https://romankurnovskii.com/en/tags/mongodb/":{title:"mongodb",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/mongodb/"},"https://romankurnovskii.com/en/tags/serverless/":{title:"serverless",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/serverless/"},"https://romankurnovskii.com/en/posts/serverless-flask-lambda-api-gateway-mongodb/":{title:"Serverless: Flask\u002bAPI Gateway\u002bLambda\u002bMongoDB",tags:["aws","flask","serverless","mongodb","lambda"],content:`mongodb free tier vs documentdb
Project structure 1- src 2 - app.py 3 - mongo.py 4- .env 5- requirements.txt 6- serverless.yml Sources
Add AIM user Setup specific user for serverless deployment
username: serverless-deployer
aws aim documentation Set policy Create:
ServerLessDeployerPolicyGroup ServerLessDeployerPolicy Policy:
1{ 2 \u0026#34;Statement\u0026#34;: [ 3 { 4 \u0026#34;Action\u0026#34;: [ 5 \u0026#34;apigateway:*\u0026#34;, 6 \u0026#34;cloudformation:CancelUpdateStack\u0026#34;, 7 \u0026#34;cloudformation:ContinueUpdateRollback\u0026#34;, 8 \u0026#34;cloudformation:CreateChangeSet\u0026#34;, 9 \u0026#34;cloudformation:CreateStack\u0026#34;, 10 \u0026#34;cloudformation:CreateUploadBucket\u0026#34;, 11 \u0026#34;cloudformation:DeleteStack\u0026#34;, 12 \u0026#34;cloudformation:Describe*\u0026#34;, 13 \u0026#34;cloudformation:EstimateTemplateCost\u0026#34;, 14 \u0026#34;cloudformation:ExecuteChangeSet\u0026#34;, 15 \u0026#34;cloudformation:Get*\u0026#34;, 16 \u0026#34;cloudformation:List*\u0026#34;, 17 \u0026#34;cloudformation:UpdateStack\u0026#34;, 18 \u0026#34;cloudformation:UpdateTerminationProtection\u0026#34;, 19 \u0026#34;cloudformation:ValidateTemplate\u0026#34;, 20 \u0026#34;dynamodb:CreateTable\u0026#34;, 21 \u0026#34;dynamodb:DeleteTable\u0026#34;, 22 \u0026#34;dynamodb:DescribeTable\u0026#34;, 23 \u0026#34;dynamodb:DescribeTimeToLive\u0026#34;, 24 \u0026#34;dynamodb:UpdateTimeToLive\u0026#34;, 25 \u0026#34;ec2:AttachInternetGateway\u0026#34;, 26 \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, 27 \u0026#34;ec2:CreateInternetGateway\u0026#34;, 28 \u0026#34;ec2:CreateNetworkAcl\u0026#34;, 29 \u0026#34;ec2:CreateNetworkAclEntry\u0026#34;, 30 \u0026#34;ec2:CreateRouteTable\u0026#34;, 31 \u0026#34;ec2:CreateSecurityGroup\u0026#34;, 32 \u0026#34;ec2:CreateSubnet\u0026#34;, 33 \u0026#34;ec2:CreateTags\u0026#34;, 34 \u0026#34;ec2:CreateVpc\u0026#34;, 35 \u0026#34;ec2:DeleteInternetGateway\u0026#34;, 36 \u0026#34;ec2:DeleteNetworkAcl\u0026#34;, 37 \u0026#34;ec2:DeleteNetworkAclEntry\u0026#34;, 38 \u0026#34;ec2:DeleteRouteTable\u0026#34;, 39 \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, 40 \u0026#34;ec2:DeleteSubnet\u0026#34;, 41 \u0026#34;ec2:DeleteVpc\u0026#34;, 42 \u0026#34;ec2:Describe*\u0026#34;, 43 \u0026#34;ec2:DetachInternetGateway\u0026#34;, 44 \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, 45 \u0026#34;events:DeleteRule\u0026#34;, 46 \u0026#34;events:DescribeRule\u0026#34;, 47 \u0026#34;events:ListRuleNamesByTarget\u0026#34;, 48 \u0026#34;events:ListRules\u0026#34;, 49 \u0026#34;events:ListTargetsByRule\u0026#34;, 50 \u0026#34;events:PutRule\u0026#34;, 51 \u0026#34;events:PutTargets\u0026#34;, 52 \u0026#34;events:RemoveTargets\u0026#34;, 53 \u0026#34;iam:AttachRolePolicy\u0026#34;, 54 \u0026#34;iam:CreateRole\u0026#34;, 55 \u0026#34;iam:DeleteRole\u0026#34;, 56 \u0026#34;iam:DeleteRolePolicy\u0026#34;, 57 \u0026#34;iam:DetachRolePolicy\u0026#34;, 58 \u0026#34;iam:GetRole\u0026#34;, 59 \u0026#34;iam:PassRole\u0026#34;, 60 \u0026#34;iam:PutRolePolicy\u0026#34;, 61 \u0026#34;iot:CreateTopicRule\u0026#34;, 62 \u0026#34;iot:DeleteTopicRule\u0026#34;, 63 \u0026#34;iot:DisableTopicRule\u0026#34;, 64 \u0026#34;iot:EnableTopicRule\u0026#34;, 65 \u0026#34;iot:ReplaceTopicRule\u0026#34;, 66 \u0026#34;kinesis:CreateStream\u0026#34;, 67 \u0026#34;kinesis:DeleteStream\u0026#34;, 68 \u0026#34;kinesis:DescribeStream\u0026#34;, 69 \u0026#34;lambda:*\u0026#34;, 70	\u0026#34;logs:CreateLogDelivery\u0026#34;, 71 \u0026#34;logs:CreateLogGroup\u0026#34;, 72 \u0026#34;logs:DeleteLogGroup\u0026#34;, 73 \u0026#34;logs:DescribeLogGroups\u0026#34;, 74 \u0026#34;logs:DescribeLogStreams\u0026#34;, 75 \u0026#34;logs:FilterLogEvents\u0026#34;, 76 \u0026#34;logs:GetLogEvents\u0026#34;, 77 \u0026#34;logs:PutSubscriptionFilter\u0026#34;, 78 \u0026#34;s3:CreateBucket\u0026#34;, 79 \u0026#34;s3:DeleteBucket\u0026#34;, 80 \u0026#34;s3:DeleteBucketPolicy\u0026#34;, 81 \u0026#34;s3:DeleteObject\u0026#34;, 82 \u0026#34;s3:DeleteObjectVersion\u0026#34;, 83 \u0026#34;s3:GetObject\u0026#34;, 84 \u0026#34;s3:GetObjectVersion\u0026#34;, 85 \u0026#34;s3:ListAllMyBuckets\u0026#34;, 86 \u0026#34;s3:ListBucket\u0026#34;, 87 \u0026#34;s3:PutBucketNotification\u0026#34;, 88 \u0026#34;s3:PutBucketPolicy\u0026#34;, 89 \u0026#34;s3:PutBucketTagging\u0026#34;, 90 \u0026#34;s3:PutBucketWebsite\u0026#34;, 91 \u0026#34;s3:PutEncryptionConfiguration\u0026#34;, 92 \u0026#34;s3:PutObject\u0026#34;, 93 \u0026#34;sns:CreateTopic\u0026#34;, 94 \u0026#34;sns:DeleteTopic\u0026#34;, 95 \u0026#34;sns:GetSubscriptionAttributes\u0026#34;, 96 \u0026#34;sns:GetTopicAttributes\u0026#34;, 97 \u0026#34;sns:ListSubscriptions\u0026#34;, 98 \u0026#34;sns:ListSubscriptionsByTopic\u0026#34;, 99 \u0026#34;sns:ListTopics\u0026#34;, 100 \u0026#34;sns:SetSubscriptionAttributes\u0026#34;, 101 \u0026#34;sns:SetTopicAttributes\u0026#34;, 102 \u0026#34;sns:Subscribe\u0026#34;, 103 \u0026#34;sns:Unsubscribe\u0026#34;, 104 \u0026#34;states:CreateStateMachine\u0026#34;, 105 \u0026#34;states:DeleteStateMachine\u0026#34; 106 ], 107 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 108 \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; 109 } 110 ], 111 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34; 112} Create user copy the API Key \u0026amp; Secret
Need during setup aws cli/serverless
Create serverless.yml In the root folder create:
1org: romankurnovskii 2app: app-name 3service: app-service-name 4 5frameworkVersion: \u0026#39;3\u0026#39; 6 7useDotenv: true 8 9custom: 10 wsgi: 11 app: src/app.app 12 packRequirements: false 13 14provider: 15 name: aws 16 deploymentMethod: direct 17 region: eu-west-1 18 runtime: python3.9 19 architecture: arm64 20 versionFunctions: false 21 memorySize: 128 22 23functions: 24 api: 25 handler: wsgi_handler.handler 26 events: 27 - httpApi: \u0026#39;*\u0026#39; 28 environment: 29 MONGO_CONNECTION_STRING: \${env:MONGO_CONNECTION_STRING} 30 MONGO_COLLECTION_DB_NAME: \${env:MONGO_COLLECTION_DB_NAME} 31 32package: 33 patterns: 34 - \u0026#39;!.dynamodb/**\u0026#39; 35 - \u0026#39;!.git/**\u0026#39; 36 - \u0026#39;!.vscode/**\u0026#39; 37 - \u0026#39;!.env\u0026#39; 38 - \u0026#39;!node_modules/**\u0026#39; 39 - \u0026#39;!tmp/**\u0026#39; 40 - \u0026#39;!venv/**\u0026#39; 41 - \u0026#39;!__pycache__/**\u0026#39; 42 43plugins: 44 - serverless-wsgi 45 - serverless-python-requirements Create Flask app Prerequisites 1python -m venv ./venv 2source ./venv/bin/activate App src/app.py
1from flask import Flask, ObjectId, request, jsonify, make_response 2from flask_cors import CORS 3import json 4from src.mongo import my_db 5 6 7users_collection = my_db.users 8 9 10app = Flask(__name__) 11cors = CORS(app) 12 13 14@app.route(\u0026#34;/\u0026#34;, methods=[\u0026#39;GET\u0026#39;]) 15def get_user(user_id): 16 user_id = request.args.get(\u0026#39;id\u0026#39;) 17 user = users_collection.find_one({\u0026#34;_id\u0026#34;: ObjectId(user_id)}) 18 if not user: 19 return jsonify({\u0026#39;error\u0026#39;: \u0026#39;data not found\u0026#39;}), 404 20 return jsonify({\u0026#39;user\u0026#39;: user}) 21 22 23@app.route(\u0026#39;/\u0026#39;, methods=[\u0026#39;PUT\u0026#39;]) 24def create_record(): 25 record = json.loads(request.data) 26 user_id = record.get(\u0026#39;user_id\u0026#39;, None) 27 users_collection.update_one({\u0026#34;_id\u0026#34;: ObjectId(user_id)}, record) 28 29 30@app.route(\u0026#34;/\u0026#34;) 31def hello(): 32 return jsonify(message=\u0026#39;Hello!\u0026#39;) 33 34 35@app.errorhandler(404) 36def resource_not_found(e): 37 return make_response(jsonify(error=\u0026#39;Not found!\u0026#39;), 404) 38 39 40def internal_server_error(e): 41 return \u0026#39;error\u0026#39;, 500 42 43 44app.register_error_handler(500, internal_server_error) src/mongo.py
1import os 2 3from pymongo import MongoClient 4 5MONGO_CONNECTION_STRING = os.environ.get( 6 \u0026#34;MONGO_CONNECTION_STRING\u0026#34;, default=\u0026#34;mongodb://localhost:27017/\u0026#34; 7) 8MONGO_COLLECTION_DB_NAME = os.environ.get( 9 \u0026#34;MONGO_COLLECTION_DB_NAME\u0026#34;, default=\u0026#34;test-mydb\u0026#34; 10) 11 12 13db_client = MongoClient(MONGO_CONNECTION_STRING) 14my_db = db_client[MONGO_COLLECTION_DB_NAME] .env
1MONGO_CONNECTION_STRING=mongodb+srv://login:password@cluster0.XXXXX.mongodb.net/mydb?retryWrites=true\u0026amp;w=majority 2MONGO_COLLECTION_DB_NAME=mydb src/requirements.txt
1certifi==2022.6.15 2charset-normalizer==2.1.1 3click==7.1.2 4dnspython==2.2.1 5ecdsa==0.18.0 6Flask==1.1.4 7Flask-Cors==3.0.10 8idna==3.3 9importlib-metadata==4.12.0 10itsdangerous==1.1.0 11Jinja2==2.11.3 12jmespath==1.0.1 13MarkupSafe==2.0.1 14pyasn1==0.4.8 15pymongo==4.2.0 16python-dateutil==2.8.2 17python-dotenv==0.20.0 18requests==2.28.1 19rsa==4.9 20six==1.16.0 21urllib3==1.26.12 22Werkzeug==1.0.1 23zipp==3.8.1 Deployment 1serverless login install dependencies with:
1npm install and
1pip install -r requirements.txt and then perform deployment with:
1serverless deploy After running deploy, you should see output similar to:
1Deploying app-service-name to stage dev (eu-west-1) 2 3✔ Service deployed to stack app-service-name (182s) Local development Thanks to capabilities of serverless-wsgi, it is also possible to run your application locally, however, in order to do that, you will need to first install werkzeug dependency, as well as all other dependencies listed in requirements.txt. It is recommended to use a dedicated virtual environment for that purpose. You can install all needed dependencies with the following commands:
Already in requirements.txt:
1pip install werkzeug 2pip install -r requirements.txt At this point, you can run your application locally with the following command:
1serverless wsgi serve For additional local development capabilities of serverless-wsgi plugin, please refer to corresponding GitHub repository.
`,url:"https://romankurnovskii.com/en/posts/serverless-flask-lambda-api-gateway-mongodb/"},"https://romankurnovskii.com/en/apps/npm/cognito-token-observer/":{title:"cognito-token-observer",tags:["npm"],content:"",url:"https://romankurnovskii.com/en/apps/npm/cognito-token-observer/"},"https://romankurnovskii.com/en/tags/npm/":{title:"npm",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/npm/"},"https://romankurnovskii.com/en/categories/npm-packages/":{title:"npm packages",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/npm-packages/"},"https://romankurnovskii.com/en/apps/npm/hugo-lunr-ml/":{title:"hugo-lunr-ml",tags:["npm"],content:"",url:"https://romankurnovskii.com/en/apps/npm/hugo-lunr-ml/"},"https://romankurnovskii.com/en/posts/hugo-add-search-lunr-popup/":{title:"Add search to Hugo multilingual static site with Lunr",tags:["hugo","lunr","javascript"],content:"Initial I had the need to implement search functionality on my site. Content on is in different languages.\nThe goal is to impelemnt search for all pages and separate search results for each and every language.\nHow it works Hugo generates the search index. In this case it means that we get json file with every static page on the site.\nTo make search works we need to create index. lunr.js takes care of it.\nClient send query -\u0026gt; our script \u0026ldquo;tries to find\u0026rdquo; in the index\nRender the results\nThis is how the logic looks like:\nImplementation Create search form Create popup modal where will render search results Connect Lunr.js script Generate pages data Connect search/result forms with lunr.js search TL;DR Files to change/create:\n1. `/layouts/partials/header.html` 1\u0026lt;form id=\u0026#34;search\u0026#34;\u0026gt; 2 \u0026lt;input type=\u0026#34;text\u0026#34; type=\u0026#34;search\u0026#34; id=\u0026#34;search-input\u0026#34;\u0026gt; 3\u0026lt;/form\u0026gt; 2. `/layouts/partials/components/search-list-popup.html` 1\u0026lt;div id=\u0026#34;search-result\u0026#34; tabindex=\u0026#34;-1\u0026#34; 2 class=\u0026#34;overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \u0026#34; hidden\u0026gt; 3 \u0026lt;div class=\u0026#34;relative p-4 w-full max-w-xs h-full md:h-auto\u0026#34;\u0026gt; 4 \u0026lt;div class=\u0026#34;relative bg-white rounded-lg shadow dark:bg-gray-700\u0026#34;\u0026gt; 5 \u0026lt;div class=\u0026#34;p-6\u0026#34;\u0026gt; 6 \u0026lt;h3\u0026gt;Search results\u0026lt;/h3\u0026gt; 7 \u0026lt;div id=\u0026#34;search-results\u0026#34; class=\u0026#34;prose\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 8 \u0026lt;/div\u0026gt; 9 \u0026lt;/div\u0026gt; 10 \u0026lt;/div\u0026gt; 11\u0026lt;/div\u0026gt; 3. `/layouts/partials/footer.html` 1... 2{{ $languageMode := .Site.Language }} 3\u0026lt;script src=\u0026#34;https://unpkg.com/lunr/lunr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 4\u0026lt;script src=\u0026#34;/js/search.js?1\u0026#34; languageMode={{ $languageMode }} \u0026gt;\u0026lt;/script\u0026gt; 5 6{{ partial \u0026#34;components/search-list-popup.html\u0026#34; . }} 7... 4. `/layouts/_default/index.json` 1[ 2 {{- range $index, $page := .Site.RegularPages.ByTitle -}} 3 {{- if gt $index 0 -}} , {{- end -}} 4 {{- $entry := dict \u0026#34;uri\u0026#34; $page.RelPermalink \u0026#34;title\u0026#34; $page.Title -}} 5 {{- $entry = merge $entry (dict \u0026#34;description\u0026#34; .Description) -}} 6 {{- $entry = merge $entry (dict \u0026#34;content\u0026#34; (.Plain | htmlUnescape)) -}} 7 {{- $entry | jsonify -}} 8 {{- end -}} 9] 5. `config.yaml` 1# config.yaml 2# need for search popup service / creates search.json index fo lunr.js 3 4outputFormats: 5 SearchIndex: 6 baseName: search 7 mediaType: application/json 8 9outputs: 10 home: 11 - HTML 12 - RSS 13 - SearchIndex 6. `static/js/search.js` 1const languageMode = window.document.currentScript.getAttribute(\u0026#39;languageMode\u0026#39;); 2const MAX_SEARCH_RESULTS = 10 3 4let searchIndex = {} 5let pagesStore = {} 6 7// Need to create ONLY once , maybe before push | during build 8const createIndex = (documents) =\u0026gt; { 9 searchIndex = lunr(function () { 10 this.field(\u0026#34;title\u0026#34;); 11 this.field(\u0026#34;content\u0026#34;); 12 this.field(\u0026#34;description\u0026#34;); 13 this.field(\u0026#34;uri\u0026#34;); 14 15 this.ref(\u0026#39;uri\u0026#39;) 16 17 documents.forEach(function (doc) { 18 pagesStore[doc[\u0026#39;uri\u0026#39;]] = doc[\u0026#39;title\u0026#39;] 19 this.add(doc) 20 }, this) 21 }) 22 23} 24 25const loadIndexData = () =\u0026gt; { 26 const url = `/${languageMode}/search.json`; 27 28 var xmlhttp = new XMLHttpRequest(); 29 xmlhttp.onreadystatechange = function () { 30 if (this.readyState == 4 \u0026amp;\u0026amp; this.status == 200) { 31 const pages_content = JSON.parse(this.responseText); 32 createIndex(pages_content) 33 } 34 }; 35 36 xmlhttp.open(\u0026#34;GET\u0026#34;, url, true); 37 xmlhttp.send(); 38} 39 40const search = (text) =\u0026gt; { 41 let result = searchIndex.search(text) 42 return result 43} 44 45const hideSearchResults = (event, divBlock) =\u0026gt; { 46 event.preventDefault() 47 if (!divBlock.contains(event.target)) { 48 divBlock.style.display = \u0026#39;none\u0026#39;; 49 divBlock.setAttribute(\u0026#39;class\u0026#39;, \u0026#39;hidden\u0026#39;) 50 } 51} 52 53// TODO refactor 54const renderSearchResults = (results) =\u0026gt; { 55 const searchResultsViewBlock = document.getElementById(\u0026#39;search-result\u0026#39;) 56 57 // hide on move mouse from results block 58 document.addEventListener(\u0026#39;mouseup\u0026#39;, (e) =\u0026gt; hideSearchResults(e, searchResultsViewBlock)); 59 60 const searchResultsDiv = document.getElementById(\u0026#39;search-results\u0026#39;) 61 searchResultsDiv.innerHTML = \u0026#39;\u0026#39; 62 63 searchResultsViewBlock.style.display = \u0026#39;initial\u0026#39;; 64 searchResultsViewBlock.removeAttribute(\u0026#39;hidden\u0026#39;) 65 66 67 const resultsBlock = document.createElement(\u0026#39;ul\u0026#39;) 68 69 for (let post of results) { 70 const url = post[\u0026#39;ref\u0026#39;] 71 const title = pagesStore[url] 72 73 let commentBlock = document.createElement(\u0026#39;li\u0026#39;) 74 75 let link = document.createElement(\u0026#39;a\u0026#39;,) 76 let linkText = document.createTextNode(title); 77 link.appendChild(linkText) 78 link.href = url 79 80 commentBlock.appendChild(link) 81 resultsBlock.appendChild(commentBlock) 82 } 83 84 searchResultsDiv.appendChild(resultsBlock) 85 86} 87 88 89const searchFormObserver = () =\u0026gt; { 90 var form = document.getElementById(\u0026#34;search\u0026#34;); 91 var input = document.getElementById(\u0026#34;search-input\u0026#34;); 92 93 form.addEventListener(\u0026#34;submit\u0026#34;, function (event) { 94 event.preventDefault(); 95 var term = input.value.trim(); 96 if (!term) { 97 return 98 } 99 100 const search_results = search(term, languageMode); 101 renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) 102 103 }, false); 104} 105 106// create indexes 107loadIndexData() 108 109searchFormObserver() Search form I am going to add search form to the header part. For thios purpose edit header.html file in the path /layouts/partials/header.html\nSet form id: search. By this id script can find this form\nMinimal form for work:\n1\u0026lt;form id=\u0026#34;search\u0026#34;\u0026gt; 2 \u0026lt;input type=\u0026#34;text\u0026#34; type=\u0026#34;search\u0026#34; id=\u0026#34;search-input\u0026#34;\u0026gt; 3\u0026lt;/form\u0026gt; I use Tailwind, so this is how my form looks like:\n1\u0026lt;div class=\u0026#34;relative pt-4 md:pt-0\u0026#34;\u0026gt; 2 \u0026lt;form id=\u0026#34;search\u0026#34; class=\u0026#34;flex items-center\u0026#34;\u0026gt; 3 \u0026lt;label for=\u0026#34;search-input\u0026#34; class=\u0026#34;sr-only\u0026#34;\u0026gt;Search\u0026lt;/label\u0026gt; 4 \u0026lt;div class=\u0026#34;relative w-full\u0026#34;\u0026gt; 5 \u0026lt;input type=\u0026#34;text\u0026#34; type=\u0026#34;search\u0026#34; id=\u0026#34;search-input\u0026#34; class=\u0026#34;bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full pl-10 p-2.5 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500\u0026#34; placeholder=\u0026#34;Search\u0026#34; required\u0026gt; 6 \u0026lt;/div\u0026gt; 7 \u0026lt;/form\u0026gt; 8\u0026lt;/div\u0026gt; Modal with results By default this modal window is hidden. So don\u0026rsquo;t need to add this to any page. But need to add somewhere.\n1. Create .html component\npath: /layouts/partials/components/search-list-popup.html\nFor modal block to show or hide I use id: search-result\nFor block with search results id is: search-results\nContent:\n1\u0026lt;div id=\u0026#34;search-result\u0026#34; tabindex=\u0026#34;-1\u0026#34; 2 class=\u0026#34;overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs \u0026#34; hidden\u0026gt; 3 \u0026lt;div class=\u0026#34;relative p-4 w-full max-w-xs h-full md:h-auto\u0026#34;\u0026gt; 4 \u0026lt;div class=\u0026#34;relative bg-white rounded-lg shadow dark:bg-gray-700\u0026#34;\u0026gt; 5 \u0026lt;div class=\u0026#34;p-6\u0026#34;\u0026gt; 6 \u0026lt;h3\u0026gt;Search results\u0026lt;/h3\u0026gt; 7 \u0026lt;div id=\u0026#34;search-results\u0026#34; class=\u0026#34;prose\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 8 \u0026lt;/div\u0026gt; 9 \u0026lt;/div\u0026gt; 10 \u0026lt;/div\u0026gt; 11\u0026lt;/div\u0026gt; 2. Add component to the site\nAdd this component to the footer. File path: /layouts/partials/footer.html\n1... 2 {{ partial \u0026#34;components/search-list-popup.html\u0026#34; . }} 3... Connect Lunr.js Add link to this script to the footer template too\nPart of the footer template:\n1... 2 \u0026lt;script src=\u0026#34;https://unpkg.com/lunr/lunr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 3 {{ partial \u0026#34;components/search-list-popup.html\u0026#34; . }} 4... Generate pages data Hugo can generate the search index the same way it generates RSS feeds for example, it’s just another output format.\n1. Generate script\nThis generator is for multilingual site\nCreates json in each language catalog in format:\n1[{\u0026#34;title\u0026#34;:\u0026#34;title01\u0026#34;,...}] Fepends on fileds inckluded in the layout /layouts/_default/index.json\nCreate file /layouts/_default/index.json\n1[ 2 {{- range $index, $page := .Site.RegularPages.ByTitle -}} 3 {{- if $page.IsTranslated -}} 4 {{ if gt (index $page.Translations 0).WordCount 0 }} 5 {{ range .Translations }} 6 {{- if gt $translatedCount 0 -}} , {{- end -}} 7 {{- $entry := dict \u0026#34;uri\u0026#34; .RelPermalink \u0026#34;title\u0026#34; .Title -}} 8 {{- $entry = merge $entry (dict \u0026#34;description\u0026#34; .Description) -}} 9 {{- $entry = merge $entry (dict \u0026#34;content\u0026#34; (.Plain | htmlUnescape)) -}} 10 {{- $entry | jsonify -}} 11 {{ $translatedCount = add $translatedCount 1 }} 12 {{ end}} 13 {{ end }} 14 {{- end -}} 15 {{- end -}} 16 ] Creates search.json file with page indexes in /public/search.json\n2. Set index file path\nUpdate config.yaml file:\n1# config.yaml 2# need for search popup service / creates search.json index fo lunr.js 3 4outputFormats: 5 SearchIndex: 6 baseName: search 7 mediaType: application/json 8 9outputs: 10 home: 11 - HTML 12 - RSS 13 - SearchIndex Connect search/result forms with lunr.js search Create file in the path: static/js/search.js\n1const languageMode = window.document.currentScript.getAttribute(\u0026#39;languageMode\u0026#39;); 2const MAX_SEARCH_RESULTS = 10 3 4let searchIndex = {} 5let pagesStore = {} 6 7// Need to create ONLY once , maybe before push | during build 8const createIndex = (documents) =\u0026gt; { 9 searchIndex = lunr(function () { 10 this.field(\u0026#34;title\u0026#34;); 11 this.field(\u0026#34;content\u0026#34;); 12 this.field(\u0026#34;description\u0026#34;); 13 this.field(\u0026#34;uri\u0026#34;); 14 15 this.ref(\u0026#39;uri\u0026#39;) 16 17 documents.forEach(function (doc) { 18 pagesStore[doc[\u0026#39;uri\u0026#39;]] = doc[\u0026#39;title\u0026#39;] 19 this.add(doc) 20 }, this) 21 }) 22 23} 24 25const loadIndexData = () =\u0026gt; { 26 const url = `/${languageMode}/search.json`; 27 28 var xmlhttp = new XMLHttpRequest(); 29 xmlhttp.onreadystatechange = function () { 30 if (this.readyState == 4 \u0026amp;\u0026amp; this.status == 200) { 31 const pages_content = JSON.parse(this.responseText); 32 createIndex(pages_content) 33 } 34 }; 35 36 xmlhttp.open(\u0026#34;GET\u0026#34;, url, true); 37 xmlhttp.send(); 38} 39 40const search = (text) =\u0026gt; { 41 let result = searchIndex.search(text) 42 return result 43} 44 45const hideSearchResults = (event, divBlock) =\u0026gt; { 46 event.preventDefault() 47 if (!divBlock.contains(event.target)) { 48 divBlock.style.display = \u0026#39;none\u0026#39;; 49 divBlock.setAttribute(\u0026#39;class\u0026#39;, \u0026#39;hidden\u0026#39;) 50 } 51} 52 53// TODO refactor 54const renderSearchResults = (results) =\u0026gt; { 55 const searchResultsViewBlock = document.getElementById(\u0026#39;search-result\u0026#39;) 56 57 // hide on move mouse from results block 58 document.addEventListener(\u0026#39;mouseup\u0026#39;, (e) =\u0026gt; hideSearchResults(e, searchResultsViewBlock)); 59 60 const searchResultsDiv = document.getElementById(\u0026#39;search-results\u0026#39;) 61 searchResultsDiv.innerHTML = \u0026#39;\u0026#39; 62 63 searchResultsViewBlock.style.display = \u0026#39;initial\u0026#39;; 64 searchResultsViewBlock.removeAttribute(\u0026#39;hidden\u0026#39;) 65 66 67 const resultsBlock = document.createElement(\u0026#39;ul\u0026#39;) 68 69 for (let post of results) { 70 const url = post[\u0026#39;ref\u0026#39;] 71 const title = pagesStore[url] 72 73 let commentBlock = document.createElement(\u0026#39;li\u0026#39;) 74 75 let link = document.createElement(\u0026#39;a\u0026#39;,) 76 let linkText = document.createTextNode(title); 77 link.appendChild(linkText) 78 link.href = url 79 80 commentBlock.appendChild(link) 81 resultsBlock.appendChild(commentBlock) 82 } 83 84 searchResultsDiv.appendChild(resultsBlock) 85 86} 87 88 89const searchFormObserver = () =\u0026gt; { 90 var form = document.getElementById(\u0026#34;search\u0026#34;); 91 var input = document.getElementById(\u0026#34;search-input\u0026#34;); 92 93 form.addEventListener(\u0026#34;submit\u0026#34;, function (event) { 94 event.preventDefault(); 95 var term = input.value.trim(); 96 if (!term) { 97 return 98 } 99 100 const search_results = search(term, languageMode); 101 renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS)) 102 103 }, false); 104} 105 106// create indexes 107loadIndexData() 108 109searchFormObserver() Next need to add this file to the site: /layouts/partials/footer.html\nNow footer looks like this:\n1... 2{{ $languageMode := .Site.Language }} 3\u0026lt;script src=\u0026#34;https://unpkg.com/lunr/lunr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 4\u0026lt;script src=\u0026#34;/js/search.js?1\u0026#34; languageMode={{ $languageMode }} \u0026gt;\u0026lt;/script\u0026gt; 5 6{{ partial \u0026#34;components/search-list-popup.html\u0026#34; . }} 7... ",url:"https://romankurnovskii.com/en/posts/hugo-add-search-lunr-popup/"},"https://romankurnovskii.com/en/tags/lunr/":{title:"lunr",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/lunr/"},"https://romankurnovskii.com/en/posts/hugo-add-image-zoomin/":{title:"Hugo resize a picture on click",tags:["hugo"],content:`Introduction Hugo by default uses parsing of markdown files. This means that we get the html code as it is written in markdown.
In order to understand which images we can enhance, we add a separate tag/key/id to those images
Tools To implement the functionality, we need to:
Write/connect a script/handler that will perform the zoomin effect on the images we need Add the necessary metadata to the images, so the script can find them zoomin script To add the ability to zoom on click, we will use the medium-zoom package.
This package provides this functionality in a non-loaded, handy style.
Demo
Script logic The script finds images with id and so understands to apply the zoomin property to those images
Possible id:
zoom-default zoom-margin zoom-background zoom-scrollOffset zoom-trigger zoom-detach zoom-center Connecting the scripts In order for the script to work, we need to connect the logic as well as the handler.
Hugo has a static folder in the root of the project, which can be used to store static files (styles, scripts) and used to connect them to the site. If there is no such folder, you can create one.
In the static folder create a folder zoom-image and add two scripts to it
static/js/zoom-image/index.js 1const zoomDefault = mediumZoom(\u0026#39;#zoom-default\u0026#39;) 2const zoomMargin = mediumZoom(\u0026#39;#zoom-margin\u0026#39;, { margin: 48 }) 3const zoomBackground = mediumZoom(\u0026#39;#zoom-background\u0026#39;, { background: \u0026#39;#212530\u0026#39; }) 4const zoomScrollOffset = mediumZoom(\u0026#39;#zoom-scrollOffset\u0026#39;, { 5 scrollOffset: 0, 6 background: \u0026#39;rgba(25, 18, 25, .9)\u0026#39;, 7}) 8 9// Trigger the zoom when the button is clicked 10const zoomToTrigger = mediumZoom(\u0026#39;#zoom-trigger\u0026#39;) 11const button = document.querySelector(\u0026#39;#button-trigger\u0026#39;) 12button.addEventListener(\u0026#39;click\u0026#39;, () =\u0026gt; zoomToTrigger.open()) 13 14// Detach the zoom after having been zoomed once 15const zoomToDetach = mediumZoom(\u0026#39;#zoom-detach\u0026#39;) 16zoomToDetach.on(\u0026#39;closed\u0026#39;, () =\u0026gt; zoomToDetach.detach()) 17 18// Observe zooms to write the history 19const observedZooms = [ 20 zoomDefault, 21 zoomMargin, 22 zoomBackground, 23 zoomScrollOffset, 24 zoomToTrigger, 25 zoomToDetach, 26] 27 28// Log all interactions in the history 29const history = document.querySelector(\u0026#39;#history\u0026#39;) 30 31observedZooms.forEach(zoom =\u0026gt; { 32 zoom.on(\u0026#39;open\u0026#39;, event =\u0026gt; { 33 const time = new Date().toLocaleTimeString() 34 history.innerHTML += \`\u0026lt;li\u0026gt;Image \u0026#34;\u0026lt;em\u0026gt;\${event.target.alt 35 }\u0026lt;/em\u0026gt;\u0026#34; was zoomed at \${time}\u0026lt;/li\u0026gt;\` 36 }) 37 38 zoom.on(\u0026#39;detach\u0026#39;, event =\u0026gt; { 39 const time = new Date().toLocaleTimeString() 40 history.innerHTML += \`\u0026lt;li\u0026gt;Image \u0026lt;em\u0026gt;\u0026#34;\${event.target.alt 41 }\u0026#34;\u0026lt;/em\u0026gt; was detached at \${time}\u0026lt;/li\u0026gt;\` 42 }) 43}) static/js/zoom-image/placeholders.js 1// Show placeholders for paragraphs 2const paragraphs = [].slice.call(document.querySelectorAll(\u0026#39;p.placeholder\u0026#39;)) 3 4paragraphs.forEach(paragraph =\u0026gt; { 5 // eslint-disable-next-line no-param-reassign 6 paragraph.innerHTML = paragraph.textContent 7 .split(\u0026#39; \u0026#39;) 8 .filter(text =\u0026gt; text.length \u0026gt; 4) 9 .map(text =\u0026gt; \`\u0026lt;span class=\u0026#34;placeholder__word\u0026#34;\u0026gt;\${text}\u0026lt;/span\u0026gt;\`) 10 .join(\u0026#39; \u0026#39;) 11}) CDN script You can download the script, or you can upload it
Script Link
Adding to template In order for these scripts to work in the website template, they must be connected.
I use for this the template baseof.html. I simply add links to the scripts in body of the template.
1 # baseof.html 2 3 ... 4 \u0026lt;/footer\u0026gt; 5 6 \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js\u0026#34; defer\u0026gt;\u0026lt;/script\u0026gt; 7 \u0026lt;script src=\u0026#34;/js/zoom-image/placeholders.js\u0026#34; defer\u0026gt;\u0026lt;/script\u0026gt; 8 \u0026lt;script src=\u0026#34;/js/zoom-image/index.js\u0026#34; defer\u0026gt;\u0026lt;/script\u0026gt; 9 10 \u0026lt;/body\u0026gt; 11\u0026lt;/html\u0026gt; image ID Hugo allows you to change the parsing behavior of markdown files with hooks. You can read more about render-hooks at website.
In the *layouts folder.
Let\u0026rsquo;s add the file render-image.html to the following path layouts -\u0026gt; _default -\u0026gt; _markup file code:
1\u0026lt;p class=\u0026#34;md__image\u0026#34;\u0026gt; 2 \u0026lt;img src=\u0026#34;{{ .Destination | safeURL }}\u0026#34; id=\u0026#34;zoom-default\u0026#34; alt=\u0026#34;{{ .Text }}\u0026#34; {{ with .Title}} title=\u0026#34;{{ . }}\u0026#34; {{ end }} /\u0026gt; 3\u0026lt;/p\u0026gt; We only added id=\u0026quot;zoom-default\u0026quot; to the default code
Result Your browser does not support the video tag. Process `,url:"https://romankurnovskii.com/en/posts/hugo-add-image-zoomin/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeguru/automating-code-reviews-amazon-codeguru/":{title:"Automating Code Reviews with Amazon CodeGuru",tags:[],content:`Lab Automating Code Reviews with Amazon CodeGuru Associating Amazon CodeGuru with a CodeCommit Repository 1. Navigate to the Amazon CodeCommit console.
2. Click java-web-app:
3. Notice that at the moment, only a README file has been committed to the master branch. Next, you\u0026rsquo;ll associate CodeGuru with this repository, so that CodeGuru can begin to analyze the code therein.
4. Go to the CodeGuru dashboard.
5. Click Associate Repository and run analysis:
6. Select AWS CodeCommit as the provider, choose java-web-app from the repository dropdown, enter _master _into Source branch and click Associate:
In roughly one minute, you\u0026rsquo;ll see that CodeGuru has associated with your repository:
Triggering an Amazon CodeGuru Review 1. Navigate to :8080 in your browser. Note: This is the IP of an EC2 instance that can be found in the EC2 console.
2. Click the file icon in the top left to open the file tree:
Note: During the creation of this lab, two things were performed automatically. One is that the CodeCommit repository you visited earlier was cloned to the directory you\u0026rsquo;re looking at in the IDE now. Another is that the framework for a Java web app was added in addition to the single README you saw. This is so that you can see the benefits of CodeGuru without having to work heavily with code.
In this lab step, you\u0026rsquo;ll push all the new code to the nearly-empty Code Commit repository, to trigger a CodeGuru review.
3. Open the terminal in your IDE:
4. In the terminal, add the new files to a Git branch, and commit and push the changes:
1cd /cloudacademy/lab 2git add . 3git checkout -b trigger_branch 4git commit -m \u0026#34;trigger a CodeGuru analysis by pushing Java code\u0026#34; 5git push origin trigger_branch This will create a Git commit that includes all the Java files in a branch called trigger_branch, so that you can make a pull request in CodeCommit. Since CodeGuru analyses are triggered by pull requests, this is what will trigger a CodeGuru analysis.
5. Back on the CodeCommit dashboard, click Create pull request:
6. Set the Destination to master and the source to trigger_branch and click Compare:
7. Type Trigger a CodeGuru Reviewer Analysis into the Title field and click Create pull request:
This will create a pull request and trigger a CodeGuru review.
Viewing Amazon CodeGuru Comments 1. If you weren\u0026rsquo;t automatically brought to the pull request details page after creating your pull request, click Pull Requests beneath Repositories on the left side of the page:
2. Click the only available pull request:
3. Notice the section mentioning CodeGuru Reviewer:
This section will display in each pull request made in any repository associated with CodeGuru. As of the time this lab was released, CodeGuru is still in preview. As the section on your pull request details tab mentions, because it\u0026rsquo;s in preview mode, CodeGuru can take a while to process a pull request. There isn\u0026rsquo;t a way to track its progress, and you currently won\u0026rsquo;t be alerted when that processing begins or finishes.
4. Select the Changes tab:
5. In the Go to file filter, enter dockerservlet and click the result to navigate to the file:
You may need to scroll down the page to find the DockerServlet.java file changes. This file is known to have CodeGuru Reviewer comments that usually appear a few minutes after creating the pull request.
6. Scroll down to line 60 to see an example of a comment from CodeGuru Reviewer (If you don\u0026rsquo;t see any comment you may try refreshing the page every minute until one appears):
You can then make updates as you see fit, and submit more pull requests to see if you\u0026rsquo;ve addressed CodeGuru\u0026rsquo;s suggestions.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codeguru/automating-code-reviews-amazon-codeguru/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/opensearch-service/build-log-aggregation-system/":{title:"Build A Log Aggregation System in AWS",tags:[],content:`Lab Monitor Like a DevOps Pro: Build A Log Aggregation System in AWS Navigating to Your Cloud\u0026rsquo;s Lambda Function 1. In the AWS Management Console search bar, enter Cloud Formation, and click the CloudFormation result under Services:
This will bring you to the CloudFormation Stacks table.
There will be one stack named cloudacademylabs in the table with a Status of CREATE_COMPLETE.
Note: If the stack hasn\u0026rsquo;t reached the **Status **of CREATE_COMPLETE, try refreshing the page after a minute. It only takes a minute for the stack to fully create.
2. To view details of the stack, under Stack name, click the cloudacademylabs link.
3. Click the Resources tab:
Your Physical IDs will be different than in the supplied image. Note in the **Type **column that a DynamoDB Table, a Lambda Function, and **IAM **resources to grant the Lambda access to the DynamoDB Table have all been created. You will be querying the DynamoDB table via Lambda function invocations to create CloudWatch Logs, that will be aggregated and searchable via a user interface (UI).
4. Click on the Outputs tab, and open the DynamoLambdaConsoleLink link in the Value column:
This takes you to the Lambda function Console.
Creating Some Logs Using AWS Lambda 1. Briefly look around the Lambda function console:
The **Designer **gives a visual representation of the AWS resources that trigger the function (there are none in this case), and the AWS resources the function has access to (CloudWatch Logs, and DynamoDB). The actual code that is executed by the function is farther down in the Function **code **section. You don\u0026rsquo;t need to worry about the actual implementation details of the function for this Lab.
2. To configure a test event to trigger the function, scroll down to the Code source section and click Test:
3. In the Configure test event form, enter the following values before scrolling to the bottom and click Save:
Event name:TestPutEvent Enter the following in the code editor at the bottom of the form: Copy code
1{ 2 \u0026#34;fn\u0026#34;: \u0026#34;PUT\u0026#34;, 3 \u0026#34;data\u0026#34;: { 4 \u0026#34;id\u0026#34;: \u0026#34;12345\u0026#34;, 5 \u0026#34;name\u0026#34;: \u0026#34;foobar\u0026#34; 6 } 7} The PUT object event will update the DynamoDB database with an object with the given id.
4. To run your function with your test event, click Test again:
After a few seconds, in the code editor, a tab called Execution results will load:
The function succeeded and the Function Logsarea displays the logs that were generated and automatically sent to CloudWatch Logs by AWS Lambda.
5. To view the Amazon CloudWatch logs, click the Monitor tab, and then click View logs in CloudWatch:
Note: The Lab\u0026rsquo;s CloudFormation stack outputs also include a link to the Log Group if you need to access it at a later time.
Manually Viewing Logs in Amazon CloudWatch 1. Observe the Log Streams in the CloudWatch log group for the Lambda function you invoked:
The rows in the table are different Log Streams for the log group.
Each log stream corresponds to log events from the same source. AWS Lambda creates a new log event for every Lambda invocation. However, it is possible to have multiple log streams for a single Lambda function since the log stream corresponds to the container actually running the function.
Behind the scenes, a Lambda is run in a container. After a period of inactivity, the container is unloaded and the following requests will be served by a new container, thus creating a new log stream. Depending on how many times you invoked the test command in the previous step, you will see one or more rows in the log stream.
2. Click on the latest Log Stream.
The log stream is a record of event Messages ordered in Time:
3. Enter _PUT _into the **Filter events **search bar and click enter:
4. Click the triangle to expand the event that matches the filter.
You will see the JSON formatted message:
The outermost data attribute wraps the test event you configured.
5. Click custom to display the custom time range filter available in CloudWatch Logs:
Observe the time-based options in the dialog box that displays:
The filter by text and by time capabilities are the tools that are available for sifting through logs in CloudWatch Logs. The text filters support some forms of conditions that can be expressed through a syntax specific to CloudWatch. These capabilities are handy, but you will see that there are more powerful tools available for log aggregation and retrieval.
Launching the OpenSearch Domain The first thing you need is an Amazon OpenSearch cluster/domain. Using the Amazon OpenSearch Service has the following benefits:
It\u0026rsquo;s distributed and resilient It supports aggregations It supports free-text search It\u0026rsquo;s managed and takes care of most of the operational complexities of operating a cluster In 2021 AWS renamed Amazon ElasticSearch Service to Amazon OpenSearch Service. You may see references to ElasticSearch in the Amazon Management Console. You should assume that ElasticSearch and OpenSearch refer to the same AWS service.
The following diagram illustrates the overall design of the AWS Lab environment and the part that you are building in this lab step is highlighted in the lower-left corner in the AWS cloud:
1. In the search bar at the top, enter OpenSearch, and under Services, click the Amazon OpenSearch Service result:
2. To begin creating your cluster, on the right-hand side of the welcome page, click Create domain:
The terms OpenSearch domain and an OpenSearch cluster can be used almost interchangeably. The former is the logical search resource, and the latter is the actual servers that are launched to create a domain.
The Create domain form will load. 3. In the Name section, in the Domain name textbox, enter ca-labs-domain-###, replacing ### with some random numbers:
4. In the Deployment type section, select the following:
Deployment type: Select Development and testing Version: Select **6.8 **under ElasticSearch In this short-lived lab, you are using a Development and testing deployment because it allows public access and reliability isn\u0026rsquo;t a concern. In a production environment, you will want to use a Production deployment to get the full availability benefits and meet security requirements.
5. In the Auto-Tune section, select Disable.
In this short-lived lab, Auto-Tune is not necessary.
6. In the Data nodes section, enter and select the following and leave remaining defaults:
Instance type: Select t3.small.search Number of nodes: Enter 1 The storage type values correspond to the storage types available for Amazon EC2 instances.
When deploying a cluster that uses multiple nodes, you can specify that the nodes are deployed in two or three availability zones. Deploying in multiple availability zones makes the cluster highly available and more reliable in the case of failures of outages.
7. Scroll down to the Network section, and select Public access:
In this lab, you are creating a publicly available Amazon OpenSearch Service cluster for convenience. Be aware that you can also deploy a cluster into an Amazon Virtual Private Cloud (VPC) and receive the network isolation and security advantages of using a VPC.
8. In the **Fine-grained access control **section, uncheck the **Enable fine-grained access control **box.
9. In a new browser tab, enter the following URL:
https://checkip.amazonaws.com/
You will see an IP address displayed. This is the public IPv4 address of your internet connection. You will use this IP address to restrict access to your Amazon OpenSearch Service cluster.
10. Scroll down to the Access Policy section and under Domain access policy, select Configure domain level access policy:
You will see a policy editor form display with the tabs Visual editor and JSON.
11. In the Visual editor tab, enter and select the following:
Type: Select IPv4 address Principal: Enter the IP address you saw on the Check IP Page Action: Select Allow You are specifying an access policy that allows access to the cluster from your IP address. In a non-lab environment, you could deploy the cluster into an Amazon VPC and configure private or public access using a VPC\u0026rsquo;s networking features.
12. To finish creating your cluster, scroll to the bottom and click Create:
A page displaying details of your cluster will load and you will see a green notification that you have successfully created a cluster.
13. In the General information section, observe the Domain status:
AWS is setting up and deploying your cluster. This process can take up to 15 or 30 minutes to complete.
12. To see the latest status of your Amazon OpenSearch Service cluster, refresh the page in your browser.
Refresh the page for your domain periodically to check if it has finished deploying.
Whilst waiting for the domain to finish provisioning, feel free to consult the Amazon OpenSearch Service Developer Guide to learn more about the OpenSearch service.
When the cluster has been provisioned you will see the Domain status change to Active:
Sending CloudWatch Logs to OpenSearch 1. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:
2. In the left-hand menu, under Logs, click on Log groups:
3. Select the log group beginning with /aws/lambda/cloudacademylabs-DynamoLambda-:
Next, you will create a subscription filter to send the log data to your ElasticSearch domain.
4. Click Actions, in the menu that opens, under Subscription filters, click Create Amazon OpenSearch Service subscription filter:
The Create Amazon OpenSearch Service subscription filter form will load.
5. In the Choose destination section, select the following:
Select account: Ensure **This account **is selected Amazon OpenSearch Service cluster: Select the cluster you created previously After selecting the Amazon OpenSearchService cluster, the Lambda function section will appear.
6. In the Lambda IAM Execution Role drop-down select LambdaElasticSearch:
7. In the Configure log format and filters section enter the following:
Log Format: Select Amazon Lambda Subscription filter name: ca-lab-filter The default Subscription Filter Pattern matches the timestamp, request_id, and event JSON. The **Test Pattern **button is available to see which events match the pattern.
8. To start sending the logs to ElasticSearch, at the bottom, click Start streaming:
Momentarily, you will see a notification that the subscription filter has been created and logs are being streamed to OpenSearch:
Discovering and Searching Events 1. Navigate back to the Lambda function you invoked earlier and click the Test button a few times to submit more PUT events:
2. Click the arrow on the Test button and click Configure test event:
3. In the **Configure test events **form, click the radio button for Create new test event and enter the following non-default values:
Event name:TestGetEvent Enter the following in the code editor at the bottom of the form: 1{ 2\u0026#34;fn\u0026#34;: \u0026#34;GET\u0026#34;, 3\u0026#34;id\u0026#34;: \u0026#34;12345\u0026#34; 4} You will submit more test events of a different type - GET operations on the object that was PUT in the database. This gives two different event types to look at in Kibana (the Log Aggregation UI).
4. Click Save.
5. Click Test several times to make GET events.
6. Return to the Amazon OpenSearch Search Console for the domain you created and click the link under Kibana URL:
7. In the Add Data to Kibana section, on the right-hand side under Use Elasticsearch data, click Connect to your Elasticsearch index:
The log data is stored in OpenSearch, but you need to tell Kibana which index to use for discovering the data.
8. In the **Create an index pattern **wizard, enter the following value and click Next step:
Index pattern: cwl-* The pattern will match the daily CloudWatch Logs (cwl) indices that are created in Amazon OpenSearch.
9. In the second step, enter the following value and click Create index pattern:
Index pattern: Select @timestamp The Time filter field name allows Kibana to determine which attribute represents the timestamp of each event. The confirmation page displays all of the fields in the log data index:
Now that the index settings for Kibana are configured, you can begin using the Log Aggregation system!
10. Click Discover in the sidebar menu on the left of the page.
11. Explore the Discover interface:
You see some events and a graph. These are your aggregated log events! The system is online! Notice the search bar up top. It is initially empty so all log events will show up. But what if you only want to see the PUT events for objects containing 12345?
12. Enter PUT 12345 in the search bar and press enter:
The matching terms in the event show up highlighted, and the bar graph updates to show only the count of PUT 12345 events that you made by clicking Test in the Lambda interface.
13. Click on the timestamp range in the upper-right corner to display the time filter:
Just as with CloudWatch Logs, you can filter the logs by time. However, in Kibana you can also drag on the bar chart to select a time range visually: Visualizing Aggregated Events 1. Click Visualize in the Kibana sidebar menu.
2. Click Create a visualization:
3. Select **Area **chart visualization:
4. In the **From a New Search, Select Index **area, click on the *cwl- **index name:
If you had any saved searches in the system, you could use them to make this Visualization from this step.
On the left-hand side, the visualization configuration tools will appear:
5. Enter the following values in the visualization configuration:
Select buckets type: X-Axis Aggregation: Date Histogram (to track log trends over time) Field: @timestamp Interval: Auto To make the graph more interesting, you will split the PUTs and GETs and display each stacked in on the chart with different colors. This requires a sub-buckets.
6. Click Add sub-buckets below the rest of the X-Axis settings, and enter the following values:
Select buckets type: Split Series Sub Aggregation: Terms (Terms splits the data based on the unique values of a field) Field: $event.data.fn.keyword (The test requests used the fn key for request type, which maps to the $event.data.fn.keyword field in OpenSearch) 7. Click the play button to apply the changes and produce the visualization:
It will look something like the image below, with two regions in an area graph corresponding to GET and PUT event count over time:
To use the visualization in a Dashboard in the next step, you need to save the visualization.
8. Click Save in the top toolbar:
9. Enter PUTs and GETs Over Time in the Save Visualization field, and click Save:
Creating a Kibana Dashboard 1. Click on Dashboard in the sidebar menu.
2. Click Create a dashboard:
3. Click Add to add saved visualizations to the dashboard:
4. Select the PUT and GETs Over Time visualization:
The visualization is added to the dashboard, but the size may not be what you like. You can adjust the size of the visualization by dragging the arrow in the lower-right corner:
5. Click Save and enter the following values before clicking the revealed **Save **button:
Title: Log Dashboard Description: Lambda API Logs You\u0026rsquo;ve done it! The Dashboard will always contain the up-to-date statistics for your GET and PUT events that run through the Lambda function:
6. Return to the Lambda console and create as many test events as you want.
7. Refresh the Kibana dashboard and see the new requests in the visualization:
You can also configure Auto-refresh to avoid having to refresh the view:
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/opensearch-service/build-log-aggregation-system/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticloadbalancing/create-amazon-load-balancing/":{title:"Create Classic Load Balancer",tags:[],content:`Practice Creating Classic Load Balancer Planning the Classic Load Balancer When you connected to the AWS account provided in the former step, you had a few things that were already deployed. This is the current infrastructure that was already deployed for you:
You already have a VPC with some subnets and 2 EC2 instances running inside the VPC in different Availability Zones. Both instances are inside the same Security Group called , which is allowing HTTP access from port 80 to anywhere (0.0.0.0/0). Each EC2 instance is running the same web application. We want to configure an LB to create a central point of access to our application, and we also want to configure our architecture in a way that users can only access the application through the ELB.
In the end, we should have a solution similar to this one:
To do that we will have to create and configure a Classic Load Balancer, and properly configure the needed Security Groups to make sure that our application will work as expected.
Creating a Classic Load Balancer and Registering EC2 Instances A Classic Load Balancer allows traffic to be balanced across many Amazon EC2 instances, it performs this balancing at the request and connection level.
1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. In the left-hand menu, under Load Balancing, click Load Balancers:
3. To start creating your classic load balancer, click Create Load Balancer:
Three tiles will be displayed detailing the different types of load balancer supported by Amazon EC2.
4. At the bottom of the page, click Classic Load Balancer:
5. In the Classic Load Balancer tile, click Create:
A multi-step wizard will open allowing you to configure and customize your load balancer.
6. Under Basic Configuration, enter the following values:
Load Balancer name: Enter classic-elb Enable advanced VPC configuration: checked Be aware there are limitations on the name field, only the characters a-z, A-Z, 0-9 and hyphens are allowed.
Create LB Inside lets you select which VPC you want the load balancer to be created in, leave this at the default.
The Create an internal load balancer option determines whether the load balancer can accept public internet traffic or not. If checked, the load balancer will have a private IP address and will only be able to accept traffic from another source inside the VPC.
The default Listener Configuration, listening on port eighty (HTTP), is all that is required for this lab.
7. Under Select Subnets, click the plus icon next to each subnet.
As you click for each subnet, it will move from the Available subnets table, to the Selected subnets table:
An Availability Zone, often referred to as an AZ, helps make your infrastructure more reliable. You can think of each zone as a separate data center (in many cases they are exactly that), they are guaranteed to have redundant power, networking, and connectivity within an AWS region.
To learn more about regions, availability zones, and redundancy in AWS, visit the documentation here.
Each subnet is mapped to one availability zone. It\u0026rsquo;s important to configure the selected subnets correctly. If a subnet containing an EC2 instance is not selected, the load balancer will not be able to communicate with that EC2 instance. 8. To move to the next step of the wizard, click Next: Assign Security Groups:
9. In the form, enter and select the following values:
Assign a security group: Select Create a new security group Security group name: Enter elb-sg Description: Enter Security group for the classic load balancer You will see a default security group rule allowing traffic on port eighty.
10. In the default security group rule, in the Source drop-down, select Anywhere:
11. To advance to the next page of the wizard, click Next: Configure Security Settings:
This wizard step display\u0026rsquo;s a warning that your load balancer isn\u0026rsquo;t configured to use HTTPS or SSL.
It\u0026rsquo;s strongly recommended that you always enable encrypted traffic on your load balancers for security reasons. Configuring SSL is beyond the scope of this lab. If you would like to learn more about SSL and load balancing, it\u0026rsquo;s covered in the Using Elastic Load Balancing \u0026amp; EC2 Auto Scaling to Support AWS Workloads course.
12. To move to the next wizard step, click Next: Configure Health Check:
13. In the Ping Path field, replace the contents with /:
By default, the fields on this page specify that the health check will be performed using the HTTP protocol on port eighty. This means the load balancer will assume an instance is healthy when the instance returns a 200 OK response.
The Advanced Details allow you to further customize different aspects of the health check:
Response Timeout: How long to the load balancer should wait for a response from the EC2 instance. Interval: Amount of time between health checks. Unhealthy threshold: The number of consecutive failed healthy checks that must occur before the load balancer declares the EC2 instance unhealthy. Healthy threshold: The number of consecutive health checks that must occur before declaring an EC2 instance healthy. To learn more about Elastic Load Balancing health checks, see the AWS documentation here.
14. To move to the next wizard step, click Next: Add EC2 Instances:
This step of the wizard displays the EC2 instances that currently exist and can be added to the load balancer:
15. Select the instances named web-node:
Take a look at the configuration options on this page:
Cross-Zone Load Balancing ensures that your LB distributes incoming requests evenly across all instances in its enabled Availability Zones. This means that the LB will ignore the default of round-robin and will also take into consideration the Availability Zone in which the instance is running. This reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone and improves your application\u0026rsquo;s ability to handle the loss of one or more instances.
Connection Draining is used to ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open.
Leave these options at their defaults.
16. To advance to the next wizard step, click Next: Add Tags:
In a non-lab environment, it is best practice to add tags to resources you create. Tags help make managing, organizing, and filtering resources in AWS easier.
To read more about tagging resources in AWS, see this document from AWS.
17. To proceed to the review step, click Review and Create:
This page allows you to review the load balancing settings you have configured:
18. To create your load balancer, click Create:
You will see a notification that your load balancer has been successfully created:
19. To return to the EC2 management console, click Close:
Configuring Security Groups for Load Balanced EC2 Instances 1. In the list of load balancers, ensure your load balancer is selected:
You will see some tabs beneath the list and the Description tab will be selected.
This tab shows general information about your load balancer.
2. To view information about instances registered with this load balancer, click the Instances tab:
You will see the instances and availability zones listed:
The instances will have a status of InService. This means the load balancer is performing successful health checks on the instances.
Note: If you see the Status as OutOfService then the instances are still be registered. Wait a minute or two and then click the refresh icon in the top-right corner.
3. To see the DNS of your load balancer, click the Description tab.
4. Copy the domain name from the value of the DNS name field:
Warning: Don\u0026rsquo;t include the (A Record) part of the value when copying.
5. In a new browser tab, paste the domain name, and press enter.
You will see an instance Id displayed:
Note: Your instance Id will be different.
An application has been pre-installed on the EC2 instances that will respond to web requests with the instance Id of the instance serving the request.
To see the Id of the other EC2 instance, refresh the page. If the Id doesn\u0026rsquo;t change, you may need to open an incognito or private browsing tab and visit the DNS name again.
Seeing the Id change shows that the load balancer is working as expected, routing traffic to both registered instances.
Leave this tab open and remember this is the tab for the load balancer, you will use it again later in the lab step.
6. In the left-hand menu, under Instances, click Instances:
You will see two instances named web-node with a status of Running:
7. Select one of the instances:
You will see tabs displayed below the list of instances.
8. In the Details tab, in the Public IPv4 DNS field, click the copy icon:
The public DNS name of the EC2 has been copied to your clipboard.
9. In a new browser tab, paste the DNS name and press enter.
You will see an instance Id displayed again.
However, this time, because you are accessing the instance directly if you refresh or visit the DNS name in an incognito or private browsing tab, the Id won\u0026rsquo;t change.
Note that you are accessing the instance directly, this is allowed by the security group associated with the EC2 instances. Allowing load-balanced instances to be publicly accessible is a bad security practice, and there is rarely a good reason for it.
In the rest of this lab step, you will modify the EC2 instance\u0026rsquo;s security group to only allow traffic from the load balancer.
Leave this browser tab open and remember this is the tab for an EC2 instance, you will use this tab again later.
Navigate to Load Balancers in the EC2 Management Console. 11. Ensure the classic-elb load balancer is selected.
12. In the Description tab, scroll down to the Security section:
This is the security group you configured when you created the load balancer.
13. In the left-hand menu, under Network \u0026amp; Security, click Security Groups:
You will see a list of security groups:
14. Select the SG which has the Group Name starting with cloudacademylabs- .
This is the security group of the EC2 instances.
You will see tabs displayed beneath the list.
15. In the row of tabs, click Inbound rules:
16. To modify the rules of this security group, click Edit inbound rules:
You want to allow only connections coming from the load balancer to the instances, however, the balancer doesn\u0026rsquo;t have a particular IP address associated with it so you can\u0026rsquo;t specify an IP address here. Instead, you will restrict the access by using the security group you created for the balancer.
You will change the current rule to deny access to anywhere and allow it only to members of the load balancer\u0026rsquo;s security group.
17. Delete the existing rule, and create a new one whose Type is HTTP. In the Source drop-down, ensure Custom is selected and in the box next to it, select elb-sg:
18. To save your changes, in the bottom-right, click Save rules:
With your rule saved, reload the browser tab with the DNS of the load balancer.
This will continue to work, you will see an instance Id displayed.
19. Reload the browser tab with the DNS of an instance in the address bar:
The exact behavior will vary depending upon your web browser.
Most likely you see the loading symbol in the browser tab spinning indefinitely:
If you wait long enough, your browser will report that it timed out trying to reach the instance:
Checking Your Load Balancer\u0026rsquo;s Behavior During Instance Failures Navigate to Instances in the EC2 Management Console. You will see two instances named web-node listed.
2. To stop an instance, right-click one of them.
3. In the menu that appears, click Instance state, and then click Stop instance:
You will see a dialog box asking you to confirm that you want to stop the instance.
4. To confirm, click Stop:
The instance\u0026rsquo;s Instance state column will change to Stopping. A few moments later you will see it changed to Stopped:
Stopping the instance will make it fail your load balancer\u0026rsquo;s health checks.
Navigate to Load Balancers in the EC2 Management Console. 6. Ensure the classic-elb load balancer is selected.
7. In the row of tabs below the load balancer list, click Instances:
Look at the Status column in the instances table, one of the instances will still be InService, and the other will be OutOfService:
This means that there is only one instance serving the application, and therefore all the requests will be forwarded to the same instance.
You can test this behavior by clicking on the Description tab and accessing the **DNS name **of the load balancer in a new browser tab. Your request will be served by the instance that you didn\u0026rsquo;t stop.
Leave the browser tab with the load balancer\u0026rsquo;s DNS name open. You will test it again after starting the stopped instance.
8. To start the stopped instance, in the left-hand menu, under Instances, click Instances:
9. Right-click the stopped instance.
10. Click Instance state, and click Start instance:
Note: You can also access this menu using the Actions button in the top-right.
The Instance state column will change to Pending, and a few moments later, to Running.
Test accessing the load balancer by it\u0026rsquo;s DNS name again. This time, you will see that both instances are serving requests.
Note: You may need to open the load balancer\u0026rsquo;s domain name in an incognito or private browsing tab to see both instance Ids.
Monitoring your Classic Load Balancer Navigate to Load Balancers in the EC2 Management Console. 2. In the list of load balancers, ensure the classic-elb load balancer is selected, and click the Monitoring tab:
You will see a number of graphs of different CloudWatch metrics.
The Elastic Load Balancing (ELB) service reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, the load balancing service measures and sends its metrics in sixty-second intervals. If there are no requests flowing through the load balancer, or no data for a metric, the metric is not reported.
There are a few metrics related to a Classic Load Balancer, and most are self-explanatory if you are familiar with HTTP requests. If some of them are unfamiliar to you, visit the Amazon AWS documentation to read more.
The metrics called HealthyHostCount, and **UnHealthyHostCount **will count the number of Healthy and Unhealthy instances respectively. These metrics can be useful for you to identify a major problem in your AWS account. A healthy instance is one that is passing the health checks performed by the load balancer.
You could use CloudWatch Alarms to notify you when you have less than 2 instances running your application, though to be clear this is not a general rule: the number of instances that might identify a problem will vary depending on your environment.
Also notice that in these metrics, there is no way of seeing the Availability Zone to which the Healthy/Unhealthy instance belongs. In our lab, we stopped an instance for a few minutes, therefore you should be able to see something like this:
If the Healthy Hosts metric reaches zero, that means that people won\u0026rsquo;t see anything when accessing your load balancer, and it is probable that you have a big problem in your infrastructure.
The Average Latency metric might be useful to identify potential issues in your setup. Maybe everything is working in your application, but you notice an increase in this metric. If you haven\u0026rsquo;t changed anything in your application, that can be a potential issue - maybe you haven\u0026rsquo;t provisioned enough EC2 instances, or you even have lots of instances but they don\u0026rsquo;t have enough power to serve your increasing traffic.
The other metrics can be very useful for troubleshooting specific scenarios and will vary depending on your setup.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/elasticloadbalancing/create-amazon-load-balancing/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codestar/develop-and-deploy-app-with-codestar/":{title:"Develop and Deploy an Application with AWS CodeStar",tags:[],content:`Lab Develop and Deploy an Application with AWS CodeStar Creating an AWS CodeStar Project 1. In the AWS Management Console search bar, enter CodeStar, and click the CodeStar result under Services:
2. On the welcome page, click Create project.
Take a moment to see all of the different templates available in AWS CodeStar.
3. Check the following boxes on the left filter bar to narrow down the listed templates:
AWS services: EC2 Application category: Web application Programming languages: Node.js The choice of **Application category **and Programming language will be driven by the requirements of your project and skills available to you. The choice of AWS services may not be as easy. Some guidelines for choosing between the alternatives are:
AWS Elastic Beanstalk: A good choice for a fully managed application environment running on EC2 servers. This option allows you to stay focused on your code. Amazon EC2: Preferable when you want to host the application on servers that you manage yourself, including on-premise servers. AWS Lambda: Choose this option if you want to run a serverless application. 4. Select the **Express.js **project template:
Express.js is a popular Node.js web application framework.
5. In the next step of the Create project wizard, enter the following:
Project name: ca-app-\u0026lt;Unique_String\u0026gt; (Replace \u0026lt;Unique_String\u0026gt; with a 6 characters. The name must be unique for the region because of AWS CodeCommit repository name restrictions) Project ID: Accept the default value The instructions in this Lab use ca-app for the project name, but you should use a different name or the project creation may fail if it is already in use.
6. Make sure that CodeCommit is selected under Project repository:
You will see the EC2 Configuration section of the form.
7. Ensure the following values are selected:
Instance type: _t2.micro _(default value) VPC: Select the non-default VPC (The VPC without \u0026ldquo;(Default)\u0026rdquo;),or the VPC with only two subnets if there is no (Default) label Subnet: Select the subnet in the us-west-2a availability zone If you can\u0026rsquo;t see which subnet is in us-west-2a hover your mouse over each subnet.
8. Click Next and then** Create Project**:
Connecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. To see available instances, click Instances in the left-hand menu:
The instances list page will open, and you will see an instance named cloudacademylabs:
If you don\u0026rsquo;t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.
3. Right-click the cloudacademylabs instance, and click Connect:
The Connect to your instance form will load.
4. In the form, ensure the EC2 Instance Connect tab is selected:
You will see the instance\u0026rsquo;s Instance ID and Public IP address displayed.
5. In the User name textbox, enter ec2-user:
Note: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:
If you see an error it\u0026rsquo;s likely that the environment hasn\u0026rsquo;t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:
A browser-based shell will open in a new window ready for you to use.
Keep this window open, you will use it in later lab steps.
You can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.
Touring the AWS CodeStar Project Website 1. Observe the tiles that are included in your Dashboard:
IDE: References for how to get started with a variety of integrated development environments (IDEs) under Access your project code. You will simply use the EC2 instance to edit the code interact with CodeCommit in this lab.
Repository: You can see the main details related to the code repository here. The most recent code commits for the selected branch:
The View commits button opens the detailed view list of the commits. Currently, there is only a master branch and the initial commit to display. The committer, AWS CodeStar, made the initial commit during the project creation. Each commit also includes a button on the right to view the code changes in AWS CodeCommit. You will look at the code in a future Lab Step.
Pipeline: This shows a graphical representation of the release pipeline for your project:
Any time you commit a code change to the master branch, the pipeline will automatically deploy your application. As your application grows and the requirements for your release pipeline change, you can modify the pipeline by clicking Edit. For example, you may want to add an automated test stage, invoke an AWS Lambda function, or modify the deployment group to deploy to an Auto Scaling group. The **Release change **button can be used to force a deployment of the latest commit. That can be useful if you modify the pipeline or something went wrong with the release. If something does go wrong with a pipeline stage, you will see the bar on the left turn red.
Monitoring: This shows the CPUUtilization and other metrics of the EC2 instance where your application is deployed.
Issues: This Lab doesn\u0026rsquo;t include a JIRA project, but for projects requiring issue tracking you can find link to JIRA from here.
3. Click View application in the upper-right to view the application included in the template:
Depending on your time of day, the background will change. You will commit a code change later to modify the appearance of the application.
4. Look at the Project resources tab under the Overview.
The most interesting thing to see here is the list of all the Project Resources created by the project template:
AWS CodeStar saved you a lot of time compared to manually configuring everything that is included. Notice that AWS CloudFormation includes a stack resource. That is how AWS CodeStar works behind the scenes. Each project template creates a stack in AWS CloudFormation. Of course, you don\u0026rsquo;t need to know any of the details. AWS CodeStar does everything for you so you can focus on development.
If you need to delete an AWS CodeStar project, you can do so from the CodeStar project page. You will be given a choice of keeping the associated resources or also deleting the associated resources.
Developing Your AWS CodeStar Project 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:
2. Click on Users in the left navigation panel.
3. In the Users table, click on student.
Note: You will see error messages. This is normal. You only have the permissions required to complete the Lab.
4. Click on the Security credentials tab.
5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section, and click Generate credentials:
This will show a pop-up dialog showing you your credentials. 6. Click Download credentials:
Your browser will download a file containing a username and password. Keep this file, you will use the credentials to connect to your AWS CodeStar repository.
7. Return to your AWS CodeStar project\u0026rsquo;s Repository tab and click HTTPS under Clone repository:
This copies the HTTPS url of the CodeCommit repository to your clipboard.
8. Paste the repository into the file with your code repository credentials.
You will use this URL later to access your repository.
9. Return to the SSH shell connected to the dev-instance EC2 instance and enter cd to ensure you are in your home directory of /home/ec2-user.
Refresh the instance connect browser tab if the session has expired.
10. To tell Git to cache your credentials for a few hours, enter the following command:
1git config --global credential.helper \u0026#39;cache --timeout=10800\u0026#39; 11. Tell Git your user name:
1git config --global user.name \u0026#34;student\u0026#34; This name will show up on the commits in your project dashboard.
12. To clone your AWS CodeStar project repository, enter:
1git clone \u0026lt;YOUR_PROJECT_REPOSITORY_URL\u0026gt; Replace \u0026lt;YOUR_PROJECT_REPOSITORY_URL\u0026gt; with the URL you copied in a previous instruction.
Your URL will be similar to https://git-codecommit.us-west-2.amazonaws.com/v1/repos/ca-app.
13. When prompted, enter the Username and Password you saved in a text file earlier in this Lab Step.
Tip: The password generated by AWS is long and it is easy to make a typo when entering it. To avoid errors copy and paste the password.
14. Change the repository directory name to ca-app:
1mv ca-app-\u0026lt;Unique_string\u0026gt; ca-app Note: Change ca-app-\u0026lt;Unique_string\u0026gt; to the name of your repository.
This won\u0026rsquo;t change the repository name. It will only simplify the instructions at the command-line by not having to enter your unique string following ca-app in this and later Lab Steps. 15. Change into the directory:
1cd ca-app 16. Enter ls to get a quick overview of the project structure.
There are several files:
app.js: JavaScript file that starts the server appspec.yml: Configuration file that instructs AWS CodeDeploy what steps to perform to deploy your application package.json: Metadata and dependencies related to your project README.md: Text file explaining the project template There is no need to get into the details of the file contents at this time. However, it is good to know that the appspec.yml file specifies scripts that run during the deployment of your application. The scripts are contained in one of the two project directories:
public: Static assets used for your application scripts: Scripts executed by AWS CodeDeploy during the deployment of your application Now you can get the server running on your development machine.
17. Install the project dependencies using Node package manager (npm) and start the Node.js server:
1npm install 2node app.js While the server is running you won\u0026rsquo;t be able to enter new commands. That won\u0026rsquo;t be a problem. Now you can test that the development server is serving the application.
Navigate to Instances in the EC2 service in the AWS Console. 19. Select the instance named cloudacademylabs:
In the Description tab, you will see a field called Public DNS (IPv4).
20. To copy the public DNS, click the click the copy icon under Public IPv4 DNS:
21. Open a new browser taband paste the public DNS and append :3000 to the end and press enter:
Now that you verified the application works on the development machine, you can make some code changes.
22. Return to the SSH shell and press Ctrl+C to kill the running Node.js server.
23. Enter the following multiline command at the shell prompt to update a file in the project:
1echo \u0026#39;var idx = Math.floor(new Date().getHours()); 2var body = document.getElementsByTagName(\u0026#34;body\u0026#34;)[0]; 3var idxStep = 1; 4var refreshRate = 1000; 5 6function adjustIdx() { 7 if (idx \u0026lt;= 0) { 8 // Start increasing idx 9 idxStep = 1; 10 } else if (idx \u0026gt;= 23) { 11 // Start decreasing idx 12 idxStep = -1; 13 } 14 idx += idxStep; 15 body.className = \u0026#34;heaven-\u0026#34; + idx; 16} 17 18body.className = \u0026#34;heaven-\u0026#34; + idx; 19setInterval(adjustIdx, refreshRate);\u0026#39; \u0026gt; public/js/set-background.js 24. Test the changes by running the server again with node app.js and refresh the browser tab with your development application. You will see a similar page as the previous one, but the color will change roughly once a second.
25. Stop the Node.js server with Ctrl+C.
26. View the local repository status:
1git status This tells you that you are on the master branch and working from the initial code commit. The output also shows the set-background.js file was modified. You need to add the file to stage it before committing.
27. Add the modified file to the staged changes in the commit:
1git add public/js/set-background.js 28. Commit the staged changes to the local repository and add a short message about the changes:
1git commit -m \u0026#34;animation\u0026#34; 29. Push the changes in your local repository to the remote AWS CodeStar project repository so they are synchronized:
1git push Now that you have made a change to your code, you will see how the changes are deployed in the next Lab Step.
Summary In this Lab Step, you committed a code change to your AWS CodeStar project repository. You created the required credentials and tested the application on your development server.
Deploying Your AWS CodeStar Project 1. Return to your AWS CodeStar project view.
There are a few things to notice since you were here last:
Your commit is now visible in the **Repository **\u0026gt; **Most recent commit **tile Your Monitoring \u0026gt; CPUUtilization tile might show some spikes if your application has already been deployed Your Pipeline tab may show one of the pipeline stages In progress or you may see a recent timestamp inside each stage box telling you the new version has been deployed. If you missed the release flowing through the stages of the pipeline, click Release change and click Continue in the pop-up.
2. To inspect the code, in the Repository tab, click the most recent Commit ID:
Your commit Id will be different.
3. Look at the code changes:
Additions appear in green and removals would appear in red, if any were present. This is an easy way to keep track of what is happening to the code in your AWS CodeStar project.
4. Navigate back to the Pipeline tab. Click on AWS Code Deploy under Deploy:
This opens your application in AWS CodeDeploy:
You can see the Deployment Groups created for deploying your application. In this case there will be just one with a **Name **ending in -Env. The **Status **column will tell you if your last deployment **Succeeded **or failed. The time of the Last attempted deployment and Last successful deployment are also recorded.
5. Click the name of your deployment group beginning with ca-app:
Notice that by default Rollback enabled is false. That means if your deployment fails, AWS CodeDeploy will not attempt to deploy the last successful version. That is something you might consider changing when you use AWS CodeStar for one of your projects.
6. Scroll down the page and inspect the Deployment group deployment history section.
Each deployment that was attempted to be deployed is recorded here along with a link to where the artifacts are located on Amazon S3.
7. Click on the most recent deployment in the Deployment Id column:
Your deployment will have a different deployment id.
This opens a page with details of the most recent deployment:
Deployment status: shows the state of the deployment operation Deployment details: shows information similar to what you saw on the AWS CodeDeploy application page Revision details: shows information about the revision deployed, including the location in AWS S3 Deployment lifecycle events: tells you the start and end times as well as the **Duration **of the deployment 8. To view the deployment life-cycle events, click View events down the bottom:
9. To view the events, scroll down to the event list:
You will see events similar to the above.
In case of a failed deployment, one of the events will record the failure and provide a link under the Logs column to investigate the command and logs related to the failure. If you recall, the appspec.yml file in the code project was used to instruct AWS CodeDeploy on how to deploy your application. Your project provides different scripts to run for some of the events listed in the table.
10. Finally, return to the AWS CodeStar and click View application.
You will see the latest version of your application including the animation commit deployed and available to the world.
Managing Your AWS CodeStar Project Team 1. Return to your AWS CodeStar project\u0026rsquo;s Overview and click on Add team members:
2. Click on the **User **drop-down menu and click on Logan.
3. Set the team member values for Logan to:
Email address: test@cloudacademy.com Project Role: Contributor Remote Access: Checked (This allows the team member to upload an SSH public key to connect to EC2 instances) The difference between the default Project Roles is:
Viewer: Access to the project dashboard and able to view a few project resources Contributor: Everything Viewer can access plus view, modify, and access all project resources Owner: Everything Contributor has plus adding and removing team members, and deleting the project 4. Click Add team member:
After adding a team member, you will be asked to create a profile for yourself.
5. In the Create user profile form, enter the following values before clicking Create user profile:
Display name: student Email address: student@cloudacademy.com You will see Logan and **student **appear in the **Team members **list.
6. Click Add team member and select Bessie from the drop-down menu.
7. Enter the following values and click Add:
Email address: bessie@cloudacademy.com Project Role: Viewer Remote Access: Unchecked Now you can briefly experience the differences between the project roles.
8. At the top of this Lab page, click on the Open Environment button.
This will sign you out of the student user and allow you to sign in as a different user.
9. Log in to AWS using the team member in the viewer role:
User Name: Bessie Password: Lab-Viewer1 Navigate to AWS CodeStar in the AWS Console. 11. Click on your project name.
Observe that the viewer role has access to view the same tabs as your student user.
13. Click on the Repository \u0026gt; Commit ID and see that a viewer is allowed to view code changes.
14. Return to the project Pipeline section and click Release change, then Release.
You will receive an error message stating that you are not authorized to perform that action:
15. At the top of this Lab page, click on the Open Environment button and sign in again with the following credentials:
User Name: Logan Password: Lab-Contributor1 The user Logan is in the contributor role, which has additional permissions than the viewer role.
16. Click Release change, then Continue.
The contributor has permission to perform this action:
17. Click Team in the left sidebar.
Notice that you can only remove yourself from the team and not other members. That is a distinction between the contributor and owner roles.
18. One last time, in the lab, click on the Open Environment button and sign in with the student credentials given in the Credentialssection of the Lab.
Cleaning Up Your AWS CodeStar Project 1. Return to your AWS CodeStar project dashboard and click on Settings:
2. Click Delete project.
3. Enter delete in the pop-up dialog:
4. Click Delete:
In a few seconds you will return to the AWS CodeStar start page and all of the resources in the project will begin terminating. `,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codestar/develop-and-deploy-app-with-codestar/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kms/encrypting-s3-objects-using-sse-kms/":{title:"Encrypting S3 Objects Using SSE-KMS",tags:[],content:`Lab Encrypting S3 Objects Using SSE-KMS Creating a Customer Master Key (CMK) 1. In the AWS Management Console search bar, enter KMS, and click the KMS result under Services:
2. Select Customer managed** keys** in the left pane of the KMS console.
Warning: Cloud Academy cleans up the lab environment for you after a lab is completed or terminated. As a precaution, AWS prevents keys from being deleted immediately. Rather, they are queued for deletion, and an expiration period is set (of 7-30 days). For this reason, you may see residual keys from other students within the last week. For this reason, you may need to append a unique number to the Alias field in the next instruction.
3. Click Create Key, then expand **Advanced Options **and set the following values:
Key type: **Symmetric **(Symmetric keys are suitable for most data encryption applications. The same key is used for both encrypt and decrypt operations with symmetric key algorithms.) Key usage: Encrypt and decrypt Advanced options: Key Material Origin: Leave as KMS (default). AWS will generate the key material for encryption. Note that another common use case is for customers to generate their own keys, and have AWS keep a back up encrypted copy and help manage them with KMS. Regionality:** Single-Region key** 4. Click Next to advance to the Add Labels page of the wizard.
5. Set the following values before clicking Next (leave the default values for other fields)
Alias: _calabs-CMK-key _(Append a unique number to the key\u0026rsquo;s Alias if needed to be unique. For example, calabs-CMK-key2.) Description: 6. Click Next to advance to Define Key Administrative Permissions and leave the default values.
Administrative permissions allow users and roles to administer CMKs but not to perform cryptographic operations. In production environments, this is sometimes used to easily grant limited access to other users. The **Allow key administrators to delete this key **checkbox makes it explicit if deleting keys is allowed, since the key can\u0026rsquo;t be recovered once deleted, making recovery of encrypted data impossible. Note that key deletion is not immediate and first enters into a pending state before the key is deleted. The delete operation can be canceled while in the pending state.
These settings generate a key policy. The default policy allows IAM policies to grant access the key, which is why you don\u0026rsquo;t require selecting your student user as an administrator. The lab IAM policy of your student user allows you to perform the required actions of the lab.
7. Click Next to advance to Define Key Usage Permissions.
Usage permissions grant access to perform cryptographic operations such as encrypting and decrypting. Enterprises usually have different permissions for administrators and users, hence the wizard walks you through defining both.
Notice that you can grant access to the key so other AWS accounts can use it for encryption/decryption. 8. Click **Next **to preview the key policy and then click Finish when ready. The CMK is created.
9. Confirm the key created correctly and that the Status is Enabled:
Encrypting S3 Data using Server-Side Encryption with KMS Managed Keys (SSE-KMS) You will upload a file and encrypt it using SSE-KMS in this lab step.
1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:
2. Click the name of the bucket the Cloud Academy lab environment created for you (name begins with cloudacademylabs-ssekms):
3. Click Upload.
4. Click Add files and select a small file, or download this sample file and select it.
5. Expand the Properties tab and scroll until the Server-side encryption settings.
6. Check the Specify an encryption key checkbox. 7. Check the AWS Key Management Service key (SSE-KMS) checkbox and then the Choose from your AWS KMS keys checkbox:
8. Choose the AWS KMS key you previously generated:
9. Click on Upload.
10. Click Close and then click the name of the object to open its properties panel: You can verify the object is encrypted using SSE-KMS by checking that the Encryption field is AWS-KMS.
Enforcing S3 Encryption Using Bucket Policies 1. In the S3 bucket console, click the Permissions tab followed by Bucket Policy to open the Bucket policy editor:
Bucket policies are IAM policies applied to a bucket rather than to a user or role as is conventionally done with IAM policies. Similar to how a key policy applied to the CMK. These are examples of resource-based policies in AWS.
2. Paste the following bucket policy into the policy editor:
1{ 2 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 3 \u0026#34;Id\u0026#34;: \u0026#34;RequireSSEKMS\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Sid\u0026#34;: \u0026#34;DenyUploadIfNotSSEKMSEncrypted\u0026#34;, 7 \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, 8 \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, 9 \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, 10 \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::\u0026lt;Your_Bucket_Name\u0026gt;/*\u0026#34;, 11 \u0026#34;Condition\u0026#34;: { 12 \u0026#34;StringNotEquals\u0026#34;: { 13 \u0026#34;s3:x-amz-server-side-encryption\u0026#34;: \u0026#34;aws:kms\u0026#34; 14 } 15 } 16 } 17 ] 18} This policy denies (\u0026quot;Effect\u0026quot;: \u0026quot;Deny\u0026quot;) all users\u0026rsquo; (\u0026quot;Principal\u0026quot;: \u0026quot;*\u0026quot;) uploads (\u0026quot;Action\u0026quot;: \u0026quot;s3:PutObject\u0026quot;) to the bucket (\u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::\u0026lt;Your_Bucket_Name\u0026gt;/*\u0026quot;) if the s3:x-amz-server-side-encryption is not set to aws:kms, which corresponds to SSE-KMS. The lab provides you with the policy but you could recreate it using the policy generator linked to beneath the policy editor.
3. Replace \u0026lt;Your_Bucket_Name\u0026gt; with the name of your lab bucket (it begins with cloudacademylabs-ssekms- and can be copied from the S3 console):
4. Click Save changes to save the policy and have it start being enforced.
5. Click the **Objects **tab followed by Upload.
6. Click Add files and select a small file, or download this sample file and select it.
7. Click Upload and observe the image does not appear in the bucket contents table.
Clicking upload without configuring any properties of the object uses the default of no encryption.
You can see the upload Failed. 8. Retry the upload but this time use the Set properties step to configure **Encryption **to AWS KMS master-key using your CMK.
The upload now succeeds since the bucket policy condition is satisfied:
The policy does not require the use of your CMK however, so the default S3 KMS key in the region is also allowed. You can change the policy condition to enforce a specific CMK is used.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kms/encrypting-s3-objects-using-sse-kms/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sqs/fan-out-orders-with-sns-sqs/":{title:"Fan-Out Orders using Amazon SNS and SQS",tags:[],content:`Lab Fan-Out Orders using Amazon SNS and SQS Creating an Amazon SNS Topic and Amazon SQS Queues Here\u0026rsquo;s a diagram of what you will build and configure in this lab step:
In the search bar at the top, enter SNS and under Services, click the Simple Notification Service result: In the Create topic card on the right, in the Topic name textbox, enter new-orders and click Next step: The Create topic form will load.
By default, the Type of topic selected will be Standard. This is the most scalable topic type. The cost of this scalability is that message order and exactly-once delivery attempts can not be guaranteed.
If you are building a solution requires strict message ordering and exactly-once message delivery, you should use a FIFO type topic.
Standard is fine for this lab.
Click the black triangle next to Access policy - optional to expand the section: In the Access policy section, under Define who can publish messages to the topic, select Everyone: Under Define who can subscribe to this topic, select Everyone: You are using a permissive access policy to save time and because the focus of this lab is on demonstrating the fan-out scenario.
In a non-lab environment, you should carefully consider the access policy required and make sure if conforms with your company or organization\u0026rsquo;s security requirements.
Scroll to the bottom of the page, and click Create topic: You will see a page load displaying details of your newly created topic:
In the order processing system your are building, this Amazon SNS topic is where orders are published to. In a non-lab environment it would most likely be a web application or other application that accepts orders that will publish messages to this topic.
Next, you will create two queues using Amazon Simple Queue Service and subscribe them to your Amazon SNS topic.
Open a new tab by right-clicking the AWS icon in the top-left and selecting Open in new tab. Note: The above instruction may vary slightly depending upon the web browser you are using.
In the search bar at the top, enter SQS, and under Services, click the Simple Queue Service result: In the middle right of the screen, in the Get started card, click Create queue: The Create queue form will open.
In the Name textbox, enter orders-for-inventory: Scroll down to the bottom, click Create queue: You will see a web page load showing you details of your newly created Amazon SQS queue:
You will now create a second Amazon SQS queue for analytics.
To navigate to the Queues list page, at the top-left, click Queues: On the right-hand side, click Create queue.
Repeat the queue creation process, only this time enter orders-for-analytics as the Name of the queue.
Return to the Queues list page by clicking Queues in the top-left.
You will see the two queues you have created:
Click the radio button for the orders-for-analytics queue.
On the right-hand side, click Actions and click Subscribe to Amazon SNS topic:
The Subscribe to Amazon SNS topic form will load.
In the Choose a topic drop down, select the topic ending with new-orders: This is the Amazon SNS topic you created earlier.
Click Save to finish subscribing this queue to your topic.
At the top-left, click Queues again.
Repeat the topic subscription process for your orders-for-inventory Amazon SQS queue.
You now have both of your Amazon SQS queues subscribed to your Amazon SNS topic. Any messages published to the topic will fan-out to both queues.
Connecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. To see available instances, click Instances in the left-hand menu:
The instances list page will open, and you will see an instance named cloudacademylabs:
If you don\u0026rsquo;t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.
3. Right-click the cloudacademylabs instance, and click Connect:
The Connect to your instance form will load.
4. In the form, ensure the EC2 Instance Connect tab is selected:
You will see the instance\u0026rsquo;s Instance ID and Public IP address displayed.
5. In the User name textbox, enter ec2-user:
Note: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:
If you see an error it\u0026rsquo;s likely that the environment hasn\u0026rsquo;t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:
A browser-based shell will open in a new window ready for you to use.
Keep this window open, you will use it in later lab steps.
You can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.
Publishing and Processing Messages In the terminal, enter the following command: 1aws sns list-topics You will see one topic displayed:
Note: Your TopicArn will have a different account identifier.
By default, the AWS command-line interface tool uses the JSON format for responses. This response contains an array of Topics with one element. The element consists of a TopicArn.
Arn is short for Amazon Resource Name. An ARN is used to uniquely identify resources in AWS.
In this lab, the EC2 instance has been configured with an IAM role that has permissions to interact with Amazon SNS topics and Amazon SQS queues.
Store the value of the TopicArn attribute in a shell variable (topic_arn): 1topic_arn=$(aws sns list-topics --query \u0026#39;Topics[0].TopicArn\u0026#39; --output text) The above command uses the --query option to select only the value of the TopicArn and the --output option is used to specify plaintext format which removes the quotation marks from the value.
To publish a message, enter the following, utilizing the ARN you stored in the topic_arn shell variable: 1aws sns publish \\ 2--topic-arn $topic_arn \\ 3--message \u0026#34;1 x Widget @ 21.99 USD\\n2 x Widget Cables @ 5.99 USD\u0026#34; In response, you will see a MessageId:
Note: Your message identifier will be different.
You have successfully published an order message to your Amazon Simple Notification Service topic.
In this lab, you are using the AWS command-line interface tool to simulate an application publishing an order message.
In a non-lab environment, the message could be published by a web application that accepts orders from customers.
To list Amazon Simple Queue Service queues, enter the following command: 1aws sqs list-queues You will see a JSON response:
The queues that you created earlier are listed.
Store each of the QueueUrls in shell variables: 1analytics_queue_url=$(aws sqs list-queues --query \u0026#39;QueueUrls[0]\u0026#39; --output text) 2inventory_queue_url=$(aws sqs list-queues --query \u0026#39;QueueUrls[1]\u0026#39; --output text) To retrieve a message from the orders-for-analytics queue, enter the following command, utilizing the analytics queue URL you stored previously: 1aws sqs receive-message \\ 2 --queue-url $analytics_queue_url You will see a JSON response containing an array with one Message:
The response contains the following fields:
Body: A JSON representation of the message ReceiptHandle: You are required to supply this to delete a message after processing MD5OfBody: An MD5 hash of the message body MessageId: The message identifier that Amazon SNS saw when pushing the message to the queues Note that this is not the same as the MessageId that Amazon SNS returned to you when you published to the topic Repeat the previous instruction, using the orders-for-inventory queue but store the message response in a shell variable (for use later) and output the shell variable (using Python\u0026rsquo;s JSON tool to pretty print it): 1inventory_message=$(aws sqs receive-message --queue-url $inventory_queue_url) 2echo $inventory_message | python -m json.tool You will see the same message displayed again.
The message you published to the Amazon SNS topic has been sent to the Amazon SQS queues you subscribed the topic. This is an example of fanning out a message to multiple receivers.
In a non-lab environment, you could have worker applications constantly running and asking the Amazon SQS queues for more messages. One worker may be updating an inventory database for the order, whilst another worker could be recording the order details in a data lake for future analysis.
Using Amazon SNS and Amazon SQS like this allows you to build scalable systems that are decoupled and resilient. If a worker went offline, messages would queue up in the Amazon SQS queues. When the worker is available again, it can pick up new messages where it left off.
You can also have multiple worker applications, to help ensure there\u0026rsquo;s no downtime in message processing.
After successfully processing a message, a worker application should delete the message to prevent it from being processed again.
Store the value of the ReceiptHandle attribute in a shell variable: 1receipt_handle=$(echo $inventory_message | python -m json.tool | grep ReceiptHandle | cut -d\\\u0026#34; -f 4) To delete a message, enter the following command for the orders-for-inventory queue: 1aws sqs delete-message \\ 2 --queue-url $inventory_queue_url \\ 3 --receipt-handle $receipt_handle Return to your browser tab with the Amazon SQS management console open. Note: If the SQS management console appears to only have one SQS queue, click the refresh button above the table:
The correct number of SQS queues will be displayed after a refresh.
Navigate to the Queues list and click the orders-for-inventory queue.
In the top-right, click Send and receive messages:
Verify that in that Receive messages section, under Messages available, it says 0. This is the queue you deleted a message for, simulating a long-running background application that receives an Amazon SQS message and then deletes the message after processing.
Repeat the last three instructions for the orders-for-analytics queue and verify Messages available is 1: This is the queue you did not delete the message for. The message is still available to be picked up for processing by an application receiving messages from the queue.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sqs/fan-out-orders-with-sns-sqs/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudformation/initializing-ec2-with-cloudformation/":{title:"Initializing Amazon EC2 Instances with AWS CloudFormation Init",tags:[],content:`Lab Initializing Amazon EC2 Instances with AWS CloudFormation Init Establishing Desired EC2 Instance State with AWS CloudFormation Init 1. In the AWS Console search bar, search for cloudformation and click the CloudFormation result under Services:
2. Click the Create stack dropdown menu and select With new resources:
3. In the Create stack form, in the Specify template section, ensure **Amazon S3 URL **is selected for the Template source.
4. Paste in the following URL in the Amazon S3 URL field:
1AWSTemplateFormatVersion: \u0026#39;2010-09-09\u0026#39; 2Description: Provision a Single Amazon EC2 Instance with CFN Helper Scripts 3 4Parameters: 5 AmiID: 6 Description: The ID of the AMI. 7 Type: AWS::SSM::Parameter::Value\u0026lt;AWS::EC2::Image::Id\u0026gt; 8 Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2 9 10Resources: 11 WebServer: 12 Type: AWS::EC2::Instance 13 Properties: 14 ImageId: !Ref AmiID 15 InstanceType: t3.micro 16 SecurityGroupIds: 17 - !Ref WebServerSecurityGroup 18 UserData: 19 # Update aws-cfn-bootstrap 20 # Run cfn-init to initialize WebServer content 21 # Return cfn-init run result to CloudFormation upon completion 22 Fn::Base64: 23 !Sub | 24 #!/bin/bash -xe 25 yum update -y aws-cfn-bootstrap 26 /opt/aws/bin/cfn-init -v --stack \${AWS::StackName} --resource WebServer --region \${AWS::Region} 27 /opt/aws/bin/cfn-signal -e $? --stack \${AWS::StackName} --resource WebServer --region \${AWS::Region} 28 CreationPolicy: 29 ResourceSignal: 30 Count: 1 31 Timeout: PT5M 32 Metadata: 33 AWS::CloudFormation::Init: 34 config: 35 packages: 36 yum: 37 httpd: [] 38 files: 39 \u0026#34;/var/www/html/index.html\u0026#34;: 40 content: | 41 \u0026lt;center\u0026gt; 42 \u0026lt;h1\u0026gt;Cloud Academy EC2 Instance\u0026lt;/h1\u0026gt; 43 \u0026lt;h3\u0026gt;This content has been initialized with \u0026lt;a href=\u0026#34;https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-helper-scripts-reference.html\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;AWS CloudFormation Helper Scripts\u0026lt;/a\u0026gt;\u0026lt;/h3\u0026gt; 44 \u0026lt;/center\u0026gt; 45 mode: \u0026#39;000644\u0026#39; 46 services: 47 sysvinit: 48 httpd: 49 enabled: \u0026#39;true\u0026#39; 50 ensureRunning: \u0026#39;true\u0026#39; 51 52 WebServerSecurityGroup: 53 Type: AWS::EC2::SecurityGroup 54 Properties: 55 GroupDescription: SSH and HTTP 56 SecurityGroupIngress: 57 - CidrIp: 0.0.0.0/0 58 FromPort: 22 59 IpProtocol: tcp 60 ToPort: 22 61 - CidrIp: 0.0.0.0/0 62 FromPort: 80 63 IpProtocol: tcp 64 ToPort: 80 65 66Outputs: 67 WebServerPublicDNS: 68 Description: Public DNS of EC2 instance 69 Value: !GetAtt WebServer.PublicDnsName The CloudFormation stack template is stored in a public S3 bucket. The EC2 instance resource definition is shown below:
The WebServer EC2 instance is defined above. It is a size t3.micro instance that references a WebServerSecurityGroup resource for its security group and the AmiID parameter for its image ID. Both of these referenced configurations are defined elsewhere in the template. The UserData script defined next performs the following tasks once the EC2 instance is created:
Updates the aws-cfn-bootstrap package to retrieve the latest version of the helper scripts Runs the cfn-init helper script to execute the WebServer instance Metadata scripts Runs the cfn-signalhelper script to notify CloudFormation after all the service(s) (Apache in this case) is installed and configured on the EC2 instance Note: The cfn-init helper script is not executed automatically. You must run the cfn-init script within the EC2 instance UserData in order to execute your metadata scripts.
The cfn-signal helper script works hand-in-hand with the CreationPolicy configuration. The ResourceSignal property has a Count of 1 and a Timeout of PT5M. This instructs CloudFormation to wait for up to 5 minutes to receive 1 resource signal from the EC2 instance.
The cfn-signal helper script call in the UserData uses $? to retrieve the return code of the previous script. If the cfn-init script is successful and the EC2 instance is configured properly, cfn-signal returns a success to CloudFormation which then transitions the EC2 instance to the CREATE_COMPLETEstatus. If the cfn-init script is unsuccessful or the timeout of 5 minutes expires before receiving a signal, then the EC2 instance will be transitioned to a CREATE_FAILEDstatus and the stack deployment will fail. The EC2 instance Metadata configuration is the same as the previous lab step. It defines a AWS::CloudFormation::Init script to install the httpd package using yum, generate an index.html file within /var/www/html/ and start the httpd service to serve the content from the EC2 instance.
5. Click **Next **to continue:
6. Enter web-server-stack for the Stack name and click Next:
7. You will not be configuring additional stack options. Scroll to the bottom of the page and click Next.
8. On the review page, scroll to the bottom and click **Create stack **to deploy your stack:
Your stack will begin deploying and you will be brought to the Events page of your web-server-stack:
The stack can take up to 3 minutes to deploy successfully. 9. If the Events section does not automatically refresh after 3 minutes, click the refresh icon:
The WebServer instance remains in a CREATE_IN_PROGRESS status until CloudFormation receives a SUCCESS signal from the instance. In the screenshot above, the UniqueId of i-0fd18c8deb52983d5 belongs to the WebServer instance. After the success signal is received, the WebServer instance is transitioned into the CREATE_COMPLETE status. Without the CloudFormation signal helper script, CloudFormation would have transitioned the EC2 instance to a completed status when the resource was created instead of waiting until the Apache service has been installed and running on the instance. 10. Click the Outputs tab on the web-server-stack page:
11. Right-click and open the WebServerPublicDNS URL in a new browser tab:
The HTMLpage generated in the cfn-init script is now being served from the Apache server running within your WebServer EC2 instance:
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudformation/initializing-ec2-with-cloudformation/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudwatch/introduction-to-cloudwatch/":{title:"Introduction to CloudWatch",tags:[],content:`Lab Introduction to CloudWatch Explore CloudWatch 1. AWS has done an excellent job defining CloudWatch key concepts. Read the abbreviated excerpt from their official documentation below to obtain an understanding of Metrics, Namespaces and Alarms: Metrics
A metric is the fundamental concept in CloudWatch and represents a time-ordered set of data points. These data points can be either your custom metrics or metrics from other services in AWS. You or AWS products publish metric data points into CloudWatch and you retrieve statistics about those data points as an ordered set of time-series data. Metrics exist only in the region in which they are created.
Think of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another.
Namespaces
CloudWatch namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.
Note: In this lab you will see namespaces that AWS has created for you, and a custom namespace created by the steps performed in this lab.
Alarms
You can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.
Alarms invoke actions for sustained state changes only. CloudWatch alarms will not invoke actions simply because they are in a particular state. The state must have changed and been maintained for a specified number of periods.
The interested student can take a look at the full version of the documentation here. Due to time constraints, you should look at additional documentation once you have completed the lab.
2. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:
3. Click Metrics \u0026gt; All metrics in the left navigation pane. At this point, there are most likely no custom namespaces. But several AWS namespaces may already be established for you. What metrics are listed on the **All metrics **tab depends on a couple of factors:
How quickly you arrived at this view after starting your lab. This lab creates an EC2 instance and EBS volume when you start the lab. After a couple of minutes of delay, metrics for the EC2 and EBS namespaces are included. How recently your Cloud Academy AWS account has been used to complete other Cloud Academy labs. If the AWS account you logged in to recently completed other labs, you may see namespace related to metrics collected in those labs. 4. Spend a few minutes to explore what metrics and namespaces look like in the CloudWatch console. Simply select any namespace and then any particular metric. As an example, the EC2 namespace and **CPUUtilization **metric for the HighCPUInstance are selected in the image below:
Note: The image above is for illustrative purposes only, you do not need to choose the same instance or metric to explore CloudWatch metrics.
The longer the instance has been running, the more data points will appear in the graph. By default, EC2 metrics are collected every five minutes. You may need to adjust the displayed timeline to 1 week (1w) or further in the past to see some metrics.
Monitoring EC2 Instances 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. Click **Instances **from the navigation pane and select the box near the instance name. A wealth of instance information is displayed in the Details tab:
When you started the Lab, Cloud Academy configured the lab environment for you. This includes a medium instance named HighCPUInstance.
Note: Your information will vary. There is additional instance information not shown in the example above.
3. Switch to the Monitoring tab and take a look at the standard metrics:
Note: If you don\u0026rsquo;t see an instance yet, it\u0026rsquo;s possible that it\u0026rsquo;s still provisioning in the background. Refresh the page every minute or so until it appears.
These are the standard metrics that CloudWatch monitors for all your EC2 instances. Please refer to the documentation for details. (Due to possible time constraints, please look up additional information in the documentation after completing this lab.)
You should be aware that all the metrics in this tab related to Disk (Disk Reads, Disk Read Operations, Disk Writes, Disk Write Operations) pertain to ephemeral storage disks. Those metrics will not represent anything if you have launched an EBS backed instance. To see the metrics related to EBS volumes you need to look elsewhere. Next you will take a look at the metrics of the EBS volume for this particular instance.
Note: Ephemeral storage is also known as instance storage. It is temporary storage that is added to your instance, unlike EBS which is an attached volume that is permanent in nature.
4. To enable and disable detailed monitoring, click Manage detailed monitoring:
The Detailed monitoring page will open :
Here you can enable and disable detailed monitoring by checking or unchecking the Enable checkbox followed by clicking Save. 5. Click Cancel as we will not be enabling detailed monitoring in this lab:
6. Reselect the HighCPUInstance , click the Storage tab. Scroll down and click on the Volume Id (lower right):
7. Select the volume and click on the Monitoring tab to see the metrics for this EBS volume:
As you can see, Amazon does quite a bit out of the box with respect to monitoring EC2 Instances and EBS volumes. However, you can enable Detailed Monitoring for even more control over the monitoring frequency of EC2 instances. CloudWatch monitors EC2 instances every 5 minutes by default. If you need more frequent monitoring, you can enable CloudWatch\u0026rsquo;s Detailed Monitoring feature to monitor your instances every minute. You can enable Detailed Monitoring during the instance launch or change it anytime afterwards. Note: Detailed Monitoring does come with an associated cost.
Install the EC2 Monitoring Scripts 1. Navigate to EC2 Instances by clicking here.
2. Click on Launch instances:
3. In the Application and OS Images section, select the Amazon Linux option under Quick Start:
4. In the Instance Type section, you should not change any options. Simply make sure the default **t2.micro **is selected:
5. In the Key pair section, select the keypair:
Note: Your keypair may differ from the screenshot. Reminder: The PEM or PPK formatted key pair can be downloaded directly from the Your lab data section of the Cloud Academy Lab page at any time.
6. Scroll down and expand the Advanced details section. Under IAM instance profile, select the IAM role provided. It will have a name that looks similar to cloudacademylabs-EC2MonitoringRole-XXXXXXXXXX :
7. Scroll down to Detailed CloudWatch monitoring and select Enable:
8. Scroll down to User data and copy and paste the following bash script code in the User data (As text) field:
This is where the magic happens. Next you will insert the code to execute during the instance launch. However, in order to send metrics to CloudWatch, you need to configure some credentials first. You can use either Access Keys or IAM roles for this task. In this Lab, you will follow the best practices and use IAM roles. There is an instance role already created in you account configured with the proper permissions.
Copy code
#!/bin/bash yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 wget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip unzip CloudWatchMonitoringScripts-1.2.2.zip rm CloudWatchMonitoringScripts-1.2.2.zip echo \u0026ldquo;*/1 * * * * /aws-scripts-mon/mon-put-instance-data.pl \u0026ndash;mem-util \u0026ndash;disk-space-util \u0026ndash;disk-path=/ \u0026ndash;from-cron\u0026rdquo; \u0026gt; monitoring.txt crontab monitoring.txt rm monitoring.txt
This bash script will get executed the first time the instances launches. In summary, the script will:
Install Perl libraries Retrieve and install the AWS CloudWatch Monitoring scripts Configure crontab to run the monitoring script every minute 9. In the Summary section, click Launch instance:
A confirmation page will let you know that your instance is launching:
10. Click View all instances.
Notice the Name for the new instance is blank by default. Although not mandatory, it is helpful to have a name. Move your mouse into the blank space in the Name column. It turns to an edit pencil. Use the pencil to change your Instance Name to Monitoring Scripts:
Wait until the** Instance State** is R****unning for the new Instance. It typically takes less than one minute for the state to transition from P****ending to R****unning.
11. Navigate back to **CloudWatch by clicking here and clickAll metrics **from the navigation pane. Notice that there is a new namespace called System/Linux under Custom namespaces:
This name is configured when you send the custom metrics.
Note: If you don\u0026rsquo;t see the new Namespace wait a few minutes and refresh the page. CloudWatch takes some time to display the information in the dashboard. Recall that the newly installed monitoring scripts send data every minute based on the crontab configuration setup in the User data bash script for the instance.
12. Click on the new System/Linux namespace:
There are two metrics being monitored by CloudWatch in the custom System/Linux namespace. (Filesystem, InstanceId, MountPath and InstanceId)
13. Click the metric on the left (Filesystem\u0026hellip;), then select the checkbox so the first metric is graphed.
14. Click Linux System, so the Metrics path is All \u0026gt; Linux System again. Now select the metric on the right (InstanceId) and select its checkbox as well. 15. Switch to the Graphed metrics tab. If you selected both metrics correctly the tab will include a \u0026ldquo;(2)\u0026rdquo; at the end of it indicating how many metrics are graphed. Your graph should look similar to the following:
It is simple to customize the display to meet your needs for the metrics displayed.
16. Click the custom graph period drop-down above the graph display area and select **15 **from the **Minutes **row:
17. Select the Period drop-down column menu for each metric in the lower Graphed metrics tab and choose 1 Minute:
You can now see the highest resolution metrics that are being sent to CloudWatch every minute. (You may need to refresh the chart after setting the new periods)
18. Select Maximum for the **Statistic **column. Instead of an average of the datapoints, the maximum will be graphed. (Note: In the lab example it is probably the same since the disk really has not been touched) Your configuration should look like:
Creating Your First CloudWatch Alarm 1. Navigate to CloudWatch by clicking here, click on Alarms \u0026gt; All Alarms in the left pane:
There are no Alarms configured, so there are no records found. Further, the three types of Alarms are all at zero (0).
Note: More information on Alarm states will be covered soon.
2. Click Create Alarm and click Select metric. Select the EC2 namespace:
Many different metrics are displayed for both the HighCPUInstance and the Monitoring Scripts instances.
3. Click Per-instance metrics, scroll down and select the metric with **HighCPUInstance **under **Instance Name and CPUUtilization **under Metric Name:
Tip: You may need to use the arrows in the upper right to find the HighCPUInstance on another page. Alternatively, you can make note of the last 3 or 4 characters in the InstanceId from the EC2 console, then enter those in the Search Metrics field. The search applies to all pages of information.
Once selected it is graphed immediately. Notice that you could tailor the graph to a specific Time Range (upper-right). For example, the time range can be specified in Relative or Absolute terms. 4. Click Select metric when ready. 5. Under Conditions, set the following values leaving the defaults for the rest:
_Whenever High CPU is\u0026hellip;: _Greater/Equal Than\u0026hellip;: 50 An alarm watches for a metric to go beyond an allowable value range when monitored over time. If violated the alarm\u0026rsquo;s state is changed. There are three possibles states for an alarm:
OK—The metric is within the defined threshold
In alarm—The metric is outside of the defined threshold
Insufficient data—The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state
6. Click **Next **and fill out the form as described:
Alarm state trigger: In alarm **Select an SNS topic: **Create new topic Insert a valid e-mail and click on Create topic.
7. Click Next and fill the form as described before clicking Next:
Define a unique name: High CPU Alarm description: When CPU utilization \u0026gt;= 50% Tip: Be sure to use your valid email address in the Email list field so you can verify the Alarm later. AWS Simple Notification Service (SNS) is used to send the email when the alarm is triggered. However, you will not need to configure anything in SNS.
8. Click Create alarm when ready.
9. Check for an email from AWS Notifications. Open up the email and click the Confirm subscription link:
You should receive a subscription confirmation. (For example, a confirmation message from Amazon Simple Notification Service (SNS) in a new browser tab if using a browser-based email client like Gmail.) To summarize, you have created a new alarm, along with a new SNS topic. Since you subscribed to this new SNS topic, every time the state of the alarm switches to ALARM you will receive an email. You may not receive an alarm email if the time it took to confirm the SNS topic subscription took longer than the time it took for the alarm to trigger. Emails will only be sent to subscribers at the time of the alarm transition.
10. You should be put to the Alarm page:
Note your Alarm state may differ. For example, you may be in an **Insufficient data **state briefly and then transition either to In alarm or OK. Troubleshooting Tip: If the state of your alarm does not change to In alarm almost immediately, it is probably because you picked the incorrect instance. The HighCPUInstance is designed to trigger an alarm due to a high CPU utilization metric. The Monitoring Scripts instance is not taxed at all. To remedy the situation you can either create a new alarm with the correct instance, lower the threshold to something artificially low (1), or change the \u0026gt;= to \u0026lt;= (which is not very realistic but will test the alarm).
11. Click the Alarm. You can see very useful information about the alarm itself. In the Details tab, there is a general overview of the alarm, and in the History tab you can see up to the last 50 states of the alarm:
12. After an **In alarm **state is raised, check for an email titled ALARM: \u0026ldquo;High CPU Alarm\u0026rdquo; in US West - Oregon from AWS Notifications.
Again, you may not have received an email because the alarm triggers before you had time to subscribe to the notification topic. Don\u0026rsquo;t worry, in the next Lab Step, you will reuse the topic for another alarm. Because you are already subscribed, you will receive an email. You could also retrigger the alarm by editing the alarm to trigger when CPUUtilization is \u0026gt;= 500 (which can never happen for the single CPU instance). Wait five minutes until the alarm is disabled, then edit the alarm to trigger when CPUUtilization \u0026gt;= 50.
13. Now move to the History tab:
Your History is likely similar to the example shown above. The oldest event is the furthest down. In succession, the Alarm was created; the state changed from INSUFFICIENT DATA to ALARM; SNS sent off an email notification.
15. Spend a few minutes exploring the latest alarm history and try to understand what is going on with each entry. You can see more details for each entry by clicking the date.
Create an Alarm using the EC2 console 1. Navigate to EC2 Instances by clicking here.
2. Select the Monitoring Scripts instance, then switch to the Status Checks tab:
3. Click **Actions **\u0026gt; Create Status Check Alarm:
This dialog is similar in function to the create alarm wizard you saw in an earlier Lab Step.
4. For the Alarm notification, select the SNS topic name you set up before.
Other fields can be kept at their defaults. The Alarm thresholds section uses Status check failed: either to trigger the alarm for either instance or system status check failures:
5. Click Create when ready. An alarm creation confirmation message is displayed:
Now you know two different ways to create alarms: one from CloudWatch and the other from the EC2 console. Next, you will learn how to attach EC2 actions to alarms.
6. Return to the** Alarms by clicking here. **Notice that the first alarm you created is stuck in the In alarm state.
The alarm is stuck in the In alarm state because the instance is running an application that consumes 100% of the CPU utilization. Clearly an indicator that something may have gone wrong with the instance. Imagine that you are managing a production environment and you have an instance that is becoming unavailable intermittently because of high CPU utilization. You would like to receive a notification every time the CPU utilization is high, but this can happen anytime, in the middle of the night, or during a weekend or holiday. It would be helpful to have a pre-defined action in this case \u0026ndash; at least until you find a definitive solution for the problem.
To help you address the scenario, you can set EC2 actions on your alarms. 8. Select the High CPU alarm and then Actions \u0026gt; Edit:
To make your alarm more suitable to the training environment needs, set a new EC2 Action to Reboot this instance whenever the state of this alarm is ALARM.
9. Click on Next and click on **Add EC2 action **under EC2 action. Select Reboot this instance.
10. Click Update alarm when ready.
Although the changes have been made to the alarm, the alarm remains in the In alarm state. CloudWatch will only perform actions when the state transitions to the **In alarm **state from another state. In the next instruction, you will modify the alarm to quickly have it change to the **OK **state and then change it back to return to the **In alarm **state. 11. Select the **High CPU **Alarm and choose Actions \u0026gt; Edit. Toggle the relationship from \u0026gt;= to \u0026lt;= and click Update alarm:
12. Refresh the page to ensure the alarm has transitioned to the **OK **state. Then toggle the condition back to \u0026gt;= and save the alarm to have it transition to the In alarm state.
_Note: _The state change may not be immediate and may take up to 2 minutes.
13. Navigate back to the Instances by clicking here and watch CloudWatch reboot the instance when the Alarm Status changes to In alarm.
In case you miss it, you can return to the alarm in CloudWatch and see the Reboot Action listed in the **History **section:
14. Check your email client and confirm that you received a notification of the alarm:
Sharing CloudWatch Metrics with others 1. Go to CloudWatch by clicking here and click on **Metrics **\u0026gt; All metrics.
2. Select an interesting metric, such as the DiskspaceUtilization metric for the Monitoring Scripts instance, and click Actions \u0026gt; Share:
This metric can be found under** System/Linux** \u0026gt; Filesystem, InstanceId, MountPath.
3. In the Share Graph URL dialog, right-click and copy the URL, then Close the dialog:
4. The URL for the specific graph you were looking at is copied into the clipboard. You can paste it into a test email to confirm this. For example:
The URL is quite complex. To confirm that it is indeed correct, test it out in another browser tab.
5. Open another browser tab. Paste the URL into the address field and refresh your browser. You should see the exact same graph as the one you shared earlier. Notice that you need to be logged into the AWS console in order to view the information referenced by the URL. For security reasons, you can only share URLs with other AWS Identity and Access Management (IAM) users who have the appropriate CloudWatch IAM permissions in your AWS account.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/cloudwatch/introduction-to-cloudwatch/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codecommit/introduction-codecommit/":{title:"Introduction to CodeCommit",tags:[],content:`Lab https://cloudacademy.com/lab/introduction-codecommit/
Create a repository 1. In the AWS Management Console search bar, enter CodeCommit, and click the CodeCommit result under Services:
2. Click Create repository:
3. In the Create repository form enter the following values accepting the defaults for values not specified:
Repository name: lab-repository You can leave the Description field empty for this lab. Usually this field would contain a short description of the purpose of the repository. Attaching meaningful descriptions to repositories makes managing large numbers of repositories easier.
4. Click Create to create the repository.
Creating credentials to access your repository 1. In the AWS Management Console search bar, enter IAM, and click the IAM result under Services:
2. Under Access Management, click Users in the left-hand sidebar menu:
3. In the IAM user list, click student:
4. Click the Security credentials tab:
5. Scroll down to the HTTPS Git credentials for AWS CodeCommit section and click Generate credentials:
6. In the box that opens, click Download credentials:
Your browser will download a file called credentials.csv.
Keep these credentials saved, you will use them in later steps.
Accessing a shell with Git available 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. In the left-hand side menu, click Instances:
3. Select the instance and in the row of buttons above the instance list, click Connect:
4. In the Connect to instance dialog, ensure EC2 Instance Connect is selected and enter in the User name field:
5. To open a shell on this instance, click Connect.
EC2 Instance Connect allows you to connect to the instance over SSH using your web browser. With EC2 Instance Connect a new browser window opens an SSH shell on a Linux host that has git installed.
Keep this window open, you will use it in later steps.
Adding files to your repository 1. Navigate to the CodeCommit Console.
2. In the list of repositories, click lab-repository:
3. Click Clone URL and select Clone HTTPS in the drop-down menu that opens:
The URL of the repository has been copied to your clipboard.
4. To copy the repository to your Linux host, in your shell, type git clone followed by a space and paste the repository URL: 1git clone https://git-codecommit.us-west-2.amazonaws.com/v1/repos/lab-repository 5. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:
You can ignore the warning about cloning an empty repository.
You have copied the repository from AWS CodeCommit and stored it locally on the Linux host.
6. To change to the directory of the repository, enter the cd command:
1cd lab-repository 7. To create a file, enter the following command:
1echo \u0026#34;lab\u0026#34; \u0026gt; lab.txt With this command you have created a file called lab.txt that can be added to your local repository.
8. To add the file to your local repository, enter the following commands:
1git add lab.txt 2git commit -m \u0026#34;Lab commit\u0026#34; In Git terminology, with the first command you are \u0026ldquo;staging\u0026rdquo; the file before you commit it. This process enables you to specify which files you want to add to the repository and which ones you want to ignore when committing.
You will see output similar to the following:
You can ignore the message about your name and email address. Usually when using Git you will configure the name and email address so that your commits are labeled with these details.
You have added the lab.txt file to your local copy of the repository on the Linux host.
Pushing your commit to your remote repository 1. In the shell on the Linux host, enter the following command:
1git push In Git terminology, with this command you are \u0026ldquo;pushing\u0026rdquo; your local commit from your \u0026ldquo;local\u0026rdquo; repository to the \u0026ldquo;remote\u0026rdquo; repository that you \u0026ldquo;cloned\u0026rdquo; from.
2. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:
You have copied the file from the local repository on the Linux host, to the repository hosted in AWS CodeCommit.
3. Navigate to the CodeCommit Console.
4. In the Repositories list click lab-repository:
You will see the lab.txt file you pushed in the previous Lab step listed.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/codecommit/introduction-codecommit/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sns/aws-lambda-sns-notifications/":{title:"Process Amazon SNS Notifications with AWS Lambda",tags:[],content:`Lab Process Amazon SNS Notifications with AWS Lambda Creating an Amazon SNS Topic 1. In the AWS Management Console search bar, enter SNS, and click the Simple Notification Service result under Services:
In the left-hand side menu, click Topics: If you can\u0026rsquo;t see the left-hand menu, to expand it, click the following:
Click Create topic: In the Create topic form, ensure to have selected the Standard type, and enter the following values accepting the defaults for values not specified: Name: lab-topic You can leave the Display name field empty for this Lab. When you create topics where the recipients receive messages over SMS (Short Message Service) you are required to provide a value.
At the bottom of the form, click Create topic: Creating an AWS Lambda Function 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:
You will see the Functions list page.
Click Create function: In the Create function form, ensure Author from scratch is selected: In the Create function form, enter lab-function in the Function name field: In the Create function form, in the Runtime drop-down, select Python 3.8: In the Create function form, click Change default execution role and select Use an existing role: In the Existing role drop-down, select lambda_s3_put: The role you have selected has been pre-populated for this Lab. Usually when using Lambda you will create a specific role for your function.
To create your function, click Create function: Implementing an AWS Lambda Function to Upload to S3 Scroll down to the Code source section and double-click lambda_function.py.
In the code editor, replace the contents with the following Python code:
1from datetime import datetime 2import boto3 3 4account_id = boto3.client(\u0026#39;sts\u0026#39;).get_caller_identity()[\u0026#34;Account\u0026#34;] 5s3 = boto3.resource(\u0026#39;s3\u0026#39;) 6 7 8def lambda_handler(event, context): 9 record = event[\u0026#39;Records\u0026#39;][0][\u0026#39;Sns\u0026#39;] 10 message = record[\u0026#39;Message\u0026#39;] 11 subject = record[\u0026#39;Subject\u0026#39;] 12 13 print(\u0026#34;Subject: %s\u0026#34; % subject) 14 print(\u0026#34;Message: %s\u0026#34; % message) 15 16 s3.Object(f\u0026#34;sns-lab-bucket-{account_id}\u0026#34;, subject).put(Body=message) 17 18 return \u0026#34;SUCCESS\u0026#34; The function code you entered processes a message from SNS. The code uploads a file into an S3 Bucket which was pre-created as a part of this lab. The name of the file will be the subject of the message and the content of the file will be the message body.
You can use a Lambda function to do many different things. Some examples include:
Process web-requests Put a custom metric into AWS CloudWatch Add or update a record in a database Post a web-request to an external service To save your changes and deploy your function, at the top of the Code source section, click Deploy: You will see a notification that your function has been deployed:
4. To add an SNS trigger, in the Function overview section, click Add trigger:
​
In the Select a trigger dropdown, enter SNS, and click the SNS result: ​
In the SNS topic drop-down, select lab-topic: ​
The SNS topic field will be filled with the ARN (Amazon Resource Name) of your SNS topic.
To add your SNS trigger, click Add: ​
You will see a notification that your trigger has been added:
In SNS terminology, by adding an SNS trigger you have \u0026ldquo;subscribed\u0026rdquo; your Lambda function to the SNS topic.
Publishing a Message to an Amazon SNS Topic Navigate to the AWS SNS service.
In the left-hand side menu, click Topics:
In the list of topics, click lab-topic: Click Publish message: In the Message details section of the form, in the Subject field, enter lab-subject: In the Message body section of the form, in the Message body to send to the endpoint textbox, enter Lab Message: Usually when you publish a message to an SNS topic, you would include meaningful data in the message body. The content of the message body is often called the \u0026ldquo;payload\u0026rdquo; of a message. In SNS, the payload can be plain text, or it can be a structured payload such as JSON, XML, or some other format. The service or device subscribed to your topic can use the data in the payload to determine what action to take in response to receiving a message.
To publish your message, click Publish message: You will see a notification, similar to the following, confirming your message has been published:
Verifying the AWS Lambda Function Processed the Message 1. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:
In the list of S3 Buckets, click the Bucket beginning with sns-lab-bucket-: In the list of objects you will see a file called lab-subject: This file was uploaded to the S3 bucket by your Lambda function.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/sns/aws-lambda-sns-notifications/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kinesis/sessionizing-clickstream-data-kinesis-data-analytics/":{title:"Sessionizing Clickstream Data with Amazon Kinesis Data Analytics",tags:[],content:`Lab Sessionizing Clickstream Data with Amazon Kinesis Data Analytics Creating an Amazon Kinesis Data Analytics Application 1. In the AWS Management Console search bar, enter Kinesis, and click the Kinesis result under Services:
You will be taken to the Amazon Kinesis dashboard.
In this lab, a Kinesis Data Stream has been pre-created for you. Under Data Streams you will see Total data streams is one:
2. In the left-hand menu, click Analytics applications and under that click SQL applications:
3. To start creating a Kinesis Data Analytics application, under Data Analytics, click Create SQL application (legacy):
You will be taken to the Create legacy SQL application form.
4. In the Application configuration section, and enter lab-application in the Application name textbox:
5. At the bottom of the page, click Create legacy SQL application:
You will be taken to a page displaying details of your application and you will see a notification that your application has been created:
You will come back to this page later in the lab to connect the pre-created Kinesis Data Stream as a data source for your Kinesis Data Analytics application.
6. To navigate to the Kinesis Data Streams list page, in the left-hand side menu, click Data streams:
You will see one data stream listed called lab-stream.
7. To view the details of the pre-created data stream, in the list, click lab-stream:
You will be taken to the Stream details page and you will see a series of tabs with Monitoring selected.
8. To see the configuration details of the data stream, click Configuration:
Take a moment to look at the details on this page, there are several Kinesis Data Stream configuration options that you should be aware of:
Data Stream capacity: The number of shards in the Data Stream. Each shard has a maximum read and write capacity. To increase the total capacity of a data stream you can add shards. Encryption: Kinesis Data Streams can be encrypted using an AWS managed or customer-managed, KMS key. Data retention: A Kinesis Data Stream can retain data for a configurable amount of time between 24 and 168 hours. Enhanced (shard-level) metrics: More detailed CloudWatch metrics can be enabled for a Data Stream, these enhanced metrics have an extra cost. In this lab, you will be working with a small amount of sample data, so there is one shard configured.
Leave these options without changing them.
Connecting to the Virtual Machine using EC2 Instance Connect 1. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:
2. To see available instances, click Instances in the left-hand menu:
The instances list page will open, and you will see an instance named cloudacademylabs:
If you don\u0026rsquo;t see a running instance then the lab environment is still loading. Wait until the Instance state is Running.
3. Right-click the cloudacademylabs instance, and click Connect:
The Connect to your instance form will load.
4. In the form, ensure the EC2 Instance Connect tab is selected:
You will see the instance\u0026rsquo;s Instance ID and Public IP address displayed.
5. In the User name textbox, enter ec2-user:
Note: Ensure there is no space after ec2-user or connect will fail. 6. To open a browser-based shell, click Connect:
If you see an error it\u0026rsquo;s likely that the environment hasn\u0026rsquo;t finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:
A browser-based shell will open in a new window ready for you to use.
Keep this window open, you will use it in later lab steps.
You can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.
Simulating a Real-Time Clickstream 1. To create a template JSON file for a click event, enter the following command into the shell:
1echo \u0026#39;{ 2 \u0026#34;user_id\u0026#34;: \u0026#34;$USER_ID\u0026#34;, 3 \u0026#34;event_timestamp\u0026#34;: \u0026#34;$EVENT_TIMESTAMP\u0026#34;, 4 \u0026#34;event_name\u0026#34;: \u0026#34;$EVENT_NAME\u0026#34;, 5 \u0026#34;event_type\u0026#34;: \u0026#34;click\u0026#34;, 6 \u0026#34;device_type\u0026#34;: \u0026#34;desktop\u0026#34; 7}\u0026#39; \u0026gt; click.json There are two parts to this command, the first uses the built-in Bash command echo to print a JSON template. The second part uses a feature of the Bash shell called redirection, it redirects the output of the echo command to a file (creating it if doesn\u0026rsquo;t exist) called click.json.
The template contains five fields, the event_type, and device_type fields are hardcoded, in a non-lab environment, you may encounter streams that come from different types of devices and streams that contain more than one type of event (clickstream events alongside sales or transaction data for example). The other fields will be populated dynamically.
2. To put records into Kinesis and simulate a clickstream, enter the following command:
1USER_IDS=(user1 user2 user3) 2EVENTS=(checkout search category detail navigate) 3for i in $(seq 1 3000); do 4 echo \u0026#34;Iteration: \${i}\u0026#34; 5 export USER_ID=\u0026#34;\${USER_IDS[RANDOM%\${#USER_IDS[@]}]}\u0026#34;; 6 export EVENT_NAME=\u0026#34;\${EVENTS[RANDOM%\${#EVENTS[@]}]}\u0026#34;; 7 export EVENT_TIMESTAMP=$(($(date +%s) * 1000)) 8 JSON=$(cat click.json | envsubst) 9 echo $JSON 10 aws kinesis put-record --stream-name lab-stream --data \u0026#34;\${JSON}\u0026#34; --partition-key 1 --region us-west-2 11 session_interval=15 12 click_interval=2 13 if ! (($i%60)); then 14 echo \u0026#34;Sleeping for \${session_interval} seconds\u0026#34; \u0026amp;\u0026amp; sleep \${session_interval} 15 else 16 echo \u0026#34;Sleeping for \${click_interval} second(s)\u0026#34; \u0026amp;\u0026amp; sleep \${click_interval} 17 fi 18done You will see the templated JSON and also the JSON response from Kinesis for each record put into the Data Stream:
This command simulates a real-time click-stream with the following characteristics:
Creates three thousand events Events have a two-second interval between them After every sixty events (two minutes) there is a fifteen-second interval, later you will assume a gap of ten seconds or more is a session boundary The command has a number of parts:
Setup of sample user ids and event types at the beginning A loop that will execute three thousand times and a sleep statement Statements that randomly select a user id and an event type, and assign them along with the current timestamp to variables A statement that uses the envsubst command to substitute defined environment variables in the JSON template A statement invoking the AWS command-line interface tool, putting the templated JSON record into the Kinesis Data Stream A condition at the end of the loop that either sleeps for a few seconds or, periodically for longer, simulating the end of a session Leave the command running.
Navigate to Kinesis Data Analytics in the AWS Management Console. 4. In the list of applications, to expand the application, click lab-application:
5. To connect your Data Analytics application to the pre-created Data Stream, click **Configure under Source stream **form:
The Configure source for lab-application form will load.
6. Under Source, ensure Kinesis data stream is selected:
7. In the Kinesis data stream, click Browse to select the radio button for lab-stream and click Choose:
8. Under Access permissions, select Choose from IAM roles that Kinesis Data Analytics can assume:
9. In the IAM role list, select the role beginning with cloudacademy-lab-data-analytics:
If you don\u0026rsquo;t see the above role listed click the refresh button:
10. To start discovering the schema of the records you added to the Data Stream, click Discover schema:
After a moment or two, you will see a notification that the discovery was successful and below, some of the records will be displayed:
11. To finish connecting your Data Analytics application to your Data Stream, click Save changes:
You will be redirected to the page for your Kinesis Data Analytics application. Leave this page open in a browser tab.
Sessionizing the Clickstream Data using Amazon Kinesis Data Analytics 1. Return to the page for your Kinesis Data Analytics application in the AWS Management Console.
2. To start your application and expand the Steps to configure your application, click Configure SQL:
3. In the SQL code editor, replace the existing contents with the following SQL commands
1CREATE OR REPLACE STREAM \u0026#34;INTERMEDIATE_SQL_STREAM\u0026#34; 2( 3 \u0026#34;event_timestamp\u0026#34; TIMESTAMP, 4 \u0026#34;user_id\u0026#34; VARCHAR(7), 5 \u0026#34;device_type\u0026#34; VARCHAR(10), 6 \u0026#34;session_timestamp\u0026#34; TIMESTAMP 7); 8 9 10CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP1\u0026#34; AS INSERT INTO \u0026#34;INTERMEDIATE_SQL_STREAM\u0026#34; 11SELECT STREAM 12 TO_TIMESTAMP(\u0026#34;event_timestamp\u0026#34;) as \u0026#34;event_timestamp\u0026#34;, 13 \u0026#34;user_id\u0026#34;, 14 \u0026#34;device_type\u0026#34;, 15 CASE WHEN (\u0026#34;event_timestamp\u0026#34; - lag(\u0026#34;event_timestamp\u0026#34;, 1) OVER (PARTITION BY \u0026#34;user_id\u0026#34; ROWS 1 PRECEDING)) \u0026gt; (10 * 1000) THEN 16 TO_TIMESTAMP(\u0026#34;event_timestamp\u0026#34;) 17 WHEN (\u0026#34;event_timestamp\u0026#34; - lag(\u0026#34;event_timestamp\u0026#34;, 1) OVER (PARTITION BY \u0026#34;user_id\u0026#34; ROWS 1 PRECEDING)) IS NULL THEN 18 TO_TIMESTAMP(\u0026#34;event_timestamp\u0026#34;) 19 ELSE NULL 20 END AS \u0026#34;session_timestamp\u0026#34; 21FROM \u0026#34;SOURCE_SQL_STREAM_001\u0026#34;; These statements do the following:
Defines an intermediate stream to insert data into called INTERMEDIATE_SQL_STREAM Creates a PUMP that selects data from the source stream The SELECT statement uses the LAG function to determine if there is a ten-second interval between the last event and the current event The LAG function statements are used with PARTITION statements to restrict the LAG function by the user You should know that Kinesis Data Analytics natively assumes Unix timestamps include milliseconds. The stream you simulated is providing timestamps with milliseconds. This is why the CASE WHEN statement that checks for a ten-second interval includes (10 * 1000), it\u0026rsquo;s multiplying ten by one thousand to get ten seconds in milliseconds.
Tip: you can increase the height of the SQL editor text-box by dragging the grey bar at the bottom.
4. To execute the SQL statements, click Save and run application:
The query will take up to a couple of minutes to execute and start returning results.
Occasionally you may see an error caused by the fifteen-second interval, if you do, re-run the query by clicking Save and run application again.
Take a look at the results. Notice that only some records have a value for session_timestamp. This is because the CASE WHEN statement in the query supplies a value of null when:
The interval between event timestamps is less than ten seconds There is no preceding event Also notice that below the SQL Code editor, there are two streams, the INTERMEDIATE_SQL_STREAM, and an error_stream. The error stream is where any errors that occur during the execution of the SQL will be delivered to.
5. In the SQL editor window, under the current SQL statements, add the following:
1CREATE OR REPLACE STREAM \u0026#34;DESTINATION_SQL_STREAM\u0026#34; ( 2 \u0026#34;user_id\u0026#34; CHAR(7), 3 \u0026#34;session_id\u0026#34; VARCHAR(50), 4 \u0026#34;session_time\u0026#34; VARCHAR(20), 5 \u0026#34;latest_time\u0026#34; VARCHAR(20) 6); 7 8 9CREATE OR REPLACE PUMP \u0026#34;STREAM_PUMP2\u0026#34; AS INSERT INTO \u0026#34;DESTINATION_SQL_STREAM\u0026#34; 10SELECT STREAM 11 \u0026#34;user_id\u0026#34;, 12 \u0026#34;user_id\u0026#34;||\u0026#39;_\u0026#39;||\u0026#34;device_type\u0026#34;||\u0026#39;_\u0026#39;||TIMESTAMP_TO_CHAR(\u0026#39;HH:mm:ss\u0026#39;, LAST_VALUE(\u0026#34;session_timestamp\u0026#34;) IGNORE NULLS OVER 13 (PARTITION BY \u0026#34;user_id\u0026#34; RANGE INTERVAL \u0026#39;24\u0026#39; HOUR PRECEDING)) AS \u0026#34;session_id\u0026#34;, 14 TIMESTAMP_TO_CHAR(\u0026#39;HH:mm:ss\u0026#39;, \u0026#34;session_timestamp\u0026#34;) AS \u0026#34;session_time\u0026#34;, 15 TIMESTAMP_TO_CHAR(\u0026#39;HH:mm:ss\u0026#39;, \u0026#34;event_timestamp\u0026#34;) AS \u0026#34;latest_time\u0026#34; 16FROM \u0026#34;INTERMEDIATE_SQL_STREAM\u0026#34; 17WHERE \u0026#34;user_id\u0026#34; = \u0026#39;user1\u0026#39;; These SQL statements do the following:
Creates a stream called DESTINATION_SQL_STREAM Creates a PUMP that selects from the INTERMEDIATE_SQL_STREAM Constructs a session_id by combining the user, device type and time Restricts the query to user1 using a WHERE clause Something else to note about these statements is that the session and event timestamps are being converted to times.
6. To run the updated query, click Save and run application.
You will see results similar to:
Your times will be different.
Notice that the session_time values are more than ten seconds apart. And that the seconds\u0026rsquo; interval of the latest_time column between the rows that have a session time, is ten seconds or less.
7. To see only the rows for new sessions, replace the last line of the query with the following:
1WHERE \u0026#34;session_timestamp\u0026#34; IS NOT NULL; This change to the WHERE clause of the last SQL statement removes the restriction of the query to user1, and removes rows where the value of session_timestamp is null.
8. Click Save and run application to re-run your query.
You will see results similar to the following:
Your results will be different.
The results now contain only session boundary rows for each of the users.
Leave this browser tab open with the query running in Kinesis Data Analytics.
Creating an AWS Lambda function to Store Sessions in an Amazon DynamoDB Table 1. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:
2. To start creating your function, click Create function:
3. Under Create function, ensure Author from scratch is selected:
4. Under Basic information, in the Function name text-box, enter lab-function:
5. In the Runtime drop-down, select the latest Python 3.x version available.
6. To expand the role selection form, click Change default execution role.
7. Under Execution role, select the Use an existing role radio button:
8. To assign an execution role, in the Existing role drop-down, select the role called cloudacademy-lab-lambda:
9. To create your function, click Create function:
You will be taken a page where you can configure your function, and you will see a notification that your function has been successfully created:
10. Scroll down to the Code source section and in the code editor double-click the lambda_function.py file.
11. To update your Lambda function\u0026rsquo;s implementation, replace the code in the editor window with the following:
1from __future__ import print_function 2import boto3 3import base64 4from json import loads 5 6dynamodb_client = boto3.client(\u0026#39;dynamodb\u0026#39;) 7 8table_name = \u0026#34;CloudAcademyLabs\u0026#34; 9 10def lambda_handler(event, context): 11 payload = event[\u0026#39;records\u0026#39;] 12 output = [] 13 success = 0 14 failure = 0 15 16 for record in payload: 17 try: 18 payload = base64.b64decode(record[\u0026#39;data\u0026#39;]) 19 data_item = loads(payload) 20 21 ddb_item = { 22 \u0026#39;session_id\u0026#39;: { \u0026#39;S\u0026#39;: data_item[\u0026#39;session_id\u0026#39;] }, 23 \u0026#39;session_time\u0026#39;: { \u0026#39;S\u0026#39;: data_item[\u0026#39;session_time\u0026#39;] }, 24 \u0026#39;user_id\u0026#39;: { \u0026#39;S\u0026#39;: data_item[\u0026#39;user_id\u0026#39;] } 25 } 26 27 dynamodb_client.put_item(TableName=table_name, Item=ddb_item) 28 29 success += 1 30 output.append({\u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;Ok\u0026#39;}) 31 except Exception: 32 failure += 1 33 output.append({\u0026#39;recordId\u0026#39;: record[\u0026#39;recordId\u0026#39;], \u0026#39;result\u0026#39;: \u0026#39;DeliveryFailed\u0026#39;}) 34 35 print(\u0026#39;Successfully delivered {0} records, failed to deliver {1} records\u0026#39;.format(success, failure)) 36 return {\u0026#39;records\u0026#39;: output} This python code processes a record from Kinesis Data Analytics and puts it into a DynamoDB table.
The implementation is based on one provided by AWS. The only change is the statements that construct the ddb_item. They have been modified to match the data being supplied by your Kinesis Data Analytics application.
12. To deploy your function, at the top, click Deploy:
You will see a notification that your function has been deployed:
13. To configure a timeout for your function, click the Configuration tab, and click Edit:
14. Under Timeout, enter 1 in the min text-box, and 0 in the sec text-box:
You are updating the timeout because the default of three seconds is too low when processing data from Kinesis Data Analytics, and may lead to failures caused by the function timing out. AWS recommends setting a higher timeout to avoid such failures. 15. To save your function\u0026rsquo;s updated timeout, click Save:
You will see a notification that your change to the timeout has been saved:
Configuring Amazon Kinesis Data Analytics to Use Your AWS Lambda Function as a Destination 1. Navigate to Kinesis Data Analytics in the AWS Management Console.
2. In the list of applications, to expand the application, click lab-application:
3. To begin configuring your Lambda as a destination, expand the Steps to configure your application and click Add destination:
The Configure destination form will load.
4. Under Destination select AWS Lambda function:
5. Under AWS Lambda function, click Browse and check radio box for lab-function followed by clicking Choose:
This is the Lambda function you created in the previous lab step.
6. Under Access permissions, ensure Choose from IAM roles that Kinesis Data Analytics can assume is selected:
7. In the IAM role drop-down, select the role called cloudacademy-lab-lambda:
This is a role that has been pre-created for this lab and allows Kinesis Data Analytics to invoke your Lambda function.
8. In the In-application stream section, under Connect in-application stream, select Choose an existing in-application stream:
9. In the In-application stream name drop-down, select DESTINATION_SQL_STREAM:
10. To finish connecting your Kinesis Data Analytics application to your Lambda function, click Save changes:
Your Kinesis Data Analytics application is being updated. Please be aware that it can take up to three minutes to complete.
Once complete the details page for Kinesis Data Analytics application will load.
11. In the AWS Management Console search bar, enter DynamoDB, and click the DynamoDB result under Services:
12. In the left-hand menu, click Tables:
13. In the list of tables, click CloudAcademyLabs:
This table was pre-created as a part of this lab.
14. To see items in the DynamoDB table, click the **Explore Table **Items button:
You will see the items in the table listed similar to:
These items have been inserted into the DyanmoDB table by your Lambda function, it\u0026rsquo;s being invoked by your Kinesis Data Analytics application.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/kinesis/sessionizing-clickstream-data-kinesis-data-analytics/"},"https://romankurnovskii.com/en/posts/photos/22-07-02-israel-haifa-bahai-gardens/":{title:"Israel - Haifa - Bahai Gardens",tags:[],content:`Google maps Route
`,url:"https://romankurnovskii.com/en/posts/photos/22-07-02-israel-haifa-bahai-gardens/"},"https://romankurnovskii.com/en/categories/photos/":{title:"Photos",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/photos/"},"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/dynamodb/introduction-dynamodb/":{title:"Introduction to DynamoDB",tags:["aws","dynamodb"],content:`Lab
Creating a DynamoDB Table with a Partition Key 1. From the AWS Management Console, in the search bar at the top, enter DynamoDB, and under Services, click the DynamoDB result:
The Amazon DynamoDB product overview page will load.
2. To start creating a new DyanmoDB table, on the right-hand side, click Create table:
3. In the Table details section, enter the following:
Table Name: Partition Key: Enter Name and ensure type is 4. In the Settings section, select Customize settings:
Choosing this option allows you to specify values for the table\u0026rsquo;s read and write capacities.
5. In the Read/write capacity settings section, under Capacity mode, select Provisioned and enter the following:
Read Capacity: Provisioned capacity units: Write Capacity: Provisioned capacity units: Accept the defaults for all other options on this page.
6. Scroll to the bottom and click Create table:
The Tables list view will load and you will see a notification that your table is being created. After a 30 seconds or so, you will see a success notification:
Creating a DynamoDB Table with Local and Global Secondary Indexes 1. On the right-hand side of the page, click Create table:
2. Enter the following in the Table details section:
Table name: Partition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select 3. In the Settings section, select Customize settings.
4. Under Read/write capacity settings, ensure Provisioned is selected for Capacity mode, and enter the following:
Read capacity: Provisioned capacity units: Write capacity: Provisioned capacity units: 5. Scroll down to the Secondary indexes section and click Create local index:
The New local secondary index dialog box will appear.
6. Enter the following to configure your local secondary index:
Sort Key: Name: Enter Type: Select Attribute projections: Select An LSI (Local Secondary Index) has the same partition key as the table\u0026rsquo;s primary key and will share the provisioned capacity of the table in contrast to global secondary indexes which provision their own capacity.
7. To finish creating the local secondary index, at the bottom, click Create index:
8. Scroll to the bottom and click Create table.
After roughly 30 seconds you will the table become active:
In contrast to a Local Secondary Index, a Global Secondary Index is an index with a partition and sort key that can be different from those in the table. It is considered \u0026ldquo;global\u0026rdquo; because queries on the index can span all of the data in a table, across all partitions.
9. Click Create table once more to start creating another table.
10. Enter the following in the Table details section:
Table Name: Partition key: Name: Enter Type_: _Select Sort key: Name: Enter Type: Select 11. In the Settings section, select Customize settings.
12. In the Read/write capacity settings section, ensure the Capacity mode is Provisioned, and enter the following:
Read capacity: Provisioned capacity units: Enter Write capacity: Provisioned capacity units: Enter 13. Scroll down to the Secondary indexes section, and click Create global index:
The New global secondary index dialog form will appear.
14. Enter the following:
Partition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 15. To finish creating the global secondary index, at the bottom, click Create index.
16. Click Create global index again and enter the following:
Partition key: Name: Enter Type: Select Sort key: Name: Enter Type: Select Attribute projections: Select 17. To finish creating the global secondary index, at the bottom, click Create index.
18. Scroll to the bottom and click Create table.
Once again, you will see your table created after roughly 30 seconds.
Inserting Items Into a DynamoDB Table 1. In the left-hand menu, click Explore items:
2. In the Tables list, select You will see nothing under Items returned because there are no items stored.
3. On the right-hand side, click Create item:
The Create item form will load and you will see a list of Attributes.
4. In the Value textbox next to Name - Partition key, enter a name for your forum (can be anything you wish):
5. To add another attribute for this item, click Add new attribute and select String from the list of types:
6. In the Attribute name textbox, enter Description and in the Value textbox, enter any value you\u0026rsquo;d like:
7. At the bottom, click Create item:
8. Repeat steps 3-7 three more times so that end up with four entries in the table:
9. Select the table and click Create Item.
10. Provide any values you\u0026rsquo;d like for , and , keeping in mind that the value must match the name of one of your forums.
Note: is a \u0026quot; \u0026quot; table with the Local Secondary Index. For being able to save a item, you have to provide:
(the table Primary Key) (the table Sort Key) (the Local Secondary Index Sort Key) Note: You will have to click Add new attribute to add the CreationDate attribute and specify a value.
11. At the bottom, click Create item.
12. Repeat steps 9-11 three more times until you have four items in the table:
Editing DynamoDB Table Items 1. On the Explore items page, select the table:
2. Select any item in the table and click on its name to get to the Item editor page:
3. Click inside any value and make an update to its contents:
Warning: Note that modifying the partition key will result in changing the values of the item keys. This will delete and recreate the item with new keys.
4. At the bottom of the page, click Save changes:
Querying a DynamoDB Table 1. In the left-hand menu, click PartiQL editor:
The PartiQL editor page will load.
PartiQL is a SQL (Structured Query Language) compatible language for Amazon DynamoDB. As well as querying tables, you can use it to insert new items and update existing ones.
2. Under Tables, click the three dots next to the and click Scan table:
The Query 1 editor will be populated with a PartiQL query that selects all items from the .
3. To execute the PartiQL table, under the editor, click Run:
4. Scroll down to see the results under Items returned:
Notice that you have a choice of viewing the results in tabular form or in JSON (Java Script Object Notation):
5. To query for a specific item, replace the contents of the Query 1 editor with the following, and click Run:
1SELECT * FROM \u0026#34;Thread\u0026#34; WHERE \u0026#34;Subject\u0026#34; = \u0026#39;Intro to cool stuff\u0026#39; This time, you will only see items returned that satisfy the value of the WHERE condition.
Note: Change the value of the WHERE condition to match an item you created if you don\u0026rsquo;t see a result.
PartiQL supports most standard features of SQL which means you can query, select, and sort your data in sophisticated ways.
Typically, using the Amazon DynamoDB Console to query items is useful for one-off reports and debugging or troubleshooting. Like most databases, DynamoDB can be accessed programmatically by other systems and software applications through either the AWS SDK (software development kit) or DyanmoDB\u0026rsquo;s HTTP API (application programming interface).
You can learn more about using PartiQL with Amazon DynamoDB by visiting the Working with PartiQL Query Language section of the Amazon DynamoDB developer guide.
Deleting a DynamoDB Table 1. In the left-hand menu, click Tables:
2. In the Tables table, select the Thread table:
3. On the right-hand side, click Delete:
The Delete table confirmation modal will appear.
Notice that you have the ability to create a backup for a table before deleting it.
4. In the confirmation textbox, enter delete and click Delete table:
You will see a message summarizing the deletion:
5. To continue, click Go to tables:
6. To update the Tables table, click the refresh icon:
You will now see only two tables listed.
`,url:"https://romankurnovskii.com/en/docs/aws-certified-developer-associate/dynamodb/introduction-dynamodb/"},"https://romankurnovskii.com/en/tags/cheatsheet/":{title:"cheatsheet",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cheatsheet/"},"https://romankurnovskii.com/en/posts/python-snippets/":{title:"Python Cheat Sheet",tags:["Python","CheatSheet"],content:" 1# Single line comments start with a number symbol. 2 3\u0026#34;\u0026#34;\u0026#34; Multiline strings can be written 4 using three \u0026#34;s, and are often used 5 as documentation. 6\u0026#34;\u0026#34;\u0026#34; 7 8#################################################### 9## 1. Primitive Datatypes and Operators 10#################################################### 11 12# You have numbers 133 # =\u0026gt; 3 14 15# Math is what you would expect 161 + 1 # =\u0026gt; 2 178 - 1 # =\u0026gt; 7 1810 * 2 # =\u0026gt; 20 1935 / 5 # =\u0026gt; 7.0 20 21# Integer division rounds down for both positive and negative numbers. 225 // 3 # =\u0026gt; 1 23-5 // 3 # =\u0026gt; -2 245.0 // 3.0 # =\u0026gt; 1.0 # works on floats too 25-5.0 // 3.0 # =\u0026gt; -2.0 26 27# The result of division is always a float 2810.0 / 3 # =\u0026gt; 3.3333333333333335 29 30# Modulo operation 317 % 3 # =\u0026gt; 1 32# i % j have the same sign as j, unlike C 33-7 % 3 # =\u0026gt; 2 34 35# Exponentiation (x**y, x to the yth power) 362**3 # =\u0026gt; 8 37 38# Enforce precedence with parentheses 391 + 3 * 2 # =\u0026gt; 7 40(1 + 3) * 2 # =\u0026gt; 8 41 42# Boolean values are primitives (Note: the capitalization) 43True # =\u0026gt; True 44False # =\u0026gt; False 45 46# negate with not 47not True # =\u0026gt; False 48not False # =\u0026gt; True 49 50# Boolean Operators 51# Note \u0026#34;and\u0026#34; and \u0026#34;or\u0026#34; are case-sensitive 52True and False # =\u0026gt; False 53False or True # =\u0026gt; True 54 55# True and False are actually 1 and 0 but with different keywords 56True + True # =\u0026gt; 2 57True * 8 # =\u0026gt; 8 58False - 5 # =\u0026gt; -5 59 60# Comparison operators look at the numerical value of True and False 610 == False # =\u0026gt; True 621 == True # =\u0026gt; True 632 == True # =\u0026gt; False 64-5 != False # =\u0026gt; True 65 66# None, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False. 67# All other values are True 68bool(0) # =\u0026gt; False 69bool(\u0026#34;\u0026#34;) # =\u0026gt; False 70bool([]) # =\u0026gt; False 71bool({}) # =\u0026gt; False 72bool(()) # =\u0026gt; False 73bool(set()) # =\u0026gt; False 74bool(4) # =\u0026gt; True 75bool(-6) # =\u0026gt; True 76 77# Using boolean logical operators on ints casts them to booleans for evaluation, but their non-cast value is returned 78# Don\u0026#39;t mix up with bool(ints) and bitwise and/or (\u0026amp;,|) 79bool(0) # =\u0026gt; False 80bool(2) # =\u0026gt; True 810 and 2 # =\u0026gt; 0 82bool(-5) # =\u0026gt; True 83bool(2) # =\u0026gt; True 84-5 or 0 # =\u0026gt; -5 85 86# Equality is == 871 == 1 # =\u0026gt; True 882 == 1 # =\u0026gt; False 89 90# Inequality is != 911 != 1 # =\u0026gt; False 922 != 1 # =\u0026gt; True 93 94# More comparisons 951 \u0026lt; 10 # =\u0026gt; True 961 \u0026gt; 10 # =\u0026gt; False 972 \u0026lt;= 2 # =\u0026gt; True 982 \u0026gt;= 2 # =\u0026gt; True 99 100# Seeing whether a value is in a range 1011 \u0026lt; 2 and 2 \u0026lt; 3 # =\u0026gt; True 1022 \u0026lt; 3 and 3 \u0026lt; 2 # =\u0026gt; False 103# Chaining makes this look nicer 1041 \u0026lt; 2 \u0026lt; 3 # =\u0026gt; True 1052 \u0026lt; 3 \u0026lt; 2 # =\u0026gt; False 106 107# (is vs. ==) is checks if two variables refer to the same object, but == checks 108# if the objects pointed to have the same values. 109a = [1, 2, 3, 4] # Point a at a new list, [1, 2, 3, 4] 110b = a # Point b at what a is pointing to 111b is a # =\u0026gt; True, a and b refer to the same object 112b == a # =\u0026gt; True, a\u0026#39;s and b\u0026#39;s objects are equal 113b = [1, 2, 3, 4] # Point b at a new list, [1, 2, 3, 4] 114b is a # =\u0026gt; False, a and b do not refer to the same object 115b == a # =\u0026gt; True, a\u0026#39;s and b\u0026#39;s objects are equal 116 117# Strings are created with \u0026#34; or \u0026#39; 118\u0026#34;This is a string.\u0026#34; 119\u0026#39;This is also a string.\u0026#39; 120 121# Strings can be added too 122\u0026#34;Hello \u0026#34; + \u0026#34;world!\u0026#34; # =\u0026gt; \u0026#34;Hello world!\u0026#34; 123# String literals (but not variables) can be concatenated without using \u0026#39;+\u0026#39; 124\u0026#34;Hello \u0026#34; \u0026#34;world!\u0026#34; # =\u0026gt; \u0026#34;Hello world!\u0026#34; 125 126# A string can be treated like a list of characters 127\u0026#34;Hello world!\u0026#34;[0] # =\u0026gt; \u0026#39;H\u0026#39; 128 129# You can find the length of a string 130len(\u0026#34;This is a string\u0026#34;) # =\u0026gt; 16 131 132# You can also format using f-strings or formatted string literals (in Python 3.6+) 133name = \u0026#34;Reiko\u0026#34; 134f\u0026#34;She said her name is {name}.\u0026#34; # =\u0026gt; \u0026#34;She said her name is Reiko\u0026#34; 135# You can basically put any Python expression inside the braces and it will be output in the string. 136f\u0026#34;{name} is {len(name)} characters long.\u0026#34; # =\u0026gt; \u0026#34;Reiko is 5 characters long.\u0026#34; 137 138# None is an object 139None # =\u0026gt; None 140 141# Don\u0026#39;t use the equality \u0026#34;==\u0026#34; symbol to compare objects to None 142# Use \u0026#34;is\u0026#34; instead. This checks for equality of object identity. 143\u0026#34;etc\u0026#34; is None # =\u0026gt; False 144None is None # =\u0026gt; True 145 146# None, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False. 147# All other values are True 148bool(0) # =\u0026gt; False 149bool(\u0026#34;\u0026#34;) # =\u0026gt; False 150bool([]) # =\u0026gt; False 151bool({}) # =\u0026gt; False 152bool(()) # =\u0026gt; False 153bool(set()) # =\u0026gt; False 154 155#################################################### 156## 2. Variables and Collections 157#################################################### 158 159# Python has a print function 160print(\u0026#34;I\u0026#39;m Python. Nice to meet you!\u0026#34;) # =\u0026gt; I\u0026#39;m Python. Nice to meet you! 161 162# By default the print function also prints out a newline at the end. 163# Use the optional argument end to change the end string. 164print(\u0026#34;Hello, World\u0026#34;, end=\u0026#34;!\u0026#34;) # =\u0026gt; Hello, World! 165 166# Simple way to get input data from console 167input_string_var = input(\u0026#34;Enter some data: \u0026#34;) # Returns the data as a string 168 169# There are no declarations, only assignments. 170# Convention is to use lower_case_with_underscores 171some_var = 5 172some_var # =\u0026gt; 5 173 174# Accessing a previously unassigned variable is an exception. 175# See Control Flow to learn more about exception handling. 176some_unknown_var # Raises a NameError 177 178# if can be used as an expression 179# Equivalent of C\u0026#39;s \u0026#39;?:\u0026#39; ternary operator 180\u0026#34;yay!\u0026#34; if 0 \u0026gt; 1 else \u0026#34;nay!\u0026#34; # =\u0026gt; \u0026#34;nay!\u0026#34; 181 182# Lists store sequences 183li = [] 184# You can start with a prefilled list 185other_li = [4, 5, 6] 186 187# Add stuff to the end of a list with append 188li.append(1) # li is now [1] 189li.append(2) # li is now [1, 2] 190li.append(4) # li is now [1, 2, 4] 191li.append(3) # li is now [1, 2, 4, 3] 192# Remove from the end with pop 193li.pop() # =\u0026gt; 3 and li is now [1, 2, 4] 194# Let\u0026#39;s put it back 195li.append(3) # li is now [1, 2, 4, 3] again. 196 197# Access a list like you would any array 198li[0] # =\u0026gt; 1 199# Look at the last element 200li[-1] # =\u0026gt; 3 201 202# Looking out of bounds is an IndexError 203li[4] # Raises an IndexError 204 205# You can look at ranges with slice syntax. 206# The start index is included, the end index is not 207# (It\u0026#39;s a closed/open range for you mathy types.) 208li[1:3] # Return list from index 1 to 3 =\u0026gt; [2, 4] 209li[2:] # Return list starting from index 2 =\u0026gt; [4, 3] 210li[:3] # Return list from beginning until index 3 =\u0026gt; [1, 2, 4] 211li[::2] # Return list selecting every second entry =\u0026gt; [1, 4] 212li[::-1] # Return list in reverse order =\u0026gt; [3, 4, 2, 1] 213# Use any combination of these to make advanced slices 214# li[start🔚step] 215 216# Make a one layer deep copy using slices 217li2 = li[:] # =\u0026gt; li2 = [1, 2, 4, 3] but (li2 is li) will result in false. 218 219# Remove arbitrary elements from a list with \u0026#34;del\u0026#34; 220del li[2] # li is now [1, 2, 3] 221 222# Remove first occurrence of a value 223li.remove(2) # li is now [1, 3] 224li.remove(2) # Raises a ValueError as 2 is not in the list 225 226# Insert an element at a specific index 227li.insert(1, 2) # li is now [1, 2, 3] again 228 229# Get the index of the first item found matching the argument 230li.index(2) # =\u0026gt; 1 231li.index(4) # Raises a ValueError as 4 is not in the list 232 233# You can add lists 234# Note: values for li and for other_li are not modified. 235li + other_li # =\u0026gt; [1, 2, 3, 4, 5, 6] 236 237# Concatenate lists with \u0026#34;extend()\u0026#34; 238li.extend(other_li) # Now li is [1, 2, 3, 4, 5, 6] 239 240# Check for existence in a list with \u0026#34;in\u0026#34; 2411 in li # =\u0026gt; True 242 243# Examine the length with \u0026#34;len()\u0026#34; 244len(li) # =\u0026gt; 6 245 246 247# Tuples are like lists but are immutable. 248tup = (1, 2, 3) 249tup[0] # =\u0026gt; 1 250tup[0] = 3 # Raises a TypeError 251 252# Note that a tuple of length one has to have a comma after the last element but 253# tuples of other lengths, even zero, do not. 254type((1)) # =\u0026gt; \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; 255type((1,)) # =\u0026gt; \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; 256type(()) # =\u0026gt; \u0026lt;class \u0026#39;tuple\u0026#39;\u0026gt; 257 258# You can do most of the list operations on tuples too 259len(tup) # =\u0026gt; 3 260tup + (4, 5, 6) # =\u0026gt; (1, 2, 3, 4, 5, 6) 261tup[:2] # =\u0026gt; (1, 2) 2622 in tup # =\u0026gt; True 263 264# You can unpack tuples (or lists) into variables 265a, b, c = (1, 2, 3) # a is now 1, b is now 2 and c is now 3 266# You can also do extended unpacking 267a, *b, c = (1, 2, 3, 4) # a is now 1, b is now [2, 3] and c is now 4 268# Tuples are created by default if you leave out the parentheses 269d, e, f = 4, 5, 6 # tuple 4, 5, 6 is unpacked into variables d, e and f 270# respectively such that d = 4, e = 5 and f = 6 271# Now look how easy it is to swap two values 272e, d = d, e # d is now 5 and e is now 4 273 274 275# Dictionaries store mappings from keys to values 276empty_dict = {} 277# Here is a prefilled dictionary 278filled_dict = {\u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2, \u0026#34;three\u0026#34;: 3} 279 280# Note keys for dictionaries have to be immutable types. This is to ensure that 281# the key can be converted to a constant hash value for quick look-ups. 282# Immutable types include ints, floats, strings, tuples. 283invalid_dict = {[1,2,3]: \u0026#34;123\u0026#34;} # =\u0026gt; Raises a TypeError: unhashable type: \u0026#39;list\u0026#39; 284valid_dict = {(1,2,3):[1,2,3]} # Values can be of any type, however. 285 286# Look up values with [] 287filled_dict[\u0026#34;one\u0026#34;] # =\u0026gt; 1 288 289# Get all keys as an iterable with \u0026#34;keys()\u0026#34;. We need to wrap the call in list() 290# to turn it into a list. We\u0026#39;ll talk about those later. Note - for Python 291# versions \u0026lt;3.7, dictionary key ordering is not guaranteed. Your results might 292# not match the example below exactly. However, as of Python 3.7, dictionary 293# items maintain the order at which they are inserted into the dictionary. 294list(filled_dict.keys()) # =\u0026gt; [\u0026#34;three\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;one\u0026#34;] in Python \u0026lt;3.7 295list(filled_dict.keys()) # =\u0026gt; [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] in Python 3.7+ 296 297 298# Get all values as an iterable with \u0026#34;values()\u0026#34;. Once again we need to wrap it 299# in list() to get it out of the iterable. Note - Same as above regarding key 300# ordering. 301list(filled_dict.values()) # =\u0026gt; [3, 2, 1] in Python \u0026lt;3.7 302list(filled_dict.values()) # =\u0026gt; [1, 2, 3] in Python 3.7+ 303 304# Check for existence of keys in a dictionary with \u0026#34;in\u0026#34; 305\u0026#34;one\u0026#34; in filled_dict # =\u0026gt; True 3061 in filled_dict # =\u0026gt; False 307 308# Looking up a non-existing key is a KeyError 309filled_dict[\u0026#34;four\u0026#34;] # KeyError 310 311# Use \u0026#34;get()\u0026#34; method to avoid the KeyError 312filled_dict.get(\u0026#34;one\u0026#34;) # =\u0026gt; 1 313filled_dict.get(\u0026#34;four\u0026#34;) # =\u0026gt; None 314# The get method supports a default argument when the value is missing 315filled_dict.get(\u0026#34;one\u0026#34;, 4) # =\u0026gt; 1 316filled_dict.get(\u0026#34;four\u0026#34;, 4) # =\u0026gt; 4 317 318# \u0026#34;setdefault()\u0026#34; inserts into a dictionary only if the given key isn\u0026#39;t present 319filled_dict.setdefault(\u0026#34;five\u0026#34;, 5) # filled_dict[\u0026#34;five\u0026#34;] is set to 5 320filled_dict.setdefault(\u0026#34;five\u0026#34;, 6) # filled_dict[\u0026#34;five\u0026#34;] is still 5 321 322# Adding to a dictionary 323filled_dict.update({\u0026#34;four\u0026#34;:4}) # =\u0026gt; {\u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2, \u0026#34;three\u0026#34;: 3, \u0026#34;four\u0026#34;: 4} 324filled_dict[\u0026#34;four\u0026#34;] = 4 # another way to add to dict 325 326# Remove keys from a dictionary with del 327del filled_dict[\u0026#34;one\u0026#34;] # Removes the key \u0026#34;one\u0026#34; from filled dict 328 329# From Python 3.5 you can also use the additional unpacking options 330{\u0026#39;a\u0026#39;: 1, **{\u0026#39;b\u0026#39;: 2}} # =\u0026gt; {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2} 331{\u0026#39;a\u0026#39;: 1, **{\u0026#39;a\u0026#39;: 2}} # =\u0026gt; {\u0026#39;a\u0026#39;: 2} 332 333 334 335# Sets store ... well sets 336empty_set = set() 337# Initialize a set with a bunch of values. Yeah, it looks a bit like a dict. Sorry. 338some_set = {1, 1, 2, 2, 3, 4} # some_set is now {1, 2, 3, 4} 339 340# Similar to keys of a dictionary, elements of a set have to be immutable. 341invalid_set = {[1], 1} # =\u0026gt; Raises a TypeError: unhashable type: \u0026#39;list\u0026#39; 342valid_set = {(1,), 1} 343 344# Add one more item to the set 345filled_set = some_set 346filled_set.add(5) # filled_set is now {1, 2, 3, 4, 5} 347# Sets do not have duplicate elements 348filled_set.add(5) # it remains as before {1, 2, 3, 4, 5} 349 350# Do set intersection with \u0026amp; 351other_set = {3, 4, 5, 6} 352filled_set \u0026amp; other_set # =\u0026gt; {3, 4, 5} 353 354# Do set union with | 355filled_set | other_set # =\u0026gt; {1, 2, 3, 4, 5, 6} 356 357# Do set difference with - 358{1, 2, 3, 4} - {2, 3, 5} # =\u0026gt; {1, 4} 359 360# Do set symmetric difference with ^ 361{1, 2, 3, 4} ^ {2, 3, 5} # =\u0026gt; {1, 4, 5} 362 363# Check if set on the left is a superset of set on the right 364{1, 2} \u0026gt;= {1, 2, 3} # =\u0026gt; False 365 366# Check if set on the left is a subset of set on the right 367{1, 2} \u0026lt;= {1, 2, 3} # =\u0026gt; True 368 369# Check for existence in a set with in 3702 in filled_set # =\u0026gt; True 37110 in filled_set # =\u0026gt; False 372 373# Make a one layer deep copy 374filled_set = some_set.copy() # filled_set is {1, 2, 3, 4, 5} 375filled_set is some_set # =\u0026gt; False 376 377 378#################################################### 379## 3. Control Flow and Iterables 380#################################################### 381 382# Let\u0026#39;s just make a variable 383some_var = 5 384 385# Here is an if statement. Indentation is significant in Python! 386# Convention is to use four spaces, not tabs. 387# This prints \u0026#34;some_var is smaller than 10\u0026#34; 388if some_var \u0026gt; 10: 389 print(\u0026#34;some_var is totally bigger than 10.\u0026#34;) 390elif some_var \u0026lt; 10: # This elif clause is optional. 391 print(\u0026#34;some_var is smaller than 10.\u0026#34;) 392else: # This is optional too. 393 print(\u0026#34;some_var is indeed 10.\u0026#34;) 394 395 396\u0026#34;\u0026#34;\u0026#34; 397For loops iterate over lists 398prints: 399 dog is a mammal 400 cat is a mammal 401 mouse is a mammal 402\u0026#34;\u0026#34;\u0026#34; 403for animal in [\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;mouse\u0026#34;]: 404 # You can use format() to interpolate formatted strings 405 print(\u0026#34;{} is a mammal\u0026#34;.format(animal)) 406 407\u0026#34;\u0026#34;\u0026#34; 408\u0026#34;range(number)\u0026#34; returns an iterable of numbers 409from zero to the given number 410prints: 411 0 412 1 413 2 414 3 415\u0026#34;\u0026#34;\u0026#34; 416for i in range(4): 417 print(i) 418 419\u0026#34;\u0026#34;\u0026#34; 420\u0026#34;range(lower, upper)\u0026#34; returns an iterable of numbers 421from the lower number to the upper number 422prints: 423 4 424 5 425 6 426 7 427\u0026#34;\u0026#34;\u0026#34; 428for i in range(4, 8): 429 print(i) 430 431\u0026#34;\u0026#34;\u0026#34; 432\u0026#34;range(lower, upper, step)\u0026#34; returns an iterable of numbers 433from the lower number to the upper number, while incrementing 434by step. If step is not indicated, the default value is 1. 435prints: 436 4 437 6 438\u0026#34;\u0026#34;\u0026#34; 439for i in range(4, 8, 2): 440 print(i) 441 442\u0026#34;\u0026#34;\u0026#34; 443To loop over a list, and retrieve both the index and the value of each item in the list 444prints: 445 0 dog 446 1 cat 447 2 mouse 448\u0026#34;\u0026#34;\u0026#34; 449animals = [\u0026#34;dog\u0026#34;, \u0026#34;cat\u0026#34;, \u0026#34;mouse\u0026#34;] 450for i, value in enumerate(animals): 451 print(i, value) 452 453\u0026#34;\u0026#34;\u0026#34; 454While loops go until a condition is no longer met. 455prints: 456 0 457 1 458 2 459 3 460\u0026#34;\u0026#34;\u0026#34; 461x = 0 462while x \u0026lt; 4: 463 print(x) 464 x += 1 # Shorthand for x = x + 1 465 466# Handle exceptions with a try/except block 467try: 468 # Use \u0026#34;raise\u0026#34; to raise an error 469 raise IndexError(\u0026#34;This is an index error\u0026#34;) 470except IndexError as e: 471 pass # Pass is just a no-op. Usually you would do recovery here. 472except (TypeError, NameError): 473 pass # Multiple exceptions can be handled together, if required. 474else: # Optional clause to the try/except block. Must follow all except blocks 475 print(\u0026#34;All good!\u0026#34;) # Runs only if the code in try raises no exceptions 476finally: # Execute under all circumstances 477 print(\u0026#34;We can clean up resources here\u0026#34;) 478 479# Instead of try/finally to cleanup resources you can use a with statement 480with open(\u0026#34;myfile.txt\u0026#34;) as f: 481 for line in f: 482 print(line) 483 484# Writing to a file 485contents = {\u0026#34;aa\u0026#34;: 12, \u0026#34;bb\u0026#34;: 21} 486with open(\u0026#34;myfile1.txt\u0026#34;, \u0026#34;w+\u0026#34;) as file: 487 file.write(str(contents)) # writes a string to a file 488 489with open(\u0026#34;myfile2.txt\u0026#34;, \u0026#34;w+\u0026#34;) as file: 490 file.write(json.dumps(contents)) # writes an object to a file 491 492# Reading from a file 493with open(\u0026#39;myfile1.txt\u0026#39;, \u0026#34;r+\u0026#34;) as file: 494 contents = file.read() # reads a string from a file 495print(contents) 496# print: {\u0026#34;aa\u0026#34;: 12, \u0026#34;bb\u0026#34;: 21} 497 498with open(\u0026#39;myfile2.txt\u0026#39;, \u0026#34;r+\u0026#34;) as file: 499 contents = json.load(file) # reads a json object from a file 500print(contents) 501# print: {\u0026#34;aa\u0026#34;: 12, \u0026#34;bb\u0026#34;: 21} 502 503 504# Python offers a fundamental abstraction called the Iterable. 505# An iterable is an object that can be treated as a sequence. 506# The object returned by the range function, is an iterable. 507 508filled_dict = {\u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2, \u0026#34;three\u0026#34;: 3} 509our_iterable = filled_dict.keys() 510print(our_iterable) # =\u0026gt; dict_keys([\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;, \u0026#39;three\u0026#39;]). This is an object that implements our Iterable interface. 511 512# We can loop over it. 513for i in our_iterable: 514 print(i) # Prints one, two, three 515 516# However we cannot address elements by index. 517our_iterable[1] # Raises a TypeError 518 519# An iterable is an object that knows how to create an iterator. 520our_iterator = iter(our_iterable) 521 522# Our iterator is an object that can remember the state as we traverse through it. 523# We get the next object with \u0026#34;next()\u0026#34;. 524next(our_iterator) # =\u0026gt; \u0026#34;one\u0026#34; 525 526# It maintains state as we iterate. 527next(our_iterator) # =\u0026gt; \u0026#34;two\u0026#34; 528next(our_iterator) # =\u0026gt; \u0026#34;three\u0026#34; 529 530# After the iterator has returned all of its data, it raises a StopIteration exception 531next(our_iterator) # Raises StopIteration 532 533# We can also loop over it, in fact, \u0026#34;for\u0026#34; does this implicitly! 534our_iterator = iter(our_iterable) 535for i in our_iterator: 536 print(i) # Prints one, two, three 537 538# You can grab all the elements of an iterable or iterator by calling list() on it. 539list(our_iterable) # =\u0026gt; Returns [\u0026#34;one\u0026#34;, \u0026#34;two\u0026#34;, \u0026#34;three\u0026#34;] 540list(our_iterator) # =\u0026gt; Returns [] because state is saved 541 542 543#################################################### 544## 4. Functions 545#################################################### 546 547# Use \u0026#34;def\u0026#34; to create new functions 548def add(x, y): 549 print(\u0026#34;x is {} and y is {}\u0026#34;.format(x, y)) 550 return x + y # Return values with a return statement 551 552# Calling functions with parameters 553add(5, 6) # =\u0026gt; prints out \u0026#34;x is 5 and y is 6\u0026#34; and returns 11 554 555# Another way to call functions is with keyword arguments 556add(y=6, x=5) # Keyword arguments can arrive in any order. 557 558# You can define functions that take a variable number of 559# positional arguments 560def varargs(*args): 561 return args 562 563varargs(1, 2, 3) # =\u0026gt; (1, 2, 3) 564 565# You can define functions that take a variable number of 566# keyword arguments, as well 567def keyword_args(**kwargs): 568 return kwargs 569 570# Let\u0026#39;s call it to see what happens 571keyword_args(big=\u0026#34;foot\u0026#34;, loch=\u0026#34;ness\u0026#34;) # =\u0026gt; {\u0026#34;big\u0026#34;: \u0026#34;foot\u0026#34;, \u0026#34;loch\u0026#34;: \u0026#34;ness\u0026#34;} 572 573 574# You can do both at once, if you like 575def all_the_args(*args, **kwargs): 576 print(args) 577 print(kwargs) 578\u0026#34;\u0026#34;\u0026#34; 579all_the_args(1, 2, a=3, b=4) prints: 580 (1, 2) 581 {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 4} 582\u0026#34;\u0026#34;\u0026#34; 583 584# When calling functions, you can do the opposite of args/kwargs! 585# Use * to expand tuples and use ** to expand kwargs. 586args = (1, 2, 3, 4) 587kwargs = {\u0026#34;a\u0026#34;: 3, \u0026#34;b\u0026#34;: 4} 588all_the_args(*args) # equivalent to all_the_args(1, 2, 3, 4) 589all_the_args(**kwargs) # equivalent to all_the_args(a=3, b=4) 590all_the_args(*args, **kwargs) # equivalent to all_the_args(1, 2, 3, 4, a=3, b=4) 591 592# Returning multiple values (with tuple assignments) 593def swap(x, y): 594 return y, x # Return multiple values as a tuple without the parenthesis. 595 # (Note: parenthesis have been excluded but can be included) 596 597x = 1 598y = 2 599x, y = swap(x, y) # =\u0026gt; x = 2, y = 1 600# (x, y) = swap(x,y) # Again parenthesis have been excluded but can be included. 601 602# Function Scope 603x = 5 604 605def set_x(num): 606 # Local var x not the same as global variable x 607 x = num # =\u0026gt; 43 608 print(x) # =\u0026gt; 43 609 610def set_global_x(num): 611 global x 612 print(x) # =\u0026gt; 5 613 x = num # global var x is now set to 6 614 print(x) # =\u0026gt; 6 615 616set_x(43) 617set_global_x(6) 618 619 620# Python has first class functions 621def create_adder(x): 622 def adder(y): 623 return x + y 624 return adder 625 626add_10 = create_adder(10) 627add_10(3) # =\u0026gt; 13 628 629# There are also anonymous functions 630(lambda x: x \u0026gt; 2)(3) # =\u0026gt; True 631(lambda x, y: x ** 2 + y ** 2)(2, 1) # =\u0026gt; 5 632 633# There are built-in higher order functions 634list(map(add_10, [1, 2, 3])) # =\u0026gt; [11, 12, 13] 635list(map(max, [1, 2, 3], [4, 2, 1])) # =\u0026gt; [4, 2, 3] 636 637list(filter(lambda x: x \u0026gt; 5, [3, 4, 5, 6, 7])) # =\u0026gt; [6, 7] 638 639# We can use list comprehensions for nice maps and filters 640# List comprehension stores the output as a list which can itself be a nested list 641[add_10(i) for i in [1, 2, 3]] # =\u0026gt; [11, 12, 13] 642[x for x in [3, 4, 5, 6, 7] if x \u0026gt; 5] # =\u0026gt; [6, 7] 643 644# You can construct set and dict comprehensions as well. 645{x for x in \u0026#39;abcddeef\u0026#39; if x not in \u0026#39;abc\u0026#39;} # =\u0026gt; {\u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;} 646{x: x**2 for x in range(5)} # =\u0026gt; {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} 647 648 649#################################################### 650## 5. Modules 651#################################################### 652 653# You can import modules 654import math 655print(math.sqrt(16)) # =\u0026gt; 4.0 656 657# You can get specific functions from a module 658from math import ceil, floor 659print(ceil(3.7)) # =\u0026gt; 4.0 660print(floor(3.7)) # =\u0026gt; 3.0 661 662# You can import all functions from a module. 663# Warning: this is not recommended 664from math import * 665 666# You can shorten module names 667import math as m 668math.sqrt(16) == m.sqrt(16) # =\u0026gt; True 669 670# Python modules are just ordinary Python files. You 671# can write your own, and import them. The name of the 672# module is the same as the name of the file. 673 674# You can find out which functions and attributes 675# are defined in a module. 676import math 677dir(math) 678 679# If you have a Python script named math.py in the same 680# folder as your current script, the file math.py will 681# be loaded instead of the built-in Python module. 682# This happens because the local folder has priority 683# over Python\u0026#39;s built-in libraries. 684 685 686#################################################### 687## 6. Classes 688#################################################### 689 690# We use the \u0026#34;class\u0026#34; statement to create a class 691class Human: 692 693 # A class attribute. It is shared by all instances of this class 694 species = \u0026#34;H. sapiens\u0026#34; 695 696 # Basic initializer, this is called when this class is instantiated. 697 # Note that the double leading and trailing underscores denote objects 698 # or attributes that are used by Python but that live in user-controlled 699 # namespaces. Methods(or objects or attributes) like: __init__, __str__, 700 # __repr__ etc. are called special methods (or sometimes called dunder methods) 701 # You should not invent such names on your own. 702 def __init__(self, name): 703 # Assign the argument to the instance\u0026#39;s name attribute 704 self.name = name 705 706 # Initialize property 707 self._age = 0 708 709 # An instance method. All methods take \u0026#34;self\u0026#34; as the first argument 710 def say(self, msg): 711 print(\u0026#34;{name}: {message}\u0026#34;.format(name=self.name, message=msg)) 712 713 # Another instance method 714 def sing(self): 715 return \u0026#39;yo... yo... microphone check... one two... one two...\u0026#39; 716 717 # A class method is shared among all instances 718 # They are called with the calling class as the first argument 719 @classmethod 720 def get_species(cls): 721 return cls.species 722 723 # A static method is called without a class or instance reference 724 @staticmethod 725 def grunt(): 726 return \u0026#34;*grunt*\u0026#34; 727 728 # A property is just like a getter. 729 # It turns the method age() into a read-only attribute of the same name. 730 # There\u0026#39;s no need to write trivial getters and setters in Python, though. 731 @property 732 def age(self): 733 return self._age 734 735 # This allows the property to be set 736 @age.setter 737 def age(self, age): 738 self._age = age 739 740 # This allows the property to be deleted 741 @age.deleter 742 def age(self): 743 del self._age 744 745 746# When a Python interpreter reads a source file it executes all its code. 747# This __name__ check makes sure this code block is only executed when this 748# module is the main program. 749if __name__ == \u0026#39;__main__\u0026#39;: 750 # Instantiate a class 751 i = Human(name=\u0026#34;Ian\u0026#34;) 752 i.say(\u0026#34;hi\u0026#34;) # \u0026#34;Ian: hi\u0026#34; 753 j = Human(\u0026#34;Joel\u0026#34;) 754 j.say(\u0026#34;hello\u0026#34;) # \u0026#34;Joel: hello\u0026#34; 755 # i and j are instances of type Human, or in other words: they are Human objects 756 757 # Call our class method 758 i.say(i.get_species()) # \u0026#34;Ian: H. sapiens\u0026#34; 759 # Change the shared attribute 760 Human.species = \u0026#34;H. neanderthalensis\u0026#34; 761 i.say(i.get_species()) # =\u0026gt; \u0026#34;Ian: H. neanderthalensis\u0026#34; 762 j.say(j.get_species()) # =\u0026gt; \u0026#34;Joel: H. neanderthalensis\u0026#34; 763 764 # Call the static method 765 print(Human.grunt()) # =\u0026gt; \u0026#34;*grunt*\u0026#34; 766 767 # Static methods can be called by instances too 768 print(i.grunt()) # =\u0026gt; \u0026#34;*grunt*\u0026#34; 769 770 # Update the property for this instance 771 i.age = 42 772 # Get the property 773 i.say(i.age) # =\u0026gt; \u0026#34;Ian: 42\u0026#34; 774 j.say(j.age) # =\u0026gt; \u0026#34;Joel: 0\u0026#34; 775 # Delete the property 776 del i.age 777 # i.age # =\u0026gt; this would raise an AttributeError 778 779 780#################################################### 781## 6.1 Inheritance 782#################################################### 783 784# Inheritance allows new child classes to be defined that inherit methods and 785# variables from their parent class. 786 787# Using the Human class defined above as the base or parent class, we can 788# define a child class, Superhero, which inherits the class variables like 789# \u0026#34;species\u0026#34;, \u0026#34;name\u0026#34;, and \u0026#34;age\u0026#34;, as well as methods, like \u0026#34;sing\u0026#34; and \u0026#34;grunt\u0026#34; 790# from the Human class, but can also have its own unique properties. 791 792# To take advantage of modularization by file you could place the classes above in their own files, 793# say, human.py 794 795# To import functions from other files use the following format 796# from \u0026#34;filename-without-extension\u0026#34; import \u0026#34;function-or-class\u0026#34; 797 798from human import Human 799 800 801# Specify the parent class(es) as parameters to the class definition 802class Superhero(Human): 803 804 # If the child class should inherit all of the parent\u0026#39;s definitions without 805 # any modifications, you can just use the \u0026#34;pass\u0026#34; keyword (and nothing else) 806 # but in this case it is commented out to allow for a unique child class: 807 # pass 808 809 # Child classes can override their parents\u0026#39; attributes 810 species = \u0026#39;Superhuman\u0026#39; 811 812 # Children automatically inherit their parent class\u0026#39;s constructor including 813 # its arguments, but can also define additional arguments or definitions 814 # and override its methods such as the class constructor. 815 # This constructor inherits the \u0026#34;name\u0026#34; argument from the \u0026#34;Human\u0026#34; class and 816 # adds the \u0026#34;superpower\u0026#34; and \u0026#34;movie\u0026#34; arguments: 817 def __init__(self, name, movie=False, 818 superpowers=[\u0026#34;super strength\u0026#34;, \u0026#34;bulletproofing\u0026#34;]): 819 820 # add additional class attributes: 821 self.fictional = True 822 self.movie = movie 823 # be aware of mutable default values, since defaults are shared 824 self.superpowers = superpowers 825 826 # The \u0026#34;super\u0026#34; function lets you access the parent class\u0026#39;s methods 827 # that are overridden by the child, in this case, the __init__ method. 828 # This calls the parent class constructor: 829 super().__init__(name) 830 831 # override the sing method 832 def sing(self): 833 return \u0026#39;Dun, dun, DUN!\u0026#39; 834 835 # add an additional instance method 836 def boast(self): 837 for power in self.superpowers: 838 print(\u0026#34;I wield the power of {pow}!\u0026#34;.format(pow=power)) 839 840 841if __name__ == \u0026#39;__main__\u0026#39;: 842 sup = Superhero(name=\u0026#34;Tick\u0026#34;) 843 844 # Instance type checks 845 if isinstance(sup, Human): 846 print(\u0026#39;I am human\u0026#39;) 847 if type(sup) is Superhero: 848 print(\u0026#39;I am a superhero\u0026#39;) 849 850 # Get the Method Resolution search Order used by both getattr() and super() 851 # This attribute is dynamic and can be updated 852 print(Superhero.__mro__) # =\u0026gt; (\u0026lt;class \u0026#39;__main__.Superhero\u0026#39;\u0026gt;, 853 # =\u0026gt; \u0026lt;class \u0026#39;human.Human\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;) 854 855 # Calls parent method but uses its own class attribute 856 print(sup.get_species()) # =\u0026gt; Superhuman 857 858 # Calls overridden method 859 print(sup.sing()) # =\u0026gt; Dun, dun, DUN! 860 861 # Calls method from Human 862 sup.say(\u0026#39;Spoon\u0026#39;) # =\u0026gt; Tick: Spoon 863 864 # Call method that exists only in Superhero 865 sup.boast() # =\u0026gt; I wield the power of super strength! 866 # =\u0026gt; I wield the power of bulletproofing! 867 868 # Inherited class attribute 869 sup.age = 31 870 print(sup.age) # =\u0026gt; 31 871 872 # Attribute that only exists within Superhero 873 print(\u0026#39;Am I Oscar eligible? \u0026#39; + str(sup.movie)) 874 875#################################################### 876## 6.2 Multiple Inheritance 877#################################################### 878 879# Another class definition 880# bat.py 881class Bat: 882 883 species = \u0026#39;Baty\u0026#39; 884 885 def __init__(self, can_fly=True): 886 self.fly = can_fly 887 888 # This class also has a say method 889 def say(self, msg): 890 msg = \u0026#39;... ... ...\u0026#39; 891 return msg 892 893 # And its own method as well 894 def sonar(self): 895 return \u0026#39;))) ... (((\u0026#39; 896 897if __name__ == \u0026#39;__main__\u0026#39;: 898 b = Bat() 899 print(b.say(\u0026#39;hello\u0026#39;)) 900 print(b.fly) 901 902 903# And yet another class definition that inherits from Superhero and Bat 904# superhero.py 905from superhero import Superhero 906from bat import Bat 907 908# Define Batman as a child that inherits from both Superhero and Bat 909class Batman(Superhero, Bat): 910 911 def __init__(self, *args, **kwargs): 912 # Typically to inherit attributes you have to call super: 913 # super(Batman, self).__init__(*args, **kwargs) 914 # However we are dealing with multiple inheritance here, and super() 915 # only works with the next base class in the MRO list. 916 # So instead we explicitly call __init__ for all ancestors. 917 # The use of *args and **kwargs allows for a clean way to pass arguments, 918 # with each parent \u0026#34;peeling a layer of the onion\u0026#34;. 919 Superhero.__init__(self, \u0026#39;anonymous\u0026#39;, movie=True, 920 superpowers=[\u0026#39;Wealthy\u0026#39;], *args, **kwargs) 921 Bat.__init__(self, *args, can_fly=False, **kwargs) 922 # override the value for the name attribute 923 self.name = \u0026#39;Sad Affleck\u0026#39; 924 925 def sing(self): 926 return \u0026#39;nan nan nan nan nan batman!\u0026#39; 927 928 929if __name__ == \u0026#39;__main__\u0026#39;: 930 sup = Batman() 931 932 # Get the Method Resolution search Order used by both getattr() and super(). 933 # This attribute is dynamic and can be updated 934 print(Batman.__mro__) # =\u0026gt; (\u0026lt;class \u0026#39;__main__.Batman\u0026#39;\u0026gt;, 935 # =\u0026gt; \u0026lt;class \u0026#39;superhero.Superhero\u0026#39;\u0026gt;, 936 # =\u0026gt; \u0026lt;class \u0026#39;human.Human\u0026#39;\u0026gt;, 937 # =\u0026gt; \u0026lt;class \u0026#39;bat.Bat\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;) 938 939 # Calls parent method but uses its own class attribute 940 print(sup.get_species()) # =\u0026gt; Superhuman 941 942 # Calls overridden method 943 print(sup.sing()) # =\u0026gt; nan nan nan nan nan batman! 944 945 # Calls method from Human, because inheritance order matters 946 sup.say(\u0026#39;I agree\u0026#39;) # =\u0026gt; Sad Affleck: I agree 947 948 # Call method that exists only in 2nd ancestor 949 print(sup.sonar()) # =\u0026gt; ))) ... ((( 950 951 # Inherited class attribute 952 sup.age = 100 953 print(sup.age) # =\u0026gt; 100 954 955 # Inherited attribute from 2nd ancestor whose default value was overridden. 956 print(\u0026#39;Can I fly? \u0026#39; + str(sup.fly)) # =\u0026gt; Can I fly? False 957 958 959 960#################################################### 961## 7. Advanced 962#################################################### 963 964# Generators help you make lazy code. 965def double_numbers(iterable): 966 for i in iterable: 967 yield i + i 968 969# Generators are memory-efficient because they only load the data needed to 970# process the next value in the iterable. This allows them to perform 971# operations on otherwise prohibitively large value ranges. 972# NOTE: `range` replaces `xrange` in Python 3. 973for i in double_numbers(range(1, 900000000)): # `range` is a generator. 974 print(i) 975 if i \u0026gt;= 30: 976 break 977 978# Just as you can create a list comprehension, you can create generator 979# comprehensions as well. 980values = (-x for x in [1,2,3,4,5]) 981for x in values: 982 print(x) # prints -1 -2 -3 -4 -5 to console/terminal 983 984# You can also cast a generator comprehension directly to a list. 985values = (-x for x in [1,2,3,4,5]) 986gen_to_list = list(values) 987print(gen_to_list) # =\u0026gt; [-1, -2, -3, -4, -5] 988 989 990# Decorators 991# In this example `beg` wraps `say`. If say_please is True then it 992# will change the returned message. 993from functools import wraps 994 995 996def beg(target_function): 997 @wraps(target_function) 998 def wrapper(*args, **kwargs): 999 msg, say_please = target_function(*args, **kwargs) 1000 if say_please: 1001 return \u0026#34;{} {}\u0026#34;.format(msg, \u0026#34;Please! I am poor :(\u0026#34;) 1002 return msg 1003 1004 return wrapper 1005 1006 1007@beg 1008def say(say_please=False): 1009 msg = \u0026#34;Can you buy me a beer?\u0026#34; 1010 return msg, say_please 1011 1012 1013print(say()) # Can you buy me a beer? 1014print(say(say_please=True)) # Can you buy me a beer? Please! I am poor :( ",url:"https://romankurnovskii.com/en/posts/python-snippets/"},"https://romankurnovskii.com/en/posts/howto-install-rhel-9-free/":{title:"How to Download and Install Linux RHEL 9 for Free",tags:["linux","rhel"],content:`Red Hat Enterprise Linux 9 (RHEL 9), codenamed Plow, has gone public (GA). Red Hat announced it on May 18, 2022. It replaced the beta version, which had been in existence since November 3, 2021.
RHEL 9 is the first few releases in the Red Hat family. It is the first major release since IBM acquired Red Hat in July 2019, and the first major release since abandoning the CentOS project in favor of CentOS Stream, which is now RHEL\u0026rsquo;s predecessor.
RHEL 9 is the latest major version of RHEL and comes with a 5.14 kernel, lots of new software packages and a host of improvements. It emphasizes security, stability, flexibility and reliability.
Description. RHEL 9 ships with new versions of software including Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0, and many more.
Preparing for installation Registration on the Red Hat portal Red Hat Developer Subscription is a free Red Hat Developer Program offer designed for individual developers who want to take full advantage of Red Hat Enterprise Linux.
It gives developers access to all versions of Red Hat Enterprise Linux, as well as other Red Hat products such as add-ons, software updates and security bugs.
First of all, make sure you have an active Red Hat account. If you don\u0026rsquo;t already have an account, go to the Red Hat Customer Portal, click on \u0026ldquo;Register\u0026rdquo; and fill out your information to create a Red Hat account. Downloading the installation image After creating a Red Hat account, you can start downloading RHEL 9. To download Red Hat Enterprise Linux 9 absolutely free, go to Red Hat Developer Portal and log in with your account credentials. Then go to the download RHEL 9 page and click on the download button shown below.
I\u0026rsquo;m using a MacBook M1, so I download the RHEL 9 image for the M1 processor aarch64 Virtual machine I use the free UTM virtual machine as a virtual machine to install RHEL 9. You can install using Homebrew by running the command brew install --cask utm.
Installing Red Hat Enterprise Linux 9 Setting up the UTM virtual machine In UTM click Create a New Virtual Machine -\u0026gt; Virtualize Choose the downloaded RHEL 9 image and click Continue. Main Setup Menu The marked fields need to be filled in
Create Root Password User Creation. Create the user you want to log in with. Connect to Red Hat. Here we will use the account created above.
Here you will enter your account data and click Register. Press Done
Under Installation Destination choose your default drive.
We can now continue with the installation. A Begin installation button will appear on the main screen
After installation is complete, we will have to reboot the system. Sometimes rebooting will unload the installation image again. It\u0026rsquo;s necessary to either disable the disk in the installer setup or reboot the UTM.
Running Red Hat Enterprise Linux 9 Enter your password and see the RHEL 9 desktop To access the applications, click the Activities button in the upper left corner
Configuring Red Hat Enterprise Linux 9 Checking the ROOT user In a Linux system users belong to different groups which have certain rights. If during the installation process we did not check the checkbox to make the user an administrator, by default he will not be able to install some system programs.
Exit and log in as root (the same user we created earlier on the main screen). Press Log out Now log in as root. The user may not be listed. Press Not listed and enter the account data. Open terminal and check Configuring system settings Button to minimize the application The first thing that seems unusual about using the GUI is that there are no buttons to minimize windows Install the necessary package
1yum install gnome-tweaks -y After installation, the Tweaks application will appear. Find it by searching.
There are many other tweaks in the app as well. We will show you the minimize buttons for the applications.
Let\u0026rsquo;s go to Windows titlebars and set the Maximize, Minimize options
User access to install applications To avoid constantly switching to a root user to install applications, we can give the normal user access to install applications. We will continue to do this as root. Open /etc/sudoers and add the user
1sudo vi /etc/sudoers Add user data to the end of the file. My user name: rhel-user
1rhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf Let\u0026rsquo;s install Visual Studio Code as a normal user Installation consists of the following steps:
adding the desired repository. Rights to add the repository (changing the files in the directory is still only for root user) Downloading and installing. First step is done as root user Go to https://code.visualstudio.com/docs/setup/linux
Copy the code and run it in the terminal
1sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc 2sudo sh -c \u0026#39;echo -e \u0026#34;[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\u0026#34; \u0026gt; /etc/yum.repos.d/vscode.repo\u0026#39; Switch to user rhel-user. This can also be done in the terminal. Updating the repositories Install VSCode 1su rhel-user 2dnf check-update 3 4sudo dnf install code References https://developers.redhat.com/products/rhel/getting-started https://www.redhat.com/sysadmin/install-linux-rhel-9 `,url:"https://romankurnovskii.com/en/posts/howto-install-rhel-9-free/"},"https://romankurnovskii.com/en/tags/linux/":{title:"Linux",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/linux/"},"https://romankurnovskii.com/en/categories/linux/":{title:"Linux",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/linux/"},"https://romankurnovskii.com/en/categories/os/":{title:"OS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/os/"},"https://romankurnovskii.com/en/tags/rhel/":{title:"rhel",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/rhel/"},"https://romankurnovskii.com/en/tags/docker/":{title:"docker",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/docker/"},"https://romankurnovskii.com/en/categories/docker/":{title:"docker",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/docker/"},"https://romankurnovskii.com/en/posts/docker-commands/":{title:"Top Docker Commands",tags:["docker","cheatsheet"],content:`most popular docker images ## lists the images docker pull imagename ## Pull an image or a repository from a registry docker ps -a ## See a list of all containers, even the ones not running docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \u0026quot;imagename\u0026quot; mapping port 4000 to 80 docker rmi ## removes the image docker rm ## removes the container docker stop ## stops the container docker volume ls ## lists the volumes docker kill ## kills the container docker logs ## see logs docker inspect ## shows all the info of a container docker docker cp ## Copy files/folders between a container and the local filesystem docker pull imagename ## Pull an image or a repository from a registry docker build -t imagename . ## Create image using this directory's Dockerfile docker run -p 4000:80 imagename ## Run \u0026quot;imagename\u0026quot; mapping port 4000 to 80 docker run -d -p 4000:80 imagename ## Same thing, but in detached mode docker exec -it [container-id] bash ## Enter a running container docker ps ## See a list of all running containers docker stop \u0026lt;hash\u0026gt; ## Gracefully stop the specified container docker ps -a ## See a list of all containers, even the ones not running docker kill \u0026lt;hash\u0026gt; ## Force shutdown of the specified container docker rm \u0026lt;hash\u0026gt; ## Remove the specified container from this machine docker rm -f \u0026lt;hash\u0026gt; ## Remove force specified container from this machine docker rm $(docker ps -a -q) ## Remove all containers from this machine docker images -a ## Show all images on this machine docker rmi \u0026lt;imagename\u0026gt; ## Remove the specified image from this machine docker rmi $(docker images -q) ## Remove all images from this machine docker top \u0026lt;container-id\u0026gt; ## Display the running processes of a container docker logs \u0026lt;container-id\u0026gt; -f ## Live tail a container's logs docker login ## Log in this CLI session using your Docker credentials docker tag \u0026lt;image\u0026gt; username/repository:tag ## Tag \u0026lt;image\u0026gt; for upload to registry docker push username/repository:tag ## Upload tagged image to registry docker run username/repository:tag ## Run image from a registry docker system prune ## Remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes. (Docker 17.06.1-ce and superior) docker system prune -a ## Remove all unused containers, networks, images not just dangling ones (Docker 17.06.1-ce and superior) docker volume prune ## Remove all unused local volumes docker network prune ## Remove all unused networks docker compose docker-compose up # Create and start containers docker-compose up -d # Create and start containers in detached mode docker-compose down # Stop and remove containers, networks, images, and volumes docker-compose logs # View output from containers docker-compose restart # Restart all service docker-compose pull # Pull all image service docker-compose build # Build all image service docker-compose config # Validate and view the Compose file docker-compose scale \u0026lt;service_name\u0026gt;=\u0026lt;replica\u0026gt; # Scale special service(s) docker-compose top # Display the running processes docker-compose run -rm -p 2022:22 web bash # Start web service and runs bash as its command, remove old container. docker services docker service create \u0026lt;options\u0026gt; \u0026lt;image\u0026gt; \u0026lt;command\u0026gt; # Create new service docker service inspect --pretty \u0026lt;service_name\u0026gt; # Display detailed information Service(s) docker service ls # List Services docker service ps # List the tasks of Services docker service scale \u0026lt;service_name\u0026gt;=\u0026lt;replica\u0026gt; # Scale special service(s) docker service update \u0026lt;options\u0026gt; \u0026lt;service_name\u0026gt; # Update Service options docker stack docker stack ls # List all running applications on this Docker host docker stack deploy -c \u0026lt;composefile\u0026gt; \u0026lt;appname\u0026gt; # Run the specified Compose file docker stack services \u0026lt;appname\u0026gt; # List the services associated with an app docker stack ps \u0026lt;appname\u0026gt; # List the running containers associated with an app docker stack rm \u0026lt;appname\u0026gt; # Tear down an application docker machine docker-machine create --driver virtualbox myvm1 # Create a VM (Mac, Win7, Linux) docker-machine create -d hyperv --hyperv-virtual-switch \u0026quot;myswitch\u0026quot; myvm1 # Win10 docker-machine env myvm1 # View basic information about your node docker-machine ssh myvm1 \u0026quot;docker node ls\u0026quot; # List the nodes in your swarm docker-machine ssh myvm1 \u0026quot;docker node inspect \u0026lt;node ID\u0026gt;\u0026quot; # Inspect a node docker-machine ssh myvm1 \u0026quot;docker swarm join-token -q worker\u0026quot; # View join token docker-machine ssh myvm1 # Open an SSH session with the VM; type \u0026quot;exit\u0026quot; to end docker-machine ssh myvm2 \u0026quot;docker swarm leave\u0026quot; # Make the worker leave the swarm docker-machine ssh myvm1 \u0026quot;docker swarm leave -f\u0026quot; # Make master leave, kill swarm docker-machine start myvm1 # Start a VM that is currently not running docker-machine stop $(docker-machine ls -q) # Stop all running VMs docker-machine rm $(docker-machine ls -q) # Delete all VMs and their disk images docker-machine scp docker-compose.yml myvm1:~ # Copy file to node's home dir docker-machine ssh myvm1 \u0026quot;docker stack deploy -c \u0026lt;file\u0026gt; \u0026lt;app\u0026gt;\u0026quot; # Deploy an app Options for popular commands docker build Docs Build an image from a Dockerfile.
1docker build [DOCKERFILE PATH] Example
Build an image tagged my-org/my-image where the Dockerfile can be found at /tmp/Dockerfile.
1docker build -t my-org:my-image -f /tmp/Dockerfile Flags
--file -f Path where to find the Dockerfile --force-rm Always remove intermediate containers --no-cache Do not use cache when building the image --rm Remove intermediate containers after a successful build (this is true) by default --tag -t Name and optionally a tag in the ‘name:tag’ format docker run Docs
Creates and starts a container in one operation. Could be used to execute a single command as well as start a long-running container.
Example
1docker run -it ubuntu:latest /bin/bash This will start a ubuntu container with the entrypoint /bin/bash. Note that if you do not have the ubuntu image downloaded it will download it before running it.
Flags
-it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) --rm Automatically remove the container when it exit. Otherwise it will be stored and visible running docker ps -a. --detach -d Run container in background and print container ID --volume -v Bind mount a volume. Useful for accessing folders on your local disk inside your docker container, like configuration files or storage that should be persisted (database, logs etc.). docker exec Docs
Execute a command inside a running container.
1docker exec [CONTAINER ID] Example
1docker exec [CONTAINER ID] touch /tmp/exec_works Flags
--detach -d Detached mode: run command in the background -it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) docker images Docs
List all downloaded/created images.
1docker images Flags
-q Only show numeric IDs docker inspect Docs
Shows all the info of a container.
1docker inspect [CONTAINER ID] docker logs Docs
Gets logs from container.
1docker logs [CONTAINER ID] Flags
--details Log extra details --follow -f Follow log output. Do not stop when end of file is reached, but rather wait for additional data to be appended to the input. --timestamps -t Show timestamps docker ps Docs
Shows information about all running containers.
1docker ps Flags
--all -a Show all containers (default shows just running) --filter -f Filter output based on conditions provided, docker ps -f=\u0026quot;name=\u0026quot;example\u0026quot; --quiet -q Only display numeric IDs docker rmi Docs
Remove one or more images.
1docker rmi [IMAGE ID] Flags
--force -f Force removal of the image Snippets A collection of useful tips and tricks for Docker.
Delete all containers NOTE: This will remove ALL your containers.
1docker container prune OR, if you\u0026rsquo;re using an older docker client:
1docker rm $(docker ps -a -q) Delete all untagged containers 1docker image prune OR, if you\u0026rsquo;re using an older docker client:
1docker rmi $(docker images | grep \u0026#39;^\u0026lt;none\u0026gt;\u0026#39; | awk \u0026#39;{print $3}\u0026#39;) Remove all docker images with none tag 1docker rmi --force $(docker images --filter \u0026#34;dangling=true\u0026#34; -q) See all space Docker take up 1docker system df Get IP address of running container 1docker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4 Kill all running containers 1docker kill $(docker ps -q) Resources docs.docker.com docker-cheat-sheet docker-cheat-sheet https://sourabhbajaj.com/mac-setup/Docker/ `,url:"https://romankurnovskii.com/en/posts/docker-commands/"},"https://romankurnovskii.com/en/tags/cli/":{title:"CLI",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/cli/"},"https://romankurnovskii.com/en/categories/cli/":{title:"CLI",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/cli/"},"https://romankurnovskii.com/en/categories/macos/":{title:"MacOS",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/macos/"},"https://romankurnovskii.com/en/tags/tar/":{title:"tar",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/tar/"},"https://romankurnovskii.com/en/posts/cheat-sheet-command-tar/":{title:"Tar command Cheat Sheet",tags:["Linux","CLI","tar","cheatsheet"],content:"Common options z	compress with gzip c	create an archive u	append files which are newer than the corresponding copy ibn the archive f	filename of the archive v	verbose, display what is inflated or deflated a	unlike of z, determine compression based on file extension Create tar named archive.tar containing directory 1tar cf archive.tar /path/files Concatenate files into a single tar 1tar -cf archive.tar /path/files Extract the contents from archive.tar 1tar xf archive.tar Create a gzip compressed tar file name archive.tar.gz 1tar czf archive.tar.gz /path/files Extract a gzip compressed tar file 1tar xzf archive.tar.gz Create a tar file with bzip2 compression 1tar cjf archive.tar.bz2 /path/files Extract a bzip2 compressed tar file 1tar xjf archive.tar.bz2 List content of tar file 1tar -tvf archive.tar ",url:"https://romankurnovskii.com/en/posts/cheat-sheet-command-tar/"},"https://romankurnovskii.com/en/posts/howto-rename-files-in-python/":{title:"How to rename files in Python",tags:["Python"],content:`Learn different ways to rename files in Python using the os and pathlib modules.
os.rename Rename files with os
You can use
1os.rename(old_name, new_name) For example we can combine it with os.path.splitext() to get the base name and file extension, and then combine it to a new name:
1import os 2for file in os.listdir(): 3 name, ext = os.path.splitext(file) 4 new_name = f\u0026#34;{name}_new{ext}\u0026#34; 5 os.rename(file, new_name) pathlib Rename files with pathlib
The same could be achieved with the pathlib module and
1Path.rename(new_name) With a Path object we can access .stem and .suffix:
1from pathlib import Path 2for file in os.listdir(): 3 f = Path(file) 4 new_name = f\u0026#34;{f.stem}_new{f.suffix}\u0026#34; 5 f.rename(new_name) shutil.move The shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module.
1import shutil 2 3old_source = \u0026#39;/Users/r/Desktop/old_source.txt\u0026#39; 4new_source = \u0026#39;/Users/r/Desktop/new_source.txt\u0026#39; 5 6newFileName = shutil.move(old_source, new_source) 7 8print(\u0026#34;New file:\u0026#34;, newFileName) 9# New file: /Users/r/Desktop/new_source.txt `,url:"https://romankurnovskii.com/en/posts/howto-rename-files-in-python/"},"https://romankurnovskii.com/en/categories/programming/":{title:"programming",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/programming/"},"https://romankurnovskii.com/en/posts/howto-create-deepclone-js/":{title:"How to create a deep clone of an object in JavaScript",tags:["JavaScript"],content:`We can use recursion. Use Object.assign() and an empty object ({}) to create a shallow clone of the original. Use Object.keys() and Array.prototype.forEach() to determine which key-value pairs need to be deep cloned.
1const deepClone = obj =\u0026gt; { 2 let clone = Object.assign({}, obj); 3 Object.keys(clone).forEach( 4 key =\u0026gt; (clone[key] = typeof obj[key] === \u0026#39;object\u0026#39; ? deepClone(obj[key]) : obj[key]) 5 ); 6 return Array.isArray(obj) \u0026amp;\u0026amp; obj.length 7 ? (clone.length = obj.length) \u0026amp;\u0026amp; Array.from(clone) 8 : Array.isArray(obj) 9 ? Array.from(obj) 10 : clone; 11}; 1const a = { foo: \u0026#39;bar\u0026#39;, obj: { a: 1, b: 2 } }; 2const b = deepClone(a); // a !== b, a.obj !== b.obj `,url:"https://romankurnovskii.com/en/posts/howto-create-deepclone-js/"},"https://romankurnovskii.com/en/tags/mac/":{title:"mac",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/mac/"},"https://romankurnovskii.com/en/posts/mac-setup-development/":{title:"Mac Setup 2022",tags:["mac","mac setup web developer","mac setup javascript"],content:`MacBook Pro Specification 13-inch Apple M1 Pro M1 2020 16 GB RAM 512 GB SSD QWERTY = English/Hebrew macOS Monterey (Update always) Homebrew Install Homebrew as package manager for macOS:
1## paste in terminal and follow the instructions 2/bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Update everything in Homebrew to recent version:
1brew update Add additional source for casks:
1brew tap homebrew/cask-versions Install GUI applications (read more about these in GUI Applications):
1brew install --cask \\ 2 google-chrome \\ 3 firefox \\ 4 visual-studio-code \\ 5 all-in-one-messenger \\ 6 sublime-text \\ 7 docker \\ 8 rectangle \\ 9 discord \\ 10 vlc \\ 11 figma \\ 12 grammarly \\ 13 macx-youtube-downloader \\ 14 notion \\ 15 postman \\ 16 tor-browser \\ 17 transmission \\ 18 utm \\ 19 viber \\ 20 yandex-disk \\ 21 zoom \\ 22 mongodb-compass \\ 23 disk-inventory-x \\ 24 obs \\ 25 spotify \\ 26 iterm2 \\ 27 deepl \\ 28 syncthing Install terminal applications (read more about these in Terminal Applications):
1brew install \\ 2 git \\ 3 ffmpeg \\ 4 nvm \\ 5 jupyterlab Additional GUI Applications Kotatogram Kotatogram - Experimental fork of Telegram Desktop. Folders with features
GUI Applications Google Chrome Google Chrome (web development, web browsing)
Preferences set default browser always show bookmarks import bookmarks from previous machine Chrome Developer Tools Network -\u0026gt; only \u0026ldquo;Fetch/XHR\u0026rdquo; Search Shortcuts. Add Shortucts for different search engines. chrome://settings/searchEngines Yandex, search only in Russia. Shortcut: vv url: https://yandex.ru/{yandex:searchPath}?text=%s\u0026amp;{yandex:referralID}\u0026amp;lr=101443\u0026amp;rstr=-225 Youtube Shortcut: yy url: https://www.youtube.com/results?search_query=%s\u0026amp;page={startPage?}\u0026amp;utm_source=opensearch Chrome Extensions Google Translate DeepL Translate - AI translator React Developer Tools Pocket - The easiest, fastest way to capture articles, videos, and more. Session Buddy (Manage Browser Tabs and Bokmarks) LanguageTool (multilingual grammar, style, and spell checker) RSS Feed Reader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) Inoreader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) 30 Seconds of Knowledge (random code snippet on a new tab) JSON Formatter picture-in-picture (yutube/video above other screens) Visual CSS Editor (Customize any website visually) Squish - AI-powered summary tool. Turn any body of text into a few sentences with one click. Zotero - Add/sync scientific PDF documents Video Downloader Plus Opus●Guide (Step-by-step for instructions) Disk Inventory X Disk Inventory X (disk usage utility for macOS)
Docker Docker (Docker, see setup)
used for running databases (e.g. PostgreSQL, MongoDB) in container without cluttering the Mac Preferences enable \u0026ldquo;Use Docker Compose\u0026rdquo; Firefox Firefox (web development)
Visual Studio Code Visual Studio Code (web development IDE)
Settings
Sublime Text Sublime Text (editor)
Maccy Maccy (clipboard manager)
enable \u0026ldquo;Launch at Login\u0026rdquo; OBS OBS (for video recording and live streaming)
for Native Mac Screen recorder Base (Canvas) 2880x1800 (Ratio: 16:10) Output 1728x1080 Spotify Spotify
Syncthing syncthing - Sync folders/files between devices. I use to backup all photos/video from mobile to PC
Transmission Transmission (A torrent client that I use. Very minimal in its UI but very powerful and has all the features that I need)
UTM UTM (Virtual machines UI using QEMU)
download ubuntu for arm, doc On error with shared folder: Could not connect: Connection refused open in browser: http://127.0.0.1:9843/ For Debian install spice-webdavd for shared folder. https://packages.debian.org/search?keywords=spice-webdavd, https://github.com/utmapp/UTM/issues/1204 1sudo apt install spice-vdagent spice-webdavd -y VLC VLC (video player)
use as default for video files Terminal Applications nvm nvm (node version manager)
jupyterlab jupyterlab (Jupyter - python development, fast code snippets)
jupyter notebook - to start jupyter notebook ffmpeg ffmpeg (Converting video and audio)
compress video: 1ffmpeg -i input.mp4 -c:v libx264 -crf 23 -preset slow -c:a aac -b:a 192k output.mp4 2# or 3ffmpeg -i input.mp4 output.avi convert video to .gif: 1- ffmpeg \\ 2-i input.mp4 \\ 3-ss 00:00:00.000 \\ 4-pix_fmt rgb24 \\ 5-r 10 \\ 6-s 960x540 \\ 7-t 00:00:10.000 \\ 8output.gif NVM for Node/npm The node version manager (NVM) is used to install and manage multiple Node versions. After you have installed it via Homebrew in a previous step, type the following commands to complete the installation:
1echo \u0026#34;source $(brew --prefix nvm)/nvm.sh\u0026#34; \u0026gt;\u0026gt; ~/.zshrc 2 3source ~/.zshrc 4## or alias 5## zshsource Now install the latest LTS version on the command line:
1nvm install \u0026lt;latest LTS version from https://nodejs.org/en/\u0026gt; Afterward, check whether the installation was successful and whether the node package manager (npm) got installed along the way:
1node -v \u0026amp;\u0026amp; npm -v Update npm to its latest version:
1npm install -g npm@latest And set defaults for npm:
1npm set init.author.name \u0026#34;your name\u0026#34; 2npm set init.author.email \u0026#34;you@example.com\u0026#34; 3npm set init.author.url \u0026#34;example.com\u0026#34; If you are a library author, log in to npm too:
1npm adduser That\u0026rsquo;s it. If you want to list all your Node.js installation, type the following:
1nvm list If you want to install a newer Node.js version, then type:
1nvm install \u0026lt;version\u0026gt; --reinstall-packages-from=$(nvm current) 2nvm use \u0026lt;version\u0026gt; 3nvm alias default \u0026lt;version\u0026gt; Optionally install yarn if you use it as alternative to npm:
1npm install -g yarn 2yarn -v If you want to list all globally installed packages, run this command:
1npm list -g --depth=0 That\u0026rsquo;s it. You have a running version of Node.js and its package manager.
OH MY ZSH MacOS already comes with zsh as default shell. Install Oh My Zsh for an improved (plugins, themes, \u0026hellip;) experience. Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.
Install:
1sh -c \u0026#34;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026#34; Update everything (e.g. plugins) in Oh My Zsh to recent version:
1omz update Install fonts for themes:
1brew tap homebrew/cask-fonts 2brew install --cask font-hack-nerd-font iTerm2 Install theme Theme description 1brew install romkatv/powerlevel10k/powerlevel10k 2echo \u0026#34;source $(brew --prefix)/opt/powerlevel10k/powerlevel10k.zsh-theme\u0026#34; \u0026gt;\u0026gt;~/.zshrc Enable suggestions 1git clone https://github.com/zsh-users/zsh-autosuggestions \${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 2echo \u0026#34;plugins=(zsh-autosuggestions)\u0026#34; \u0026gt;\u0026gt;~/.zshrc Open new tab(CMD+T)/restart iTerm to proceed with theme setup
Terminal Script and Aliases Update .zprofile. Еhe changes will take effect after restarting the terminal
1vi ~/.zprofile Automatic software updates Add script to zprofile that updates everything:
Update, upgrade all and cleanup softwareupdate - system software update tool We can execute this command on strartup, but i prefer handle it. When I kick of upd command in terminal, it will update everythin I need:
1alias upd=\u0026#39;brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i\u0026#39; Add aliases to latest versions pip \u0026amp; python
1alias pip=pip3 2alias python=python3 Final view of .zprofile
1... 2alias pip=pip3 3alias python=python3 4alias upd=\u0026#39;omz update; brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i\u0026#39; Links https://www.robinwieruch.de/mac-setup-web-development/ https://sourabhbajaj.com/mac-setup/iTerm/ack.html https://www.engineeringwithutsav.com/blog/spice-up-your-macos-terminal `,url:"https://romankurnovskii.com/en/posts/mac-setup-development/"},"https://romankurnovskii.com/en/tags/mac-setup-javascript/":{title:"mac setup javascript",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/mac-setup-javascript/"},"https://romankurnovskii.com/en/tags/mac-setup-web-developer/":{title:"mac setup web developer",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/mac-setup-web-developer/"},"https://romankurnovskii.com/en/tags/css/":{title:"css",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/css/"},"https://romankurnovskii.com/en/tags/html/":{title:"html",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/html/"},"https://romankurnovskii.com/en/tags/markdown/":{title:"markdown",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/markdown/"},"https://romankurnovskii.com/en/posts/markdown-syntax/":{title:"Markdown Cheat Sheet",tags:["markdown","css","html","themes"],content:`This article offers an example of the basic Markdown syntax that can be used and also shows whether the basic elements of HTML are decorated with CSS.
Headers 1Header 1 2======== 3Header 2 4-------- Header 1 Header 2 1# h1 2## h2 3### h3 4#### h4 5##### h5 6###### h6 h1 h2 h3 h4 h5 h6 Paragraph To insert an empty string, you need to put the word wrap symbol twice (press Enter)
1Lorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit! 2 3Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae. Images 1![Image alt text](/path/to/img.jpg) 2![Image alt text](/path/to/img.jpg \u0026#34;title\u0026#34;) 3![Image alt text][img] 4 5[img]: http://foo.com/img.jpg Emphasis 1*italic* 2_italic_ 3 4**bold** 5__bold__ 6***bold italic*** 7___bold italic___ 8 9~~strikethrough~~ 10 11\`code\` italic italic
bold bold bold italic bold italic
strikethrough
code
Links 1[link](http://google.com) 2 3[link][google] 4[google]: http://google.com 5 6\u0026lt;http://google.com\u0026gt; Blockquotes The blockquote element represents the content that is quoted from another source, optionally with a quotation that must be in the element footer or cite, and optional line changes such as annotations and abbreviations.
Block quote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Notethat you can use the syntax Markdown inside the block quote.
Block quote with authorship Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.
— Rob Pike1
1\u0026gt;This is an example quote, 2\u0026gt;in which before each line 3\u0026gt;angle bracket is used. 4 5\u0026gt;This is an example quote, 6in which the corner bracket is placed only before the beginning of the new paragraph. 7\u0026gt;Second paragraph. This is an example quote, in which before each line angle bracket is used.
This is an example quote, in which the corner bracket is placed only before the beginning of the new paragraph. Second paragraph.
1\u0026gt; Level One Citation 2\u0026gt;\u0026gt; Second Level Citation 3\u0026gt;\u0026gt;\u0026gt; Third Level Citation 4\u0026gt; 5\u0026gt;Level One Citation Level One Citation
Second Level Citation
Third Level Citation
Level One Citation
Tables 1 | Name | Age | 2 | ----- | --- | 3 | Bob | 27 | 4 | Alice | 23 | Name Age Bob 27 Alice 23 The cells in the delimitation row use only symbols - and :. The symbol : is placed at the beginning, at the end, or on both sides of the cell contents of the dividing row to indicate the alignment of the text in the corresponding column on the left, right side, or center.
1| Column on the left | Column on the right | Column on the center | 2| :----------------- | ------------------: | :------------------: | 3| Text | Text | Text | Column on the left Column on the right Column on the center Text Text Text Markdown inside the table 1| Italics | Bold | Code | 2| --------- | -------- | ------ | 3| *italics* | **bold** | \`code\` | Italics Bold Code italics bold code Code Blocks Code block with inverted quotes 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Code block with four spaces indent \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code Unit with Hugo Internal Shorted Backlight 1\u0026lt;!doctype html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 5 \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Lists 1* Item 1 2* Item 2 3 4- Item 1 5- Item 2 6 7- [ ] Checkbox off 8- [x] Checkbox on 9 101. Item 1 112. Item 2 Item 1 Item 2 Item 1
Item 2
Checkbox off
Checkbox on
Item 1 Item 2 Make the headers uniform. At the end of the title, do not put a point.
Correct Wrong Getting the Creating a Cluster Get the Creating a Cluster Get Create Cluster If you want to describe the sequence of actions, use the numbered list. At the end of the lines, put a period.
If the order of items is not important, use the marked list. Make it one of the ways:
If the entries in the list are separate sentences, start them with a capital letter and put a period at the end. If the introductory phrase and the list make up one sentence, the entries in the list should start with a lowercase letter and end with a semicolon. The last list item ends with a dot. If the list consists of parameter names or values (without explanation), do not put characters at the end of lines. Ordered list First item Second item Third item To create an ordered numbered list, use the digits with the symbol . or ). The recommended markup format is 1 and ..
11. First item 21. Second item 31. Third item First item Second item Third item To create a nested ordered list, add a indent to the entries in the child list. The allowed indentation is from two to five spaces. The recommended indent size is four spaces.
For example, markup:
11. First paragraph 2 1. Sub-paragraph 3 1. Sub-paragraph 41. Second paragraph First paragraph Sub-paragraph Sub-paragraph Second paragraph Unordered list List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other elemnts - abbr, sub, sup, kbd, mark GIF is a bitmap image format.
H2O
Xn + Yn = Zn
Press CTRL+ALT+Delete to end the session.
Most salamanders are nocturnal, and hunt for insects, worms, and other small creatures.
Most \u0026lt;mark\u0026gt;salamanders\u0026lt;/mark\u0026gt; are nocturnal 💡 Data structure is a container that stores data in a specific format. This container decides how the outside world can read or change this data.
Resources Style from Google The above quote is taken from Rob Pike’s book talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`,url:"https://romankurnovskii.com/en/posts/markdown-syntax/"},"https://romankurnovskii.com/en/tags/themes/":{title:"themes",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/themes/"},"https://romankurnovskii.com/en/tags/emigration/":{title:"emigration",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/emigration/"},"https://romankurnovskii.com/en/tags/it/":{title:"it",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/it/"},"https://romankurnovskii.com/en/posts/diploma/":{title:"IT courses 2020",tags:["study","it","emigration"],content:`Interim metrics still in process
For 2020:
Time spent studying/practicing: ~5500 hours `,url:"https://romankurnovskii.com/en/posts/diploma/"},"https://romankurnovskii.com/en/tags/study/":{title:"study",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/study/"},"https://romankurnovskii.com/en/categories/study/":{title:"study",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/study/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/apm-server/readme/":{title:"",tags:[],content:`APM Server extension The APM Server receives data from APM agents and transforms them into Elasticsearch documents that can be visualised in Kibana.
Usage To include APM Server in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the apm-server-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/apm-server/apm-server-compose.yml up Meanwhile, you can navigate to the APM application in Kibana and follow the setup instructions to get started.
Connecting an agent to APM Server The most basic configuration to send traces to APM server is to specify the SERVICE_NAME and SERVICE_URL. Here is an example Python Flask configuration:
1import elasticapm 2from elasticapm.contrib.flask import ElasticAPM 3 4from flask import Flask 5 6app = Flask(__name__) 7app.config[\u0026#39;ELASTIC_APM\u0026#39;] = { 8 # Set required service name. Allowed characters: 9 # a-z, A-Z, 0-9, -, _, and space 10 \u0026#39;SERVICE_NAME\u0026#39;: \u0026#39;PYTHON_FLASK_TEST_APP\u0026#39;, 11 12 # Set custom APM Server URL (default: http://localhost:8200) 13 \u0026#39;SERVER_URL\u0026#39;: \u0026#39;http://apm-server:8200\u0026#39;, 14 15 \u0026#39;DEBUG\u0026#39;: True, 16} Configuration settings for each supported language are available in the APM documentation: APM Agents.
Checking connectivity and importing default APM dashboards On the Kibana home page, click Add APM under the Observability panel. Click Check APM Server status to confirm the server is up and running. Click Check agent status to verify your agent has registered properly. Click Load Kibana objects to create an index pattern for APM. Click Launch APM to be taken to the APM dashboard. See also Running APM Server on Docker
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/apm-server/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/curator/readme/":{title:"",tags:[],content:`Curator Elasticsearch Curator helps you curate or manage your indices.
Usage If you want to include the Curator extension, run Docker Compose from the root of the repository with an additional command line argument referencing the curator-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/curator/curator-compose.yml up This sample setup demonstrates how to run curator every minute using cron.
All configuration files are available in the config/ directory.
Documentation Curator Reference
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/curator/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/enterprise-search/readme/":{title:"",tags:[],content:`Enterprise Search extension Elastic Enterprise Search is a suite of products for search applications backed by the Elastic Stack.
Requirements 2 GB of free RAM, on top of the resources required by the other stack components and extensions. Enterprise Search exposes the TCP port 3002 for its Web UI and API.
Usage Generate an encryption key Enterprise Search requires one or more encryption keys to be configured before the initial startup. Failing to do so prevents the server from starting.
Encryption keys can contain any series of characters. Elastic recommends using 256-bit keys for optimal security.
Those encryption keys must be added manually to the config/enterprise-search.yml file. By default, the list of encryption keys is empty and must be populated using one of the following formats:
1secret_management.encryption_keys: 2 - my_first_encryption_key 3 - my_second_encryption_key 4 - ... 1secret_management.encryption_keys: [my_first_encryption_key, my_second_encryption_key, ...] ℹ️ To generate a strong encryption key, for example using the AES-256 cipher, you can use the OpenSSL utility or any other online/offline tool of your choice:
1$ openssl enc -aes-256 -P 2 3enter aes-256-cbc encryption password: \u0026lt;a strong password\u0026gt; 4Verifying - enter aes-256-cbc encryption password: \u0026lt;repeat your strong password\u0026gt; 5... 6 7key=\u0026lt;generated AES key\u0026gt; Enable Elasticsearch\u0026rsquo;s API key service Enterprise Search requires Elasticsearch\u0026rsquo;s built-in API key service to be enabled in order to start. Unless Elasticsearch is configured to enable TLS on the HTTP interface (disabled by default), this service is disabled by default.
To enable it, modify the Elasticsearch configuration file in elasticsearch/config/elasticsearch.yml and add the following setting:
1xpack.security.authc.api_key.enabled: true Configure the Enterprise Search host in Kibana Kibana acts as the management interface to Enterprise Search.
To enable the management experience for Enterprise Search, modify the Kibana configuration file in kibana/config/kibana.yml and add the following setting:
1enterpriseSearch.host: http://enterprise-search:3002 Start the server To include Enterprise Search in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the enterprise-search-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml up Allow a few minutes for the stack to start, then open your web browser at the address http://localhost:3002 to see the Enterprise Search home page.
Enterprise Search is configured on first boot with the following default credentials:
user: enterprise_search password: changeme Security The Enterprise Search password is defined inside the Compose file via the ENT_SEARCH_DEFAULT_PASSWORD environment variable. We highly recommend choosing a more secure password than the default one for security reasons.
To do so, change the value ENT_SEARCH_DEFAULT_PASSWORD environment variable inside the Compose file before the first boot:
1enterprise-search: 2 3 environment: 4 ENT_SEARCH_DEFAULT_PASSWORD: {{some strong password}} ⚠️ The default Enterprise Search password can only be set during the initial boot. Once the password is persisted in Elasticsearch, it can only be changed via the Elasticsearch API.
For more information, please refer to User Management and Security.
Configuring Enterprise Search The Enterprise Search configuration is stored in config/enterprise-search.yml. You can modify this file using the Default Enterprise Search configuration as a reference.
You can also specify the options you want to override by setting environment variables inside the Compose file:
1enterprise-search: 2 3 environment: 4 ent_search.auth.source: standard 5 worker.threads: \u0026#39;6\u0026#39; Any change to the Enterprise Search configuration requires a restart of the Enterprise Search container:
1$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml restart enterprise-search Please refer to the following documentation page for more details about how to configure Enterprise Search inside a Docker container: Running Enterprise Search Using Docker.
See also Enterprise Search documentation
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/enterprise-search/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/filebeat/readme/":{title:"",tags:[],content:`Filebeat Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or Logstash for indexing.
Usage To include Filebeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the filebeat-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up Configuring Filebeat The Filebeat configuration is stored in config/filebeat.yml. You can modify this file with the help of the Configuration reference.
Any change to the Filebeat configuration requires a restart of the Filebeat container:
1$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml restart filebeat Please refer to the following documentation page for more details about how to configure Filebeat inside a Docker container: Run Filebeat on Docker.
See also Filebeat documentation
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/filebeat/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/logspout/readme/":{title:"",tags:[],content:`Logspout extension Logspout collects all Docker logs using the Docker logs API, and forwards them to Logstash without any additional configuration.
Usage If you want to include the Logspout extension, run Docker Compose from the root of the repository with an additional command line argument referencing the logspout-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/logspout/logspout-compose.yml up In your Logstash pipeline configuration, enable the udp input and set the input codec to json:
1input { 2 udp { 3 port =\u0026gt; 5000 4 codec =\u0026gt; json 5 } 6} Documentation https://github.com/looplab/logspout-logstash
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/logspout/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/metricbeat/readme/":{title:"",tags:[],content:`Metricbeat Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server. Metricbeat takes the metrics and statistics that it collects and ships them to the output that you specify, such as Elasticsearch or Logstash.
Usage To include Metricbeat in the stack, run Docker Compose from the root of the repository with an additional command line argument referencing the metricbeat-compose.yml file:
1$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml up Configuring Metricbeat The Metricbeat configuration is stored in config/metricbeat.yml. You can modify this file with the help of the Configuration reference.
Any change to the Metricbeat configuration requires a restart of the Metricbeat container:
1$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml restart metricbeat Please refer to the following documentation page for more details about how to configure Metricbeat inside a Docker container: Run Metricbeat on Docker.
See also Metricbeat documentation
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/metricbeat/readme/"},"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/readme/":{title:"",tags:[],content:`Extensions Third-party extensions that enable extra integrations with the Elastic stack.
`,url:"https://romankurnovskii.com/en/docs/90daysofdevops/monitoring/elastic-stack/extensions/readme/"},"https://romankurnovskii.com/en/apps/":{title:"Apps",tags:[],content:"",url:"https://romankurnovskii.com/en/apps/"},"https://romankurnovskii.com/en/tags/aws-amplify/":{title:"AWS Amplify",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/aws-amplify/"},"https://romankurnovskii.com/en/categories/aws-amplify/":{title:"AWS Amplify",tags:[],content:"",url:"https://romankurnovskii.com/en/categories/aws-amplify/"},"https://romankurnovskii.com/en/posts/cloud-exam-quizz/amplify-setup-project/":{title:"AWS Amplify - project setup with Github",tags:["AWS","AWS Amplify","Github"],content:`Preface For Amplify project I use eu-west region github repo has to be ready private or public New project goto https://eu-west-1.console.aws.amazon.com/amplify/home?region=eu-west-1#/
New app → Host web app → Github
Add access to github repo Select repository Come back to Amplify and try again to choose repo Click Next
Update amplify.yml for node.js project
1version: 1 2frontend: 3 phases: 4 preBuild: 5 commands: 6 - yarn install 7 build: 8 commands: 9 - yarn run build 10 artifacts: 11 baseDirectory: build 12 files: 13 - \u0026#39;**/*\u0026#39; 14 cache: 15 paths: 16 - node_modules/**/* Next → Save and deploy Amplify starts to build project and generates project url.
Once build done you can open project.
`,url:"https://romankurnovskii.com/en/posts/cloud-exam-quizz/amplify-setup-project/"},"https://romankurnovskii.com/en/posts/cloud-exam-quizz/amplify-custom-domain/":{title:"AWS Amplify - Set custom domain",tags:["AWS","AWS Amplify"],content:`You can use any custom domain with Amplify and no need register it with AWS Route53.
I am adding domain at the setup app stage. Another way is from console.
Click Domain management. or
Add domain Write domain name -\u0026gt; Configure domain -\u0026gt; Save Nest starts SSL configuration process. Amplify provides with DNS data that you need to write in the domain register account. Once SSL creation starts you can get domain data
Action -\u0026gt; View DNS records
Copy provided data (DNS records) and then set it in the domain registrar panel. Go to domain registrar Set dns servers to default In my case panel looks like this: Save Go to amplify and check for updates. Amplify checks DNS server and if everything is correct (CNAME set) it will proceed to the next step. SSL configuration passed, waiting up to 30 min for domain activation
Once done we can check url: https://cloud-exam-prepare.com Check url: cloud-exam-prepare.com
Resources:
https://docs.aws.amazon.com/amplify/latest/userguide/to-add-a-custom-domain-managed-by-a-third-party-dns-provider.html `,url:"https://romankurnovskii.com/en/posts/cloud-exam-quizz/amplify-custom-domain/"},"https://romankurnovskii.com/en/docs/archive/":{title:"Docs",tags:[],content:" Docs EN | RU Posts EN | RU ",url:"https://romankurnovskii.com/en/docs/archive/"},"https://romankurnovskii.com/en/tags/github/":{title:"Github",tags:[],content:"",url:"https://romankurnovskii.com/en/tags/github/"},"https://romankurnovskii.com/en/authors/michael-cade/":{title:"Michael Cade",tags:[],content:" ",url:"https://romankurnovskii.com/en/authors/michael-cade/"},"https://romankurnovskii.com/en/posts/":{title:"Notes",tags:[],content:`List style view
`,url:"https://romankurnovskii.com/en/posts/"},"https://romankurnovskii.com/en/posts/archive/":{title:"Posts Archive",tags:[],content:" Docs EN | RU Posts EN | RU ",url:"https://romankurnovskii.com/en/posts/archive/"},"https://romankurnovskii.com/en/docs/":{title:"Roadmaps",tags:[],content:`List style view
`,url:"https://romankurnovskii.com/en/docs/"},"https://romankurnovskii.com/en/authors/roman-kurnovskii/":{title:"Roman Kurnovskii",tags:[],content:" ",url:"https://romankurnovskii.com/en/authors/roman-kurnovskii/"},"https://romankurnovskii.com/en/search/":{title:"Search page",tags:[],content:"",url:"https://romankurnovskii.com/en/search/"}}</script><script src=https://unpkg.com/lunr/lunr.js></script>
<script src=/js/search-page.js?v3></script>
<script src=/js/base.js?5 languagemode=en></script></div></footer><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js defer></script>
<script src=/js/zoom-image/placeholders.js?4 defer></script>
<script src=/js/zoom-image/index.js?4 defer></script></body></html>