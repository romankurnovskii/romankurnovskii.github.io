<!doctype html><html lang=ru dir=ltr><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>33. Пакет requests | Карманная книга по Python | Roman Kurnovskii</title><meta name=generator content="Hugo "><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css rel=stylesheet crossorigin=anonymous><script src=https://unpkg.com/flowbite@1.5.1/dist/flowbite.js></script>
<script src=https://unpkg.com/axios@0.27.2/dist/axios.min.js></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/js/all.min.js integrity=" sha512-8pHNiqTlsrRjVD4A/3va++W1sMbUHwWxxRPWNyVlql3T+Hgfd81Qc6FC5WMXDC+tSauxxzp1tgiAvSKFu1qIlA==" defer crossorigin></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.2/css/all.min.css integrity="sha512-1sCRPdkRXhBV2PBLUdRb4tMg1w2YPf37qatUFeS7zlBy7jJI8Lf4VHwWfZZfpXtYSLy85pkm9GaYVYMfw5BC1A==" media=print onload='this.media="all",this.onload=null' crossorigin><link rel=stylesheet href=https://romankurnovskii.com/css/yalla.min.f80e520817e3748497aee69b3de34487d9f8a5f209edcab72610ce39329ee9f51d18836d82fcf35ac7dc1207432bf6b0.css><script defer src=https://romankurnovskii.com/js/yalla.min.e11e7612701aa90cb2ebcc925db8682fa8bf6a533ef79fb3658949f1f677290795ee80541ca370d10d65dd5fd04e40b8.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href=#ZgotmplZ as=style onload='this.onload=null,this.rel="stylesheet"'><link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/bash.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/javascript.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/python.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/go.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.6.0/build/languages/yaml.min.js crossorigin></script>
<link rel=stylesheet href=https://romankurnovskii.com/css/highlightjs.min.1ee3514a5c54c5959f452405db8232111af2f81ee27cc1b3f5a5be8d23a5f4ac9c8d257e6cbbb2cb6e9ac4c505430307.css media=print onload='this.media="all",this.onload=null'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@9.1.3/dist/mermaid.min.js crossorigin></script>
<link rel=preconnect href=https://www.google-analytics.com crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-26F2C5ZR4Y"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-26F2C5ZR4Y")</script><script type=text/javascript>(function(e,t,n,s,o,i,a){e[o]=e[o]||function(){(e[o].a=e[o].a||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)})(window,document,"script","https://mc.yandex.ru/metrika/tag.js?v2","ym"),ym(87734724,"init",{clickmap:!0,trackLinks:!0,accurateTrackBounce:!0,webvisor:!0})</script><noscript><div><img src=https://mc.yandex.ru/watch/87734724 style=position:absolute;left:-9999px alt></div></noscript><link rel=icon type=image/png sizes=32x32 href=https://romankurnovskii.com/images/icon_huc02d7296c9eb9353758cb2467c0d17b0_10773_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://romankurnovskii.com/images/icon_huc02d7296c9eb9353758cb2467c0d17b0_10773_180x180_fill_box_center_3.png><meta name=description content="Python 101"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Roadmaps","item":"https://romankurnovskii.com/ru/docs/"},{"@type":"ListItem","position":2,"name":"Карманная книга по Python","item":"https://romankurnovskii.com/ru/docs/python101/"},{"@type":"ListItem","position":3,"name":"33. Пакет requests","item":"https://romankurnovskii.com/ru/docs/python101/chapter33_requests/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://romankurnovskii.com/ru/docs/python101/chapter33_requests/"},"headline":"33. Пакет requests | Карманная книга по Python | Roman Kurnovskii","datePublished":"2022-06-28T00:00:00+00:00","dateModified":"2023-01-12T22:53:50+02:00","wordCount":498,"publisher":{"@type":"Person","name":"Roman Kurnovskii","logo":{"@type":"ImageObject","url":"https://romankurnovskii.com/images/icon.png"}},"description":"Python 101"}</script><meta property="og:title" content="33. Пакет requests | Карманная книга по Python | Roman Kurnovskii"><meta property="og:type" content="article"><meta property="og:image" content="https://romankurnovskii.com/images/icon.png"><meta property="og:url" content="https://romankurnovskii.com/ru/docs/python101/chapter33_requests/"><meta property="og:description" content="Python 101"><meta property="og:locale" content="ru"><meta property="og:site_name" content="Roman Kurnovskii"><meta property="article:published_time" content="2022-06-28T00:00:00+00:00"><meta property="article:modified_time" content="2023-01-12T22:53:50+02:00"><meta property="article:section" content="docs"><script>var callback=function(){alert("A callback was triggered")}</script><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");const isDark2=JSON.parse(localStorage.getItem("DarkMode")||"false");isDark2?storageColorScheme="Dark":storageColorScheme="Light",((storageColorScheme=="Auto"||storageColorScheme==null)&&window.matchMedia("(prefers-color-scheme: dark)").matches||storageColorScheme=="Dark")&&document.getElementsByTagName("html")[0].classList.add("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ru/ class="me-6 text-primary-text text-xl font-bold">Roman Kurnovskii</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><ul id=menu><li class=parent><a href=/ru/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Заметки</a><ul class=child><li class=parent><a href=/ru/categories/roadmaps/>Руководства <span class=expand>»</span></a><ul class=child><li class=parent><a href=/ru/docs/python101/>Справочник Python</a></li><li class=parent><a href=/ru/docs/webrtc/>Справочник WebRTC</a></li><li class=parent><a href=/ru/docs/disser/>Диссертация</a></li></ul></li><li class=parent><a href=/ru/categories/programming/>Программирование <span class=expand>»</span></a><ul class=child><li class=parent><a href=/en/docs/algorithms-101/>Algorithms [EN]</a></li><li class=parent><a href=/ru/categories/hugo/>Hugo</a></li><li class=parent><a href=/ru/categories/javascript/>JavaScript</a></li><li class=parent><a href=/ru/categories/python/>Python</a></li></ul></li><li><a href=/en/p/links/>Links</a></li></ul></li><li class=parent><a href=/ru/apps/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Приложения</a></li><li class=parent><a href=/ru/p/publications class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Публикации</a></li></ul></div><div class=flex><div class="relative pt-4 md:pt-0"><form id=search class="flex items-center"><label for=search-input class=sr-only>Search</label><div class="relative w-full"><input type=text type=search id=search-input class="bg-gray-50 border border-gray-300 text-gray-900 text-sm rounded-lg focus:ring-blue-500 focus:border-blue-500 block w-full pl-10 p-2.5 dark:bg-gray-700 dark:border-gray-600 dark:placeholder-gray-400 dark:text-white dark:focus:ring-blue-500 dark:focus:border-blue-500" placeholder=Search required></div></form></div><div class="relative pt-4 ps-2 md:pt-0"><button class="p-1 rounded border dark:border-slate-700 hover:bg-slate-200 dark:hover:bg-slate-700" onclick='location.href="/ru/search/"'>
<img width=13 src=https://img.icons8.com/ios-glyphs/30/null/search-more.png></button></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>document.addEventListener("DOMContentLoaded",()=>{switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-2xl lg:px-2 xl:px-4"><div class=lg:pt-8><div class="flex flex-col md:flex-row bg-secondary-bg rounded"><div class="md:w-1/4 lg:w-1/5 border-e"><div class="sticky top-16 pt-6"><div id=sidebar-title class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text"><span class=font-semibold>Содержание</span>
<i class='fas fa-caret-right ms-1'></i></div><div id=sidebar-toc class="hidden md:block overflow-y-auto mx-6 md:mx-0 pe-2 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent"><div class="flex flex-wrap ms-1 -me-2 p-1 bg-secondary-bg md:bg-primary-bg rounded"><a class=hover:text-yalla href=https://romankurnovskii.com/ru/docs/python101/>Карманная книга по Python</a></div><aside aria-label=Sidebar><div class="overflow-y-auto py-4 px-3"><ul id=doc__nav__sidebar class=space-y-1><li><a href=https://romankurnovskii.com/ru/docs/python101/01-intro/><span>Введение</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/01-part_i/><span>Часть I - Основы</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter1_idle/><span>1. Программирование IDLE</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter2_strings/><span>2. Все о строках</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter3_lists_dicts/><span>3. Списки, кортежи и словари</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter4_conditionals/><span>4. Условия</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter5_loops/><span>5. Циклы</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter6_comprehensions/><span>6. Генераторы в Python</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter7_exception_handling/><span>7. Обработка исключений</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter8_file_io/><span>8. Работа с файлами</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter9_imports/><span>9. Импортирование</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter10_functions/><span>10. Функции</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter11_classes/><span>11. Классы</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter12_introspection/><span>12. Интроспекция</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/02-part_ii/><span>Часть II - Стандартные модули</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter13_csv/><span>13. Модуль csv</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter14_config_parser/><span>14. Модуль configparser</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter15_logging/><span>15. Логирование</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter20_sys/><span>20. Модуль sys</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter16_os/><span>16. Модуль os</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter17_smtplib/><span>17. Модуль email / smtplib</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter18_sqlite/><span>18. Модуль sqlite</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter19_subprocess/><span>19. Модуль subprocess</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter21_thread/><span>21. Модуль потоков Thread</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter21-1_asyncio/><span>21-1. Модуль asyncio</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter22_time/><span>22. Работа с датами и временем</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter23_xml/><span>23. Модуль xml</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter24_debugging/><span>24. Отладчик Python</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/03-part_iii/><span>Часть III - Промежуточные вопросы и ответы</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter25_decorators/><span>25. Декораторы</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter26_lambda/><span>26. Лямбда</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter27_profiling/><span>27. Профилирование кода</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter28_testing/><span>28. Введение в тестирование</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter29_pip/><span>29. Установка пакетов</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/04-part_iv/><span>Часть IV - Советы, приемы и учебные пособия</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter30_configobj/><span>30. ConfigObj</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter31_lxml/><span>31. Парсинг XML с помощью lxml</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter32_pylint/><span>32. Анализ кода Python</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter33_requests/ class=bg-tertiary-bg><span>33. Пакет requests</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter34_sqlalchemy/><span>34. Пакет SQLAlchemy</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter35_virtualenv/><span>35. virtualenv</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter36_creating_modules_and_packages/><span>36. Создание модулей и пакетов</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/05-part_v/><span>Часть V - Упаковка и распространение</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter37_pypi_packaging/><span>37. Как добавить пакет в PyPI</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter38_eggs/><span>38. Python egg</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter39_wheels/><span>39. Python wheels</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter40_py2exe/><span>40. py2exe</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter41_bb_freeze/><span>41. bbfreeze</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter42_cx_freeze/><span>42. cx_Freeze</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter43_pyinstaller/><span>43. PyInstaller</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter44_creating_an_installer/><span>44. Создание программы установки</span></a></li><li><a href=https://romankurnovskii.com/ru/docs/python101/chapter45_hosting/><span>45. Хостинг Python приложения</span></a></li></ul></div></aside></div></div></div><div class="w-full md:w-3/4 lg:w-4/5 pb-2 pt-2 md:pt-2"><div class=flex><div class="w-full lg:w-3/4 px-6"><article class=prose><h1>33. Пакет requests</h1><div class="text-tertiary-text text-sm not-prose mt-2 flex flex-row flex-wrap items-center"><div class=me-4><i class="fa-regular fa-calendar-days"></i>
<span>Обновлено: 2023-01-12</span></div><div class=me-4><i class="fa-regular fa-clock"></i>
<span>3 мин</span></div><div id=page__views__div class="me-4 my-2"><i class="fa-regular fa-eye me-1"></i>
<span id=page__views></span></div></div><div class="block lg:hidden"><h3 class=text-lg>Содержание</h3><div class=break-words><nav id=TableOfContents><ul><li><a href=#использование-requests>Использование requests</a></li><li><a href=#как-отправить-веб-форму>Как отправить веб-форму</a></li><li><a href=#ресурсы>Ресурсы</a></li></ul></nav></div></div><p>Пакет <code>requests</code> - это более питоническая замена для собственного <code>urllib</code> в Python. API пакета requests во многом проще в работе.</p><h2 id=использование-requests>Использование requests</h2><p>Давайте рассмотрим несколько примеров использования пакета requests. Мы будем использовать серию небольших фрагментов кода, чтобы помочь объяснить, как использовать эту библиотеку.</p><pre><code class=language-python>&gt;&gt;&gt; r = requests.get(&quot;http://www.google.com&quot;)

</code></pre><p>Этот пример возвращает объект <strong>Response</strong>. Вы можете использовать методы объекта Response, чтобы узнать много нового о том, как можно использовать запросы. Давайте воспользуемся функцией <strong>dir</strong> в Python, чтобы посмотреть, какие методы нам доступны:</p><pre><code class=language-python>&gt;&gt;&gt; dir(r)
['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__',
'__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',
'__getstate__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__lt__',
'__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__',
'__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__',
'__weakref__', '_content', '_content_consumed', 'apparent_encoding', 'close',
'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history',
'iter_content', 'iter_lines', 'json', 'links', 'ok', 'raise_for_status', 'raw',
'reason', 'request', 'status_code', 'text', 'url']
</code></pre><p>Если вы запустите следующий метод, вы сможете увидеть исходный код веб-страницы:</p><pre><code class=language-python>&gt;&gt;&gt; r.text
</code></pre><p>Вывод этой команды слишком длинный, чтобы включать его в книгу, поэтому обязательно попробуйте сами. Если вы хотите взглянуть на заголовки веб-страниц, вы можете выполнить следующее:</p><pre><code class=language-python>&gt;&gt;&gt; r.headers
</code></pre><p>Обратите внимание, что атрибут <strong>headers</strong> возвращает диктоподобный объект и не является вызовом функции. Мы не показываем вывод, так как заголовки веб-страниц имеют тенденцию быть слишком широкими, чтобы правильно отображаться в книге. В объекте Response есть множество других замечательных функций и атрибутов. Например, вы можете получить cookies, ссылки на странице и status_code, который вернула страница.</p><p>Пакет requests поддерживает следующие типы HTTP-запросов: POST, GET, PUT, DELETE, HEAD и OPTIONS. Если страница возвращает json, вы можете получить к нему доступ, вызвав метод json объекта Response. Давайте рассмотрим практический пример.</p><h2 id=как-отправить-веб-форму>Как отправить веб-форму</h2><p>В этом разделе мы сравним отправку веб-формы с помощью requests и urllib. Давайте начнем с изучения того, как отправить веб-форму. Мы будем выполнять веб-поиск на сайте <strong>duckduckgo.com</strong> по термину <em>python</em> и сохранять результат в виде HTML-файла. Мы начнем с примера, в котором используется urllib:</p><pre><code class=language-python>import urllib.request
import urllib.parse
import webbrowser

data = urllib.parse.urlencode({'q': 'Python'})
url = 'http://duckduckgo.com/html/'
full_url = url + '?' + data
response = urllib.request.urlopen(full_url)
with open(&quot;results.html&quot;, &quot;wb&quot;) as f:
    f.write(response.read())

webbrowser.open(&quot;results.html&quot;)
</code></pre><p>Первое, что вам нужно сделать, когда вы хотите отправить веб-форму, - это выяснить, как называется форма и каков url, на который вы будете отправлять сообщение. Если вы перейдете на сайт duckduckgo и просмотрите источник, вы заметите, что его действие указывает на относительную ссылку &ldquo;/html&rdquo;. Таким образом, наш url будет &ldquo;<a href=http://duckduckgo.com/html%22>http://duckduckgo.com/html"</a>. Поле ввода имеет имя &ldquo;q&rdquo;, поэтому, чтобы передать duckduckgo поисковый запрос, мы должны конкатенировать url с полем &ldquo;q&rdquo;. Результаты считываются и записываются на диск. Теперь давайте выясним, чем отличается этот процесс при использовании пакета requests.</p><p>Пакет requests выполняет отправку форм немного более элегантно. Давайте посмотрим:</p><pre><code class=language-python>import requests

url = 'https://duckduckgo.com/html/'
payload = {'q':'python'}
r = requests.get(url, params=payload)
with open(&quot;requests_results.html&quot;, &quot;wb&quot;) as f:
    f.write(r.content)
</code></pre><p>При использовании запросов вам просто нужно создать словарь с именем поля в качестве ключа и поисковым термином в качестве значения. Затем вы используете <strong>requests.get</strong> для выполнения поиска. Наконец, вы используете полученный объект requests, &ldquo;r&rdquo;, и обращаетесь к его свойству content, которое сохраняете на диске.</p><h2 id=ресурсы>Ресурсы</h2><ul><li><a href=https://www.w3schools.com/python/module_requests.asp>https://www.w3schools.com/python/module_requests.asp</a></li></ul></article><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script><div class="-mx-2 mt-4 flex flex-col border-t border-b pb-1 px-2 pt-4 md:flex-row md:justify-between"><div><span class="text-primary-text block font-bold">Ранее</span>
<a href=https://romankurnovskii.com/ru/docs/python101/chapter32_pylint/ class=block>32. Анализ кода Python</a></div><div class="mt-4 md:mt-0 md:text-right"><span class="text-primary-text block font-bold">Далее</span>
<a href=https://romankurnovskii.com/ru/docs/python101/chapter34_sqlalchemy/ class=block>34. Пакет SQLAlchemy</a></div></div><div id=open_comments_block></div><script src=/opencomments.js renderdivid=open_comments_block server=https://eyt4njm3se.execute-api.eu-west-1.amazonaws.com/comments awstag defer></script></div><div class="hidden lg:block lg:w-1/4"><div class="bg-primary-bg prose sticky top-7 z-10 hidden
px-6 lg:block"><h3 class=text-lg>Содержание</h3></div><div class="sticky-toc hidden px-0 pb-6 lg:block"><nav id=TableOfContents><ul><li><a href=#использование-requests>Использование requests</a></li><li><a href=#как-отправить-веб-форму>Как отправить веб-форму</a></li><li><a href=#ресурсы>Ресурсы</a></li></ul></nav><div class="feedback-links mt-4 text-sm"><hr><div><a href=https://github.com/romankurnovskii/romankurnovskii.github.io/edit/main/content/docs/python101/chapter33_requests.ru.md title="Edit this page" target=_blank><i class="fas fa-edit me-1"></i>
<span>Редактировать страницу</span></a></div><div><a href="https://github.com/romankurnovskii/romankurnovskii.github.io/issues/new/?body=File:%20[/content/docs/python101/chapter33_requests.ru.md/]%28/https:/romankurnovskii.com/ru/docs/python101/chapter33_requests/%29" title="Request issue" target=_blank><i class="fas fa-check me-1" aria-hidden=true></i>
<span>Запросить изменения</span></a></div></div></div><script>window.addEventListener("DOMContentLoaded",()=>{enableStickyToc()})</script></div></div></div></div></div><script>document.addEventListener("DOMContentLoaded",()=>{changeSidebarHeight(),switchDocToc()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2022 <a href=https://romankurnovskii.com>Roman Kurnovskii</a> personal page
&#183;
<a href=https://romankurnovskii.com/ru/index.xml target=_blank>RSS</a>
&#183;
<a href=https://gohugo.io>Hugo</a> 9c1f119 2023-01-12</p></p></div><script src=/js/search.js?v5 languagemode=ru></script><div id=search-result tabindex=-1 class="overflow-y-auto overflow-x-hidden fixed top-0 right-0 left-0 z-50 max-w-xs" hidden><div class="relative p-4 w-full max-w-xs h-full md:h-auto"><div class="relative bg-white rounded-lg shadow dark:bg-gray-700"><div class=p-6><h3>Search results</h3><div id=search-results class=prose></div></div></div></div></div><script>window.store={"https://romankurnovskii.com/ru/docs/python101/01-intro/":{title:"Введение",tags:[],content:`Добро пожаловать в Python 101! Я написал эту книгу, чтобы помочь вам изучить Python 3. Она не претендует на роль исчерпывающего справочника. Напротив, ее цель - познакомить вас со строительными блоками Python, чтобы вы могли сами написать что-нибудь полезное. Многие учебники по программированию учат только языку, но не идут дальше этого. Я постараюсь не только ознакомить вас с основами, но и показать, как создавать полезные программы. Теперь вы можете задаться вопросом, почему простого изучения основ недостаточно. По моему опыту, когда я заканчиваю читать вводный текст, мне хочется что-то создать, но я не знаю как! У меня есть знания, но нет клея, чтобы добраться из точки А в точку Б. Я считаю, что важно не только научить вас основам, но и охватить материал среднего уровня.
Таким образом, эта книга будет состоять из пяти частей:
В первой части будут рассмотрены основы Python. Вторая часть будет посвящена небольшому подмножеству стандартной библиотеки Python Третья часть - материал среднего уровня Четвертая часть будет представлять собой серию небольших уроков Пятая часть будет посвящена упаковке и распространению Python. Позвольте мне потратить несколько минут на объяснение того, что предлагает каждая часть. В первой части мы рассмотрим следующее:
Типы Python (строки, списки, массивы и т.д.) Условные операторы Циклы Понимание списков и словарей Обработка исключений Файловый ввод-вывод Функции и классы Во второй части мы поговорим о некоторых элементах стандартной библиотеки Python. Стандартная библиотека - это то, что поставляется в комплекте с Python. Она состоит из модулей, которые вы можете импортировать для получения дополнительной функциональности. Например, вы можете импортировать модуль math, чтобы получить некоторые математические функции высокого уровня. Я буду выбирать модули, которые чаще всего использую в повседневной работе, и объяснять, как они работают. Причина, по которой я считаю это хорошей идеей, заключается в том, что это обычные, повседневные модули, о которых, я думаю, вам будет полезно знать в начале вашего обучения Python. В этом разделе также будут рассмотрены различные способы установки модулей сторонних производителей. Наконец, я расскажу о том, как создавать собственные модули и пакеты и почему вы захотите сделать это в первую очередь. Вот некоторые модули, которые мы рассмотрим:
csv ConfigParser логирование os smtplib / email подпроцесс sys поток / очереди time / datetime В третьей части мы рассмотрим промежуточные вопросы. Это темы, которые полезно знать, но не обязательно уметь программировать на Python. Будут рассмотрены следующие темы:
отладчик Python (pdb) декораторы лямбда-функция профилирование кода введение в тестирование Четвертая часть будет состоять из небольших уроков, которые помогут вам научиться использовать Python на практике. Таким образом, вы научитесь создавать программы на Python, которые действительно могут сделать что-то полезное! Знания, полученные из этих уроков, вы сможете использовать для создания собственных скриптов. В конце каждого урока будут приведены идеи по усовершенствованию этих мини-приложений, так что у вас будет то, что вы сможете опробовать самостоятельно. Вот несколько пакетов сторонних разработчиков, которые мы будем рассматривать:
pip и easy_install configobj lxml requests virtualenv pylint / pychecker SQLAlchemy В пятой части мы расскажем о том, как взять свой код и передать его своим друзьям, семье и всему миру! Вы узнаете следующее:
Как превратить ваши многократно используемые скрипты в Python \u0026ldquo;eggs\u0026rdquo;, \u0026ldquo;wheels\u0026rdquo; и многое другое. Как загрузить свое творение в Python Package Index (PyPI) Как создавать двоичные исполняемые файлы, чтобы вы могли запускать свое приложение без Python Как создать программу установки для вашего приложения Главы и разделы могут быть не одинаковой длины. Хотя каждая тема будет хорошо освещена, не каждая тема требует одинакового количества страниц.
Краткая история языка Python Я думаю, это поможет узнать предысторию языка программирования Python. Python был создан в конце 1980-х годов. Все согласны с тем, что его создателем является Гвидо ван Россум, он написал его как преемника языка программирования ABC, которым он пользовался. Гвидо назвал язык в честь одного из своих любимых комедийных артистов: Монти Пайтон. Язык был выпущен только в 1991 году, и за это время он сильно вырос в плане количества включенных модулей и пакетов. На момент написания этой статьи существует две основные версии Python: серия 2.x и 3.x (иногда известная как Python 3000) . Серия 3.x не имеет обратной совместимости с 2.x, поскольку при создании 3.x была идея избавиться от некоторых идиосинкразий оригинала. Текущими версиями являются 2.7.12 и 3.5.2. Большинство функций из 3.x были перенесены в 2.x; однако 3.x получает большую часть текущих разработок Python, так что это версия будущего.
Некоторые люди думают, что Python предназначен только для написания небольших скриптов для склеивания \u0026ldquo;настоящего\u0026rdquo; кода, как C++ или Haskell. Однако вы найдете Python полезным практически в любой ситуации. Python используют многие известные компании, такие как Google, NASA, LinkedIn, Industrial Light \u0026amp; Magic и многие другие. Python используется не только в бэкенде, но и в фронтенде. Если вы новичок в области компьютерных наук, то бэкенд-программирование - это то, что находится за кулисами; такие вещи, как обработка баз данных, создание документов и т.д. Фронтенд-программирование - это красивые вещи, с которыми знакомо большинство пользователей, например, веб-страницы или пользовательские интерфейсы для настольных компьютеров. Например, есть несколько действительно хороших наборов инструментов графического интерфейса Python, таких как wxPython, PySide и Kivy. Есть также несколько веб-фреймворков, таких как Django, Pyramid и Flask. Вы возможно удивитесь, узнав, что Django используется для Instagram и Pinterest. Если вы пользовались этими или другими сайтами, то вы использовали что-то, работающее на Python, даже не подозревая об этом!
Об авторе Возможно, вам интересно, кто я такой и почему я могу быть достаточно осведомлен о Python, чтобы писать о нем, поэтому я решил дать вам немного информации о себе. Я начал программировать на Python весной 2006 года по работе. Моим первым заданием было перенести сценарии входа в Windows с Kixtart на Python. Вторым моим проектом был перенос кода VBA (по сути, графического интерфейса поверх продуктов Microsoft Office) на Python, так я впервые познакомился с wxPython. С тех пор я использую Python, занимаясь различными видами бэкэнд-программирования и фронтэнд-интерфейсами для настольных компьютеров.
Я понял, что один из способов запомнить, как делать определенные вещи на Python, - это писать о них, и так появился мой блог о Python: http://www.blog.pythonlibrary.org/. По мере того как я писал, я получал отзывы от читателей, и в итоге я расширил блог, включив в него советы, учебники, новости Python и обзоры книг по Python. Я регулярно сотрудничаю с издательством Packt Publishing в качестве технического рецензента, что означает, что я стараюсь проверять книги на наличие ошибок до их публикации. Я также писал для сайтов Developer Zone (DZone) и i-programmer, а также для Python Software Foundation. В ноябре 2013 года DZone опубликовал книгу The Essential Core Python Cheat Sheet, которую я написал в соавторстве.
Условные обозначения Как и большинство технических книг, эта включает в себя несколько условностей, о которых вам необходимо знать. Новые темы и терминология будут выделены жирным шрифтом. Вы также увидите несколько примеров, которые выглядят следующим образом:
\u0026gt;\u0026gt;\u0026gt; myString = \u0026quot;Welcome to Python!\u0026quot; Символ \u0026raquo;\u0026gt; - это символ подсказки Python. Вы увидите его в интерпретаторе Python и в IDLE. Подробнее о каждом из них вы узнаете в первой главе. Другие примеры кода будут показаны аналогичным образом, но без символа \u0026raquo;\u0026gt;.
Требования Вам потребуется рабочая установка Python 3. Большинство машин Linux и Mac поставляются с уже установленным Python. Однако если у вас нет Python, вы можете загрузить его копию с сайта http://python.org/download/. На сайте есть актуальные инструкции по установке, поэтому в этой книге я не буду приводить никаких инструкций по установке. Любые дополнительные требования будут объяснены позже в книге.
Обратная связь с читателями Я приветствую отзывы о моих работах. Если вы хотите сообщить мне, что вы думаете о книге, вы можете отправить комментарии по следующему адресу:
comments@pythonlibrary.org
Ошибки Я изо всех сил стараюсь не публиковать свои работы с ошибками, но время от времени это случается. Если вы заметили ошибку в этой книге, не стесняйтесь сообщить мне об этом, написав по следующему адресу:
errata@pythonlibrary.org
`,url:"https://romankurnovskii.com/ru/docs/python101/01-intro/"},"https://romankurnovskii.com/ru/docs/python101/01-part_i/":{title:"Часть I - Основы",tags:[],content:`В первой части мы изучим основы языка программирования Python. Этот раздел книги должен подготовить вас к использованию всех строительных блоков Python, чтобы вы были готовы уверенно взяться за следующие разделы.
Давайте рассмотрим, что мы будем изучать:
IDLE Строки Списки, словари и кортежи Условные операторы Циклы Генераторы Обработка исключений Файловый ввод-вывод Импорт модулей и пакетов Функции Классы В первой главе этого раздела вы познакомитесь со встроенной средой разработки Python, которая называется IDLE. В следующих двух главах мы рассмотрим некоторые типы Python, такие как строки, списки и словари. После этого мы рассмотрим условные операторы в Python и такие циклы Python как for и while.
Во второй половине этого раздела мы рассмотрим генераторы, такие как генераторы списков и словарей. Затем мы рассмотрим возможности Python по обработке исключений и работу Python с файлами. Далее мы узнаем, как импортировать готовые модули и пакеты. Последние две главы посвящены функциям и классам Python.
`,url:"https://romankurnovskii.com/ru/docs/python101/01-part_i/"},"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/":{title:"AWS Certified Developer (DVA-C01 -\u003e DVA-C02)",tags:["AWS"],content:`TL;DR Passed exam in one month. Created an app with questions and progress that helped me a lot Note The AWS Certified Developer - Associate exam is changing February 28, 2023. The last date to take the current exam is February 27, 2023.
To keep the docs up to date I will add new and latest information.
DVA-C01 vs DVA-C02 new domain: Domain 3: Deployment focus will be on testing and deploying your code into different environments including development, test, and production environments. You’ll need to know how CloudFormation, the AWS Cloud Development Kit (CDK), and AWS SAM are used to deploy applications. Domains 4 and 5 (“Refactoring” along with “Monitoring and Troubleshooting”) from the DVA-C01 exam guide have been consolidated into Domain 4 (“Troubleshooting and Optimization”) in the DVA-C02 exam guide Plus 2% questions in Development with AWS Services domain Test questions for DVA-C02 - here and here
Criteria In order to pass the exam, you must score more than 720/1000 (unspecified) points. Criterion will be a minimum threshold of 75/100%, unless conditions change in preparation.
Study Plan Find out what the exam requirements are Have a list of topics that will be on the exam Practice each service for comprehension Read extra theory that will not be covered during practice Go through the test general questions Repeat 3-5 repeat until the result of failed block greater than 80 points Entrypoint:
AWS Certified Developer Exam Information Prepare The AWS website has:
Exam Preparation Guide DVA-C01 From 27 Feb 2023 Exam Preparation Guide DVA-C02 To pass the exam, you need to know certain services from the 4 domains: Development with AWS Services, Security, Deployment, Refactoring, Monitoring and Troubleshooting
List of services on the exam Version 2.1 DVA-C01 Version 1.0 DVA-C02
Analytics:
Amazon Athena (new in DVA-C02) Amazon OpenSearch Service (Amazon Elasticsearch Service) Amazon Kinesis Application Integration:
AWS AppSync (new in DVA-C02) Amazon EventBridge (Amazon CloudWatch Events) Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Compute:
Amazon EC2 AWS Elastic Beanstalk AWS Lambda AWS Serverless Application Model (AWS SAM) (new in DVA-C02) Containers:
AWS Copilot (new in DVA-C02) Amazon Elastic Container Registry (Amazon ECR) Amazon Elastic Container Service (Amazon ECS) Amazon Elastic Kubernetes Services (Amazon EKS) Database:
Amazon Aurora (new in DVA-C02) Amazon DynamoDB Amazon ElastiCache Amazon MemoryDB for Redis (new in DVA-C02) Amazon RDS Developer Tools:
AWS Amplify (new in DVA-C02) AWS Cloud9 (new in DVA-C02) AWS CloudShell (new in DVA-C02) AWS CodeArtifact AWS CodeBuild AWS CodeCommit AWS CodeDeploy Amazon CodeGuru AWS CodePipeline AWS CodeStar AWS Fault Injection Simulator AWS X-Ray Management and Governance:
AWS AppConfig (new in DVA-C02) AWS Cloud Development Kit (AWS CDK) (new in DVA-C02) AWS CloudFormation AWS CloudTrail (new in DVA-C02) Amazon CloudWatch Amazon CloudWatch Logs (new in DVA-C02) AWS Command Line Interface (AWS CLI) (new in DVA-C02) AWS Systems Manager (new in DVA-C02) Networking and Content Delivery:
Amazon API Gateway Amazon CloudFront Elastic Load Balancing Amazon Route 53 (new in DVA-C02) Amazon VPC (new in DVA-C02) Security, Identity, and Compliance:
AWS Certificate Manager (ACM) (new in DVA-C02) AWS Certificate Manager Private Certificate Authority (new in DVA-C02) Amazon Cognito AWS Identity and Access Management (IAM) AWS Key Management Service (AWS KMS) AWS Secrets Manager (new in DVA-C02) AWS Security Token Service (AWS STS) (new in DVA-C02) AWS WAF (new in DVA-C02) Storage:
Amazon Elastic Block Store (Amazon EBS) (new in DVA-C02) Amazon Elastic File System (Amazon EFS) (new in DVA-C02) Amazon S3 Amazon S3 Glacier (new in DVA-C02) Training plan Opened a training plan for any tutorial to understand where to start learning. Have chosen cloudacademy service (but for example FreeCodeCamp has a free course with content).
Another option is to use free AWS Workshops
AWS Developer - Associate (DVA-C01) Certification Preparation
Don\u0026rsquo;t see coverage of the following services, so I add them to the block when related topics are covered:
Analytics:
Amazon Elasticsearch Service (Amazon ES) -\u0026gt; OpenSearch Service Developer Tools:
AWS CodeArtifact AWS Fault Injection Simulator My roadmap The following is my roadmap for the study. There may be adjustments.
AWS Identity and Access Management (IAM) Amazon EC2 AWS Elastic Beanstalk AWS Lambda Amazon S3 Amazon DynamoDB Amazon ElastiCache Amazon RDS Amazon API Gateway Amazon CloudFront Elastic Load Balancing (ELB) Amazon Kinesis Amazon OpenSearch Service (Amazon Elasticsearch Service) Amazon CloudWatch AWS CloudFormation AWS CodeCommit AWS CodeDeploy AWS CodeBuild AWS CodePipeline Amazon CodeGuru AWS CodeStar AWS CodeArtifact AWS X-Ray AWS Fault Injection Simulator Amazon Elastic Container Registry (Amazon ECR) Amazon Elastic Container Service (Amazon ECS) AWS Fargate Amazon Elastic Kubernetes Services (Amazon EKS) Amazon Cognito Route 53 AWS Key Management Service (AWS KMS) Amazon EventBridge (Amazon CloudWatch Events) Amazon Simple Notification Service (Amazon SNS) Amazon Simple Queue Service (Amazon SQS) AWS Step Functions Resources AWS Certified Developer A brief overview of the official documentation Exam Preparation Guide Sample Exam Questions https://github.com/itsmostafa/certified-aws-developer-associate-notes https://github.com/arnaudj/mooc-aws-certified-developer-associate-2020-notes FreeCodeCamp Youtube - AWS Certified Developer - Associate 2020 How-To Labs from AWS AWS Ramp-Up guides: Downloadable AWS Ramp-Up Guides offer a variety of resources to help you build your skills and knowledge of the AWS Cloud. Coursera\u0026rsquo;s AWS Courses(Free to enroll via audit): AWS also provides various specializations in partnership with coursera AWS Architecture center: Provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more. This expert guidance was contributed by cloud architecture experts from AWS, including AWS Solutions Architects, Professional Services Consultants, and Partners. AWS Whitepapers: Expand your knowledge of the cloud with AWS technical content authored by AWS and the AWS community, including technical whitepapers, technical guides, reference material, and reference architecture diagrams. Back to Basics: Back to Basics\u0026rsquo; is a video series that explains, examines, and decomposes basic cloud architecture pattern best practices. AWS Heroes Content Library: AWS Hero authored content including blogs, videos, slide presentations, podcasts, and more. https://amazon.qwiklabs.com/catalog AWS Workshops: This website lists workshops created by the teams at Amazon Web Services (AWS). Workshops are hands-on events designed to teach or introduce practical skills, techniques, or concepts which you can use to solve business problems. https://wellarchitectedlabs.com/ https://testseries.edugorilla.com/tests/1359/aws-certified-developer-associate Community posts https://dev.to/romankurnovskii/aws-certified-developer-associate-prepare-2np https://www.reddit.com/user/romankurnovskii/comments/x8rgig/what_is_the_topics_order_to_cover_to_get_prepared/?utm_source=share\u0026amp;utm_medium=web2x\u0026amp;context=3 https://twitter.com/romankurnovskii/status/1567746601136832512 `,url:"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/"},"https://romankurnovskii.com/ru/docs/python101/chapter1_idle/":{title:"1. Программирование IDLE",tags:[],content:`Глава 1 - Программирование IDLE Использование IDLE Python поставляется с собственным редактором кода: IDLE (Integrated Development and Learning Environment). Существует предание, что название IDLE происходит от имени Эрика Айдла, актера из \u0026ldquo;Монти Пайтона\u0026rdquo;. IDE - это редактор для программистов, который обеспечивает цветную подсветку ключевых слов языка, автозаполнение, \u0026ldquo;экспериментальный\u0026rdquo; отладчик и множество других интересных вещей. Вы можете найти IDE к большинству популярных языков, а некоторые IDE работают с несколькими языками.
IDLE - это своего рода легкая IDE, но в ней есть все перечисленные элементы. Она позволяет программисту писать на Python и отлаживать свой код довольно просто. Причина, по которой я называю её \u0026ldquo;легким\u0026rdquo;, заключается в том, что отладчик очень базовый. В нем отсутствуют другие функции, которые программисты, имеющие опыт работы с такими продуктами, как Visual Studio, пропустят. Вам также будет интересно узнать, что IDLE был создан с помощью Tkinter, набора инструментов графического интерфейса Python, который поставляется вместе с Python.
Чтобы открыть IDLE, вам нужно найти его, и вы увидите что-то вроде этого:
Да, это оболочка Python, в которой вы можете набирать короткие сценарии и сразу же видеть их вывод, и даже взаимодействовать с кодом в реальном времени. Компиляция кода не требуется, так как Python является интерпретируемым языком и выполняется в интерпретаторе Python. Давайте сейчас напишем вашу первую программу. Введите следующее после командной строки (\u0026raquo;\u0026gt;) в IDLE:
print(\u0026quot;Hello from Python!\u0026quot;) Вы только что написали свою первую программу! Все, что делает ваша программа, это записывает строку на экран, но позже вы увидите как это полезно.
В Python 3 оператор print превратился в функцию print, поэтому скобки необходимы. Что такое функции, вы узнаете в главе 10.
Если вы хотите сохранить свой код в файл, зайдите в меню Файл и выберите Новое окно (или нажмите CTRL+N). Теперь вы можете набрать свою программу и сохранить ее здесь. Главное преимущество использования оболочки Python заключается в том, что вы можете экспериментировать с небольшими фрагментами кода, сразу видеть как он себя поведёт, прежде чем поместите его в реальную программу. Экран редактора кода выглядит немного иначе, чем на скриншоте IDLE выше:
Теперь мы уделим немного времени рассмотрению других полезных возможностей IDLE.
Python поставляется с большим количеством модулей и пакетов, которые можно импортировать для добавления новых возможностей. Например, вы можете импортировать модуль math для всех видов хороших математических функций, таких как квадратные корни, косинусы и т.д. В меню File вы найдете Path Browser, который пригодится для того, чтобы понять, где Python ищет импорт модулей. Python сначала ищет в том же каталоге, что и запущенный скрипт, чтобы узнать, есть ли там файл, который нужно импортировать. Затем он проверяет предопределенный список других мест. Вы можете добавлять и удалять эти места. Браузер путей покажет вам, где эти файлы находятся на вашем жестком диске, если вы что-то импортировали. Мой Path Browser выглядит следующим образом:
Далее находится Class Browser, который поможет вам ориентироваться в коде. Честно говоря, было бы логичнее назвать этот пункт меню \u0026ldquo;Браузер модулей\u0026rdquo;, так как это гораздо ближе к тому, что вы будете делать на самом деле. В действительности это то, что не очень полезно для вас сейчас, но будет полезно в будущем. Когда у вас много строк кода в одном файле это очень поможет, дав \u0026ldquo;древовидный\u0026rdquo; интерфейс. Обратите внимание, что вы не сможете загрузить Class Browser, пока не сохраните свою программу.
Меню Edit содержит типичные функции, такие как Копировать, Вырезать, Вставить, Отменить, Повторить и Выбрать все. Оно также содержит различные способы поиска кода и поиска и замены. Наконец, в меню Правка есть несколько пунктов, которые показывают вам различные вещи, такие как выделение круглых скобок или отображение списка автозаполнения.
Меню Format содержит множество полезных функций. В нем есть несколько полезных пунктов, позволяющих делать отступы и вычеты * в коде, а также комментировать код. Это довольно полезно при тестировании кода. Комментирование кода может быть очень полезным, когда у вас много кода и вам нужно выяснить, почему он работает неправильно. Комментирование части кода и повторный запуск скрипта могут помочь вам понять, где вы ошиблись. Вы просто медленно продвигаетесь по пути, не комментируя ничего, пока не найдете ошибку. Это напомнило мне о том, что вы, возможно, заметили, что на главном экране IDLE есть меню Debugger.
Это удобно для отладки, но только в окне Shell. К сожалению, вы не можете использовать отладчик в главном меню редактирования. Однако вы можете запустить модуль с включенной отладкой так, чтобы иметь возможность взаимодействовать с объектами вашей программы. Это будет полезно, например, в циклах, где вы пытаетесь определить текущее значение элемента внутри цикла. Если вы используете tkinter для создания пользовательского интерфейса (UI), вы можете не включать вызов mainloop() (который может блокировать UI), чтобы иметь возможность отлаживать пользовательский интерфейс. Наконец, если при запущенном отладчике возникает исключение, вы можете дважды щелкнуть по нему, чтобы перейти непосредственно к коду, в котором произошло исключение.
Если вам нужен более универсальный отладчик, вам следует либо найти другую IDE, либо попробовать отладчик Python, находящийся в библиотеке pdb.
Что такое комментарии? Комментарий - это способ оставить неисполняемый код, который документирует то, что вы делаете в своем коде. Каждый язык программирования использует различные символы для обозначения начала и конца комментария. Как выглядят комментарии в Python? Комментарий - это все, что начинается с восьмеричного символа (т.е. знака хеша или фунта). Ниже приведен пример некоторых комментариев в действии:
# This is a comment before some code print(\u0026quot;Hello from Python!\u0026quot;) print(\u0026quot;Winter is coming\u0026quot;) # this is an in-line comment Комментарии можно писать в строке сами по себе или после оператора, как, например, во втором операторе print выше. Интерпретатор Python игнорирует комментарии, поэтому вы можете писать в них все, что захотите. Большинство программистов, с которыми я встречался, не очень часто используют комментарии. Однако я настоятельно рекомендую использовать комментарии не только для себя, но и для всех остальных, кому в будущем придется поддерживать или улучшать ваш код. Я понял, что собственные комментарии полезны, когда вернулся к сценарию, который я написал 6 месяцев назад. Обнаружив, что работаю с не закомментированным кодом, я жалел, что не написал комментариев раньше, чтобы быстрее разобраться в коде сейчас.
Примерами хороших комментариев могут быть пояснения к сложным утверждениям кода или добавление пояснений к аббревиатурам в коде. Иногда вам нужно оставить комментарий, чтобы объяснить, почему вы сделали что-то определенным образом, потому что это просто не очевидно*.
Теперь нам нужно вернуться к рассмотрению опций меню IDLE:
В меню Run есть несколько удобных опций. С его помощью можно вызвать оболочку Python Shell, проверить код на наличие ошибок или запустить его. В меню Options не так много пунктов. В нем есть пункт Настроить, который позволяет изменить цвет подсветки кода, шрифт и сочетания клавиш. Кроме того, есть опция Code Context, которая полезна тем, что помещает в окно редактирования накладку, показывающую, в каком классе или функции вы сейчас находитесь. Мы будем объяснять функции и классы в конце первой части. Вы увидите как эта функция полезна, когда у в функции много кода, а название прокручивается за пределы верхней части экрана. При включенной опции этого не произойдет. Конечно, если функция слишком велика, чтобы поместиться на одном экране, то, возможно, она стала слишком длинной и пора разбить её на несколько функций. Еще один интересный пункт в диалоге настроек находится на вкладке General, где вы можете добавить другую документацию. Это означает, что вы можете добавить URL-адреса к документации сторонних разработчиков, например, SQLAlchemy или pillow, и получить ее в IDLE. Чтобы получить доступ к новой документации, просто перейдите в меню Help.
Меню Windows показывает список открытых в данный момент окон и позволяет переключаться между ними.
Последнее, но не менее важное меню - это меню Help, где вы можете узнать об IDLE, получить помощь по работе с самой IDLE или загрузить локальную копию документации по Python. Документация объясняет, как работает каждый элемент Python, и является довольно исчерпывающей в своем охвате. Меню Help, вероятно, наиболее полезно тем, что вы можете получить доступ к документации, даже если вы не подключены к Интернету. Вы можете искать в документации, находить HOWTO, читать о любой из встроенных библиотек и узнавать столько нового, что у вас голова начнет кружиться.
Другие советы Когда вы увидите примеры кода в следующих главах, вы можете написать и запустить их в IDLE. Я писал все свои программы в IDLE первые пару лет своей жизни программиста на Python и был вполне доволен этим. Однако существует множество бесплатных IDE для Python и несколько IDE, за которые нужно платить. Если вы хотите обойтись дешевой, вам стоит обратить внимание на Eclipse+PyDev, VSCode, Atom, Sublime Text или даже Notepad++. Из платных - PyCharm, но есть и ознакомительная версия. Они имеют гораздо больше возможностей, таких как интеграция с репозиториями кода, лучшие отладчики, помощь в рефакторинге и т.д.
Visual Studio Code с набором отличных плагинов - самый универсальный и бесплатный инструмент.
В этой книге мы будем использовать IDLE в наших примерах, потому что она поставляется вместе с Python и обеспечивает общий тестовый стенд. Я по-прежнему считаю, что IDLE имеет лучшую, наиболее последовательную подсветку кода из всех IDE, которые я использовал. Подсветка кода важна, на мой взгляд, потому что она помогает избежать использования одного из ключевых слов Python (или встроенных модулей) для имени переменной. Если вам интересно, вот список этих ключевых слов:
and del from not while as elif global or with assert else if pass yield break except import print class exec in raise continue finally is return def for lambda try Давайте посмотрим, что произойдет, когда мы напечатаем несколько вещей на языке Python:
Как вы можете видеть, IDLE все кодирует цветом. Ключевое слово имеет пурпурный цвет, строка текста - зеленый, комментарий - красный, а вывод функции печати - синий.
Подведение итогов В этой главе мы узнали, как использовать интегрированную среду разработки Python, IDLE. Мы также узнали, что такое комментарии и как их использовать. На данном этапе вы должны быть достаточно знакомы с IDLE, чтобы использовать ее в остальных частях этой книги. Существует множество других интегрированных сред разработки (IDE) для Python.
На данный момент мы готовы двигаться дальше и начать изучать различные типы данных Python. В следующей главе мы начнем со строк.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter1_idle/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day01/":{title:"1. DevOps - общее представление",tags:[],content:`Введение - День 1 Первый день из 90, чтобы получить хорошее базовое понимание DevOps и инструментов.
Этот путь обучения начался для меня несколько лет назад, но тогда я сосредоточился на платформах виртуализации и облачных технологиях. В основном я изучал инфраструктуру как код и управление конфигурацией приложений с помощью Terraform и Chef.
Перенесемся в март 2021 года. Мне представилась прекрасная возможность сосредоточить свои усилия на стратегии Cloud Native в Kasten by Veeam. Это должно было стать огромным фокусом на Kubernetes и DevOps, а также на сообществе, окружающем эти технологии. Я начал свое обучение и быстро понял, что помимо изучения основ Kubernetes и контейнеризации существует очень широкий мир, и именно тогда я начал общаться с сообществом и узнавать все больше и больше о культуре, инструментах и ​​​​процессах DevOps, поэтому я начал публично документировать некоторые области, которые я хотел изучить.
Начнем наше путешествие Если вы прочитаете приведенный выше блог, вы увидите, что это содержание высокого уровня для моего учебного пути, и я скажу, что на данный момент я не являюсь экспертом ни в одном из этих разделов, но я хотел поделиться некоторыми БЕСПЛАТНЫМИ ресурсами. а некоторые платные, но вариант для обоих, так как у всех разные обстоятельства.
В течение следующих 90 дней я хочу задокументировать эти ресурсы и охватить эти основополагающие области. Я бы хотел, чтобы сообщество также приняло участие, поделилось своим путешествием и ресурсами, чтобы мы могли учиться публично и помогать друг другу.
Из начального файла readme в репозитории проекта вы увидите, что я разделил все на разделы, и в основном это 12 недель плюс 6 дней. Первые 6 дней мы будем изучать основы DevOps в целом, прежде чем погрузиться в некоторые конкретные области, этот список ни в коем случае не является исчерпывающим, и мы снова будем рады, если сообщество поможет сделать этот ресурс полезным.
Еще один ресурс, которым я поделюсь на этом этапе, который, я думаю, каждый должен внимательно изучить и, возможно, создать свою собственную карту ума для себя, своих интересов и позиции:
DevOps Roadmap
Я нашел это отличным ресурсом, когда создавал свой первоначальный список и сообщение в блоге по этой теме. Вы также можете заметить, что помимо 12 тем, которые я перечислил здесь, в этом репозитории, есть и другие разделы, требующие более подробного рассмотрения.
Первые шаги - или что такое DevOps? Есть так много статей в блогах и видео на YouTube, которые можно перечислить здесь, но поскольку мы начинаем 90-дневное испытание и сосредоточиваемся на том, чтобы тратить около часа в день на изучение чего-то нового или о DevOps, я подумал, что было бы хорошо получить некоторые из высокого уровня «что такое DevOps» для начала.
Во-первых, DevOps — это не инструмент. Вы не можете купить его, это не номер программного обеспечения или репозиторий GitHub с открытым исходным кодом, который вы можете скачать. Это также не язык программирования, это также не какая-то магия темного искусства.
DevOps — это способ делать более разумные вещи в разработке программного обеспечения. - Подождите\u0026hellip; Но если вы не разработчик программного обеспечения, вы должны отвернуться прямо сейчас и не погрузиться в этот проект??? Нет, совсем нет, оставайтесь\u0026hellip; Потому что DevOps объединяет разработку программного обеспечения и эксплуатацию. Ранее я упоминал, что больше занимаюсь виртуальными машинами, и это, как правило, относится к сфере эксплуатации, но в сообществе есть люди с самым разным опытом, и DevOps на 100 % принесет пользу отдельным лицам, разработчикам, специалистам по эксплуатации и Все инженеры по контролю качества могут в равной степени изучить эти передовые методы, лучше разбираясь в DevOps.
DevOps — это набор практик, которые помогают достичь цели этого движения: сократить время между фазой создания идеи продукта и его выпуском в производство для конечного пользователя или кого бы то ни было, внутренней команды или клиента.
Еще одна область, в которую мы углубимся в первую неделю, касается Методологии Agile. DevOps и Agile широко применяются вместе для обеспечения непрерывной доставки вашего Приложения.
Главный вывод заключается в том, что образ мышления или культура DevOps позволяют сократить затянувшийся процесс выпуска программного обеспечения с потенциально многих лет до возможности более частого выпуска небольших выпусков. Другой ключевой принцип, который следует здесь усвоить, заключается в том, что речь идет о разрушении разрозненности между командами, о которых я упоминал ранее, разработчиками, эксплуатацией и контролем качества.
С точки зрения DevOps, разработка, тестирование и развертывание выполняются командой DevOps.
Последнее, что я хотел бы сделать, чтобы сделать это максимально эффективным и действенным, мы должны использовать автоматизацию
Источники Я всегда открыт для добавления дополнительных ресурсов в эти файлы readme, поскольку они здесь в качестве учебного пособия.
Мой совет — посмотрите все ссылки ниже, и, надеюсь, вы тоже что-то почерпнули из текста и объяснений выше.
DevOps in 5 Minutes What is DevOps? Easy Way DevOps roadmap 2022 | Success Roadmap 2022 `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day01/"},"https://romankurnovskii.com/ru/docs/webrtc/media-devices/":{title:"Мультимедиа-устройства",tags:[],content:`Мультимедиа-устройства Начало работы с мультимедийными устройствами При web-разработке WebRTC-стандарт предоставляет API для доступа к камерам и микрофонам, подключенным к компьютеру или смартфону. Эти устройства обычно называются мультимедийными устройствами, и к ним можно получить доступ с помощью Java-скрипта через объект navigator.mediaDevices, который реализует интерфейс MediaDevices. С помощью этого объекта мы можем просмотреть все подключенные устройства, отслеживать изменения статуса устройства (когда устройство подключается или отключается) и открывать устройство для извлечения мультимедийного потока (см. ниже). Чаще всего для этого используют функцию getUserMedia(), которая возвращает промис, который будет преобразован в MediaStream для соответствующих мультимедийных устройств. Эта функция принимает один объект MediaStreamConstraints, который определяет имеющиеся требования. Например, чтобы просто открыть микрофон и камеру по умолчанию, мы должны сделать следующее:
Через промисы:
const constraints = { 'video': true, 'audio': true } navigator.mediaDevices.getUserMedia(constraints) .then(stream =\u0026gt; { console.log('Got MediaStream:', stream); }) .catch(error =\u0026gt; { console.error('Error accessing media devices.', error); }); Через async/await
const openMediaDevices = async (constraints) =\u0026gt; { return await navigator.mediaDevices.getUserMedia(constraints); } try { const stream = openMediaDevices({'video':true,'audio':true}); console.log('Got MediaStream:', stream); } catch(error) { console.error('Error accessing media devices.', error); } Обращение к getUserMedia() запускает запрос на разрешение. Если пользователь одобряет запрос, промис разрешает MediaStream, содержащий одну видео и одну аудио дорожку. Если запрос отклонен, появляется PermissionDeniedError. Если же нет подключенных устройств, появляется NotFoundError. Полный список API для интерфейса MediaDevices доступен по ссылке https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices
Обращение к мультимедиа-устройствам В более сложных приложениях, мы скорее всего захотим проверить все подключенные камеры и микрофоны и дать соответствующий отчет пользователю. Это можно сделать через запрос функции enumerateDevices(). Она возвращает промис, который преобразуется в массив MediaDevicesInfo, описывающий каждое известное мультимедиа-устройство. Через него мы можем предоставить пользовательский интерфейс пользователю, который позволит выбрать те или иные устройства. Каждый список MediaDevicesInfo содержит свойства, которые называются kind с значениями audioinput, audiooutput или videoinput, отражая, какой это тип мультимедиа-устройства.
Через промисы
function getConnectedDevices(type, callback) { navigator.mediaDevices.enumerateDevices() .then(devices =\u0026gt; { const filtered = devices.filter(device =\u0026gt; device.kind === type); callback(filtered); }); } getConnectedDevices('videoinput', cameras =\u0026gt; console.log('Cameras found', cameras)); через async/await
async function getConnectedDevices(type) { const devices = await navigator.mediaDevices.enumerateDevices(); return devices.filter(device =\u0026gt; device.kind === type) } const videoCameras = getConnectedDevices('videoinput'); console.log('Cameras found:', videoCameras); Отслеживание изменений в статусах устройств Большинство компьютеров поддерживают подключение различных устройств прямо во время работы. Это может быть веб-камера, подключенная через USB, Bluetooth-гарнитура или внешние динамики. Чтобы должным образом поддерживать все это, веб-приложение должно отслеживать изменения в статусах мультимедиа-устройств. Это можно сделать, добавив «отслеживатель» в navigator.mediaDevices для события devicechange.
// Updates the select element with the provided set of cameras function updateCameraList(cameras) { const listElement = document.querySelector(‘select#availableCameras’); listElement.innerHTML = ‘’; cameras.map(camera =\u0026gt; { const cameraOption = document.createElement(‘option’); cameraOption.label = camera.label; cameraOption.value = camera.deviceId; }).forEach(cameraOption =\u0026gt; listElement.add(cameraOption)); } // Fetch an array of devices of a certain type async function getConnectedDevices(type) { const devices = await navigator.mediaDevices.enumerateDevices(); return devices.filter(device =\u0026gt; device.kind === type) } // Get the initial set of cameras connected const videoCameras = getConnectedDevices(‘videoinput’); updateCameraList(videoCameras); // Listen for changes to media devices and update the list accordingly navigator.mediaDevices.addEventListener(‘devicechange’, event =\u0026gt; { const newCameraList = getConnectedDevices(‘video’); updateCameraList(newCameraList); }); Ограничения для мультимедиа Объект ограничений, осуществляющий интерфейс MediaStreamConstraints и который мы отправляем в качестве параметра в getUserMedia(), позволяет нам открывать мультимедиа-устройство, которое отвечает определенным требованиям. Эти требования могут быть как очень расплывчатыми (аудио и/или видео), так и очень специфичными (минимальное разрешение камеры или точный ID устройства). Рекомендуем, чтобы приложения, использующие getUserMedia() API, сначала проверяли существующие устройства, а затем определяли ограничения, которые соответствуют точному устройству через deviceID-ограничение. Устройства, по возможности, будут настроены в соответствии с ограничениями. Мы можем включить эхоподавление на микрофоне, установить определенную или минимальную ширину и высоту видео с камеры.
async function getConnectedDevices(type) { const devices = await navigator.mediaDevices.enumerateDevices(); return devices.filter(device =\u0026gt; device.kind === type) } // Open camera with at least minWidth and minHeight capabilities async function openCamera(cameraId, minWidth, minHeight) { const constraints = { 'audio': {'echoCancellation': true}, 'video': { 'deviceId': cameraId, 'width': {'min': minWidth}, 'height': {'min': minHeight} } } return await navigator.mediaDevices.getUserMedia(constraints); } const cameras = getConnectedDevices('videoinput'); if (cameras \u0026amp;\u0026amp; cameras.length \u0026gt; 0) { // Open first available video camera with a resolution of 1280x720 pixels const stream = openCamera(cameras[0].deviceId, 1280, 720); } Полную документацию для интерфейса MediaStreamConstraints можно найти по ссылке: https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamConstraints
Локальное воспроизведение Как только мультимедиа-устройство открыто и есть доступный MediaStream, мы можем назначить его для его видео- или аудио-элемента локальное воспроизведение потока.
async function playVideoFromCamera() { try { const constraints = {'video': true, 'audio': true}; const stream = await navigator.mediaDevices.getUserMedia(constraints); const videoElement = document.querySelector('video#localVideo'); videoElement.srcObject = stream; } catch(error) { console.error('Error opening video camera.', error); } } Обычно код HTML, необходимый для типичного видео-элемента с getUserMedia(), имеет атрибуты autoplay и playsinline. Атрибут autoplay запускает воспроизведение новых потоков, связанных с элементом, автоматически. Атрибут playsinline позволяет проигрывать встроенное видео вместо видео на весь экран, в некоторых мобильных браузерах. Также рекомендуем использовать controls = “false” для прямых эфиров, если у пользователя нет необходимости ставить их на паузу.
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Local video playback\u0026lt;/video\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;video id=\u0026quot;localVideo\u0026quot; autoplay playsinline controls=\u0026quot;false\u0026quot;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `,url:"https://romankurnovskii.com/ru/docs/webrtc/media-devices/"},"https://romankurnovskii.com/ru/docs/python101/chapter2_strings/":{title:"2. Все о строках",tags:[],content:`В Python существует несколько типов данных. Основные типы данных, с которыми вы, вероятно, будете чаще всего встречаться, - это строка, целое число, плавающая цифра, список, словарь и кортеж. В этой главе мы рассмотрим строковый тип данных. Вы удивитесь, как много вещей можно делать со строками в Python прямо из коробки. Существует также модуль string, который можно импортировать для получения доступа к еще большей функциональности, но мы не будем рассматривать его в этой главе. Вместо этого мы рассмотрим следующие темы:
Как создавать строки Конкатенация строк Методы работы со строками Нарезка строк Подстановка строк Как создать строку Строки обычно создаются одним из трех способов. Вы можете использовать одинарные, двойные или тройные кавычки. Давайте посмотрим!
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;Welcome to Python!\u0026quot; \u0026gt;\u0026gt;\u0026gt; another_string = 'The bright red fox jumped the fence.' \u0026gt;\u0026gt;\u0026gt; a_long_string = '''This is a multi-line string. It covers more than one line''' Строка с тройными кавычками может быть выполнена с помощью трех одинарных или трех двойных кавычек. В любом случае они позволяют программисту писать строки в несколько строк. Когда вы выведете ее, то заметите, что при выводе сохраняются разрывы строк. Если вам необходимо использовать одинарные кавычки в строке, то оберните их в двойные кавычки. Смотрите следующий пример.
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;I'm a Python programmer!\u0026quot; \u0026gt;\u0026gt;\u0026gt; otherString = 'The word \u0026quot;python\u0026quot; usually refers to a snake' \u0026gt;\u0026gt;\u0026gt; tripleString = \u0026quot;\u0026quot;\u0026quot;Here's another way to embed \u0026quot;quotes\u0026quot; in a string\u0026quot;\u0026quot;\u0026quot; Приведенный выше код демонстрирует, как можно поместить одинарные или двойные кавычки в строку. На самом деле существует еще один способ создания строки - это использование метода str. Вот как это работает:
\u0026gt;\u0026gt;\u0026gt; my_number = 123 \u0026gt;\u0026gt;\u0026gt; my_string = str(my_number) Если вы введете приведенный выше код в свой интерпретатор, то обнаружите, что вы преобразовали целочисленное значение в строку и присвоили строку переменной my_string. Это известно как преобразование. Вы можете преобразовывать некоторые типы данных в другие типы данных, например, числа в строки. Но вы также увидите, что не всегда можно сделать обратное действие, например, преобразовать строку типа \u0026lsquo;ABC\u0026rsquo; в целое число. В этом случае вы получите ошибку, подобную той, что приведена в следующем примере:
\u0026gt;\u0026gt;\u0026gt; int('ABC') Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; ValueError: invalid literal for int() with base 10: 'ABC' Мы рассмотрим обработку исключений в следующей главе, но, как вы уже догадались из сообщения, это означает, что вы не можете преобразовать литерал в целое число. Однако, если бы вы сделали
\u0026gt;\u0026gt;\u0026gt; x = int(\u0026quot;123\u0026quot;) тогда это будет работать нормально.
Следует отметить, что строка является одним из неизменяемых типов Python. Это значит, что вы не можете изменить содержимое строки после ее создания. Давайте попробуем изменить одну из них и посмотрим, что произойдет:
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;abc\u0026quot; \u0026gt;\u0026gt;\u0026gt; my_string[0] = \u0026quot;d\u0026quot; Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: 'str' object does not support item assignment Здесь мы пытаемся изменить первый символ с \u0026ldquo;a\u0026rdquo; на \u0026ldquo;d\u0026rdquo;, однако это вызывает ошибку типа TypeError, которая не дает нам этого сделать. Теперь вы можете подумать, что, присвоив новую строку той же переменной, вы изменили строку. Давайте проверим, так ли это:
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;abc\u0026quot; \u0026gt;\u0026gt;\u0026gt; id(my_string) 19397208 \u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;def\u0026quot; \u0026gt;\u0026gt;\u0026gt; id(my_string) 25558288 \u0026gt;\u0026gt;\u0026gt; my_string = my_string + \u0026quot;ghi\u0026quot; \u0026gt;\u0026gt;\u0026gt; id(my_string) 31345312 Проверяя id объекта, мы можем определить, что каждый раз, когда мы присваиваем переменной новое значение, ее идентификатор меняется.
Конкатенация строк Конкатенация - это большое слово, которое означает объединение или сложение двух вещей вместе. В данном случае мы хотим узнать, как сложить две строки вместе. Как вы уже догадались, эта операция очень проста в Python:
\u0026gt;\u0026gt;\u0026gt; string_one = \u0026quot;My dog ate \u0026quot; \u0026gt;\u0026gt;\u0026gt; string_two = \u0026quot;my homework!\u0026quot; \u0026gt;\u0026gt;\u0026gt; string_three = string_one + string_two Оператор \u0026lsquo;+\u0026rsquo; объединяет две строки в одну.
Методы работы со строками Строка - это объект в Python. Фактически, все в Python является объектом. Однако вы еще не готовы к этому. Если вы хотите узнать больше о том, что Python является объектно-ориентированным языком программирования, то вам нужно перейти к этой главе. А пока достаточно знать, что строки имеют свои собственные методы, встроенные в них. Например, допустим, у вас есть следующая строка:
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;This is a string!\u0026quot; Теперь вы хотите, чтобы эта строка была полностью в верхнем регистре. Для этого достаточно вызвать его метод upper(), как показано ниже:
\u0026gt;\u0026gt;\u0026gt; my_string.upper() Если у вас открыт интерпретатор, вы можете сделать то же самое следующим образом:
\u0026gt;\u0026gt;\u0026gt; \u0026quot;This is a string!\u0026quot;.upper() Существует множество других методов работы со строками. Например, если бы вы хотели, чтобы все было в нижнем регистре, вы бы использовали метод lower(). Если бы вы хотели удалить все пробелы в начале и в конце строки, вы бы использовали метод strip(). Чтобы получить список всех методов работы со строками, введите в интерпретатор следующую команду:
\u0026gt;\u0026gt;\u0026gt; dir(my_string) В итоге вы должны увидеть нечто похожее на это:
[‘add’, ‘class’, ‘contains’, ‘delattr’, ‘doc’, ‘eq’, ‘format’, ‘ge’, ‘getattribute’, ‘getitem’, ‘getnewargs’, ‘getslice’, ‘gt’, ‘hash’, ‘init’, ‘le’, ‘len’, ‘lt’, ‘mod’, ‘mul’, ‘ne’, ‘new’, ‘reduce’, ‘reduce_ex’, ‘repr’, ‘rmod’, ‘rmul’, ‘setattr’, ‘sizeof’, ‘str’, ‘subclasshook’, ‘_formatter_field_name_split’, ‘_formatter_parser’, ‘capitalize’, ‘center’, ‘count’, ‘decode’, ‘encode’, ‘endswith’, ‘expandtabs’, ‘find’, ‘format’, ‘index’, ‘isalnum’, ‘isalpha’, ‘isdigit’, ‘islower’, ‘isspace’, ‘istitle’, ‘isupper’, ‘join’, ‘ljust’, ‘lower’, ‘lstrip’, ‘partition’, ‘replace’, ‘rfind’, ‘rindex’, ‘rjust’, ‘rpartition’, ‘rsplit’, ‘rstrip’, ‘split’, ‘splitlines’, ‘startswith’, ‘strip’, ‘swapcase’, ‘title’, ‘translate’, ‘upper’, ‘zfill’]
Вы можете смело игнорировать методы, начинающиеся и заканчивающиеся двойными знаками, такие как add. Они не используются в повседневном кодировании на Python. Вместо этого сосредоточьтесь на других методах. Если вы хотите узнать, что делает один из них, просто попросите помощи. Например, вы хотите узнать, для чего нужна capitalize. Чтобы узнать это, введите
\u0026gt;\u0026gt;\u0026gt; help(my_string.capitalize) Это вернет следующую информацию:
Help on built-in function capitalize: capitalize(...) S.capitalize() -\u0026gt; string Возвращает копию строки S, в которой заглавным является только первый символ.
Вы только что узнали немного о теме, называемой интроспекцией. Python позволяет легко проводить интроспекцию всех своих объектов, что делает его очень удобным в использовании. По сути, интроспекция позволяет вам спрашивать Python о самом себе. В одном из предыдущих разделов вы узнали о преобразовании. Возможно, вы задавались вопросом, как определить тип переменной (например, int или string). Вы можете попросить Python рассказать вам об этом!
\u0026gt;\u0026gt;\u0026gt; type(my_string) \u0026lt;type 'str'\u0026gt; Как вы видите, переменная my_string имеет тип str!
Нарезка строка Один из пунктов, которым вы будете часто заниматься в реальном мире, - это нарезка строк. Я был удивлен, как часто мне приходилось сталкиваться с необходимостью знать, как это делается в моей повседневной работе. Давайте посмотрим, как работает нарезка на примере следующей строки:
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;I like Python!\u0026quot; Каждый символ в строке может быть доступен с помощью нарезки. Например, если я хочу получить только первый символ, я могу сделать следующее:
\u0026gt;\u0026gt;\u0026gt; my_string[0:1] Это захватит первый символ в строке до 2-го символа, но не включая его. Да, Python основан на нулях. Это будет немного проще понять, если мы обозначим позицию каждого символа в таблице:
0 |	1 |	2 |	3 |	4 |	5 |	6 |	7 |	8 |	9 |	10 | 11 |	12 | 13 I |	|	l |	i |	k |	e |	|	P |	y |	t |	h |	o |	n |	!
Таким образом, у нас есть строка длиной 14 символов, начинающаяся с нуля и заканчивающаяся тринадцатью. Давайте рассмотрим еще несколько примеров, чтобы лучше закрепить эти понятия в голове.
\u0026gt;\u0026gt;\u0026gt; my_string[:1] 'I' \u0026gt;\u0026gt;\u0026gt; my_string[0:12] 'I like Pytho' \u0026gt;\u0026gt;\u0026gt; my_string[0:13] 'I like Python' \u0026gt;\u0026gt;\u0026gt; my_string[0:14] 'I like Python!' \u0026gt;\u0026gt;\u0026gt; my_string[0:-5] 'I like Py' \u0026gt;\u0026gt;\u0026gt; my_string[:] 'I like Python!' \u0026gt;\u0026gt;\u0026gt; my_string[2:] 'like Python!' Как видно из этих примеров, мы можем сделать срез, указав только начало среза (например, my_string[2:]), конец среза (например, my_string[:1]) или оба (например, my_string[0:13]). Мы можем даже использовать отрицательные значения, которые начинаются с конца строки. Так, в примере my_string[0:-5] начинается с нуля, но заканчивается за 5 символов до конца строки.
Вам может быть интересно, где можно использовать это. Я использую его для разбора записей фиксированной ширины в файлах или иногда для разбора сложных имен файлов, которые следуют очень специфическому соглашению об именовании. Я также использовал его при разборе значений из файлов двоичного типа. Любая работа, где вам нужно обрабатывать текстовые файлы, станет проще, если вы поймете, что такое нарезка и как ее эффективно использовать.
Вы также можете получить доступ к отдельным символам в строке с помощью индексации. Вот пример:
\u0026gt;\u0026gt;\u0026gt; print(my_string[0]) Приведенный выше код выведет первый символ в строке.
Форматирование строк Форматирование строк (оно же подстановка) - это тема подстановки значений в базовую строку. В большинстве случаев вы будете вставлять строки в строки, однако вам также часто придется вставлять в строки целые и плавающие числа. Существует два различных способа выполнения этой задачи. Мы начнем со старого способа, а затем перейдем к новому.
Старый способ подстановки строк
Самый простой способ узнать, как это делается, - посмотреть несколько примеров. Итак, приступим:
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;I like %s\u0026quot; % \u0026quot;Python\u0026quot; \u0026gt;\u0026gt;\u0026gt; my_string 'I like Python' \u0026gt;\u0026gt;\u0026gt; var = \u0026quot;cookies\u0026quot; \u0026gt;\u0026gt;\u0026gt; newString = \u0026quot;I like %s\u0026quot; % var \u0026gt;\u0026gt;\u0026gt; newString 'I like cookies' \u0026gt;\u0026gt;\u0026gt; another_string = \u0026quot;I like %s and %s\u0026quot; % (\u0026quot;Python\u0026quot;, var) \u0026gt;\u0026gt;\u0026gt; another_string 'I like Python and cookies' Как вы уже, наверное, догадались, %s - это важный элемент в приведенном выше коде. Он сообщает Python, что скоро вы можете вставить текст. Если за строкой следует знак процента и другая строка или переменная, Python попытается вставить ее в строку. Вы можете вставить несколько строк, поместив несколько экземпляров %s внутрь строки. Вы увидите это в последнем примере. Обратите внимание, что при вставке более одной строки необходимо заключить в круглые скобки те строки, которые вы собираетесь вставить.
Теперь давайте посмотрим, что произойдет, если мы вставим недостаточно строк:
Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: not enough arguments for format string Упс! Мы не передали достаточно аргументов для форматирования строки! Если вы внимательно посмотрите на пример выше, то заметите, что в нем есть два экземпляра %s, поэтому, чтобы вставить в него строки, вы должны передать ему такое же количество строк! Теперь мы готовы узнать о вставке целых и плавающих чисел. Давайте посмотрим!
\u0026gt;\u0026gt;\u0026gt; my_string = \u0026quot;%i + %i = %i\u0026quot; % (1,2,3) \u0026gt;\u0026gt;\u0026gt; my_string '1 + 2 = 3' \u0026gt;\u0026gt;\u0026gt; float_string = \u0026quot;%f\u0026quot; % (1.23) \u0026gt;\u0026gt;\u0026gt; float_string '1.230000' \u0026gt;\u0026gt;\u0026gt; float_string2 = \u0026quot;%.2f\u0026quot; % (1.23) \u0026gt;\u0026gt;\u0026gt; float_string2 '1.23' \u0026gt;\u0026gt;\u0026gt; float_string3 = \u0026quot;%.2f\u0026quot; % (1.237) \u0026gt;\u0026gt;\u0026gt; float_string3 '1.24' Первый пример довольно очевиден. Мы создаем строку, которая принимает три аргумента, и передаем их. На всякий случай, если вы еще не поняли, нет, Python не выполняет никакого сложения в этом первом примере. Во втором примере мы передаем плавающее число. Обратите внимание, что на выходе получается много лишних нулей. Нам это не нужно, поэтому мы говорим Python ограничиться двумя знаками после запятой в третьем примере (\u0026quot;%.2f\u0026quot;). Последний пример показывает, что Python сделает округление за вас, если вы передадите ему плавающее число с более чем двумя знаками после запятой.
Теперь давайте посмотрим, что произойдет, если мы передадим ему плохие данные:
\u0026gt;\u0026gt;\u0026gt; int_float_err = \u0026quot;%i + %f\u0026quot; % (\u0026quot;1\u0026quot;, \u0026quot;2.00\u0026quot;) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: %d format: a number is required, not str В этом примере мы передаем ему две строки вместо целого и плавающего числа. Это вызывает ошибку TypeError и говорит нам, что Python ожидал получить число. Это относится к тому, что мы не передали целое число, поэтому давайте исправим это и посмотрим, устранит ли это проблему:
\u0026gt;\u0026gt;\u0026gt; int_float_err = \u0026quot;%i + %f\u0026quot; % (1, \u0026quot;2.00\u0026quot;) Traceback (most recent call last): File \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; TypeError: must be real number, not str Неа. Мы получаем ту же ошибку, но другое сообщение, которое говорит нам, что мы должны были передать плавающее число. Как видите, Python дает нам довольно хорошую информацию о том, что пошло не так и как это исправить. Если вы правильно исправите входные данные, то сможете запустить этот пример.
Давайте перейдем к новому методу форматирования строк!
Шаблоны и новая методика форматирования строк
Этот новый метод был фактически добавлен еще в Python 2.4 в виде шаблонов строк, но в Python 2.6 он был добавлен как обычный метод форматирования строк через метод format. Так что это не совсем новый метод, просто более новый. В любом случае давайте начнем с шаблонов!
\u0026gt;\u0026gt;\u0026gt; print(\u0026quot;%(lang)s is fun!\u0026quot; % {\u0026quot;lang\u0026quot;:\u0026quot;Python\u0026quot;}) Python is fun! Возможно, это выглядит довольно странно, но, по сути, мы просто заменили %s на %(lang)s, что, является %s с переменной внутри. Вторая часть на самом деле называется словарем Python, который мы будем изучать в следующем разделе. По сути, это пара ключ:значение, поэтому когда Python видит ключ \u0026ldquo;lang\u0026rdquo; в строке И в ключе переданного словаря, он заменяет этот ключ на его значение. Давайте рассмотрим еще несколько примеров:
\u0026gt;\u0026gt;\u0026gt; print(\u0026quot;%(value)s %(value)s %(value)s !\u0026quot; % {\u0026quot;value\u0026quot;:\u0026quot;SPAM\u0026quot;}) SPAM SPAM SPAM ! \u0026gt;\u0026gt;\u0026gt; print(\u0026quot;%(x)i + %(y)i = %(z)i\u0026quot; % {\u0026quot;x\u0026quot;:1, \u0026quot;y\u0026quot;:2}) Traceback (most recent call last): File \u0026quot;\u0026lt;stdin\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; KeyError: 'z' \u0026gt;\u0026gt;\u0026gt; print(\u0026quot;%(x)i + %(y)i = %(z)i\u0026quot; % {\u0026quot;x\u0026quot;:1, \u0026quot;y\u0026quot;:2, \u0026quot;z\u0026quot;:3}) 1 + 2 = 3 В первом примере вы заметите, что мы передали только одно значение, но оно было вставлено 3 раза! Это одно из преимуществ использования шаблонов. Во втором примере есть проблема, связанная с тем, что мы забыли передать ключ, а именно ключ \u0026ldquo;z\u0026rdquo;. Третий пример исправляет эту проблему и показывает результат. Теперь давайте посмотрим, как можно сделать нечто подобное с помощью метода формата строки!
\u0026gt;\u0026gt;\u0026gt; \u0026quot;Python is as simple as {0}, {1}, {2}\u0026quot;.format(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) 'Python is as simple as a, b, c' \u0026gt;\u0026gt;\u0026gt; \u0026quot;Python is as simple as {1}, {0}, {2}\u0026quot;.format(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;) 'Python is as simple as b, a, c' \u0026gt;\u0026gt;\u0026gt; xy = {\u0026quot;x\u0026quot;:0, \u0026quot;y\u0026quot;:10} \u0026gt;\u0026gt;\u0026gt; print(\u0026quot;Graph a point at where x={x} and y={y}\u0026quot;.format(**xy)) Graph a point at where x=0 and y=10 В первых двух примерах вы можете видеть, как мы можем передавать элементы позиционно. Если мы изменим порядок, то получим немного другой результат. В последнем примере используется словарь, как мы использовали в шаблонах выше. Однако нам нужно извлечь словарь с помощью двойной звездочки, чтобы он работал правильно.
Есть много других вещей, которые можно делать со строками, например, указывать ширину, выравнивать текст, конвертировать в различные базы и многое другое. Обязательно посмотрите некоторые из приведенных ниже ссылок для получения дополнительной информации.
Официальная документация Python по типу str Форматирование строк [Подробнее о форматировании строк(https://docs.python.org/3/library/string.html#formatexamples) Подведение итогов Мы многое рассмотрели в этой главе. Давайте подведем итоги:
Сначала мы узнали, как создавать сами строки, затем перешли к теме конкатенации строк. После этого мы рассмотрели некоторые методы, которые предоставляет нам объект string. Далее мы рассмотрели нарезку строк и закончили изучением подстановки строк.
В следующей главе мы рассмотрим еще три встроенных типа данных Python: списки, кортежи и словари. Приступаем!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter2_strings/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day02/":{title:"2. Задачи DevOps-инженера",tags:["devops","90daysofdevops","learning"],content:`Обязанности DevOps специалиста Надеюсь, вы приступили к этому после просмотра ресурсов и публикации в День 1 из #90DaysOfDevOps
В первом посте был краткий обзор, но теперь мы должны углубиться в концепцию DevOps и понять, что при создании приложения есть две основные части. У нас есть часть Разработка, где разработчики программного обеспечения программируют приложение и тестируют его. Затем у нас есть часть Операции, где приложение развертывается и поддерживается на сервере.
DevOps — это связующее звено между двумя Чтобы разобраться с DevOps или задачами, которые будет выполнять инженер DevOps, нам нужно понять инструменты или процесс, а также разобраться как они вместе они вместе взаимодейтвуют.
Все начинается с приложения! Вы увидите так много всего, что все дело в приложении, когда речь идет о DevOps.
Разработчики создадуют приложение, это можно сделать с помощью множества различных технологических стеков, и давайте пока оставим это воображению, поскольку мы вернемся к этому позже. Это также может включать множество различных языков программирования, инструменты сборки, репозиторий кода и т. д.
Будучи инженером DevOps, вы не будете программировать приложение, но хорошее понимание концепций работы разработчика и используемых им систем, инструментов и процессов является ключом к успеху.
На очень высоком уровне вам нужно будет знать, как приложение настроено для взаимодействия со всеми необходимыми службами или службами данных, а затем также добавить требования о том, как это можно или нужно протестировать.
Приложение нужно будет где-то развернуть, давайте сделаем его в целом простым и сделаем это сервером, неважно где, но сервером. Затем ожидается, что к нему будет обращаться клиент или конечный пользователь в зависимости от созданного приложения.
Этот сервер должен работать где-то локально, в общедоступном облаке, без сервера (Хорошо, я зашел слишком далеко, мы не будем рассматривать бессерверный вариант, но это вариант, и все больше и больше предприятий идут по этому пути). Кто-то должен создать настройте эти серверы и подготовьте их к запуску приложения. Теперь этот элемент может пригодиться вам как инженеру DevOps для развертывания и настройки этих серверов.
Эти серверы должны будут работать под управлением операционной системы, и, вообще говоря, это будет Linux, но у нас есть целый раздел или потратим неделю, где мы рассмотрим некоторые фундаментальные знания, которые вы должны получить.
Также вероятно, что нам нужно взаимодействовать с другими службами в нашей сети или среде, поэтому нам также необходимо иметь такой уровень знаний о сети и настройке, что в некоторой степени также может оказаться в руках инженера DevOps. Опять же, мы рассмотрим это более подробно в специальном разделе, посвященном DNS, DHCP, балансировщикам нагрузки (Load Balancing) и т. д.
Мастер на все руки Однако на этом этапе я скажу, что вам не нужно быть специалистом по сетям или инфраструктуре, вам нужны базовые знания о том, как наладить работу и общаться друг с другом, во многом так же, как, возможно, иметь базовые знания язык программирования, но вам не нужно быть разработчиком. Однако вы можете прийти к этому как специалист в какой-то области, и это отличная основа для адаптации к другим областям.
Вы также, скорее всего, не будете ежедневно управлять этими серверами или приложением.
Мы говорили о серверах, но есть вероятность, что ваше приложение будет разработано для работы в виде контейнеров, которые по-прежнему работают на сервере по большей части, но вам также потребуется понимание не только виртуализации, облачной инфраструктуры как услуги (IaaS). ), но также и контейнеризация. В эти 90 дней основное внимание будет уделяться контейнерам.
Общий обзор С одной стороны, наши разработчики создают новые функции и функции (а также исправления ошибок) для приложения.
С другой стороны, у нас есть какая-то среда, инфраструктура или серверы, которые настроены и управляются для запуска этого приложения и связи со всеми необходимыми службами.
Большой вопрос заключается в том, как нам внедрить эти функции и исправления ошибок в нашу продукцию и сделать их доступными для этих конечных пользователей?
Как мы выпускаем новую версию приложения? Это одна из основных задач для DevOps-инженера, и здесь важно не просто понять, как это сделать один раз, а нам нужно делать это непрерывно и автоматизированным, эффективным способом, который также должен включать тестирование!
На этом мы собираемся закончить этот день обучения, надеюсь, это было полезно. В течение следующих нескольких дней мы собираемся немного глубже погрузиться в некоторые другие области DevOps, а затем мы перейдем к разделам, в которых более подробно рассматриваются инструменты и процессы, а также их преимущества.
Ресурсы Я всегда открыт для добавления дополнительных ресурсов в эти файлы Readme, поскольку они здесь в качестве учебного пособия.
Мой совет - просмотреть все ссылки из списка ниже, и, надеюсь, вы также что-то почерпнули из текста и объяснений выше.
What is DevOps? - TechWorld with Nana What is DevOps? - GitHub YouTube What is DevOps? - IBM YouTube What is DevOps? - AWS What is DevOps? - Microsoft Если вы зашли так далеко, то поймете, хотите ли вы быть здесь или нет. До встречи в День 3
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day02/"},"https://romankurnovskii.com/ru/docs/webrtc/media-capture-and-constraints/":{title:"Захват мультимедиа и ограничения",tags:[],content:`Захват мультимедиа и ограничения Мультимедиа-часть WebRTC показывает, как получить доступ к оборудованию, способному записывать видео и аудио (например, камеры и микрофоны), а также как работают медиа-потоки. И помимо этого – средства отображения, которые позволяют делать захват экрана.
Мультимедиа-устройства Все камеры и микрофоны, поддерживаемые браузером, доступны и управляются через объект navigator.mediaDevices. Приложения могут получать текущий список подсоединенных устройств и отслеживать изменения, т.к. многие камеры и микрофоны подсоединены через USB, и могут подключаться/отключаться в течение работы приложения. Поскольку статус мультимедиа-устройства может меняться в любой момент времени, рекомендуем, чтоб приложения регистрировали все изменения в статусе устройства для правильной обработки статусов изменений.
Ограничения При получении доступа к мультимедиа-устройствам, хорошо бы обеспечить настолько подробные ограничения, насколько это возможно. И хотя можно открыть камеру и микрофон по умолчанию с простым ограничением, это может привести к тому, что медиапоток будет далеко не самым оптимальным для приложения.
Конкретные ограничения определяются в объекте MediaTrackConstraint (одно для аудио, одно для видео). Атрибуты в этом объекте типа ConstraintLong, ConstraintBoolean, ConstraintDouble или ConstraintDOMString. Данные могут быть как конкретным значением (например, число, Boolean или String), диапазоном (LongRange или DoubleRange с минимальным и максимальным значением) или объектом c ideal или exact определением. Для конкретных значений браузер будет пытаться выбрать что-то наиболее близкое. Для диапазонных будет использоваться лучшее значение из диапазона. Для exact – будет передаваться только тот медиа-поток, который точно соответствует заданным ограничениям.
NEAR // Camera with a resolution as close to 640x480 as possible { \u0026quot;video\u0026quot;: { \u0026quot;width\u0026quot;: 640, \u0026quot;height\u0026quot;: 480 } } RANGE // Camera with a resolution in the range 640x480 to 1024x768 { \u0026quot;video\u0026quot;: { \u0026quot;width\u0026quot;: { \u0026quot;min\u0026quot;: 640, \u0026quot;max\u0026quot;: 1024 }, \u0026quot;height\u0026quot;: { \u0026quot;min\u0026quot;: 480, \u0026quot;max\u0026quot;: 768 } } } EXACT // Camera with the exact resolution of 1024x768 { \u0026quot;video\u0026quot;: { \u0026quot;width\u0026quot;: { \u0026quot;exact\u0026quot;: 1024 }, \u0026quot;height\u0026quot;: { \u0026quot;exact\u0026quot;: 768 } } } Чтобы определить актуальную конфигурацию конкретной дорожки медиа-потока, мы можем воспользоваться запросом MediaStreamTrack.getSettings(), который возвращает набор настроек MediaTrackSettings, используемых в данные момент.
Также можно обновить ограничения дорожки с мультимедиа-устройства, которое открываем через applyConstraints(). Это позволяет приложению перенастроить устройство без прерывания текущего потока.
Захват экрана Приложение, которое потенциально может выполнять захват и запись экрана, должно использовать Display Media API. Функция getDisplayMedia() (которая является частью navigator.mediaDevices), аналогична getUserMedia() и используется, чтобы открыть содержимое дисплея (или его части, например, окна). Возвращенный MediaStream работает также, как при использовании getUserMedia().
Ограничения для getDisplayMedia() отличаются от ограничений, используемых для обычных входящих видео- и аудио-потоков.
{ video: { cursor: ‘always’ | ‘motion’ | ‘never’, displaySurface: ‘application’ | ‘browser’ | ‘monitor’ | ‘window’ } } Фрагмент кода выше показывает, как работают специальные ограничения для записи экрана. Обратите внимание, что они могут не поддерживаться некоторыми браузерами, поддерживающими отображение мультимедиа.
Потоки и дорожки MediaStream представляет собой поток медиаконтента, который состоит из аудио- и видео- дорожек (MediaStreamTrack). Можно достать все дорожки из MediaStream, вызвав команду MediaStream.getTracks(), которая возвращает массив объектов из MediaStreamTrack.
MediaStreamTrack MediaStreamTrack обладает свойством kind (audio или video, указывающий тип мультимедиа, который он воспроизводит). Каждую дорожку можно выключить, переключив ее свойство enabled. У дорожки есть логическое свойство remote, которое показывает, является ли она источником RTCPeerConnection и идет ли она от удаленного узла.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/media-capture-and-constraints/"},"https://romankurnovskii.com/ru/docs/python101/chapter3_lists_dicts/":{title:"3. Списки, кортежи и словари",tags:[],content:` В Python есть еще несколько важных типов данных, которые вы, вероятно, будете использовать каждый день. Они называются списками, кортежами и словарями. Цель этой главы - познакомить вас с каждым из этих типов данных. Они не являются особенно сложными, так что думаю, что вам будет очень легко научиться их использовать. Когда вы освоите эти три типа данных, а также строковый тип данных из предыдущей главы, вы пройдете довольно большой путь в изучении Python. Вы будете использовать эти четыре строительных блока в 99% всех приложений, которые вы будете писать.
Списки Список в Python похож на массив в других языках. В Python пустой список может быть создан следующими способами.
\u0026gt;\u0026gt;\u0026gt; my_list = [] \u0026gt;\u0026gt;\u0026gt; my_list = list() Как вы видите, список можно создать с помощью квадратных скобок или с помощью встроенной в Python функции list. Список содержит перечень элементов, таких как строки, целые числа, объекты или смесь типов. Давайте рассмотрим несколько примеров:
\u0026gt;\u0026gt;\u0026gt; my_list = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; my_list2 = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;] \u0026gt;\u0026gt;\u0026gt; my_list3 = [\u0026quot;a\u0026quot;, 1, \u0026quot;Python\u0026quot;, 5] Первый список содержит 3 целых числа, второй - 3 строки, а третий - смесь. Вы также можете создавать списки списков следующим образом:
\u0026gt;\u0026gt;\u0026gt; my_nested_list = [my_list, my_list2] \u0026gt;\u0026gt;\u0026gt; my_nested_list [[1, 2, 3], ['a', 'b', 'c']] Иногда возникает необходимость объединить два списка вместе. Первый способ - использовать метод extend:
\u0026gt;\u0026gt;\u0026gt; combo_list = [] \u0026gt;\u0026gt;\u0026gt; one_list = [4, 5] \u0026gt;\u0026gt;\u0026gt; combo_list.extend(one_list) \u0026gt;\u0026gt;\u0026gt; combo_list [4, 5] Более простой способ - просто сложить два списка вместе.
\u0026gt;\u0026gt;\u0026gt; my_list = [1, 2, 3] \u0026gt;\u0026gt;\u0026gt; my_list2 = [\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;] \u0026gt;\u0026gt;\u0026gt; combo_list = my_list + my_list2 \u0026gt;\u0026gt;\u0026gt; combo_list [1, 2, 3, 'a', 'b', 'c'] Да, это действительно так просто. Вы также можете сортировать список. Давайте потратим немного времени, чтобы посмотреть, как это сделать:
\u0026gt;\u0026gt;\u0026gt; alpha_list = [34, 23, 67, 100, 88, 2] \u0026gt;\u0026gt;\u0026gt; alpha_list.sort() \u0026gt;\u0026gt;\u0026gt; alpha_list [2, 23, 34, 67, 88, 100] А вот здесь уже есть зацепка. Вы видите ее? Давайте напишем еще один пример, чтобы сделать его очевидным:
\u0026gt;\u0026gt;\u0026gt; alpha_list = [34, 23, 67, 100, 88, 2] \u0026gt;\u0026gt;\u0026gt; sorted_list = alpha_list.sort() \u0026gt;\u0026gt;\u0026gt; sorted_list \u0026gt;\u0026gt;\u0026gt; print(sorted_list) None В этом примере мы пытаемся присвоить отсортированный список переменной. Однако, когда вы вызываете метод sort() для списка, он сортирует список на месте. Поэтому если вы попытаетесь присвоить результат другой переменной, то получите объект None, который в других языках похож на Null. Таким образом, когда вы хотите что-то отсортировать, помните, что вы сортируете на месте и не можете присвоить результат другой переменной.
Вы можете разрезать список так же, как и строку:
\u0026gt;\u0026gt;\u0026gt; alpha_list[0:3] [2, 23, 34] Этот код возвращает список, состоящий только из первых 3 элементов.
Кортежи Кортеж похож на список, но при его создании вместо квадратных скобок используются круглые скобки. Вы также можете использовать tuple встроенно. Основное отличие состоит в том, что кортеж неизменяем, а список - изменяем. Давайте рассмотрим несколько примеров:
\u0026gt;\u0026gt;\u0026gt; my_tuple = (1, 2, 3, 4, 5) \u0026gt;\u0026gt;\u0026gt; my_tuple[0:3] (1, 2, 3) \u0026gt;\u0026gt;\u0026gt; another_tuple = tuple() \u0026gt;\u0026gt;\u0026gt; abc = tuple([1, 2, 3]) Приведенный выше код демонстрирует один из способов создания кортежа с пятью элементами. Он также показывает, что вы можете выполнять нарезку кортежей. Однако вы не можете отсортировать кортеж! Последние два примера показывают, как создавать кортежи с помощью ключевого слова tuple. Первый пример просто создает пустой кортеж, в то время как второй пример содержит три элемента. Обратите внимание, что внутри него находится список. Это пример преобразования. Мы можем изменить или преобразовать элемент из одного типа данных в другой. В данном случае мы приводим список к кортежу. Если вы хотите превратить кортеж abc обратно в список, вы можете сделать следующее:
\u0026gt;\u0026gt;\u0026gt; abc_list = list(abc) Повторим, что приведенный выше код преобразует кортеж (abc) в список с помощью функции list.
Словари Словарь в Python - это, по сути, хэш-таблица или хэш-сопоставление. В некоторых языках они могут называться ассоциативной памятью или ассоциативными массивами. Они индексируются ключами, которые могут быть любого неизменяемого типа. Например, ключом может быть строка или число. Необходимо знать, что словарь - это неупорядоченный набор пар ключ:значение, и ключи должны быть уникальными. Вы можете получить список ключей, вызвав метод keys экземпляра словаря. Чтобы проверить, есть ли у словаря ключ, можно использовать ключевое слово in в Python. В некоторых старых версиях Python (точнее, в 2.3 и старше) для проверки наличия ключа в словаре используется ключевое слово has_key. Это ключевое слово устарело в Python 2.x и полностью удалено из Python 3.x.
Давайте рассмотрим, как мы создаем словарь.
\u0026gt;\u0026gt;\u0026gt; my_dict = {} \u0026gt;\u0026gt;\u0026gt; another_dict = dict() \u0026gt;\u0026gt;\u0026gt; my_other_dict = {\u0026quot;one\u0026quot;:1, \u0026quot;two\u0026quot;:2, \u0026quot;three\u0026quot;:3} \u0026gt;\u0026gt;\u0026gt; my_other_dict {'three': 3, 'two': 2, 'one': 1} Первые два примера показывают, как создать пустой словарь. Все словари заключены в фигурные скобки. Последняя строка выводится на печать, чтобы вы могли увидеть, насколько словарь неупорядочен. Теперь пришло время узнать, как получить доступ к значению в словаре.
\u0026gt;\u0026gt;\u0026gt; my_other_dict[\u0026quot;one\u0026quot;] 1 \u0026gt;\u0026gt;\u0026gt; my_dict = {\u0026quot;name\u0026quot;:\u0026quot;Mike\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;123 Happy Way\u0026quot;} \u0026gt;\u0026gt;\u0026gt; my_dict[\u0026quot;name\u0026quot;] 'Mike' В первом примере мы используем словарь из предыдущего примера и извлекаем значение, связанное с ключом \u0026ldquo;one\u0026rdquo;. Во втором примере показано, как получить значение для ключа \u0026ldquo;name\u0026rdquo;. Теперь давайте посмотрим, как определить, находится ли ключ в словаре или нет:
\u0026gt;\u0026gt;\u0026gt; \u0026quot;name\u0026quot; in my_dict True \u0026gt;\u0026gt;\u0026gt; \u0026quot;state\u0026quot; in my_dict False Таким образом, если ключ находится в словаре, Python возвращает булевое значение True. В противном случае он возвращает булево значение False. Если вам нужно получить список всех ключей в словаре, то вы делаете следующее:
\u0026gt;\u0026gt;\u0026gt; my_dict.keys() dict_keys(['name', 'address']) В Python 3 метод keys возвращает объект представления. Это дает разработчику возможность обновить словарь, и представление тоже автоматически обновится. Также обратите внимание, что при использовании ключевого слова in для тестирования членства в словаре, лучше делать это по словарю, а не по списку, возвращаемому методом keys. См. ниже:
\u0026gt;\u0026gt;\u0026gt; \u0026quot;name\u0026quot; in my_dict # this is good \u0026gt;\u0026gt;\u0026gt; \u0026quot;name\u0026quot; in my_dict.keys() # this works too, but is slower Хотя сейчас это, вероятно, не имеет для вас большого значения, в реальной рабочей ситуации секунды важны. Когда вам нужно обработать тысячи файлов, эти маленькие хитрости помогут вам сэкономить много времени в долгосрочной перспективе!
Итоги В этой главе вы только что узнали, как построить список, кортеж и словарь Python. Прежде чем двигаться дальше, убедитесь, что вы поняли все, что было написано в этом разделе. Эти понятия помогут вам при разработке программ. Вы будете каждый день создавать сложные структуры данных с помощью этих строительных блоков, если решите работать программистом Python. Каждый из этих типов данных может быть вложен в другие. Например, у вас может быть вложенный словарь, словарь кортежей, кортеж, состоящий из нескольких словарей, и так далее.
Когда вы будете готовы двигаться дальше, мы познакомимся с поддержкой условных операторов в Python.
Ссылки Множества в Python `,url:"https://romankurnovskii.com/ru/docs/python101/chapter3_lists_dicts/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day03/":{title:"3. Ориентированность на приложения",tags:["devops","cicd","tests","learning"],content:`Жизненный цикл DevOps — ориентированность на приложения По мере того, как мы будем продолжать в течение следующих нескольких недель, мы будем сталкиваться с этими названиями (Continuous Development, Testing, Deployment, Monitor) (непрерывная разработка, тестирование, развертывание, мониторинг) снова и снова. Если вы стремитесь статья инженером DevOps, то повторяемость будет тем, к чему вы привыкнете, но постоянное улучшение каждый раз — это еще одна вещь, которая делает вещи интересными.
В этом часе мы рассмотрим общий вид приложения от начала до конца, а затем вернемся назад, как в постоянном цикле.
Разработка Давайте возьмем совершенно новый пример приложения, для начала у нас ничего не создано, возможно, как разработчик вы должны обсудить с вашим клиентом или конечным пользователем требования и придумать какой-то план или требования для вашего приложения. Затем нам нужно создать согласно требованиям наше новое приложение.
Что касается инструментов на данном этапе, здесь нет никаких реальных требований, кроме выбора вашей IDE и языка программирования, который вы хотите использовать для написания своего приложения.
Как инженер DevOps, помните, что вы, вероятно, не тот, кто создает этот план или создает приложение для конечного пользователя, этим занимается опытный разработчик.
Но вам также не помешает иметь возможность прочитать часть кода, чтобы вы могли принимать наилучшие решения по инфраструктуре для своего приложения.
Ранее мы упоминали, что приложение может быть написано на любом языке. Важно, чтобы это поддерживалось с помощью системы контроля версий, это то, что мы также подробно рассмотрим позже, и, в частности, мы углубимся в Git.
Также вероятно, что над этим проектом будет работать не один разработчик, хотя это может иметь место, но даже в этом случае передовой опыт потребует репозиторий кода для хранения и совместной работы над кодом, он может быть частным или общедоступным и может быть размещен или если говорить о частном развертывании, вы наверняка слышали, как GitHub или GitLab используются в качестве репозитория кода. Мы снова рассмотрим их позже в разделе Git.
Тестирование На данном этапе у нас есть свои требования и наша задача - разработать приложение. Но нам нужно убедиться, что мы тестируем наш код во всех различных средах, которые у нас есть, или, возможно, в выбранном языке программирования.
Этот этап позволяет QA тестировать на наличие ошибок, чаще мы видим, что контейнеры используются для моделирования тестовой среды, что в целом может снизить накладные расходы на физическую или облачную инфраструктуру.
Этот этап также, вероятно, будет автоматизирован как часть следующей области — непрерывной интеграции.
Возможность автоматизировать это тестирование по сравнению с 10, 100 или даже 1000 инженерами по контролю качества, которые должны делать это вручную, говорит сама за себя, эти инженеры могут сосредоточиться на чем-то другом в стеке, чтобы гарантировать, что вы двигаетесь быстрее и разрабатываете больше функций по сравнению с тестированием ошибок и программного обеспечения. что, как правило, является задержкой для большинства традиционных выпусков программного обеспечения, использующих методологию водопада (Waterfall).
Интеграция Очень важно, что интеграция находится в середине жизненного цикла DevOps. Это практика, когда разработчикам требуется чаще вносить изменения в исходный код. Это может быть ежедневно или еженедельно.
С каждым коммитом ваше приложение может проходить этапы автоматизированного тестирования, что позволяет на раннем этапе обнаруживать проблемы или ошибки до следующего этапа.
На этом этапе вы можете сказать: «Но мы не создаем приложения, мы покупаем их в готовом виде у поставщика программного обеспечения». Не волнуйтесь, многие компании делают это и будут продолжать делать, и именно поставщик программного обеспечения будет концентрируется на трех вышеупомянутых этапах, но вы, возможно, захотите принять последний этап, поскольку это позволит быстрее и эффективнее развертывать готовые развертывания.
Я бы также сказал, что очень важно просто иметь эти вышеперечисленные знания, поскольку сегодня вы можете купить готовое программное обеспечение, но что насчет завтра или в будущем \u0026hellip; может быть, на следующей работе?
Развертывание / Deployment Итак, наше приложение создано и протестировано в соответствии с требованиями нашего конечного пользователя, и теперь нам нужно приступить к развертыванию этого приложения в рабочей среде для использования нашими конечными пользователями.
Это этап, когда код развертывается на рабочих серверах, теперь все становится чрезвычайно интересным, и именно здесь оставшиеся 86 дней мы глубже погружаемся в эти области. Потому что разные приложения требуют различного аппаратного обеспечения или конфигураций. Именно здесь Управление конфигурацией приложений и Инфраструктура как код могут сыграть ключевую роль в жизненном цикле DevOps. Возможно, ваше приложение контейнеризовано, но его также можно запустить на виртуальной машине. Это также приводит наше изучение к таким платформам, как Kubernetes, которые будут организовывать эти контейнеры и следить за тем, чтобы желаемое состояние было доступно вашим конечным пользователям.
Все эти смелые темы мы рассмотрим более подробно в течение следующих нескольких недель, чтобы лучше понять основы того, что они из себя представляют и когда их использовать.
Мониторинг / Monitoring Все быстро меняется, и у нас есть наше приложение, которое мы постоянно обновляем новыми функциями и функциями, и у нас есть наше тестирование, чтобы убедиться, что функциональность не нарушена. У нас есть приложение, работающее в нашей среде, которое может постоянно поддерживать требуемую конфигурацию и производительность.
Но теперь мы должны быть уверены, что наши конечные пользователи получают то, что им нужно. Здесь нам нужно убедиться, что производительность нашего приложения постоянно отслеживается, этот этап позволит вашим разработчикам принимать более взвешенные решения об улучшениях приложения в будущих выпусках, чтобы лучше обслуживать конечных пользователей.
Надежность также является ключевым фактором здесь, в конце концов, мы хотим, чтобы наше приложение было доступно все время, когда оно требуется. Затем это дает возможность другим областям наблюдаемости, безопасности и управления данными, которые следует постоянно контролировать, а обратную связь всегда можно использовать для улучшения, обновления и непрерывного выпуска приложения.
Некоторый вклад от сообщества здесь, в частности @_ediri, упоминает также часть этого непрерывного процесса, мы также должны привлечь команды FinOps. Приложения и данные работают и хранятся где-то, за чем вы должны постоянно следить, чтобы убедиться, что если что-то изменится с точки зрения ресурсов, ваши расходы не вызовут серьезных финансовых проблем с вашими облачными счетами.
Я думаю, что сейчас самое время упомянуть упомянутого выше «инженера DevOps». Я имею в виду, что из разговора с другими членами сообщества звание инженера DevOps не должно быть целью ни для кого, потому что на самом деле любая должность должна включать процессы DevOps и культуру, описанную здесь. DevOps следует использовать на самых разных должностях, таких как облачный инженер/архитектор, администратор виртуализации, облачный архитектор/инженер, администратор инфраструктуры. Это лишь некоторые из них, но причина использования DevOps Engineer, описанная выше, на самом деле заключалась в том, чтобы выделить объем или процесс, используемый любой из вышеперечисленных должностей, и многое другое.
Источники Я всегда открыт для добавления дополнительных ресурсов в эти файлы readme, поскольку они здесь в качестве учебного пособия.
Мой совет — посмотрите все, что ниже, и, надеюсь, вы тоже что-то почерпнули из текста и объяснений выше.
Методология разработки CI/CD Continuous Development Continuous Testing - IBM YouTube Continuous Integration - IBM YouTube Continuous Monitoring The Remote Flow FinOps Foundation - What is FinOps NOT FREE The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win До встречи в День 4
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day03/"},"https://romankurnovskii.com/ru/docs/webrtc/peer-connections/":{title:"Одноранговые соединения",tags:[],content:`Начало работы с одноранговыми соединениями Одноранговые соединения – часть спецификации WebRTC, которая занимается связью двух приложений на различных компьютерах для коммуникации через P2P-протокол. Коммуникация между узлами может быть видео-, аудио- или произвольными двоичными данными (для клиентов, поддерживающих RTCDataChannel API). Чтобы выяснить, как два узла могут быть соединены, оба клиента должны предоставить конфигурацию ICE-Server. Это или STUN, или TURN-сервер, и их роль – обеспечить ICE-кандидатов для каждого клиента, который затем передается на удаленный узел. Эта «передача» ICE-кандидатов обычно называется «сигналинг».
Сигналинг Спецификации WebRTC включают API для коммуникации с ICE-сервером (ICE =Internet Connectivity Establishment, установление интерактивного подключения), но компонент сигналинга не является частью этого сервера. Сигналинг необходим, чтобы два узла могли использовать один и тот же способ подключения. Обычно это можно решить через обычный Web API на базе HTTP (то есть службу REST или другой механизм RPC), где веб-приложения могут передавать необходимую информацию до того, как будет установлено соединение. Следующий фрагмент кода показывает, как эту придуманную службу сигналинга можно использовать для отправки и получения асинхронных сообщений. Мы будем использовать по необходимости этот прием в оставшихся примерах в этом гайде.
// Set up an asynchronous communication channel that will be // used during the peer connection setup const signalingChannel = new SignalingChannel(remoteClientId); signalingChannel.addEventListener('message', message =\u0026gt; { // New message from remote client received }); // Send an asynchronous message to the remote client signalingChannel.send('Hello!'); Сигналинг может быть реализован разными способами, и спецификация WebRTC не отдает предпочтений какому-то определенному варианту.
Инициирование одноранговых соединений Каждое одноранговое соединение управляется объектом RTCPeerConnection. Конструктор для этого класса берет в качестве параметра одиночный объект RTCConfiguration. Этот объект определяет, как одноранговое соединение устанавливается, и какую информацию должен содержать об используемых ICE-серверах.
После того, как RTCPeerConnection установлено, мы должны задать SDP-запрос/ответ, в зависимости от того, являемся мы вызывающим или принимающим узлом. После того, как SDP-запрос/ответ создан, он должен быть отправлен на удаленный узел через другой канал. Передача SDP-объектов на удаленные узлы называется сигналингом и не рассматривается в WebRTC спецификации.
Для установки однорангового соединения с вызывающей стороны, мы создаем объект RTCPeerConnection, и затем вызываем createOffer() для создания объекта RTCSessionDescription. Описание этого сеанса устанавливается как локальное описание с использованием setLocalDescription(), и затем отправляется через наш сигналинг-канал получающей стороне. Мы также устанавливаем «прослушиватель» для нашего сигналинг-канала, чтобы знать, когда получающей стороной будет получен ответ на описание нашего запрошенного сеанса.
async function makeCall() { const configuration = {'iceServers': [{'urls': 'stun:stun.l.google.com:19302'}]} const peerConnection = new RTCPeerConnection(configuration); signalingChannel.addEventListener('message', async message =\u0026gt; { if (message.answer) { const remoteDesc = new RTCSessionDescription(message.answer); await peerConnection.setRemoteDescription(remoteDesc); } }); const offer = await peerConnection.createOffer(); await peerConnection.setLocalDescription(offer); signalingChannel.send({'offer': offer}); } На получающей стороне мы ждем входящий запрос до того, как мы создали пример RTCPeerConnection. После этого мы устанавливаем полученный запрос, используя setRemoteDescription().
Далее, мы делаем запрос createAnswer() для создания ответа на полученный запрос. Этот ответ устанавливается как локальное описание через использование setLocalDescription() и затем отправляется набирающей стороне через наш сигналинг-сервер.
const peerConnection = new RTCPeerConnection(configuration); signalingChannel.addEventListener('message', async message =\u0026gt; { if (message.offer) { peerConnection.setRemoteDescription(new RTCSessionDescription(message.offer)); const answer = await peerConnection.createAnswer(); await peerConnection.setLocalDescription(answer); signalingChannel.send({'answer': answer}); } }); Как только два узла установили описания и локального, и удаленного сеансов, становятся доступны возможности удаленного узла. Это еще не означает, что соединение между узлами готово. Для работы необходимо собрать ICE-кандидатов на каждом узле и передать (по сигналинг-каналу) другому узлу.
ICE-кандидаты До того, как два узла смогут коммуницировать через WebRTC, им необходимо обменяться информацией о подключении. Так как условия сети могут отличаться в зависимости от ряда факторов, для обнаружения возможных кандидатов на соединение с узлом обычно используется внешний сервис. Этот сервис называется ICE и использует серверы STUN или TURN. STUN – это аббревиатура от Session Traversal for NAT, и обычно косвенно используется в большинстве WebRTC приложениях.
TURN (Traversal Using Relay NAT) более продвинутое решение, которое включает в себя протоколы STUN, и большинство коммерческих служб WebRTC используют TURN сервер для установки соединения между узлами.
API WebRTC напрямую поддерживает как STUN, так и TURN, и объединяется под более полным термином ICE (Internet Connectivity Establishment - «Установление подключения к Интернету»). При установке WebRTC-соединения мы обычно предоставляем один или несколько ICE-серверов в конфигурации для объекта RTCPeerConnection.
Trickle ICE После создания объекта RTCPeerConnection, исходный фреймворк использует предоставленные ICE-серверы для сбора кандидатов на установление соединения (кандидатов ICE).
Событие icegatheringstatechange на RTCPeerConnection передает информацию о том, в каком состоянии находится ICE-сбор (new, gathering или complete). Несмотря на то, что для узла возможно просто дождаться, пока ICE-сбор будет завершен, обычно гораздо эффективнее использовать метод «trickle ice» и передавать каждого вновь обнаруженного ICE-кандидата удаленному узлу. Это значительно сократит время настройки однорангового соединения и позволит начать видео-звонок с меньшими задержками.
Для сбора ICE-кандидатов, просто добавьте «прослушиватель» в событие icecandidate. Объект RTCPeerConnectionIceEvent, созданный этим «прослушивателем», будет содержать свойство candidate, представляющее нового кандидата, которого нужно отправить удаленному узлу (см. Сигналинг)
// Listen for local ICE candidates on the local RTCPeerConnection peerConnection.addEventListener(‘icecandidate’, event =\u0026gt; { if (event.candidate) { signalingChannel.send({‘new-ice-candidate’: event.candidate}); } }); // Listen for remote ICE candidates and add them to the local RTCPeerConnection signalingChannel.addEventListener(‘message’, async message =\u0026gt; { if (message.iceCandidate) { try { await peerConnection.addIceCandidate(message.iceCandidate); } catch € { console.error(‘Error adding received ice candidate’, e); } } }); Соединение установлено После того, как ICE-кандидаты получены, нужно дождаться, пока состояние нашего однорангового соединения изменится на подключенное состояние. Чтобы отследить это, добавим «прослушиватель» в наш RTCPeerConnection, где можно просматривать изменения события connectionstatechange.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/peer-connections/"},"https://romankurnovskii.com/ru/docs/python101/chapter4_conditionals/":{title:"4. Условия",tags:[],content:`В каждом компьютерном языке есть хотя бы один условный оператор. Чаще всего этот оператор представляет собой структуру if/elif/else.
В Python 3.10 добавилась структура match/case
Условный оператор проверяет, является ли утверждение истинным или ложным. Это, собственно, все, что он делает. Также рассмотрим следующие булевы операции: and, or и not. Эти операции могут изменять поведение условного оператора простыми и сложными способами, в зависимости от проекта.
Оператор if Оператор if в Python довольно прост в использовании. Давайте потратим несколько минут на рассмотрение нескольких примеров, чтобы лучше познакомиться с этой конструкцией.
\u0026gt;\u0026gt;\u0026gt; if 2 \u0026gt; 1: print(\u0026quot;This is a True statement!\u0026quot;) This is a True Statement! \`\` Это условие проверяет \u0026quot;правдивость\u0026quot; следующего утверждения: 2 \u0026gt; 1. Поскольку это утверждение оценивается как True, оно приведет к печати последней строки примера на экран или в **стандартный выход** (stdout). **Python заботится о пробелах** Язык Python очень заботится о пробелах. Вы заметили, что в нашем условном выражении выше мы отступили от кода внутри оператора if на четыре пробела. Это очень важно! Если вы не сделаете отступы между блоками кода должным образом, код не будет выполняться правильно. Он может вообще не выполниться. Также **не** смешивайте табуляцию и пробелы. IDLE будет жаловаться, что с файлом что-то не так, и вам будет трудно понять, в чем дело. Рекомендуемое количество пробелов для отступа в блоке кода - четыре. На самом деле вы можете отступать от текста на любое количество пробелов, если вы последовательны. Однако правило 4 пробелов рекомендовано в Руководстве по стилю Python, и именно его придерживаются разработчики кода Python. Давайте рассмотрим другой пример: \`\`\`python \u0026gt;\u0026gt;\u0026gt; var1 = 1 \u0026gt;\u0026gt;\u0026gt; var2 = 3 \u0026gt;\u0026gt;\u0026gt; if var1 \u0026gt; var2: print(\u0026quot;This is also True\u0026quot;) В этой статье мы сравниваем две переменные, которые отвечают на вопрос: 1 \u0026gt; 3? Очевидно, что 1 не больше 3, поэтому ничего не выводится. Но что, если мы хотим, чтобы он что-то вывел? Вот тут-то и приходит на помощь оператор else. Давайте изменим условие, чтобы добавить этот элемент:
if var1 \u0026gt; var2: print(\u0026quot;This is also True\u0026quot;) else: print(\u0026quot;That was False!\u0026quot;) Если вы запустите этот код, он выведет строку, которая следует за оператором else. Давайте сменим передачу и получим немного информации от пользователя, чтобы сделать это поинтереснее. В Python 3.x тоже есть встроенная функция input, но она пытается выполнить то, что введено как выражение Python.
value = input(\u0026quot;How much is that doggy in the window? \\n\u0026quot;) value = int(value) if value \u0026lt; 10: print(\u0026quot;That's a great deal!\u0026quot;) elif 10 \u0026lt;= value \u0026lt;= 20: print(\u0026quot;I'd still pay that...\u0026quot;) else: print(\u0026quot;Wow! That's too much!\u0026quot;) Давайте немного разложим всё это по полочкам. Первая строка запрашивает у пользователя сумму. В следующей строке он преобразует введенный пользователем результат в целое число. Так что если вы введете число с плавающей запятой, например 1.23, оно будет усечено до 1. Если вы введете не число, то получите исключение. Мы рассмотрим, как обрабатывать исключения в следующей главе, поэтому пока просто введите целое число.
В следующих нескольких строках вы увидите, как мы проверяем 3 различных случая: меньше 10, больше или равно 10, но меньше или равно 20 или что-то еще. Для каждого из этих случаев выводится своя строка. Попробуйте поместить этот код в IDLE и сохранить его. Затем запустите его несколько раз с разными входными данными, чтобы посмотреть, как он работает.
Вы можете добавить несколько операторов elif ко всему условию. Оператор else необязателен, но является хорошим вариантом по умолчанию.
match/case color = \u0026quot;yellow\u0026quot; match color: case \u0026quot;red\u0026quot;: print(\u0026quot;The color is red\u0026quot;) case \u0026quot;yellow\u0026quot;: print(\u0026quot;Wow, you picked yellow\u0026quot;) case \u0026quot;green\u0026quot;: print(\u0026quot;We are using a green color\u0026quot;) case \u0026quot;blue\u0026quot;: print(\u0026quot;Blue like the sea...\u0026quot;) match command.split(): case [\u0026quot;quit\u0026quot;]: print(\u0026quot;Goodbye!\u0026quot;) quit_game() case [\u0026quot;look\u0026quot;]: current_room.describe() case [\u0026quot;get\u0026quot;, obj]: character.get(obj, current_room) case [\u0026quot;go\u0026quot;, direction]: current_room = current_room.neighbor(direction) or/and/not Теперь мы готовы к изучению булевых операций (and, or, not). Согласно документации Python, их порядок приоритета таков: сначала or, затем and, затем not. Вот как они работают:
or означает, что если любое условие, которое \u0026ldquo;перечислено\u0026rdquo; вместе, равно True, то выполняется следующее утверждение and означает, что для выполнения следующего утверждения все утверждения должны быть True not означает, что если условие оценивается как False, то оно является True. На мой взгляд, это самый запутанный вариант. Давайте рассмотрим несколько примеров каждого из них. Мы начнем с or.
x = 10 y = 20 if x \u0026lt; 10 or y \u0026gt; 15: print(\u0026quot;This statement was True!\u0026quot;) Здесь мы создаем пару переменных и проверяем, если одна из них меньше десяти, а другая больше 15. Поскольку последняя больше 15, выполняется оператор print. Как видите, если одно или оба утверждения равны True, то выполняется оператор print. Давайте посмотрим, как работает end:
x = 10 y = 10 if x == 10 and y == 15: print(\u0026quot;This statement was True\u0026quot;) else: print(\u0026quot;The statement was False!\u0026quot;) Если вы выполните приведенный выше код, вы увидите, что первое утверждение не выполняется. Вместо него выполняется оператор под else. Почему так? Потому что мы проверяем, что x и y равны 10 и 15 соответственно. В данном случае это не так, поэтому мы переходим к else. Таким образом, когда вы соединяете два оператора вместе, оба оператора должны иметь значение True, чтобы выполнить следующий код. Также обратите внимание, что для проверки равенства в Python необходимо использовать двойной знак равенства. Одинарный знак равенства известен как оператор присваивания и предназначен только для присвоения значения переменной. Если бы вы попытались выполнить приведенный выше код с одним из операторов, имеющим только один знак равенства, вы бы получили сообщение о неправильном синтаксисе.
Обратите внимание, что вы также можете использовать or и and более чем два оператора вместе. Однако я не рекомендую этого делать, так как чем больше утверждений, тем сложнее их понять и отладить.
Теперь мы готовы рассмотреть операцию not.
my_list = [1, 2, 3, 4] x = 10 if x not in my_list: print(\u0026quot;'x' is not in the list, so this is True!\u0026quot;) В этом примере мы создаем список, содержащий четыре целых числа. Затем мы пишем тест, который спрашивает, нет ли \u0026ldquo;x\u0026rdquo; в этом списке. Поскольку \u0026ldquo;x\u0026rdquo; равно 10, оператор оценивается как True, и сообщение выводится на экран. Другим способом проверки на not является использование восклицательного знака, например, так:
x = 10 if x != 11: print(\u0026quot;x is not equal to 11!\u0026quot;) При желании вы можете комбинировать операцию not с двумя другими для создания более сложных условных операторов. Вот простой пример:
my_list = [1, 2, 3, 4] x = 10 z = 11 if x not in my_list and z != 10: print(\u0026quot;This is True!\u0026quot;) Проверка на ничто (None) Поскольку мы говорим об утверждениях, которые оцениваются в True, нам, вероятно, нужно рассказать о том, что оценивается в False. В Python есть ключевое слово False, которое я уже несколько раз упоминал. Однако пустая строка, кортеж или список также оцениваются как False. Есть также еще одно ключевое слово, которое в основном оценивается как False и называется None. Значение None используется для обозначения отсутствия значения. Это своего рода аналог Null, который можно встретить в базах данных. Давайте посмотрим на код, чтобы лучше понять, как все это работает:
empty_list = [] empty_tuple = () empty_string = \u0026quot;\u0026quot; nothing = None if empty_list == []: print(\u0026quot;It's an empty list!\u0026quot;) if empty_tuple: print(\u0026quot;It's not an empty tuple!\u0026quot;) if not empty_string: print(\u0026quot;This is an empty string!\u0026quot;) if not nothing: print(\u0026quot;Then it's nothing!\u0026quot;) Первые четыре строки задают четыре переменные. Далее мы создаем четыре условия для их проверки. Первое проверяет, действительно ли empty_list пуст. Второе условие проверяет, есть ли что-то в empty_tuple. Да, вы все правильно поняли, второе условие имеет значение True, только если кортеж не пуст! Последние два условия делают противоположное второму. Третье проверяет, является ли строка пустой, а четвертое - действительно ли переменная nothing является None.
Оператор not означает, что мы проверяем противоположное значение. Другими словами, мы проверяем, является ли значение НЕ True. Поэтому в третьем примере мы проверяем, является ли пустая строка ПО НАСТОЯЩЕМУ пустой. Вот другой способ написать то же самое:
if empty_string == \u0026quot;\u0026quot;: print(\u0026quot;This is an empty string!\u0026quot;) Чтобы действительно закрепить это, давайте установим переменную empty_string, чтобы она действительно содержала что-то:
\u0026gt;\u0026gt;\u0026gt; empty_string = \u0026quot;something\u0026quot; \u0026gt;\u0026gt;\u0026gt; if empty_string == \u0026quot;\u0026quot;: print(\u0026quot;This is an empty string!\u0026quot;) Если вы выполните это, то увидите, что ничего не будет выведено, так как мы выведем что-то только в том случае, если переменная является пустой строкой.
Обратите внимание, что ни одна из этих переменных не равна другой. Они просто оцениваются одинаково. Чтобы доказать это, мы рассмотрим пару быстрых примеров:
\u0026gt;\u0026gt;\u0026gt; empty_list == empty_string False \u0026gt;\u0026gt;\u0026gt; empty_string == nothing False Как видите, они не равны друг другу. В реальном мире вы часто будете проверять свои структуры данных на наличие данных. Некоторые программисты предпочитают просто обернуть свои структуры в обработчик исключений, и если они окажутся пустыми, они поймают исключение. Другие предпочитают использовать вышеупомянутую стратегию, когда вы действительно проверяете структуру данных на наличие в ней данных. Обе стратегии являются правильными.
Я нахожу оператор not немного запутанным и не использую его часто. Но время от времени он может оказаться полезным.
Специальные символы Строки могут содержать специальные символы, такие как табуляция или новая строка. Мы должны знать о них, поскольку иногда они могут появляться и вызывать проблемы. Например, символ новой строки определяется как \\n, а символ табуляции - как \\t. Давайте рассмотрим несколько примеров, чтобы вы лучше поняли, что они делают:
\u0026gt;\u0026gt;\u0026gt; print(\u0026quot;I have a \\n new line in the middle\u0026quot;) I have a new line in the middle \u0026gt;\u0026gt;\u0026gt; print(\u0026quot;This sentence is \\ttabbed!\u0026quot;) This sentence is tabbed! Был ли вывод таким, как вы ожидали? В первом примере в середине предложения стоит буква \u0026ldquo;n\u0026rdquo;, что заставляет вывести новую строку. Поскольку у нас есть пробел после символа новой строки, вторая строка отделена пробелом. Второй пример показывает, что происходит, когда внутри предложения есть символ табуляции.
Иногда в строке необходимо использовать управляющие символы, например, обратную косую черту. Чтобы использовать экранирующие символы, необходимо использовать обратную косую черту, поэтому в случае с обратной косой чертой необходимо ввести две обратные косые черты. Давайте посмотрим:
\u0026gt;\u0026gt;\u0026gt; print(\u0026quot;This is a backslash \\\u0026quot;) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; EOL while scanning string literal: \u0026lt;string\u0026gt;, line 1, pos 30 \u0026gt;\u0026gt;\u0026gt; print(\u0026quot;This is a backslash \\\\\u0026quot;) This is a backslash \\ Вы заметите, что первый пример сработал не очень хорошо. Python думал, что мы экранируем двойную кавычку, поэтому он не мог определить, где находится конец строки (EOL), и выдавал ошибку. Во втором примере обратная косая черта экранирована должным образом.
if name == “main” Вы увидите очень распространенный условный оператор, используемый во многих примерах Python. Вот как он выглядит
if __name__ == \u0026quot;__main__\u0026quot;: # do something! Располагается в конце файла. Это говорит Python, что вы хотите выполнить следующий код, только если эта программа будет выполнена как отдельный файл. Я часто использую эту конструкцию, чтобы проверить, что мой код работает так, как я ожидаю. Мы обсудим это позже в книге, но всякий раз, когда вы создаете сценарий Python, вы создаете модуль Python. Если вы напишете его хорошо, вы можете захотеть импортировать его в другой модуль. Когда вы импортируете модуль, он не будет выполнять код, находящийся под условным обозначением, потому что name больше не будет равен \u0026quot;main\u0026quot;.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter4_conditionals/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day04/":{title:"4. DevOps и Agile",tags:["devops","agile","scrum","kanban"],content:`DevOps и Agile Вы знаете разницу между DevOps и Agile? Они формировались как самостоятельные понятия. Но теперь эти два термина сливаются.
В этом посте мы рассмотрим важные различия между Agile и DevOps и выясним, почему они так тесно связаны.
Я думаю, что хорошее место для начала — это немного больше узнать об общем подходе, который я увидел в изучении этой области, а именно о DevOps и Agile, даже несмотря на то, что у них схожие цели и процессы. В этом разделе я, надеюсь, мы разберемся с этим.
Начнем с определений.
Разработка Agile Agile — это подход, который фокусируется на более быстром получении небольших результатов, а не на выгрзуке (релизе) одного большого обновления продукта; программное обеспечение разрабатывается итерациями (неболшими изменениями). Команда выпускает новую версию каждую неделю или месяц с дополнительными обновлениями. Итоговая цель Agile — предоставить конечным пользователям оптимальный опыт.
DevOps В течение последних нескольких дней мы освещали это несколькими различными способами описания конечных целей DevOps. DevOps обычно описывает разработку программного обеспечения и методы доставки, основанные на сотрудничестве между разработчиками программного обеспечения и специалистами по эксплуатации. Основными преимуществами DevOps являются упрощение процесса разработки и минимизация недопонимания.
В чем разница между Agile и DevOps Разница в основном в заботах. У Agile и DevOps разные интересы, но они помогают друг другу. Agile требует коротких итераций, что возможно только с автоматизацией, которую обеспечивает DevOps. Agile хочет, чтобы клиент попробовал конкретную версию и быстро дал отзыв, что возможно только в том случае, если DevOps упростит создание новой среды.
Разные участники Agile фокусируется на оптимизации взаимодействия между конечными пользователями и разработчиками, в то время как DevOps нацелен на разработчиков и членов операционной группы. Можно сказать, что Agile ориентирован на клиентов, тогда как DevOps — это набор внутренних практик.
Команда Agile обычно применяется к разработчикам программного обеспечения и руководителям проектов. Компетенции DevOps-инженеров лежат на стыке разработки, QA (обеспечения качества) и операций, поскольку они участвуют во всех этапах цикла продукта и являются частью Agile-команды.
Прикладные фреймворки В Agile есть много сред управления для достижения гибкости и прозрачности: Scrum \u0026gt; Kanban \u0026gt; Lean \u0026gt; Extreme \u0026gt; Crystal \u0026gt; Dynamic \u0026gt; Feature-Driven. DevOps фокусируется на подходе к разработке в сотрудничестве, но не предлагает конкретных методологий. Тем не менее, DevOps продвигает такие практики, как инфраструктура как код, архитектура как код, мониторинг, самовосстановление, сквозная автоматизация тестирования\u0026hellip; Но сама по себе это не структура, а практика.
Обратная связь В Agile основным источником обратной связи является конечный пользователь, тогда как в DevOps более высокий приоритет имеет обратная связь от заинтересованных сторон и самой команды.
Целевые области Agile фокусируется на разработке программного обеспечения больше, чем на развертывании и обслуживании. DevOps также фокусируется на разработке программного обеспечения, но его ценности и инструменты также охватывают этапы развертывания и после выпуска, такие как мониторинг, высокая доступность, безопасность и защита данных.
Документация Agile отдает предпочтение гибкости и поставленным задачам, а не документации и мониторингу. С другой стороны, DevOps рассматривает проектную документацию как один из основных компонентов проекта.
Риски Риски Agile вытекают из гибкости методологии. Гибкие проекты трудно предсказать или оценить, поскольку приоритеты и требования постоянно меняются.
Риски DevOps возникают из-за неправильного понимания термина и отсутствия подходящих инструментов. Некоторые люди рассматривают DevOps как набор программного обеспечения для развертывания и непрерывной интеграции, не способного изменить базовую структуру процесса разработки.
Используемые инструменты Agile-инструменты ориентированы на совместную управленческую коммуникацию, метрики и обработку отзывов. К наиболее популярным agile-инструментам относятся JIRA, Trello, Slack, Zoom, SurveyMonkey и другие.
DevOps использует инструменты для командного общения, разработки программного обеспечения, развертывания и интеграции, такие как Jenkins, GitHub Actions, BitBucket и т. д. Несмотря на то, что Agile и DevOps имеют несколько разные фокусы и области действия, ключевые значения почти идентичны, поэтому вы можете комбинировать их.
Собрать все вместе… хорошая идея или нет? Обсуждать? Сочетание Agile и DevOps дает следующие преимущества:
Гибкое управление и мощные технологии. Практики Agile помогают командам DevOps более эффективно сообщать о своих приоритетах. Стоимость автоматизации, которую вы должны заплатить за свои методы DevOps, оправдана вашим гибким требованием быстрого и частого развертывания. Это приводит к укреплению: команда, внедряющая agile-практики, улучшит сотрудничество, повысит мотивацию команды и снизит текучесть кадров. В результате вы получаете лучшее качество продукции. Agile позволяет вернуться к предыдущим этапам разработки продукта, чтобы исправить ошибки и предотвратить накопление технического долга. Принять Agile и DevOps одновременно просто выполните 7 шагов:
Объедините команды разработки и эксплуатации. Создайте команды сборки и запуска, все проблемы, связанные с разработкой и эксплуатацией, обсуждаются всей командой DevOps. Измените свой подход к спринтам и назначьте рейтинги приоритета, чтобы предлагать задачи DevOps, которые имеют такое же значение, как задачи разработки. Поощряйте команды разработчиков и эксплуатации обмениваться мнениями о рабочем процессе других команд и возможных проблемах. Включите контроль качества на все этапы разработки. Выбирайте правильные инструменты. Автоматизируйте все, что можете. Измеряйте и контролируйте, используя материальные числовые результаты. Что вы думаете? У вас разные взгляды? Я хочу услышать от разработчиков, специалистов по эксплуатации, QA или кого-либо, кто лучше разбирается в Agile и DevOps, которые могут поделиться комментариями и отзывами по этому поводу?
Источники DevOps for Developers – Day in the Life: DevOps Engineer in 2021 3 Things I wish I knew as a DevOps Engineer How to become a DevOps Engineer feat. Shawn Powers До встречи в День 5
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day04/"},"https://romankurnovskii.com/ru/docs/webrtc/remote-streams/":{title:"Удаленные потоки",tags:[],content:`Начало работы с удаленными потоками Как только RTCPeerConnection подключился к удаленному узлу, между ними можно передавать аудио- и видео-потоки. Это точка, в которой мы подключаем поток, полученный от getUserMedia(), к RTCPeerConnection. Медиаопоток состоит как минимум из одной дорожки мультимедиа, и они по отдельности добавляются в RTCPeerConnection, когда мы хотим передать данные удаленному узлу.
const localStream = await getUserMedia({vide: true, audio: true}); const peerConnection = new RTCPeerConnection(iceConfig); localStream.getTracks().forEach(track =\u0026gt; { peerConnection.addTrack(track, localStream); }); Дорожки можно добавлять в RTCPeerConnection до подключения к удаленному узлу, поэтому имеет смысл выполнить эту настройку как можно раньше, а не ждать завершения соединения.
Добавление удаленных дорожек Для получения удаленных дорожек, которые были добавлены другим узлом, мы регистрируем «прослушиватель» на локальном RTCPeerConnection, отслеживая изменения в событии track. RTCTrackEvent содержит массив объектов MediaStream, которые имеют те же значения MediaStream.id, что и соответствующие локальные потоки узла. В нашем примере каждая дорожка связана только с одним потоком.
Обратите внимание, что, хотя ID из MediaStream совпадают на обеих сторонах однорангового соединения, в общем случае это не работает для ID MediaStreamTrack.
const remoteVideo = document.querySelector('#remoteVideo'); peerConnection.addEventListener('track', async (event) =\u0026gt; { const [remoteStream] = event.streams; remoteVideo.srcObject = remoteStream; }); `,url:"https://romankurnovskii.com/ru/docs/webrtc/remote-streams/"},"https://romankurnovskii.com/ru/docs/python101/chapter5_loops/":{title:"5. Циклы",tags:[],content:`В каждом языке программирования, который я пробовал, есть какая-то конструкция циклов. В большинстве из них их больше одного. В мире Python есть два типа циклов:
цикл for и цикл while Вы увидите, что цикл for является самым популярным. Циклы используются, когда вы хотите сделать что-то много раз. Обычно вам нужно выполнить какую-то операцию или набор операций над фрагментом данных снова и снова. Вот здесь-то и приходят на помощь циклы. С их помощью очень легко применять подобную логику к вашим данным.
Давайте начнем изучать, как работают эти забавные структуры!
Цикл for Как упоминалось выше, цикл используется, когда вы хотите выполнить итерацию по чему-либо n-ное количество раз. Это будет немного проще понять, если мы рассмотрим пример. Давайте воспользуемся встроенной в Python функцией range. Функция range создаст список длиной n.
\u0026gt;\u0026gt;\u0026gt; range(5) range(0, 5) Как видите, вышеприведенная функция range принимает целое число и возвращает объект range. Функция range также принимает начальное значение, конечное значение и значение шага. Вот еще два примера:
\u0026gt;\u0026gt;\u0026gt; range(5,10) range(5, 10) \u0026gt;\u0026gt;\u0026gt; list(range(1, 10, 2)) [1, 3, 5, 7, 9] Первый пример демонстрирует, что вы можете передать начальное и конечное значение, и функция range вернет числа от начального значения до конечного, но не включая его. Таким образом, в случае 5-10 мы получим 5-9. Второй пример показывает, как использовать функцию list, чтобы заставить функцию range возвращать каждый второй элемент от 1 до 10. Таким образом, она начинает с единицы, пропускает два и т.д. Теперь вам, наверное, интересно, какое отношение это имеет к циклам. Один из простых способов показать, как работает цикл, - это использовать функцию range! Посмотрите:
\u0026gt;\u0026gt;\u0026gt; for number in range(5): print(number) 0 1 2 3 4 Что здесь произошло? Давайте прочитаем слева направо, чтобы понять это. Для каждого числа в диапазоне 5 выведите это число. Мы знаем, что если вызвать range со значением 5, то он вернет список из 5 элементов. Поэтому каждый раз, проходя через цикл, он выводит каждый из элементов. Приведенный выше цикл for был бы эквивалентен следующему:
\u0026gt;\u0026gt;\u0026gt; for number in [0, 1, 2, 3, 4]: print(number) Функция range просто делает его немного меньше. Цикл for может выполнять цикл над любым итератором Python. Мы уже видели, как он может выполнять итерацию над списком. Давайте посмотрим, может ли он также выполнять итерацию над словарем
\u0026gt;\u0026gt;\u0026gt; a_dict = {\u0026quot;one\u0026quot;:1, \u0026quot;two\u0026quot;:2, \u0026quot;three\u0026quot;:3} \u0026gt;\u0026gt;\u0026gt; for key in a_dict: print(key) three two one Когда вы используете цикл for со словарем, вы увидите, что он автоматически перебирает ключи. Нам не нужно было говорить for key в a_dict.keys() (хотя это тоже сработало бы). Python просто сделал все правильно за нас. Вам возможно интересно, почему ключи выводятся в другом порядке, чем они были определены в словаре. Как вы помните из главы 3, словари неупорядочены, поэтому при итерации по ним ключи могут располагаться в любом порядке.
Теперь, если вы знаете, что ключи можно отсортировать, вы можете сделать это до того, как начнете итерацию по ним. Давайте немного изменим словарь, чтобы посмотреть, как это работает.
\u0026gt;\u0026gt;\u0026gt; a_dict = {1:\u0026quot;one\u0026quot;, 2:\u0026quot;two\u0026quot;, 3:\u0026quot;three\u0026quot;} \u0026gt;\u0026gt;\u0026gt; keys = a_dict.keys() \u0026gt;\u0026gt;\u0026gt; keys = sorted(keys) \u0026gt;\u0026gt;\u0026gt; for key in keys: print(key) 1 2 3 Давайте немного разберемся, что делает этот код. Во-первых, мы создаем словарь, в котором ключами являются целые числа, а не строки. Затем мы извлекаем ключи из словаря. Каждый раз, когда вы вызываете метод keys(), он возвращает неупорядоченный список ключей. Если вы выведите их и обнаружите, что они расположены в порядке возрастания, то это просто случайность. Теперь у нас есть представление ключей словаря, которые хранятся в переменной keys. Мы сортируем ее, а затем с помощью цикла for перебираем ее.
Теперь мы готовы сделать все немного интереснее. Мы собираемся перебирать диапазон, но хотим вывести только четные числа. Для этого мы хотим использовать условный оператор вместо параметра step диапазона. Вот один из способов сделать это:
\u0026gt;\u0026gt;\u0026gt; for number in range(10): if number % 2 == 0: print(number) 0 2 4 6 8 Вы, вероятно, задаетесь вопросом, что здесь происходит. Что случилось со знаком процента? В Python знак % называется оператором модуляции. Когда вы используете оператор модуляции, он возвращает остаток. При делении четного числа на два остатка нет, поэтому мы выводим эти числа. Вы, скорее всего, не будете часто использовать оператор modulus, но я думаю он может быть полезным время от времени.
Теперь мы готовы к изучению цикла while.
Цикл while Цикл while также используется для повторения участков кода, но вместо того, чтобы повторять цикл n раз, он будет повторяться только до тех пор, пока не будет выполнено определенное условие. Давайте рассмотрим очень простой пример:
\u0026gt;\u0026gt;\u0026gt; i = 0 \u0026gt;\u0026gt;\u0026gt; while i \u0026lt; 10: print(i) i = i + 1 Цикл while - это что-то вроде условного оператора. Вот что означает этот код: пока переменная i меньше десяти, выведите это значение. Затем в конце мы увеличиваем значение i на единицу. Если вы запустите этот код, он должен вывести 0-9, каждый на своей строке, а затем остановиться. Если вы удалите ту часть, где мы увеличиваем значение i, то в итоге у вас получится бесконечный цикл. Это обычно плохо. Бесконечных циклов следует избегать, они известны как логические ошибки.
Существует другой способ выйти из цикла. Использованием встроенной функции break. Давайте посмотрим, как это работает:
\u0026gt;\u0026gt;\u0026gt; while i \u0026lt; 10: print(i) if i == 5: break i += 1 0 1 2 3 4 5 В этом фрагменте кода мы добавляем условие, чтобы проверить, равна ли переменная i 5 когда-либо. Если да, то мы выходим из цикла. Как видно из примера вывода, как только значение i достигает 5, код останавливается, несмотря на то, что мы сказали циклу while продолжать цикл, пока значение не достигнет 10. Вы также заметите, что мы изменили способ увеличения значения, использовав +=. Это удобное сокращение, которое можно использовать и для других математических операций, таких как вычитание (-=) и умножение (*=).
Встроенная функция break известна как инструмент управления потоком. Есть еще одна, называемая continue, которая используется для пропуска итерации или продолжения следующей итерации. Вот один из способов её использования:
i = 0 while i \u0026lt; 10: if i == 3: i += 1 continue print(i) if i == 5: break i += 1 Немного запутано, не так ли? По сути, мы добавили второе условие, которое проверяет, равно ли i 3. Если да, то мы увеличиваем переменную и продолжаем следующий цикл, который фактически пропускает вывод значения 3 на экран. Как и раньше, когда мы достигаем значения 5, мы выходим из цикла.
Есть еще одна тема, которую мы должны рассмотреть в отношении циклов, и это оператор else.
Для чего нужно else в циклах Оператор else в циклах выполняется только в случае успешного завершения цикла. В основном оператор else используется для поиска элементов:
my_list = [1, 2, 3, 4, 5] for i in my_list: if i == 3: print(\u0026quot;Item found!\u0026quot;) break print(i) else: print(\u0026quot;Item not found!\u0026quot;) В этом коде мы выходим из цикла, когда i становится равным 3. В результате оператор else будет пропущен. Если вы хотите поэкспериментировать, вы можете изменить условие так, чтобы оно искало значение, которого нет в списке, что приведет к выполнению оператора else. Честно говоря, за все годы работы программистом я ни разу не видел, чтобы кто-то использовал эту структуру. Большинство примеров, которые я видел, это блоггеры, пытающиеся объяснить, для чего она используется. Я видел несколько примеров, когда она используется для выдачи ошибки, если элемент не найден в итерабельной таблице, где был поиск. Вы можете прочитать довольно подробную статью одного из разработчиков ядра Python здесь.
Подведение итогов Надеюсь, теперь вы видите ценность циклов Python. Они упрощают повторение и довольно просты для понимания. Скорее всего, вы будете видеть цикл for гораздо чаще, чем цикл while. На самом деле, мы рассмотрим еще один способ использования цикла for в следующей главе, когда будем изучать генераторы! Если вы все еще не совсем понимаете, как все работает, возможно, вам стоит перечитать эту главу, прежде чем продолжать.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter5_loops/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day05/":{title:"5. Plan \u003e Code \u003e Build \u003e Testing \u003e Release \u003e Deploy \u003e Operate \u003e Monitor",tags:["devops","cicd","tests","learning"],content:`Сегодня мы сосредоточимся на отдельных шагах от начала до конца и на непрерывном цикле приложения в мире DevOps.
План Все начинается с процесса планирования, когда команда разработчиков собирается вместе и выясняет, какие типы функций и исправлений ошибок они собираются внедрить в следующем спринте. Это возможность для вас как инженера DevOps принять участие в этом и узнать, какие вещи будут происходить на вашем пути, с которыми вам нужно участвовать, а также повлиять на их решения или их путь и как бы помочь им работать с инфраструктура, которую вы построили, или направьте их к чему-то, что будет работать лучше для них, если они не на этом пути, и поэтому одна ключевая вещь, на которую здесь следует указать, это то, что разработчики или команда разработчиков программного обеспечения являются вашим клиентом как DevOps инженер, так что это ваша возможность поработать с вашим клиентом до того, как он пойдет по плохому пути.
Code Теперь, как только эта сессия планирования будет завершена, разработчики начинают писать код, в разработку котоого вы можете быть вовлечены, предоставляя информацю об инфрастуктуре, микросеврисах, если таковые имеются, и т.д. Когда разработчики заканчивают писать код/часть кода, они объединяют (merge) все измененияю и выгруат в репозиторий.
Build Здесь мы начнем первый из наших процессов автоматизации, потому что мы \u0026ldquo;возьмем\u0026rdquo; их код и построим (скомпилируем, \u0026ldquo;сбилдим\u0026rdquo;) его в зависимости от того, какой язык они используют, это может быть транспиляция или компиляция, а может создать образ докера из этого кода в любом случае, мы собираемся пройти этот процесс, используя наш cicd pipeline (\u0026ldquo;пайплайн\u0026rdquo;)
Testing После того, как мы его скомпилировали проект, мы проведем на нем несколько тестов. Команда разработчиков обычно пишет тесты. У вас может быть некоторый вклад в то, какие тесты пишутся, но нам нужно запустить эти тесты. Тестирование — это способ провериь и свести к минимуму появление проблем в рабочей среде. И хотя это не гарантирует полной проверки, но мы хотим максимально точно быть уверенными, что одна из новых функций не создает новых ошибок, а две другие не ломают то, что раньше работало.
Release Как только эти тесты пройдены, мы собираемся выполнить процесс выпуска, и, опять же, в зависимости от того, над каким типом приложения вы работаете, это может быть поэтапным. Код может просто находиться в репозитории GitHub или репозитории git или где-то еще, а также это может быть процесс зарузки вашего скомпилированного кода или созданного образа докера и помещения его в реестр или репозиторий, где он находится.
Deploy Следующее, что мы собираемся сделать - это \u0026ldquo;деплой\u0026rdquo; (публикация/развертывание). Развертывание похоже на конечный результат процесса. Потому что после развертывания приложения, когда мы запускаем код в производство, наш бизнес действительно осознает ценность всех временных усилий и тяжелой работы, которые вы и команда разработчиков программного обеспечения вложили в этот продукт до этого момента.
Operate После того, как код выгружен скомпилирован, мы собираемся эксплуатировать его, и эксплуатация может включать в себя что-то вроде того, что вы начинаете получать звонки от своих клиентов, которые все раздражены тем, что сайт работает медленно или их приложение работает медленно, поэтому вам нужно выяснить, почему это так. А а затем, возможно, создать автоматическое масштабирование, которое связано с увеличением количества серверов, доступных в пиковые периоды, и уменьшением количества серверов в непиковые периоды.
Monitor Все вышеперечисленные части ведут к последнему шагу - мониторингу, что важно особенно в отношении проблем, возникающих в рельном времени, автоматического масштабирования, устранения неполадок. Во время мониторига мы сохраняем данные об использовании памяти, использовании ЦП на диске, времени отклика, скорость отклика и т.д. Большая часть этого также является журналами. Журналы дают разработчикам возможность видеть, что происходит, без доступа к производственным системам.
Rince \u0026amp; Repeat Once that\u0026rsquo;s in place you go right back to the beginning to the planning stage and go through the whole thing again
Continuous Многие инструменты помогают нам достичь вышеуказанного непрерывного процесса, весь этот код и конечная цель полной автоматизации облачной инфраструктуры или любой среды часто описывается как непрерывная интеграция/непрерывная доставка/непрерывное развертывание или сокращенно «CI/CD». Позже, в течение 90 дней, мы посвятим целую неделю CI/CD с некоторыми примерами и пошаговыми руководствами, чтобы понять основы.
Continuous Delivery Continuous Delivery = Plan \u0026gt; Code \u0026gt; Build \u0026gt; Test
Continuous Integration Непрерывная интеграция - это результат описанных выше этапов непрерывной \u0026ldquo;доставки\u0026rdquo; и результат этапа выпуска. Это относится как к неудаче, так и к успеху, но это возвращается в непрерывную доставку или перемещается в непрерывное развертывание.
Continuous Integration = Plan \u0026gt; Code \u0026gt; Build \u0026gt; Test \u0026gt; Release
Continuous Deployment Если у вас есть успешный релиз, перейдите к непрерывному развертыванию, которое включает следующие этапы.
Выпуск CI выполнен успешно = непрерывное развертывание = развертывание \u0026gt; эксплуатация \u0026gt; мониторинг
Вы можете рассматривать эти три понятия выше как простой набор фаз жизненного цикла DevOps.
Этот последний фрагмент был для меня чем-то вроде подведения итогов третьего дня, но думаю, что на самом деле это проясняет для меня ситуацию.
Источники DevOps for Developers – Software or DevOps Engineer? Techworld with Nana -DevOps Roadmap 2022 - How to become a DevOps Engineer? What is DevOps? How to become a DevOps Engineer in 2021 - DevOps Roadmap До встречи в День 6
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day05/"},"https://romankurnovskii.com/ru/docs/webrtc/data-channels/":{title:"Каналы данных",tags:[],content:`Стандарт WebRTC также охватывает API для отправки произвольных данных через RTCPeerConnection. Это происходит через запрос createDataChannel() для объекта RTCPeerConnection, который возвращает объект RTCDataChannel.
const peerConnection = new RTCPeerConnection(configuration); const dataChannel = peerConnection.createDataChannel();
Удаленный узел может получать каналы данных через отслеживание события datachannel в объекте RTCPeerConnection. Полученное событие имеет тип RTCDataChannelEvent и содержит свойство channel, которое представляет RTCDataChannel между двумя узлами.
const peerConnection = new RTCPeerConnection(configuration); peerConnection.addEventListener('datachannel', event =\u0026gt; { const dataChannel = event.channel; }); События Open и Close Прежде чем канал данных можно будет использовать для отправки данных, клиент должен дождаться его открытия. Это происходит через прослушивание события open. Точно так же существует событие close, когда одна из сторон закрывает канал.
const messageBox = document.querySelector('#messageBox'); const sendButton = document.querySelector('#sendButton'); const peerConnection = new RTCPeerConnection(configuration); const dataChannel = peerConnection.createDataChannel(); // Enable textarea and button when opened dataChannel.addEventListener('open', event =\u0026gt; { messageBox.disabled = false; messageBox.focus(); sendButton.disabled = false; }); // Disable input when closed dataChannel.addEventListener('close', event =\u0026gt; { messageBox.disabled = false; sendButton.disabled = false; }); Сообщения Отправка сообщения в RTCDataChannel выполняется через вызов функции send() с данными, которые мы хотим отправить. Параметр data для этой функции может быть типа String, Blob, ArrayBuffer или ArrayBufferView.
const messageBox = document.querySelector('#messageBox'); const sendButton = document.querySelector('#sendButton'); // Send a simple text message when we click the button sendButton.addEventListener('click', event =\u0026gt; { const message = messageBox.textContent; dataChannel.send(message); }) Удаленный узел будет получать сообщения, отправленные на RTCDataChannel, через отслеживание события message.
const incomingMessages = document.querySelector('#incomingMessages'); const peerConnection = new RTCPeerConnection(configuration); const dataChannel = peerConnection.createDataChannel(); // Append new messages to the box of incoming messages dataChannel.addEventListener('message', event =\u0026gt; { const message = event.data; incomingMessages.textContent += message + '\\n'; }); `,url:"https://romankurnovskii.com/ru/docs/webrtc/data-channels/"},"https://romankurnovskii.com/ru/docs/python101/chapter6_comprehensions/":{title:"6. Генераторы в Python",tags:[],content:`В языке Python есть несколько методов создания списков и словарей, которые известны как генераторы. Существует также третий тип генератора для создания набора в Python. В этой главе мы узнаем, как использовать каждый тип генераторов. Вы увидите, что конструкция генератора основываются на знаниях, полученных из предыдущих глав, поскольку они содержат циклы и условия.
Генераторы списков Генераторы списков в Python очень удобны. Но их также бывает трудно понять, когда и зачем их использовать. Генераторы списков, как правило, сложнее для чтения, чем простое использование цикла for. Возможно, вы захотите просмотреть главу о циклах, прежде чем продолжить.
Если вы готовы, то мы потратим немного времени на рассмотрение того, как строить генераторы списков и узнаем, как их можно использовать. Генератор списка - это, по сути, однострочный цикл for, который создает структуру данных Python в виде списка. Вот простой пример:
\u0026gt;\u0026gt;\u0026gt; x = [i for i in range(5)] Давайте немного разберемся в этом. В Python есть функция range, которая может возвращать список чисел. По умолчанию она возвращает целые числа, начиная с 0 и заканчивая числом, которое вы ей передали, но не включая его. В данном случае она возвращает список, содержащий целые числа 0-4. Это может быть полезно, если вам нужно быстро создать список. Например, вы разбираете файл и ищете что-то конкретное. Вы можете использовать генератор списка в качестве своеобразного фильтра:
if [i for i in line if \u0026quot;SOME TERM\u0026quot; in i]: # do something Я использовал код, подобный этому, для быстрого просмотра файла, чтобы разобрать определенные строки или разделы файла. Когда вы добавляете функции, вы можете начать делать действительно интересные вещи. Допустим, вы хотите применить функцию к каждому элементу списка, например, вам нужно преобразовать кучу строк в целые числа:
\u0026gt;\u0026gt;\u0026gt; x = ['1', '2', '3', '4', '5'] \u0026gt;\u0026gt;\u0026gt; y = [int(i) for i in x] \u0026gt;\u0026gt;\u0026gt; y [1, 2, 3, 4, 5] Такое встречается чаще, чем вы думаете. Мне также приходилось обращаться к списку строк и вызывать строковый метод, например, strip, потому что в них были всевозможные ведущие или конечные пробелы:
\u0026gt;\u0026gt;\u0026gt; myStrings = [s.strip() for s in myStringList] Бывают случаи, когда необходимо создать генерацию вложенного списка. Одна из причин для этого - сглаживание нескольких списков в один. Этот пример взят из документации Python:
\u0026gt;\u0026gt;\u0026gt; vec = [[1,2,3], [4,5,6], [7,8,9]] \u0026gt;\u0026gt;\u0026gt; [num for elem in vec for num in elem] [1, 2, 3, 4, 5, 6, 7, 8, 9] В документации приведено несколько других интересных примеров для понимания генерации вложенных списков. Я настоятельно рекомендую взглянуть на нее! К этому моменту вы уже должны уметь применять генераторы списков в своем собственном коде и применять их хорошо. Просто используйте свое воображение, и вы начнете видеть много хороших мест, где вы тоже можете их использовать.
Теперь мы готовы перейти к работе со словарями Python!
Генераторы словарей Генераторы словарей появились в Python 3.0. Первоначально они были предложены в предложении 274 (PEP 274) по усовершенствованию Python еще в 2001 году. По своей организации они очень похожи на списки.
Лучший способ понять это - просто сделать один!
\u0026gt;\u0026gt;\u0026gt; print( {i: str(i) for i in range(5)} ) {0: '0', 1: '1', 2: '2', 3: '3', 4: '4'} Это довольно простой генератор. По сути, он создает целочисленный ключ и строковое значение для каждого элемента в диапазоне. Теперь вы можете задаться вопросом, как можно использовать генерацию словаря в реальной жизни. Марк Пилгрим упомянул, что вы можете использовать это для замены ключей и значений словаря. Например, вот так:
\u0026gt;\u0026gt;\u0026gt; my_dict = {1:\u0026quot;dog\u0026quot;, 2:\u0026quot;cat\u0026quot;, 3:\u0026quot;hamster\u0026quot;} \u0026gt;\u0026gt;\u0026gt; print( {value:key for key, value in my_dict.items()} ) {'hamster': 3, 'dog': 1, 'cat': 2} Это будет работать только в том случае, если генераторы словаря имеют неизменяемый тип, например, строку. В противном случае вы вызовете исключение.
Я также вижу, что генератор словаря может быть полезен для создания таблицы из переменных класса и их значений. Однако на данный момент мы не рассматривали классы, поэтому я не буду вас в этом запутывать.
Генератор множеств Генератор множеств создается примерно так же, как и генератор словарей. Множество Python во многом похоже на математическое множество, поскольку в нем нет повторяющихся элементов. Вы можете создать обычное множество следующим образом:
\u0026gt;\u0026gt;\u0026gt; my_list = [1, 2, 2, 3, 4, 5, 5, 7, 8] \u0026gt;\u0026gt;\u0026gt; my_set = set(my_list) \u0026gt;\u0026gt;\u0026gt; my_set set([1, 2, 3, 4, 5, 7, 8]) Как видно из примера выше, вызов set удалил дубликаты из списка. Теперь давайте перепишем этот код для использования генератора set:
\u0026gt;\u0026gt;\u0026gt; my_list = [1, 2, 2, 3, 4, 5, 5, 7, 8] \u0026gt;\u0026gt;\u0026gt; my_set = {x for x in my_list} \u0026gt;\u0026gt;\u0026gt; my_set set([1, 2, 3, 4, 5, 7, 8]) Вы заметите, что для создания генератора set мы просто заменили квадратные скобки, которые используются в генераторе списка, на фигурные скобки, которые используются в генераторе словаря.
Подведение итогов Теперь вы знаете, как использовать различные виды генераторов в Python. Вероятно, поначалу вам покажется, что наиболее полезным и популярным является генератор списка. Если вы начнете использовать свое воображение, я уверен, что вы сможете найти применение всем трем типам генераторов. Теперь мы готовы двигаться дальше и изучать обработку исключений!
Ресурсы https://vegibit.com/python-comprehension-tutorial/ `,url:"https://romankurnovskii.com/ru/docs/python101/chapter6_comprehensions/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day06/":{title:"6. DevOps - Истории",tags:["devops"],content:`DevOps - Истории компаний DevOps с самого начала считался недосягаемым для многих из нас, поскольку у нас не было среды или требований, подобных Netflix или Fortune 500, но подумайте, что теперь это начинает становиться нормой, когда мы внедряем практику DevOps внутри. любой вид бизнеса.
По второй ссылке ниже в справочных материалах вы увидите множество различных отраслей и вертикалей, использующих DevOps и оказывающих огромное положительное влияние на свои бизнес-цели.
Очевидно, что основным преимуществом здесь является DevOps, если он выполнен правильно, он должен помочь вашему бизнесу повысить скорость и качество разработки программного обеспечения.
Я хотел использовать этот день, чтобы посмотреть на успешные компании, которые внедрили практику DevOps, и поделиться некоторыми ресурсами по этому поводу. Приняли ли вы культуру DevOps в своем бизнесе? Был ли он успешным?
Я упомянул Netflix выше и коснусь их снова, поскольку это очень хорошая модель, которая даже до сих пор продвинута к тому, что мы обычно видим сегодня, но также упомяну некоторые другие известные бренды, которые, похоже, преуспевают.
Amazon В 2010 году Amazon переместила свои физические серверы в облако Amazon Web Services (AWS), что позволило им сэкономить ресурсы за счет увеличения и уменьшения емкости с очень небольшими приращениями. Мы также знаем, что это облако AWS продолжит свое существование и будет приносить огромный доход, продолжая управлять розничным филиалом компании Amazon.
Amazon внедрила в 2011 году (согласно приведенному ниже ресурсу) непрерывный процесс развертывания, при котором их разработчики могли развертывать код в любое время и на любых серверах, которые им нужны. Это позволило Amazon добиться развертывания нового программного обеспечения на производственных серверах в среднем каждые 11,6 секунды!
NetFlix Кто не пользуется NetFlix? очевидно, это огромный качественный потоковый сервис, который, по крайней мере, лично для всех, обеспечивает отличный пользовательский опыт.
Почему этот пользовательский опыт так хорош? Что ж, возможность предоставить услугу без воспоминаний, по крайней мере, о сбоях, требует скорости, гибкости и внимания к качеству.
Разработчики NetFlix могут автоматически встраивать фрагменты кода в развертываемые веб-образы, не полагаясь на ИТ-операции. По мере обновления изображений они интегрируются в инфраструктуру Netflix с помощью специально созданной веб-платформы.
Непрерывный мониторинг выполняется таким образом, что в случае сбоя развертывания образов новые образы откатываются, а трафик перенаправляется на предыдущую версию.
Ниже приводится отличная беседа, в которой подробно рассказывается о том, что нужно и чего нельзя делать, по которым Netflix живет и умирает в своих командах.
Etsy Как и у многих из нас и многих компаний, медленные и болезненные развертывания были настоящим испытанием. В том же духе мы могли бы также работать в компаниях, которые имеют много бункеров и команд, которые не очень хорошо работают вместе.
Из того, что я могу понять, по крайней мере, из чтения об Amazon и Netflix, Etsy, возможно, разрешила разработчикам развертывать свой собственный код примерно в конце 2009 года, что могло быть до двух других упомянутых. (интересный!)
Интересный вывод, который я прочитал здесь, заключался в том, что они поняли, что когда разработчики чувствуют ответственность за развертывание, они также берут на себя ответственность за производительность приложения, время безотказной работы и другие цели.
Культура обучения является ключевой частью DevOps, даже неудача может стать успехом, если извлечь уроки. (не уверен, откуда на самом деле взялась эта цитата, но она имеет смысл!)
Я добавил несколько других историй о том, как DevOps изменил правила игры в некоторых из этих чрезвычайно успешных компаний.
Источники How Netflix Thinks of DevOps 16 Popular DevOps Use Cases \u0026amp; Real Life Applications [2021] DevOps: The Amazon Story How Etsy makes DevOps work Adopting DevOps @ Scale Lessons learned at Hertz, Kaiser Permanente and lBM Interplanetary DevOps at NASA JPL Target CIO explains how DevOps took root inside the retail giant Подведем итоги наших первых дней, посвященных DevOps. DevOps — это комбинация разработки и эксплуатации, которая позволяет одной команде управлять всем жизненным циклом разработки приложения, состоящим из разработки, тестирования, развертывания, эксплуатации.
Основное внимание и цель DevOps — сократить жизненный цикл разработки, часто предоставляя функции, исправления и функциональные возможности в тесном соответствии с бизнес-целями.
DevOps — это подход к разработке программного обеспечения, с помощью которого программное обеспечение может поставляться и разрабатываться надежно и быстро. Вы также можете увидеть это как Непрерывная разработка, тестирование, развертывание, мониторинг
До встречи в День 7
На седьмой день мы погрузимся в язык программирования. Я не стремлюсь быть разработчиком, но хочу понимать, что делают разработчики.
Можем ли мы достичь этого за неделю? Вероятно, нет, но если мы потратим 7 дней или 7 часов на изучение чего-то, мы будем знать больше, чем когда мы начинали.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day06/"},"https://romankurnovskii.com/ru/docs/webrtc/turn-server/":{title:"TURN сервер",tags:[],content:`Для работы большинства приложений WebRTC необходим сервер для ретрансляции трафика между узлами, поскольку прямой сокет часто невозможен между клиентами (если только они не находятся в одной локальной сети). Обычный способ решить эту проблему — использовать TURN-сервер (Traversal Using Relay NAT), который представляет собой протокол ретрансляции сетевого трафика.
В настоящее время существует несколько вариантов TURN-серверов, доступных в Интернете, как в виде самостоятельных приложений (например, проект COTURN с открытым исходным кодом), так и в виде облачных сервисов.
Если у вас есть доступный онлайн TURN-сервер, то все, что вам нужно - это правильная RTCConfiguration для вашего клиентского приложения. Следующий фрагмент кода иллюстрирует пример конфигурации для RTCPeerConnection, где TURN-сервер hostname my-turn-server.mycompany.com работает на порту 19403.
Объект конфигурации также поддерживает свойства username и credentials для защиты доступа к серверу. Они необходимы при подключении к TURN-серверу.
const iceConfiguration = { iceServers: [ { urls: 'turn:my-turn-server.mycompany.com:19403', username: 'optional-username', credentials: 'auth-token' } ] } const peerConnection = new RTCPeerConnection(iceConfiguration); `,url:"https://romankurnovskii.com/ru/docs/webrtc/turn-server/"},"https://romankurnovskii.com/ru/docs/python101/chapter7_exception_handling/":{title:"7. Обработка исключений",tags:[],content:`Что вы делаете, когда в вашей программе происходит что-то плохое? Допустим, вы пытаетесь открыть файл, но вводите неправильный путь или запрашиваете у пользователя информацию, а он вводит какой-то мусор. Вы не хотите, чтобы ваша программа аварийно завершилась, поэтому вы реализуете обработку исключений. В Python эта конструкция обычно обернута в так называемый try/except. В этой главе мы рассмотрим следующие темы:
Общие типы исключений Обработка исключений с помощью try/except Изучим, как работает try/except/finally Исследуем, как оператор else работает в сочетании с try/except. Давайте начнем с изучения некоторых наиболее распространенных исключений, которые встречаются в Python. Примечание: ошибка и исключение - это просто разные слова, которые описывают одно и то же, когда мы говорим об обработке исключений.
Распространенные исключения Вы уже видели несколько исключений. Вот список наиболее распространенных встроенных исключений (определения взяты из документации Python):
- **Exception** (это то, на чем построены почти все остальные) - **AttributeError** - Возникает, когда ссылка на атрибут или присвоение не удается. - **IOError** - Возникает, когда операция ввода/вывода (например, оператор печати, встроенная функция open() или метод объекта файла) не выполняется по причине, связанной с вводом/выводом, например, \u0026quot;файл не найден\u0026quot; или \u0026quot;диск заполнен\u0026quot;. - **ImportError** - Возникает, когда оператор import не может найти определение модуля или когда при **импорте from ...** не удается найти имя, которое должно быть импортировано. - *IndexError** - Возникает, когда подскрипт последовательности выходит за пределы диапазона. - **KeyError** - Возникает, когда ключ отображения (словаря) не найден в наборе существующих ключей. - **KeyboardInterrupt** - Возникает, когда пользователь нажимает клавишу прерывания (обычно Control-C или Delete). - **NameError** - Возникает, когда не найдено локальное или глобальное имя. - **OSError** - Возникает, когда функция возвращает системную ошибку. - **SyntaxError** - Возникает, когда синтаксический анализатор встречает синтаксическую ошибку. - **TypeError** - Возникает, когда операция или функция применяется к объекту неподходящего типа. Связанное значение представляет собой строку, содержащую подробную информацию о несоответствии типа. - **ValueError** - Возникает, когда встроенная операция или функция получает аргумент, имеющий правильный тип, но несоответствующее значение, и ситуация не описывается более точным исключением, таким как IndexError. - **ZeroDivisionError** - Возникает, когда второй аргумент операции деления или модуляции равен нулю. Существует также множество других исключений, но, скорее всего, вы не будете сталкиваться с ними так часто. Однако, если вам интересно, вы можете прочитать о них в документации Python.
Как работать с исключениями Обработка исключений в Python очень проста. Давайте потратим немного времени на написание нескольких примеров, которые будут вызывать исключения. Мы начнем с одной из самых распространенных проблем в информатике: деление на ноль.
\u0026gt;\u0026gt;\u0026gt; 1 / 0 Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; ZeroDivisionError: integer division or modulo by zero \u0026gt;\u0026gt;\u0026gt; try: 1 / 0 except ZeroDivisionError: print(\u0026quot;You cannot divide by zero!\u0026quot;) You cannot divide by zero! Если вы вспомните уроки начальной математики, то вспомните, что нельзя делить на ноль. В Python эта операция приводит к ошибке, как видно из первой половины примера. Чтобы поймать ошибку, мы обернем операцию оператором try/except.
Голые исключения
Вот еще один способ поймать ошибку:
\u0026gt;\u0026gt;\u0026gt; try: 1 / 0 except: print(\u0026quot;You cannot divide by zero!\u0026quot;) Это не рекомендуется! В Python это известно как \u0026ldquo;голое исключение\u0026rdquo;, , что означает, что будут найдены вообще все исключения. Причина, по которой не рекомендуется так делать, заключается в том, что вы не знаете, какое именно исключение вы отлавливаете. Когда у вас есть что-то вроде except ZeroDivisionError, вы, очевидно, пытаетесь поймать ошибку деления на ноль. В коде, написанном выше, вы не можете указать, что именно вам нужно выявить.
Давайте рассмотрим еще несколько примеров.
\u0026gt;\u0026gt;\u0026gt; my_dict = {\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2, \u0026quot;c\u0026quot;:3} \u0026gt;\u0026gt;\u0026gt; try: value = my_dict[\u0026quot;d\u0026quot;] except KeyError: print(\u0026quot;That key does not exist!\u0026quot;) That key does not exist! \u0026gt;\u0026gt;\u0026gt; my_list = [1, 2, 3, 4, 5] \u0026gt;\u0026gt;\u0026gt; try: my_list[6] except IndexError: print(\u0026quot;That index is not in the list!\u0026quot;) That index is not in the list! В первом примере мы создаем словарь из 3 элементов. Затем мы пытаемся получить доступ к ключу, которого нет в словаре. Поскольку ключа нет в словаре, возникает ошибка KeyError, которую мы выявляем. Во втором примере показан список длиной 5 элементов. Мы пытаемся взять 7-й объект из индекса. Помните, что списки Python начинаются с нуля поэтому когда вы говорите [6], вы запрашиваете 7-й элемент. В любом случае поскольку элементов всего 5, возникает ошибка IndexError, которую мы также выявляем.
Вы также можете поймать несколько исключений с помощью одного оператора. Есть несколько различных способов сделать это. Давайте посмотрим:
my_dict = {\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2, \u0026quot;c\u0026quot;:3} try: value = my_dict[\u0026quot;d\u0026quot;] except IndexError: print(\u0026quot;This index does not exist!\u0026quot;) except KeyError: print(\u0026quot;This key is not in the dictionary!\u0026quot;) except: print(\u0026quot;Some other error occurred!\u0026quot;) Это самый стандартный способ выявить несколько исключений. Сначала мы попробовали открыть доступ к несуществующему ключу, которого нет в нашем словаре. При помощи try/except мы проверили код на наличие ошибки KeyError, которая находится во втором операторе except. Обратите внимание на то, что в конце кода у нас появилась «голое» исключение. Обычно это не рекомендуется, но вы, вероятно, будете сталкиваться с этим время от времени, поэтому полезно знать об этом. Также обратите внимание, что в большинстве случаев вам не нужно использовать целый блок кода для обработки нескольких исключений. Обычно, целый блок используется для выявления одного единственного исключения.
Вот еще один способ перехвата нескольких исключений:
try: value = my_dict[\u0026quot;d\u0026quot;] except (IndexError, KeyError): print(\u0026quot;An IndexError or KeyError occurred!\u0026quot;) Обратите внимание, что в этом примере мы помещаем ошибки, которые хотим перехватить, внутрь круглых скобок. Проблема этого метода в том, что трудно определить, трудно сказать какая именно ошибка произошла, поэтому рекомендуется использовать предыдущий пример.
В большинстве случаев, когда возникает исключение, необходимо уведомить пользователя, выведя сообщение на экран или записав его в журнал. В зависимости от серьезности ошибки, вам может понадобиться завершить работу программы. Иногда перед выходом из программы необходимо выполнить очистку. Например, если вы открыли соединение с базой данных, вам нужно будет закрыть его, перед выходом из программы, или вы можете закончить с открытым соединением. Другой пример - закрытие дескриптора файла, в который вы записывали данные. Подробнее о работе с файлами вы узнаете в следующей главе. Но сначала нам нужно научиться убирать за собой. Это легко сделать с оператором finally.
Оператор finally Оператор finally очень прост в использовании. Давайте рассмотрим глупый пример:
my_dict = {\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2, \u0026quot;c\u0026quot;:3} try: value = my_dict[\u0026quot;d\u0026quot;] except KeyError: print(\u0026quot;A KeyError occurred!\u0026quot;) finally: print(\u0026quot;The finally statement has executed!\u0026quot;) Если вы запустите это код, оно отобразиться и в операторе except и в finally. Это довольно просто, верно? Теперь вы можете использовать оператор finally, чтобы убрать за собой. В конце оператора finally вы также можете поместить код завершения.
try, except, или else! Оператор try/except также имеет пункт else. ОН будет выполняться только в том случае, если не возникнет ошибок. Давайте потратим несколько минут на рассмотрение нескольких примеров:
кmy_dict = {\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2, \u0026quot;c\u0026quot;:3} try: value = my_dict[\u0026quot;a\u0026quot;] except KeyError: print(\u0026quot;A KeyError occurred!\u0026quot;) else: print(\u0026quot;No error occurred!\u0026quot;)од Мы видим словарь с 3 элементами, и в try/except мы обращаемся к существующему ключу. Это работает, поэтому ошибка KeyError не возникает. Поскольку ошибки нет, else выполняется, и на экран выводится сообщение \u0026ldquo;No error occurred!\u0026rdquo;. Теперь добавим оператор finally:
my_dict = {\u0026quot;a\u0026quot;:1, \u0026quot;b\u0026quot;:2, \u0026quot;c\u0026quot;:3} try: value = my_dict[\u0026quot;a\u0026quot;] except KeyError: print(\u0026quot;A KeyError occurred!\u0026quot;) else: print(\u0026quot;No error occurred!\u0026quot;) finally: print(\u0026quot;The finally statement ran!\u0026quot;) Если вы запустите этот пример, он выполнит утверждения else и finally. В большинстве случаев оператор else не используется, поскольку любой код, следующий за try/except, будет выполнен, если не возникло ошибок. Единственное хорошее использование оператора else, которое я видел, это когда вы хотите выполнить вторую часть кода, которая также может вызвать ошибку. Конечно, если ошибка возникнет в else, то она не будет поймана.
Подведение итогов Теперь вы должны уметь обрабатывать исключения в своем коде. Если вы обнаружите, что ваш код вызывает исключение, вы будете знать, как выявить его таким образом, чтобы поймать ошибку и спокойно выйти или продолжить работу без прерывания.
Теперь мы готовы двигаться дальше и узнать о том, как работать с файлами в Python.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter7_exception_handling/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day07/":{title:"7. DevOps - изучение языка программирования",tags:["devops","golang","python"],content:`Общая картина: DevOps и изучение языка программирования Я думаю, будет справедливо сказать, что для достижения успеха в качестве инженера DevOps в долгосрочной перспективе необходимо знать хотя бы один язык программирования на базовом уровне. Я хочу провести это первое занятие в этой статье, чтобы выяснить, почему это такой важный навык, и, надеюсь, к концу этой недели или раздела вы будете лучше понимать, почему, как и что делать. делайте, чтобы продвигаться в своем учебном путешествии.
Я думаю, что если бы я спросил в социальных сетях, нужны ли вам навыки программирования для ролей, связанных с DevOps, ответ, скорее всего, будет утвердительным? Дайте мне знать, если вы думаете иначе? Хорошо, но тогда более важный вопрос, и здесь вы не получите такого четкого ответа, какой язык программирования? Наиболее распространенным ответом, который я видел здесь, был Python, или все чаще мы видим, что Golang или Go должны быть языком, который вы изучаете.
Чтобы быть успешным в DevOps, вы должны хорошо знать навыки программирования, по крайней мере, мой вывод из этого. Но мы должны понять, зачем нам это нужно, чтобы выбрать правильный путь.
Понимание зачем вам нужно изучать язык программирования Причина, по которой Python и Go так часто рекомендуются инженерам DevOps, заключается в том, что многие инструменты DevOps написаны либо на Python, либо на Go, что имеет смысл, если вы собираетесь создавать инструменты DevOps. Теперь это важно, так как это действительно определит, что вы должны изучить, и это, вероятно, будет наиболее полезным. Если вы собираетесь создавать инструменты DevOps или присоединяетесь к команде, которая занимается этим, имеет смысл выучить тот же язык. Если вы собираетесь активно участвовать в Kubernetes или контейнерах, то, скорее всего, вы захотите выберите Go в качестве языка программирования. Для меня компания, в которой я работаю (Kasten by Veeam), находится в экосистеме Cloud-Native, ориентированной на управление данными для Kubernetes, и все написано на Go.
Но тогда у вас может не быть четких рассуждений, подобных этим, чтобы выбрать, быть ли вам студентом или менять карьеру без реального решения за вас. Я думаю, что в этой ситуации вы должны выбрать тот, который, кажется, резонирует и подходит для приложений, с которыми вы хотите работать.
Помните, что я не собираюсь становиться здесь разработчиком программного обеспечения, я просто хочу немного больше узнать о языке программирования, чтобы я мог читать и понимать, что делают эти инструменты, а затем это, возможно, приведет к тому, как мы можем помочь улучшить ситуацию.
Я также хотел бы знать, как вы взаимодействуете с этими инструментами DevOps, такими как Kasten K10 или Terraform и HCL. Это то, что мы будем называть конфигурационными файлами, и именно так вы взаимодействуете с этими инструментами DevOps, чтобы что-то происходило, обычно это будет YAML. (Мы можем использовать последний день этого раздела, чтобы немного погрузиться в YAML)
Я только что отговорил себя от изучения языка программирования? Большую часть времени или в зависимости от роли вы будете помогать инженерным командам внедрять DevOps в свой рабочий процесс, много тестировать приложение и следить за тем, чтобы созданный рабочий процесс соответствовал тем принципам DevOps, которые мы упоминали в первые несколько дней. . Но на самом деле много времени уходит на устранение проблем с производительностью приложений или что-то в этом роде. Это возвращает меня к моей первоначальной точке зрения и рассуждениям: язык программирования, который мне нужно знать, — это тот, на котором написан код? Если их приложение написано на NodeJS, это не сильно поможет, если у вас есть значок Go или Python.
Почему Go Почему Golang — следующий язык программирования для DevOps? В последние годы Go стал очень популярным языком программирования. Согласно опросу StackOverflow за 2021 год, Go занял четвертое место среди самых востребованных языков программирования, сценариев и разметки, а Python был на первом месте, но выслушайте меня. StackOverflow 2021 Developer Survey – Most Wanted Link
Как я уже упоминал, некоторые из самых известных инструментов и платформ DevOps написаны на Go, такие как Kubernetes, Docker, Grafana и Prometheus.
Какие характеристики Go делают его идеальным для DevOps?
Сборка и развертывание программ Go Преимущество использования такого языка, как Python, который интерпретируется в роли DevOps, заключается в том, что вам не нужно компилировать программу Python перед ее запуском. Особенно для небольших задач автоматизации вы не хотите, чтобы процесс сборки, требующий компиляции, замедлялся, несмотря на то, что Go — компилируемый язык программирования, Go компилируется непосредственно в машинный код. Go также известен быстрым временем компиляции.
Go или Python для DevOps Программы Go статически связаны, это означает, что когда вы компилируете программу Go, все включается в один исполняемый двоичный файл, не требуется никаких внешних зависимостей, которые необходимо установить на удаленной машине, это упрощает развертывание программ Go, по сравнению с программой Python, которая использует внешние библиотеки, где вы должны убедиться, что все эти библиотеки установлены на удаленной машине, на которой вы хотите работать.
Go — это независимый от платформы язык, что означает, что вы можете создавать двоичные исполняемые файлы для * всех операционных систем, Linux, Windows, macOS и т. д., и это очень легко сделать. С Python не так просто создавать эти двоичные исполняемые файлы для конкретных операционных систем.
Go — очень производительный язык, он имеет быструю компиляцию и быстрое время выполнения с меньшим использованием ресурсов, таких как процессор и память, особенно по сравнению с python, в языке Go были реализованы многочисленные оптимизации, которые делают его таким производительным. (Ресурсы ниже)
В отличие от Python, который часто требует использования сторонних библиотек для реализации конкретной программы Python, go включает в себя стандартную библиотеку, которая имеет большую часть функций, которые вам понадобятся для DevOps, встроенных непосредственно в нее. Это включает в себя функциональную обработку файлов, веб-службы HTTP, обработку JSON, встроенную поддержку параллелизма и параллелизма, а также встроенное тестирование.
Это ни в коем случае не бросает Python под автобус, я просто излагаю свои причины выбора Go, но они не являются вышеупомянутым Go против Python, это обычно потому, что это имеет смысл, поскольку компания, в которой я работаю, разрабатывает программное обеспечение на Go, вот почему.
Я скажу, что как только как только вы выучите свой первый язык программирования, вам станет легче осваивать другие языки. Вероятно, у вас никогда не будет ни одной работы в какой-либо компании, где бы вам не приходилось иметь дело с управлением, архитектурой, оркестровкой, отладкой приложений JavaScript и Node JS.
Источники StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Теперь в течение следующих 6 дней этой темы я намерен работать с некоторыми из ресурсов, перечисленных выше, и документировать свои заметки на каждый день. Вы заметите, что они, как правило, составляют около 3 часов в качестве полного курса, я хотел поделиться своим полным списком, чтобы, если у вас есть время, вы могли двигаться вперед и работать над каждым, если позволяет время, я буду придерживаться моего часа обучения каждый день.
До встречи в День 8
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day07/"},"https://romankurnovskii.com/ru/docs/webrtc/testing/":{title:"Тестирование приложений WebRTC",tags:[],content:`При написании автоматических тестов для приложений WebRTC, существуют полезные конфигурации, которые можно включить для браузеров, и которые упростят разработку и тестирование.
Chrome При запуске автоматических тестов в Chrome полезны следующие функции:
\u0026ndash;allow-file-access-from-files — дает API-доступ для file://URLs \u0026ndash;disable-translate — отключает всплывающие окна \u0026ndash;use-fake-ui-for-media-stream — Представляет поддельные медиапотоки. Полезно при работе на CI-серверах. \u0026ndash;use-file-for-fake-audio-capture= — дает возможность использовать файл при захвате звука. \u0026ndash;use-file-for-fake-video-capture= — дает возможность использовать файл при захвате видео. \u0026ndash;headless - Запустить в автономном режиме. Полезно при работе на CI-серверах. \u0026ndash;mute-audio - Отключить аудио. Firefox При запуске автоматических тестов в Firefox, необходимо указать набор ключей предпочтений, которые будут использоваться в запущенном соединении. Ниже приведена конфигурация, используемая для автоматических тестов образцов WebRTC:
\u0026quot;prefs\u0026quot;: { \u0026quot;browser.cache.disk.enable\u0026quot;: false, \u0026quot;browser.cache.disk.capacity\u0026quot;: 0, \u0026quot;browser.cache.disk.smart_size.enabled\u0026quot;: false, \u0026quot;browser.cache.disk.smart_size.first_run\u0026quot;: false, \u0026quot;browser.sessionstore.resume_from_crash\u0026quot;: false, \u0026quot;browser.startup.page\u0026quot;: 0, \u0026quot;media.navigator.streams.fake\u0026quot;: true, \u0026quot;media.navigator.permission.disabled\u0026quot;: true, \u0026quot;device.storage.enabled\u0026quot;: false, \u0026quot;media.gstreamer.enabled\u0026quot;: false, \u0026quot;browser.startup.homepage\u0026quot;: \u0026quot;about:blank\u0026quot;, \u0026quot;browser.startup.firstrunSkipsHomepage\u0026quot;: false, \u0026quot;extensions.update.enabled\u0026quot;: false, \u0026quot;app.update.enabled\u0026quot;: false, \u0026quot;network.http.use-cache\u0026quot;: false, \u0026quot;browser.shell.checkDefaultBrowser\u0026quot;: false } `,url:"https://romankurnovskii.com/ru/docs/webrtc/testing/"},"https://romankurnovskii.com/ru/docs/python101/chapter8_file_io/":{title:"8. Работа с файлами",tags:[],content:`Эта глава знакомит с темой чтения и записи данных в файлы на жестком диске. Вы увидите, что читать и записывать файлы в Python очень просто. Давайте приступим!
Как читать файл В Python есть встроенная функция open, которую мы можем использовать для открытия файла для чтения. Создайте текстовый файл с именем \u0026ldquo;test.txt\u0026rdquo; со следующим содержимым:
This is a test file line 2 line 3 this line intentionally left blank Вот несколько примеров, которые показывают, как использовать open для чтения:
handle = open(\u0026quot;test.txt\u0026quot;) handle = open(r\u0026quot;C:\\Users\\mike\\py101book\\data\\test.txt\u0026quot;, \u0026quot;r\u0026quot;) Первый пример откроет файл с именем test.txt в режиме \u0026ldquo;только для чтения\u0026rdquo;. Это стандартный режим функции открытия файлов. Обратите внимание, что в первом примере мы не указали полный путь к файлу, который хотим открыть. Python автоматически ищет test.txt в папке, в которой запущен сценарий. Если он не найдет его, то вы получите ошибку IOError.
Во втором примере показан полный путь к файлу, но обратите внимание на то, что он начинается с \u0026ldquo;r\u0026rdquo;. Это значит, что мы указываем Python, чтобы строка обрабатывалась как исходная. Давайте посмотрим на разницу между исходной строкой и обычной:
\u0026gt;\u0026gt;\u0026gt; print(\u0026quot;C:\\Users\\mike\\py101book\\data\\test.txt\u0026quot;) C:\\Users\\mike\\py101book\\data est.txt \u0026gt;\u0026gt;\u0026gt; print(r\u0026quot;C:\\Users\\mike\\py101book\\data\\test.txt\u0026quot;) C:\\Users\\mike\\py101book\\data\\test.txt Как видно из примера, когда мы не определяем строку как исходную, мы получаем неправильный путь.. Почему это происходит? Как вы помните из главы о строках, есть некоторые специальные символы, которые должны быть экранированы, например, \u0026ldquo;n\u0026rdquo; или \u0026ldquo;t\u0026rdquo;. В данном случае мы видим \u0026ldquo;t\u0026rdquo; (т.е. табуляцию), поэтому строка послушно добавляет табуляцию к нашему пути и портит его.
Вторым аргументом во втором примере также является \u0026ldquo;r\u0026rdquo;. Это говорит open, что мы хотим открыть файл в режиме только для чтения. Другими словами, это делает то же самое, что и первый пример, но более явно. Теперь давайте действительно прочитаем файл!
Запишите следующие строки в сценарий Python и сохраните его в том же месте, где находится файл test.txt:
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;r\u0026quot;) data = handle.read() print(data) handle.close() Если вы запустите эту программу, она откроет файл и прочитает весь файл как строку в переменную data. Затем мы печатаем эти данные и закрываем дескриптор файла. Следует всегда закрывать дескриптор файла, так как неизвестно когда и какая именно программа захочет получить к нему доступ. Закрытие файла также поможет сохранить память и избежать появления странных багов в программе. Вы можете указать Python читать по строке за раз, читать все строки в список Python или читать файл по частям. Последний вариант очень удобен, когда вы имеете дело с очень большими файлами и не хотите читать их целиком, что может заполнить память компьютера.
Давайте потратим немного времени на рассмотрение различных способов чтения файлов.
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;r\u0026quot;) data = handle.readline() # read just one line print(data) handle.close() Если вы запустите этот пример, он прочитает только первую строку вашего текстового файла и распечатает ее. Это не слишком полезно, поэтому давайте попробуем метод readlines() дескриптора файла:
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;r\u0026quot;) data = handle.readlines() # read ALL the lines! print(data) handle.close() После выполнения этого кода вы увидите список Python, выведенный на экран, потому что это то, что возвращает метод readlines: список! Давайте уделим немного времени тому, как читать файл по частям.
Как читать файлы по частям Самый простой способ читать файл по частям - использовать цикл. Сначала мы научимся читать файл строка за строкой, а затем - по килобайту за раз. Для первого примера мы будем использовать цикл for:
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;r\u0026quot;) for line in handle: print(line) handle.close() Здесь мы открываем файл в дескрипторе в режиме \u0026ldquo;только чтение\u0026rdquo;, а затем используем цикл for для итерации по нему. Вы увидите, что в Python можно выполнять итерации над всеми видами объектов (строки, списки, кортежи, ключи в словаре и т.д.). Это было довольно просто, верно? Теперь давайте сделаем это по частям!
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;r\u0026quot;) while True: data = handle.read(1024) print(data) if not data: break В этом примере мы используем цикл while в Python для чтения по одному килобайту файла за раз. Как вы, вероятно, знаете, килобайт - это 1024 байта или символа. Теперь давайте представим, что мы хотим прочитать двоичный файл, например PDF.
Как прочитать двоичный файл Прочитать двоичный файл очень просто. Все что вам нужно, это изменить способ доступа к файлу:
handle = open(\u0026quot;test.pdf\u0026quot;, \u0026quot;rb\u0026quot;) Мы изменили способ доступа к файлу на rb, что означает read-binary. Вам может понадобиться читать двоичные файлы, когда вы скачиваете PDF-файлы из Интернета или передаете файлы с компьютера на компьютер.
Запись файлов в Python Если вы следили за развитием событий, то, вероятно, догадываетесь, режимы написания файлов в Python это “w” и “wb” для write-mode и write-binary-mode соответственно. Теперь давайте взглянем на простой пример того, как они применяются.
ВНИМАНИЕ: При использовании режимов \u0026ldquo;w\u0026rdquo; или \u0026ldquo;wb\u0026rdquo;, если файл уже существует, он будет перезаписан без предупреждения! Вы можете проверить, существует ли файл до того, как вы его откроете, с помощью модуля os в Python. См. раздел os.path.exists в Главе 16.
handle = open(\u0026quot;test.txt\u0026quot;, \u0026quot;w\u0026quot;) handle.write(\u0026quot;This is a test!\u0026quot;) handle.close() Это было просто! Все, что мы сделали, это изменили режим файла на \u0026ldquo;w\u0026rdquo; и вызвали метод write дескриптора файла, чтобы записать текст в файл. У дескриптора файла также есть метод writelines, который принимает список строк, которые дескриптор записывает на диск по порядку.
Использование оператора with В Python есть небольшой встроенный оператор with, который можно использовать для упрощения чтения и записи файлов. Оператор with создает то, что в Python известно как менеджер контекста, который автоматически закроет файл, когда вы закончите его обработку. Давайте посмотрим, как это работает:
with open(\u0026quot;test.txt\u0026quot;) as file_handler: for line in file_handler: print(line) Синтаксис оператора with немного странный, но вы быстро разберетесь в нем. По сути, мы делаем замену:
handle = open(\u0026quot;test.txt\u0026quot;) на это:
with open(\u0026quot;test.txt\u0026quot;) as file_handler: Вы можете выполнять все обычные операции ввода-вывода файлов, которые вы обычно делаете, пока вы находитесь внутри блока кода with. Как только вы покинете этот блок кода, дескриптор файла закроется, и вы больше не сможете его использовать. Да, вы правильно прочитали. Вам больше не нужно явно закрывать дескриптор файла, поскольку оператор with делает это автоматически! Попробуйте изменить некоторые предыдущие примеры из этой главы так, чтобы в них тоже использовался метод with.
Выявление ошибок Иногда при работе с файлами случаются неприятные вещи. Файл заблокирован, потому что его использует другой процесс, или у вас возникла какая-то ошибка разрешения. Когда это происходит, вероятно, возникнет ошибка IOError. В этом разделе мы рассмотрим, как ловить ошибки обычным способом и как ловить их с помощью оператора with. Подсказка: идея в обоих случаях практически одинакова!
try: file_handler = open(\u0026quot;test.txt\u0026quot;) for line in file_handler: print(line) except IOError: print(\u0026quot;An IOError has occurred!\u0026quot;) finally: file_handler.close() В приведенном выше примере мы помещаем обычный код в конструкции try/except. Если возникает ошибка, мы выводим сообщение на экран. Обратите внимание, что мы также закрываем файл с помощью оператора finally. Теперь мы готовы рассмотреть, как сделать то же самое, используя оператор with:
try: with open(\u0026quot;test.txt\u0026quot;) as file_handler: for line in file_handler: print(line) except IOError: print(\u0026quot;An IOError has occurred!\u0026quot;) Как вы уже догадались, мы просто обернули блок with таким же образом, как и в предыдущем примере. Разница в том, что нам не нужен оператор finally, так как менеджер контекста сделает это за нас.
Подведение итогов К этому моменту вы должны быть достаточно хорошо знакомы с работой с файлами в Python. Теперь вы знаете, как читать и записывать файлы, используя старый стиль и новый стиль with. Скорее всего, вы встретите оба стиля в реальной жизни. В следующей главе мы узнаем, как импортировать другие модули, поставляемые с Python. Это позволит нам создавать программы, используя готовые модули. Давайте начнем!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter8_file_io/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day08/":{title:"8. Настройка DevOps окружения для запуска Hello World на Go",tags:["devops","golang","hello-world"],content:`Настройка DevOps окружения для запуска Hello World на Go Прежде чем мы приступим к некоторым основам Go, мы должны установить Go на нашу рабочую станцию и сделать то, чему нас учит каждый модуль «Изучение программирования 101», а именно создать приложение Hello World. Так как здесь будут описаны шаги по установке Go на ваш ПК, мы попытаемся задокументировать процесс в картинках, чтобы людям было легко следовать за ним.
Возможные варианты установки Golang
Исполняемый файл Пакет из исходного кода Mac Os Homebrew #Homebrew install command brew install go Быстрый тьюториал для ознакомления с языком Go
Рассмотрим вараинт установки с помощью инсталляционного файла
Если мы зашли так далеко, вы, вероятно, знаете, какая операционная система рабочей станции у вас установлена, поэтому выберите соответствующую загрузку, и тогда мы сможем приступить к установке. Я использую Windows для этого пошагового руководства. На следующем шаге мы можем оставить все значения по умолчанию. (Отмечу, что на момент написания это была последняя версия, поэтому скриншоты могут быть устаревшими)
Также обратите внимание, что если у вас установлена более старая версия Go, вам придется удалить ее перед установкой, поскольку в Windows она встроена в установщик, и она будет удалена и установлена как единое целое.
После завершения вы должны открыть командную строку / терминал, и мы хотим проверить, установлен ли Go. Если вы не получите вывод, который мы видим ниже, значит, Go не установлен, и вам нужно будет повторить свои шаги.
go version Далее мы хотим проверить нашу среду на наличие Go. Это всегда полезно проверить, чтобы убедиться, что ваши рабочие каталоги настроены правильно, как вы можете видеть ниже, нам нужно убедиться, что в вашей системе есть следующий каталог.
Хорошо, давайте создадим этот каталог для простоты. Я собираюсь использовать команду mkdir в своем терминале PowerShell. Нам также нужно создать 3 папки в папке Go, как вы увидите ниже.
Теперь у нас установлен Go, и у нас есть рабочий каталог Go, готовый к действию. Теперь нам нужна интегрированная среда разработки (IDE). Сейчас есть много доступных, которые вы можете использовать, но наиболее распространенным и тем, который я использую, является Visual Studio Code или Code. Вы можете узнать больше об IDE здесь.
Если вы еще не загрузили и не установили VSCode на свою рабочую станцию, вы можете сделать это, перейдя по ссылке. Как вы можете видеть ниже, у вас есть разные варианты ОС.
Почти так же, как и при установке Go, мы собираемся загрузить и установить и сохранить значения по умолчанию. После завершения вы можете открыть VSCode, выбрать «Открыть файл» и перейти в наш каталог Go, который мы создали выше.
Вы можете получить всплывающее окно о доверии, прочитать его, если хотите, а затем нажать «Да, доверять авторам». (Позже я не несу ответственности, если вы начнете открывать вещи, которым не доверяете!)
Теперь вы должны увидеть три папки, которые мы также создали ранее, и теперь мы хотим щелкнуть правой кнопкой мыши папку src и создать новую папку с именем «Hello».
Довольно простые вещи, я бы сказал до этого момента? Теперь мы собираемся создать нашу первую программу Go, не понимая, что мы вкладываем в этот следующий этап.
Затем создайте файл с именем main.go в папке Hello. Как только вы нажмете Enter на main.go, вас спросят, хотите ли вы установить расширение Go, а также пакеты, вы также можете проверить этот пустой файл pkg, который мы сделали несколько шагов назад, и обратите внимание, что у нас должны быть новые пакеты. там сейчас?
Теперь давайте запустим это приложение Hello World, скопируйте следующий код в новый файл main.go и сохраните его.
package main import \u0026quot;fmt\u0026quot; func main() { fmt.Println(\u0026quot;Hello #90DaysOfDevOps\u0026quot;) } Я понимаю, что вышеизложенное может не иметь никакого смысла, но мы подробнее расскажем о функциях, пакетах и многом другом позже. А пока давайте запустим наше приложение. Вернувшись в терминал и в нашу папку Hello, мы можем проверить, все ли работает. Используя приведенную ниже команду, мы можем проверить, работает ли наша общая программа обучения.
go run main.go Однако на этом это не заканчивается, что, если теперь мы захотим взять нашу программу и запустить ее на других машинах с Windows? Мы можем сделать это, создав наш двоичный файл, используя следующую команду
go build main.go Попробуем запустить
#Windows ./main.exe #Linux/Mac Os ./main Источники Быстрое погружение в Golang StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Увидимся на 9-й день
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day08/"},"https://romankurnovskii.com/ru/docs/webrtc/unified-plan-transition-guide/":{title:"Формат SDP унифицированного плана – план перехода",tags:[],content:`Google планирует перевести реализацию WebRTC в Chrome с текущего SDP-формата (называемого «Plan B») на формат соответствующих стандартов («Unified Plan», draft-ietf-rtcweb-jsep) в течение следующих нескольких кварталов. План включает 5 этапов и одну временную функцию API.
Кто будет затронут? Людям, которые используют несколько аудиодорожек или несколько видеодорожек в одном PeerConnection, придется протестировать свой продукт в рамках Унифицированного Плана и, соответственно, адаптироваться. В случае, когда вызов инициируется с конечной точки не из Chrome, и на него отвечают в Chrome, форма запросов может измениться.
Людям, выполняющим детальный анализ SDP и заботящимся о msid атрибутах, придется убедиться, что их код синтаксического анализа поддерживает новый формат (a=msid). Подробная информация о том, потребуются ли изменения и как должны измениться приложения, будет зависеть от приложения. Мы думаем, что почти все приложения, которые используют только одну аудио- и одну видеодорожку для каждого RTCPeerConnection, - их эти изменения не коснутся.
Функция API Мы добавляем новую функцию в RTCConfiguration RTCPeerConnection:
enum SdpSemantics { \u0026quot;plan-b\u0026quot;, \u0026quot;unified-plan\u0026quot; }; partial dictionary RTCConfiguration { SdpSemantics sdpSemantics; } RTCConfiguration может быть передана конструктору из RTCPeerConnection, и все запросы и ответы будут в формате Унифицированного Плана. Запросы в setLocalDescription и setRemoteDescription также будут ожидать, что SDP будет в формате Унифицированного Плана; если он в устаревшем формате Chrome, то все, кроме первой звуковой дорожки и первой видеодорожки, будут игнорироваться.
Также есть флаг командной строки (–enable-features=RTCUnifiedPlanByDefault в версии Chrome M71 и выше, –enable-blink-features=RTCUnifiedPlanByDefault в более ранних версиях), который позволяет установить для этого флага значение по умолчанию в «Unified-plan».
Этапы Этап 1. Внедрение Унифицированного Плана На этом этапе Унифицированный План разрабатывался под флагом экспериментов, доступным с версии M65. До этапа 2 разумнее всего было тестировать Chrome Canary, используя «–enable-blink-features=RTCUnifiedPlan».
Этап 2. Сделать функцию API общедоступной Представлено в версии M69 (бета-август 2018 г., стабильная версия в сентябре 2018 г.)
На этом этапе значением по умолчанию флага sdpSemantics было «plan-b». На этапе 2 люди, у которых были реализации, зависящие от формата SDP, должны были протестировать, работают ли их приложения при использовании Унифицированного Плана. Для приложений, поддерживающих Firefox, это очень простое упражнение: просто делайте то же, что делали до этого в Firefox. Значение по умолчанию флага sdpSemantics можно изменить в «chrome://flags»; найдите функцию «WebRTC: Use Unified Plan SDP Semantics by default».
Этап 3 Переключите значение по умолчанию Датой перехода была версия M72 (бета-декабрь 2018 г., стабильная версия — январь 2019 г.). На этом этапе было изменено значение флага sdpSemantics по умолчанию на «unified plan». Приложения, которые обнаружили, что стали работать медленнее, переустановили флаг sdpSemantics в «plan-b», чтобы вернуться к предыдущему поведению.
Этап 4: бросьте «План Б» На этом этапе установка флага sdpSemantics в значение «plan-b» приводит к возникновению исключения. Это было сделано при переходе от версии Canary к M93. Что касается M96, исключение работает как в Canary, так и на Beta. План состоит в том, чтобы добавить его и в стабильную версию. Мы следим за использованием Plan B. На этом этапе доступна пробная версия, которая позволяет использовать план Б без создания исключений. Эта пробная версия перестала работать 29 декабря 2021 г.
Этап 5: Уберите «План Б» После окончания пробного периода Plan B будет удален из Chrome. На этом этапе флаг sdpSemantics будет удален. Попытка установить его на «plan-b» не вызовет исключение, и перестанет работать.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/unified-plan-transition-guide/"},"https://romankurnovskii.com/ru/docs/python101/chapter9_imports/":{title:"9. Импортирование",tags:[],content:`Python поставляется с большим количеством готового кода. Эти части кода известны как модули и пакеты. Модуль - это один импортируемый файл Python, а пакет состоит из двух или более модулей. Пакет может быть импортирован так же, как и модуль. Каждый раз, когда вы сохраняете собственный сценарий Python, вы создаете модуль. Это, конечно, может быть далеко не самым полезным модулем, но тем не менее. В этой главе мы узнаем, как импортировать модули, используя несколько различных методов. Давайте начнем!
import this Python предоставляет ключевое слово import для импорта модулей. Давайте попробуем:
import this Если вы запустите этот код в интерпретаторе, на выходе вы должны увидеть что-то вроде следующего:
The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren't special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you're Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it's a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let's do more of those! Вы нашли \u0026ldquo;пасхальное яйцо\u0026rdquo; в Python, известное как \u0026ldquo;Zen of Python\u0026rdquo;. На самом деле это своего рода неофициальная лучшая практика для Python. Mодуль this на самом деле ничего не делает, но он предоставляет забавный способ показать, как импортировать что-то. Давайте действительно импортируем что-то, что мы можем использовать, например, модуль math:
\u0026gt;\u0026gt;\u0026gt; import math \u0026gt;\u0026gt;\u0026gt; math.sqrt(4) 2.0 Здесь мы импортировали модуль math, а затем сделали кое-что новое. Мы вызвали одну из его функций, sqrt (т.е. квадратный корень). Чтобы вызвать метод импортированного модуля, мы должны использовать следующий синтаксис: имя_модуля.имя_метода(аргумент). В этом примере мы нашли квадратный корень из 4. Модуль math имеет множество других функций, которые мы можем использовать, например, cos (косинус), factorial, log (логарифм) и т. д. Вы можете вызывать эти функции точно так же, как и sqrt. Единственное, что вам нужно будет проверить, принимают ли они дополнительные аргументы или нет. Теперь давайте рассмотрим другой способ импорта.
Использование from для импорта Некоторым людям не нравится, что все, что они вводят, должно сопровождаться именем модуля. В Python есть решение для этого! Вы можете импортировать из модуля только те функции, которые вам нужны. Давайте представим, что мы хотим импортировать только функцию *sqrt:
\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; sqrt(16) 4.0 Это работает практически так, как читается: из модуля math импортируйте функцию sqrt. Позвольте мне объяснить это по-другому. Мы используем ключевое слово Python from для импорта функции sqrt из модуля math. Вы также можете использовать этот метод для импорта нескольких функций из модуля math:
\u0026gt;\u0026gt;\u0026gt; from math import pi, sqrt В этом примере мы импортируем и pi, и sqrt. Если вы пытались получить доступ к pi, то, возможно, заметили, что на самом деле это значение, а не функция, которую можно вызвать. Она просто возвращает значение pi. Когда вы выполняете импорт, вы можете импортировать значение, функцию или даже другой модуль! Есть еще один способ импортировать материал, который мы должны рассмотреть. Давайте узнаем, как импортировать все!
Импорт всего! Python предоставляет возможность импортировать все функции и значения из модуля. На самом деле это плохая идея, так как это может привести к загрязнению вашего пространства имен. Пространство имен - это место, где находятся все ваши переменные в течение жизни программы. Допустим, у вас есть своя переменная с именем sqrt, например, так:
\u0026gt;\u0026gt;\u0026gt; from math import sqrt \u0026gt;\u0026gt;\u0026gt; sqrt = 5 Теперь вы только что изменили функцию sqrt на переменную, которая хранит значение 5. Это называется затенением (shadowing). Это особенно хитрый способ упрощения своей жизни, когда вы выполняете импорт всего из модуля. Давайте взглянем:
\u0026gt;\u0026gt;\u0026gt; from math import * \u0026gt;\u0026gt;\u0026gt; sqrt = 5 \u0026gt;\u0026gt;\u0026gt; sqrt(16) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: 'int' object is not callable Чтобы импортировать все, вместо указания списка элементов мы просто используем подстановочный знак \u0026ldquo;*\u0026rdquo;, который означает, что мы хотим импортировать все. Если мы не знаем, что находится в модуле math, мы не поймем, что только что провалили одну из импортированных функций. Когда мы пытаемся вызвать функцию sqrt после присвоения её целому числу, мы обнаружим, что она больше не работает.
Поэтому в большинстве случаев рекомендуется импортировать элементы из модулей, используя один из предыдущих методов, упомянутых в этой главе. Из этого правила есть несколько исключений. Некоторые модули созданы для импорта с помощью метода \u0026ldquo;*\u0026rdquo;. Одним из ярких примеров является Tkinter, набор инструментов, входящий в состав Python, который позволяет создавать пользовательские интерфейсы для настольных компьютеров. Причина, по которой импортировать Tkinter таким образом якобы можно, заключается в том, что модули названы так, что вы вряд ли будете использовать их повторно.
Подведение итогов Теперь вы знаете все об импорте в Python. В состав Python входят десятки модулей, которые вы можете использовать для придания дополнительной функциональности вашим программам. Вы можете использовать встроенные модули для опроса вашей ОС, получения информации из реестра Windows, настройки утилит протоколирования, разбора XML и многого, многого другого. Мы рассмотрим некоторые из этих модулей во второй части этой книги.
В следующей главе мы рассмотрим создание собственных функций. Я думаю, что следующая тема окажется для вас очень полезной.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter9_imports/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day09/":{title:"9. Как работает hello-world на Golang",tags:["devops","golang","hello-world"],content:`Разберемся как работает hello-world Как работает Go Вчера мы прошли процедуру установки Go на ПК, а затем создали наше первое приложение Go.
В этом разделе мы собираемся глубже изучить код и понять еще несколько вещей о языке Go.
Что такое компиляция? Прежде чем мы перейдем к 6 строкам кода Hello World, которые написали вчера, нам нужно немного разобраться в компиляции.
Языки программирования, которые мы обычно используем, такие как Python, Java, Go и C++, являются языками высокого уровня. Это означает, что они удобочитаемы для человека, но когда машина пытается выполнить программу, она должна быть в форме, понятной машине. Мы должны перевести наш человекочитаемый код в машинный код, что называется компиляцией.
Из приведенного выше вы можете видеть, что мы сделали в День 8 - мы создали простой Hello World main.go, а затем использовали команду go build main.go для компиляции нашего исполняемого файла.
package main import \u0026quot;fmt\u0026quot; func main() { fmt.Println(\u0026quot;Hello #90DaysOfDevOps\u0026quot;) } Что такое пакеты? Пакет — это набор исходных файлов в одном каталоге, которые скомпилированы вместе. Мы можем упростить это еще больше, пакет — это набор файлов .go в одном каталоге. Помните нашу папку Hello из Дня 8? Когда вы попадете в более сложные программы Go, вы можете обнаружить, что у вас есть папка1, папка2 и папка3, содержащие разные файлы .go, которые составляют вашу программу с несколькими пакетами.
Мы используем пакеты, чтобы мы могли повторно использовать код других людей, нам не нужно писать все с нуля. Возможно, нам нужен калькулятор как часть нашей программы, вы, вероятно, могли бы найти существующий пакет Go, содержащий математические функции, которые вы могли бы импортировать в свой код, что в конечном итоге сэкономит вам много времени и усилий.
Go рекомендует организовывать код в пакеты, чтобы его было легко повторно использовать и поддерживать исходный код.
Hello #90DaysOfDevOps шаг за шагом Теперь давайте посмотрим на наш файл main.go Hello #90DaysOfDevOps и пройдемся по строкам. В первой строке у нас есть package main, что означает, что этот файл принадлежит пакету с именем main. Все файлы .go должны принадлежать пакету, они также должны иметь «package something» в открывающей строке.
Пакет можно назвать как угодно. Мы должны назвать этот main, так как это начальная точка программы, которая будет в этом пакете, это правило. Всякий раз, когда мы хотим скомпилировать и выполнить наш код, мы должны сообщить машине, где должно начаться выполнение. Мы делаем это, написав функцию с именем main. Машина будет искать функцию с именем main, чтобы найти точку входа в программу.
Функция — это блок кода, который может выполнять определенную задачу и может использоваться во всей программе.
Вы можете объявить функцию с любым именем, используя func, но в этом случае нам нужно назвать ее main, так как именно здесь начинается код.
Далее мы рассмотрим строку 3 нашего кода, импорт, это в основном означает, что вы хотите добавить другой пакет в свою основную программу. fmt — это стандартный пакет, используемый здесь, предоставленный Go, этот пакет содержит функцию Println(), и, поскольку мы импортировали ее, мы можем использовать ее в строке 6. Существует ряд стандартных пакетов, которые вы можете включить в свою программу и используйте или повторно используйте их в своем коде, избавляя вас от необходимости писать с нуля. Println(), который у нас есть, — это способ записи в стандартный вывод на терминал, где когда-либо исполняемый файл был успешно выполнен. Не стесняйтесь изменять сообщение между скобками (). TLDR Что такое TLDR
Строка 1 = Этот файл будет находиться в пакете с именем main, и его нужно назвать main, поскольку он включает точку входа программы. Строка 3 = Чтобы использовать Println(), мы должны импортировать пакет fmt, чтобы использовать его в строке 6. Строка 5 = фактическая начальная точка, это функция main. Строка 6 = Это позволит нам напечатать «Hello #90DaysOfDevOps» в нашей системе. Источники Стандартная библиотека Go Golang | Все Основы за 4 Часа Для Начинающих StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Увидимся на 10-й день
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day09/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/":{title:"Практика",tags:[],content:`Соединение в режиме реального времени с использованием WebRTC WebRTC – это проект с открытым исходным кодом, позволяющий передавать аудио, видео и данные в режиме реального времени в браузере или через приложения. WebRTC имеет несколько Java-скрипт API – нажмите на ссылки, чтоб посмотреть примеры
getUserMedia(): захват аудио и видео MediaRecorder: запись аудио и видео RTCPeerConnection: потоковая передача аудио и видео между пользователями RTCDataChannel: потоковая передача данных между пользователями. Где я могу использовать WebRTC?
В браузерах Firefox, Opera и Chrome на компьютере и Андроиде. Также WebRTC работает в приложениях iOS и Android.
Что такое «сигналинг»?
WebRTC использует RTCPeerConnection для обмена потоковыми данными между браузерами, но ему необходим механизм для координации обмена данными и отправки контрольных сообщений. Этот процесс называется «сигналингом». Методы сигналинга и протоколы передачи не определены в WebRTC. Поэтому в коде придется использовать Socket.IO для обмена сообщениями, но существует много других альтернатив (https://github.com/muaz-khan/WebRTC-Experiment/blob/master/Signaling.md)
Что такое STUN и TURN?
WebRTC разработан для работы в P2P, поэтому пользователи могут подключаться по самому прямому возможному маршруту. Однако WebRTC создан для работы в реальных сетях: клиентским приложениям необходимо проходить через шлюзы NAT (http://en.wikipedia.org/wiki/NAT_traversal) и брандмауэры, а P2P-сеть нуждается в резервном варианте на случай сбоя прямого соединения. В рамках этого процесса API WebRTC используют STUN-серверы для получения IP-адреса вашего компьютера и TURN-серверы для работы в качестве серверов ретрансляции в случае сбоя P2P-соединения. (WebRTC в реальном мире объясняет более подробно - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)
Безопасен ли WebRTC?
Шифрование является обязательным для всех компонентов WebRTC, а его Javasсript API могут использоваться только из безопасных источников (HTTPS или localhost). Механизмы сигналинга не определены стандартами WebRTC, поэтому важно убедиться, что вы используете безопасные протоколы.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/"},"https://romankurnovskii.com/ru/docs/python101/chapter10_functions/":{title:"10. Функции",tags:[],content:`Функция - это структура, которую вы определяете. Вы можете решать, есть ли у них аргументы или нет. Вы можете добавить аргументы в виде ключевых слов и аргументы по умолчанию. Функция - это блок кода, который начинается с ключевого слова def, имени функции и двоеточия. Вот простой пример:
def a_function(): print(\u0026quot;You just created a function!\u0026quot;) Эта функция ничего не делает, кроме вывода какого-то текста. Чтобы вызвать функцию, нужно напечатать ее имя, за которым следуют открытые и закрытые круглые скобки:
a_function() You just created a function! Просто, да?
Пустая функция (заглушка) Иногда, когда вы пишете код, вы просто хотите написать определения функций, не вставляя в них никакого кода. Я делал это в качестве своеобразного наброска. Это помогает вам увидеть, как будет выглядеть ваше приложение. Вот пример:
def empty_function(): pass Вот нечто новое: оператор pass. По сути, это операция null, то есть при выполнении pass ничего не происходит.
Передача аргументов в функцию Теперь мы готовы узнать, как создать функцию, которая может принимать аргументы, а также как передать эти аргументы в функцию. Давайте создадим простую функцию, которая может складывать два числа:
def add(a, b): return a + b add(1, 2) # 3 Все функции что-то возвращают. Если не указать ей, что она должна что-то вернуть, то она вернет None. В данном случае мы говорим ей вернуть a + b. Как видите, мы можем вызвать функцию, передав два значения. Если вы передадите недостаточно или слишком много аргументов, то получите ошибку:
add(1) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: add() takes exactly 2 arguments (1 given) Вы также можете вызвать функцию, передав ей имя аргументов:
add(a=2, b=3) # 5 total = add(b=4, a=5) print(total) # 9 Вы заметите, что не имеет значения, в каком порядке вы передаете их в функцию, лишь бы они были названы правильно. Во втором примере видно, что мы присваиваем результат функции переменной с именем total. Это обычный способ вызова функции, поскольку вы захотите что-то сделать с результатом. Вам, вероятно, интересно, что произойдет, если мы передадим аргументы с неправильными именами. Будет ли это работать? Давайте узнаем:
add(c=5, d=2) Traceback (последний последний вызов): Файл \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, строка 1, в \u0026lt;фрагменте\u0026gt;. TypeError: add() получил неожиданный аргумент ключевого слова 'c' Мы получили ошибку. Это означает, что мы передали аргумент с ключевым словом, который функция не распознала. Какое совпадение, аргументы с ключевыми словами - наша следующая тема!
Аргументы с ключевыми словами Функции также могут принимать аргументы в виде ключевых слов! На самом деле они могут принимать как обычные аргументы, так и аргументы с ключевыми словами. Значит, вы можете указать, какие ключевые слова какими являются, и передать их. Вы видели такое поведение в предыдущем примере.
def keyword_function(a=1, b=2): return a+b keyword_function(b=4, a=5) # 9 Вы также могли бы вызвать эту функцию без указания ключевых слов. Эта функция также демонстрирует концепцию аргументов по умолчанию. Каким образом? Ну, попробуйте вызвать функцию вообще без аргументов!
keyword_function() # 3 Функция вернула число 3! Почему? Причина в том, что a и b имеют значения по умолчанию 1 и 2 соответственно. Теперь давайте создадим функцию, которая имеет как обычный аргумент, так и пару аргументов в виде ключевых слов:
def mixed_function(a, b=2, c=3): return a+b+c mixed_function(b=4, c=5) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; TypeError: mixed_function() takes at least 1 argument (2 given) mixed_function(1, b=4, c=5) # 10 mixed_function(1) # 6 В приведенном выше коде есть 3 примера. Давайте рассмотрим каждый из них. В первом примере мы пытаемся вызвать нашу функцию, используя только аргументы ключевого слова. Это приведет нас к непонятной ошибке. В Traceback говорится, что наша функция принимает хотя бы один аргумент, но было передано два. Что же здесь происходит? Дело в том, что первый аргумент является обязательным, так как он ничем не задан, поэтому если вы вызываете функцию только с аргументами ключевых слов, это приводит к ошибке.
Во втором примере мы вызываем смешанную функцию с 3 значениями, называя два из них. Это работает и дает нам ожидаемый результат - 1+4+5=10. Третий пример показывает, что произойдет, если мы вызовем функцию, передав только одно значение\u0026hellip; то, которое не имеет значения по умолчанию. Это также работает, если взять значение \u0026ldquo;1\u0026rdquo; и добавить его к двум значениям по умолчанию \u0026ldquo;2\u0026rdquo; и \u0026ldquo;3\u0026rdquo;, чтобы получить результат \u0026ldquo;6\u0026rdquo;! Разве это не круто?
*args и **kwargs Вы также можете настроить функции на прием любого количества аргументов или ключевых слов, используя специальный синтаксис. Чтобы получить бесконечное количество аргументов, используйте *args, а для бесконечного количества аргументов ключевых слов - **kwargs. Слова \u0026ldquo;args\u0026rdquo; и \u0026ldquo;kwargs\u0026rdquo; не имеют значения. Это просто условность. Вы могли бы назвать их *bill и **ted, и это работало бы точно так же. Ключ здесь в количестве звездочек.
Примечание: в дополнение к соглашению о *args и **kwargs, вы также будете время от времени встречать *a и **kw.
Давайте рассмотрим небольшой пример:
def many(*args, **kwargs): print(args) print(kwargs) many(1, 2, 3, name=\u0026quot;Mike\u0026quot;, job=\u0026quot;programmer\u0026quot;) # (1, 2, 3) # {'job': 'programmer', 'name': 'Mike'} Сначала мы создадим нашу функцию, используя новый синтаксис, а затем вызовем ее с тремя обычными аргументами и двумя аргументами в виде ключевых слов. Сама функция выведет оба типа аргументов. Как видите, параметр args превращается в кортеж, а kwargs - в словарь. Вы можете увидеть этот тип кодирования в исходном тексте Python и во многих сторонних пакетах Python.
Замечание об области видимости и глобальных значениях В Python есть понятие области видимости, как и в большинстве языков программирования. Область видимости подскажет нам, когда переменная доступна для использования и где. Если мы определяем переменные внутри функции, то эти переменные могут быть использованы только внутри этой функции. После завершения функции они больше не могут быть использованы, поскольку выходят из области видимости. Давайте рассмотрим пример:
def function_a(): a = 1 b = 2 return a+b def function_b(): c = 3 return a+c print( function_a() ) print( function_b() ) Если вы запустите этот код, вы получите следующую ошибку:
NameError: global name 'a' is not defined Это происходит потому, что переменная a определена только в первой функции и недоступна во второй. Это можно обойти, сказав Python, что a является глобальной переменной. Давайте посмотрим, как это делается:
def function_a(): global a a = 1 b = 2 return a+b def function_b(): c = 3 return a+c print(function_a()) # 3 print(function_b()) # 4 Этот код будет работать, потому что мы сказали Python сделать переменную глобальной, что означает, что она будет доступна везде в нашей программе. Обычно это плохая идея не рекомендуется. А всё потому, что в этом случае трудно определить, когда переменная определена. Другая проблема заключается в том, что, определив глобальную переменную в одном месте, мы можем случайно переопределить ее значение в другом, что впоследствии может привести к логическим ошибкам, которые трудно отладить.
Советы по кодированию Одной из самых больших проблем, которую необходимо усвоить начинающим программистам, является идея \u0026ldquo;Не повторяй себя\u0026rdquo;(Don’t Repeat Yourself -DRY). Эта концепция заключается в избегании написания одного и того же кода более одного раза. Если вы обнаружите, что делаете это, то знайте, что этот кусок кода должен быть помещен в функцию. Основная причина заключается в том, что в будущем вам почти наверняка понадобится снова изменить этот кусок кода, а если он находится в нескольких местах, то чтобы изменить его, вам придется вспоминать где ещё вы его писали.
Копирование и вставка одного и того же фрагмента кода повсюду - это пример \u0026ldquo;спагетти-кода\u0026rdquo;. Старайтесь максимально избегать этого. В какой-то момент вы пожалеете об этом либо потому, что вам придется исправлять его, либо потому, что вы найдете чужой код с подобными проблемами, который вам придется поддерживать.
Подведение итогов Теперь у вас есть базовые знания, необходимые для эффективного использования функций. Вам следует попрактиковаться в создании некоторых простых функций и попробовать вызывать их различными способами. После того как вы немного поиграете с функциями или просто решите, что хорошо понимаете концепции, вы можете перейти к следующей главе, посвященной классам.
Ресурсы https://vegibit.com/python-function-tutorial/ `,url:"https://romankurnovskii.com/ru/docs/python101/chapter10_functions/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day10/":{title:"10. Окружение Go",tags:["devops","golang"],content:`Окружение Go В 8-м дне мы кратко рассмотрели рабочее пространство Go, чтобы запустить его и перейти к демонстрации «Hello #90DaysOfDevOps». Но мы должны немного рассказать о рабочем пространстве Go.
Помните, что мы выбрали значения по умолчанию, а затем прошли и создали нашу папку Go в GOPATH, который уже был определен, но на самом деле этот GOPATH можно изменить, чтобы он находился там, где вы хотите.
Если вы запустите
echo $GOPATH Вывод должен быть похож на мой (может быть с другим именем пользователя), а именно:
/home/michael/projects/go Затем здесь мы создали 3 директории. src, pkg и bin
src is where all of your Go programs and projects are stored. This handles namespacing package management for all your Go repositories. This is where you will see on our workstation we have our Hello folder for the Hello #90DaysOfDevOps project.
pkg — это место, где хранятся ваши заархивированные файлы пакетов, которые установлены или были установлены в программах. Это помогает ускорить процесс компиляции в зависимости от того, были ли изменены используемые пакеты. bin — это место, где хранятся все ваши скомпилированные двоичные файлы.
Наш Hello #90DaysOfDevOps не является сложной программой, поэтому вот пример более сложной программы Go, взятой из другого замечательного ресурса, на который стоит обратить внимание GoChronicles Компиляция и запуск кода На 9-й день мы также рассмотрели краткое введение в компиляцию кода, но здесь мы можем пойти немного глубже.
Чтобы запустить наш код, мы сначала должны его скомпилировать. В Go это можно сделать тремя способами.
go build go install go run Прежде чем мы перейдем к описанному выше этапу компиляции, нам нужно взглянуть на то, что мы получаем при установке Go.
Когда мы установили Go на 8-й день, мы установили что-то, известное как инструменты Go, которые состоят из нескольких программ, которые позволяют нам создавать и обрабатывать наши исходные файлы Go. Одним из инструментов является «Go».
Стоит отметить, что вы можете установить дополнительные инструменты, которых нет в стандартной установке Go.
Если вы откроете командную строку и наберете «go», вы должны увидеть что-то вроде изображения ниже, а затем вы увидите «Дополнительные разделы справки» ниже, и пока нам не нужно беспокоиться об этом.
Возможно, вы также помните, что мы уже использовали как минимум два из этих инструментов в День 8. Мы хотим узнать больше о сборке, установке и запуске.
go run - Эта команда компилирует и запускает основной пакет, состоящий из файлов .go, указанных в командной строке. Команда компилируется во временную папку. go build - чтобы скомпилировать пакеты и зависимости, скомпилируйте пакет в текущем каталоге. Если пакет «main», поместит исполняемый файл в текущий каталог, если нет, то он поместит исполняемый файл в папку «pkg». go build также позволяет вам создать исполняемый файл для любой платформы ОС, поддерживаемой Go. go install - то же самое, что и go build, но помещает исполняемый файл в папку bin Мы прошли через go build и go run, но не стесняйтесь запускать их снова здесь, если хотите, go install, как указано выше, помещает исполняемый файл в нашу папку bin. Надеюсь, что вы следите за мной и смотрите один из плейлистов или видеороликов ниже. Я беру их по кусочкам и перевожу в свои заметки, чтобы понять основы языка Голанг. Приведенные ниже ресурсы, вероятно, дадут вам гораздо лучшее понимание многих областей, которые вам нужны в целом, но я пытаюсь задокументировать 7 дней или 7 часов путешествия с интересными вещами, которые я нашел.
Источники StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Увидимся на 11-й день
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day10/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-get-code/":{title:"Загрузка кода",tags:[],content:`Загрузка кода Если вы знакомы с сайтом git, вы можете скачать код для данной codelab с GitHub, клонировав его: git clone https://github.com/googlecodelabs/webrtc-web
Можно также нажать на ссылку ниже для загрузки zip-файла кода: https://github.com/googlecodelabs/webrtc-web/archive/master.zip
Откройте загруженный zip-файл. Разархивируйте папку проекта (adaptive-web-media), в которой по одной папке на каждый шаг этой codelab, и есть все необходимые вам ресурсы. Вы будете выполнять все действия в папке work.
Папки step-nn содержат финальную версию для каждого шага этой codelab. Они там для справки.
Установите и проверьте веб-сервер Несмотря на то, что вы можете использовать и свой собственный веб-сервер, эта codelab подразумевает работу с веб-сервером Chrome. Если у вас он еще не установлен, вы можете инсталлировать его из Chrome Web Store https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en
После установки приложения Web Server для Chrome, нажмите на ярлык Chrome Apps на панели закладок, на странице новой вкладки или в панели запуска приложений:
Нажмите на значок Web Server
Далее вы увидите это диалоговое окно, которое позволит настроить локальный веб-сервер:
Нажмите на кнопку «Choose Folder», и выберите папку work, которую вы только что создали. Это позволит вам просматривать текущую работу в Chrome по ссылке URL, подчеркнутой в диалоговом окне в разделе Web Server URL(s). Ниже, в Options, поставьте флажок в Automatically show index.html, как показано ниже:
Затем остановите и перезапустите сервер, сдвинув флажок с надписью «Web Server: STARTED» влево, а затем снова вправо.
Теперь посетите свой рабочий сайт в браузере, кликнув на выделенный Web Server URL. Вы должны увидеть подобную страницу, которая соответствует пути work/index.html:
Очевидно, что данное приложение пока еще не делает ничего интересного – пока это просто минимальный скелет, который нужен для того, чтоб убедиться, что веб-сервер работает, как надо. На следующих этапах мы добавим функциональности в этом приложение.
С этого момента все тестирование и проверка должны выполняться с использованием этой настройки веб-сервера. Обычно достаточно просто обновить вкладку тестового браузера.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-get-code/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-overview/":{title:"Обзор",tags:[],content:`Создайте приложение для получения видео и снимков с веб-камеры, с возможностью делиться ими в P2P через WebRTC. В ходе codelab вы узнаете, как использовать основные API WebRTC и настроить сервер обмена сообщениями через Node.js.
Чему вы научитесь
получать видео с вашей веб-камеры потоковое видео через RTCPeerConnection потоковая передача данных через RTCDataChannel настраивать сигналинг для обмена сообщениями комбинировать одноранговое соединение и сигналинг делать фото и передавать его через канал данных Что понадобится
Chrome версии 47 и выше веб-сервер для Chrome https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb , и ваш собственный веб-сервер по выбору пример кода текстовый редактор базовые знания HTML, CSS и Javaskript `,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-overview/"},"https://romankurnovskii.com/ru/docs/python101/chapter11_classes/":{title:"11. Классы",tags:[],content:`Все в Python является объектом. Это означает, что каждая сущность в Python имеет методы и значения. Причина в том, что в основе всего лежит класс.
\u0026gt;\u0026gt;\u0026gt; x = \u0026quot;Some String\u0026quot; \u0026gt;\u0026gt;\u0026gt; dir(x) ['__add__', '__class__', '__contains__', '__delattr__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getslice__', '__gt__', '__hash__', '__init__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_formatter_field_name_split', '_formatter_parser', 'capitalize', 'center', 'count', 'decode', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'index', 'isalnum', 'isalpha', 'isdigit', 'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] Здесь у нас есть строка, присвоенная переменной x. Может показаться, что это не так много, но у этой строки есть много методов. Если вы используете ключевое слово dir в Python, то сможете получить список всех методов, которые можно вызвать для вашей строки.
Технически мы не должны напрямую вызывать методы, начинающиеся с символов подчеркивания, но их можно вызвать.
Это значит, что строка основана на классе, а x- это экземпляр этого класса!
В Python мы можем создавать свои собственные классы.
Создание класса Создать класс в Python очень просто. Вот очень простой пример:
class Vehicle(object): \u0026quot;\u0026quot;\u0026quot;docstring\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; pass Этот класс не делает ничего особенного, однако он является очень хорошим инструментом обучения. Например, чтобы создать класс, нам нужно использовать ключевое слово Python class, за которым следует имя класса. В Python принято, что в имени класса первая буква должна быть заглавной. Далее следует открытая скобка, за которой следует слово object и закрытая скобка. object - это то, на чем основан класс или от чего он наследуется. Он известен как базовый класс или родительский класс.
Большинство классов в Python основаны на объекте. У классов есть специальный метод init (для инициализации). Этот метод вызывается всякий раз, когда вы создаете (или инстанцируете) объект на основе данного класса. Метод init вызывается только один раз и не должен вызываться повторно внутри программы. Другим термином для init является конструктор, хотя этот термин не часто используется в Python.
Вам может быть интересно, почему я все время говорю метод, а не функция. Функция меняет свое название на \u0026ldquo;метод\u0026rdquo;, когда она находится внутри класса. Вы также заметите, что каждый метод должен иметь хотя бы один аргумент (т.е. self), чего нельзя сказать об обычной функции.
В Python 3 нам не нужно прямо указывать, что мы наследуем у объекта. Вместо этого мы могли бы написать вышеописанное следующим образом:
# Python 3.x syntax class Vehicle: \u0026quot;\u0026quot;\u0026quot;docstring\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; pass Вы заметите, что единственное различие заключается в том, что нам больше не нужны круглые скобки, если мы основываем наш класс на объекте. Давайте немного расширим определение нашего класса и наделим его некоторыми атрибутами и методами.
class Vehicle(object): \u0026quot;\u0026quot;\u0026quot;docstring\u0026quot;\u0026quot;\u0026quot; def __init__(self, color, doors, tires): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.color = color self.doors = doors self.tires = tires def brake(self): \u0026quot;\u0026quot;\u0026quot; Stop the car \u0026quot;\u0026quot;\u0026quot; return \u0026quot;Braking\u0026quot; def drive(self): \u0026quot;\u0026quot;\u0026quot; Drive the car \u0026quot;\u0026quot;\u0026quot; return \u0026quot;I'm driving!\u0026quot; Приведенный выше код добавил три атрибута и два метода. Тремя атрибутами являются:
self.color = color self.doors = doors self.tires = tires Атрибуты описывают транспортное средство. Так, автомобиль имеет цвет, некоторое количество дверей и некоторое количество шин. У него также есть два метода. Метод описывает, что делает класс. В данном случае автомобиль может тормозить(brake) и ехать(drive). Возможно, вы заметили, что все методы, включая первый, имеют забавный аргумент self. Давайте поговорим об этом!
Что такое self? Классы Python нуждаются в способе обращения к самим себе. Это не какое-то самовлюбленное созерцание класса. Напротив, это способ отличить один экземпляр от другого. Слово \u0026ldquo;self\u0026rdquo; - это способ самоописания любого объекта, в буквальном смысле. Давайте рассмотрим пример, поскольку я всегда нахожу это полезным, когда изучаю что-то новое и непонятное:
Добавьте следующий код в конец класса, который вы написали выше, и сохраните его:
if __name__ == \u0026quot;__main__\u0026quot;: car = Vehicle(\u0026quot;blue\u0026quot;, 5, 4) print(car.color) truck = Vehicle(\u0026quot;red\u0026quot;, 3, 6) print(truck.color) Условия оператора if в данном примере это стандартный способ указать Пайтону на то, что вы хотите запустить код, если он выполняется как автономный файл. Если вы импортировали свой модуль в другой скрипт, то код, расположенный ниже проверки if не заработает. В любом случае, если вы выполните этот код, вы создадите два экземпляра класса Vehicle: экземпляр легкового автомобиля и экземпляр грузовика. У каждого экземпляра будут свои атрибуты и методы.
Вот почему, когда мы выводим цвет каждого экземпляра, они отличаются. Причина в том, что класс использует аргумент self, чтобы сообщить себе, кто из них кто. Давайте немного изменим класс, чтобы сделать методы более уникальными:
class Vehicle(object): \u0026quot;\u0026quot;\u0026quot;docstring\u0026quot;\u0026quot;\u0026quot; def __init__(self, color, doors, tires, vtype): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.color = color self.doors = doors self.tires = tires self.vtype = vtype def brake(self): \u0026quot;\u0026quot;\u0026quot; Stop the car \u0026quot;\u0026quot;\u0026quot; return \u0026quot;%s braking\u0026quot; % self.vtype def drive(self): \u0026quot;\u0026quot;\u0026quot; Drive the car \u0026quot;\u0026quot;\u0026quot; return \u0026quot;I'm driving a %s %s!\u0026quot; % (self.color, self.vtype) if __name__ == \u0026quot;__main__\u0026quot;: car = Vehicle(\u0026quot;blue\u0026quot;, 5, 4, \u0026quot;car\u0026quot;) print(car.brake()) print(car.drive()) truck = Vehicle(\u0026quot;red\u0026quot;, 3, 6, \u0026quot;truck\u0026quot;) print(truck.drive()) print(truck.brake()) В этом примере мы передаем еще один параметр, чтобы сообщить классу, какой тип автомобиля мы создаем. Затем мы вызываем каждый метод для каждого экземпляра. Если вы запустите этот код, вы должны увидеть следующий результат:
car braking I'm driving a blue car! I'm driving a red truck! truck braking Это демонстрирует, как экземпляр отслеживает свой аргумент \u0026ldquo;self\u0026rdquo;. Вы также могли заметить, что мы можем получать значения атрибутов из метода init в другие методы. Причина в том, что все эти атрибуты снабжены предлогом self. Если бы мы этого не сделали, переменные вышли бы из области видимости в конце метода init.
Подклассы / Наследование Настоящая сила классов становится очевидной, когда вы переходите к подклассам. Мы уже создали подкласс, когда создавали класс на основе объекта. Другими словами, мы создали подкласс object. Поскольку объект не очень интересен, предыдущие примеры не демонстрируют возможности подклассификации.
Создадим подкласс нашего класса Vehicle:
class Car(Vehicle): \u0026quot;\u0026quot;\u0026quot; The Car class \u0026quot;\u0026quot;\u0026quot; def brake(self): \u0026quot;\u0026quot;\u0026quot; Override brake method \u0026quot;\u0026quot;\u0026quot; return \u0026quot;The car class is breaking slowly!\u0026quot; if __name__ == \u0026quot;__main__\u0026quot;: car = Car(\u0026quot;yellow\u0026quot;, 2, 4, \u0026quot;car\u0026quot;) car.brake() 'The car class is breaking slowly!' car.drive() \u0026quot;I'm driving a yellow car!\u0026quot; Для этого примера мы создали подкласс нашего класса Vehicle. Вы заметите, что мы не включили метод init или метод drive. Причина в том, что при создании подкласса Vehicle вы получаете все его атрибуты и методы, если только вы не переопределите их. Таким образом, вы заметите, что мы переопределили метод brake и заставили его делать что-то другое. Остальные методы мы оставили прежними. Поэтому, когда вы говорите машине ехать, она использует оригинальный метод, и мы узнаем, что едем на желтой машине. Разве это не здорово?
Использование значений по умолчанию родительского класса известно как наследование. Это большая тема в объектно-ориентированном программировании (ООП). Это также простой пример полиморфизма. Полиморфные классы обычно имеют одинаковые интерфейсы (т.е. методы, атрибуты), но они не контактируют друг с другом. В языке Python полиморфизм не является очень жестким в плане обеспечения точного совпадения интерфейсов. Вместо этого он следует концепции утиной типизации. Идея утиной типизации заключается в том, что если объект ходит как утка и говорит как утка, то он должен быть уткой. Поэтому в Python, пока класс имеет одинаковые имена методов, не имеет значения, если реализация методов разная.
Теперь, когда вы создаете подкласс, вы можете переопределить как можно больше или меньше функций родительского класса. Если вы полностью переопределите его, то вам, вероятно, будет лучше просто создать новый класс.
Ресурсы https://vegibit.com/python-abstract-base-classes/ `,url:"https://romankurnovskii.com/ru/docs/python101/chapter11_classes/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day11/":{title:"11. Переменные и константы в Go",tags:["devops","golang"],content:`Прежде чем мы перейдем к темам сегодняшнего дня, я хочу выразить огромную благодарность Techworld with Nana и этому фантастическому краткому путешествию по основам Go.
В 8-м дне мы настроили нашу среду, в 9-м дне мы разобрали код Hello #90DaysOfDevOps, а в 10-м дне) мы поработали с нашей рабочей средой Go и немного углубились в компиляцию и запуск кода.
Сегодня мы рассмотрим переменные, константы и типы данных при написании новой программы.
Переменные и константы в Go Давайте начнем с планирования нашего приложения, я думаю, было бы неплохо поработать над программой, которая сообщает нам, сколько дней осталось в нашем испытании #90DaysOfDevOps.
Первое, что нужно учитывать, это то, что, поскольку мы создаем наше приложение, мы приветствуем наших посетителей и даем пользователям отзывы о количестве дней, которые они выполнили, мы можем использовать термин #90DaysOfDevOps много раз на протяжении всей программы. Это отличный вариант использования переменной #90DaysOfDevOps в нашей программе.
Переменные используются для хранения значений. Как маленькая коробка с нашей сохраненной информацией или ценностями. Затем мы можем использовать эту переменную во всей программе, что также выгодно тем, что если эта задача или переменная изменится, нам нужно будет изменить это только в одном месте. Это означает, что мы могли бы перенести это на другие проблемы, с которыми мы сталкиваемся в сообществе, просто изменив значение этой переменной. Чтобы объявить это в нашей программе Go, мы определяем значение, используя ключевое слово для переменных. Это будет жить в нашем блоке кода func main, который вы увидите позже. Подробнее о Ключевых словах можно узнать здесь.
Не забудьте убедиться, что ваши имена переменных являются понятными. Если вы объявляете переменную, вы должны использовать ее, иначе вы получите ошибку. Это делается для того, чтобы избежать возможного неиспользованного кода. То же самое для неиспользуемых пакетов.
var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; С приведенным выше набором и использованием, как мы увидим в следующем фрагменте кода, вы можете видеть из вывода ниже, что мы использовали переменную.
package main import \u0026quot;fmt\u0026quot; func main() { var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Println(\u0026quot;Welcome to\u0026quot;, challenge \u0026quot;\u0026quot;) } Затем вы увидите ниже, что мы построили наш код с помощью приведенного выше примера и получили вывод, показанный ниже. Мы также знаем, что наш челендж длится как минимум 90 дней для этой задачи, но в следующей, может быть, будет 100, поэтому мы хотим определить переменную, которая поможет нам. Однако для нашей программы мы хотим определить это как константу. Константы похожи на переменные, за исключением того, что их значение не может быть изменено в коде (мы все еще можем создать новое приложение позже с этим кодом и изменить эту константу, но это 90 не изменится, пока мы запускаем наше приложение)
Добавим const в наш код и добавим еще одну строку кода, чтобы напечатать результат.
package main import \u0026quot;fmt\u0026quot; func main() { var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; const daystotal = 90 fmt.Println(\u0026quot;Welcome to\u0026quot;, challenge) fmt.Println(\u0026quot;This is a\u0026quot;, daystotal, \u0026quot;challenge\u0026quot;) } Если мы затем снова пройдем этот процесс go build и запустим, вы увидите результат.
Но это не будет концом нашей программы, мы вернемся к ней в 12-м дне, чтобы добавить больше функциональности. Теперь мы хотим добавить еще одну переменную для количества дней, в течение которых мы выполнили задание.
Ниже я добавил переменную dayscomplete с количеством завершенных дней.
package main import \u0026quot;fmt\u0026quot; func main() { var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; const daystotal = 90 var dayscomplete = 11 fmt.Println(\u0026quot;Welcome to\u0026quot;, challenge, \u0026quot;\u0026quot;) fmt.Println(\u0026quot;This is a\u0026quot;, daystotal, \u0026quot;challenge and you have completed\u0026quot;, dayscomplete, \u0026quot;days\u0026quot;) fmt.Println(\u0026quot;Great work\u0026quot;) } Давайте снова запустим go build, или вы можете просто использовать go run
Вот несколько других примеров, которые я использовал, чтобы упростить чтение и редактирование кода. До сих пор мы использовали Println, но мы можем упростить это, используя Printf, используя %v, что означает, что мы определяем наши переменные по порядку в конце строки кода. мы также используем \\n для разрыва строки.
Я использую %v, поскольку здесь используется значение по умолчанию, но есть и другие параметры, которые можно найти документации пакета fmt.
Переменные также могут быть определены в вашем коде в более простом формате. Вместо того, чтобы определять, что это var и type, вы можете закодировать это следующим образом, чтобы получить ту же функциональность, но более чистый и простой вид вашего кода. Это будет работать только для переменных, а не для констант.
func main() { challenge := \u0026quot;#90DaysOfDevOps\u0026quot; const daystotal = 90 Типы в Go В приведенных выше примерах мы не определили тип переменных, это потому, что мы можем задать им значение, Go достаточно умен, чтобы знать, что это за тип, или, по крайней мере, может сделать вывод, что это на основе значения, которое вы сохранили. . Однако, если мы хотим, чтобы пользователь ввел данные, для этого потребуется определенный тип.
До сих пор в нашем коде использовались строки и целые числа. Целые числа для количества дней и строки для названия задачи.
Также важно отметить, что каждый тип данных может выполнять разные действия и вести себя по-разному. Например, целые числа могут умножаться там, где нет строк.
Есть четыре категории
Basic type: в эту категорию попадают числа, строки и логические значения. Aggregate type: к этой категории относятся массивы и структуры. Reference type: в эту категорию попадают указатели, срезы, карты, функции и каналы. Interface type Тип данных — важная концепция в программировании. Тип данных определяет размер и тип значений переменных.
Go статически типизирован, а это означает, что после определения типа переменной он может хранить данные только этого типа.
В Go есть три основных типа данных:
bool: представляет логическое значение и может быть либо истинным, либо ложным. Numeric: представляет целые типы, значения с плавающей запятой и сложные типы. string: представляет строковое значение. Я нашел этот ресурс очень подробным о типах данных Golang by example
Я бы также посоветовал Techworld with Nana на этом этапе довольно подробно рассказать о типах данных в Go.
Если нам нужно определить тип в нашей переменной, мы можем сделать это так:
var TwitterHandle string var DaysCompleted uint Поскольку Go принимает переменные, которым задано значение, мы можем распечатать эти значения следующим образом:
fmt.Printf(\u0026quot;challenge is %T, daystotal is %T, dayscomplete is %T\\n\u0026quot;, conference, daystotal, dayscomplete) Существует много различных типов целых чисел и типов с плавающей запятой, ссылки выше подробно описывают их.
int = целые числа unint = беззнаковые целые числа floating point types = числа с плавающей запятой Источники Введение в Golang StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Далее мы начнем добавлять в нашу программу некоторые функции пользовательского ввода, чтобы программа спрашивала, сколько дней было завершено.
Увидимся завтра.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day11/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-stream-to-cam/":{title:"Потоковое видео с веб-камеры",tags:[],content:`Чему вы научитесь:
На этом шаге вы узнаете, как
получить видеопоток с вашей веб-камеры управлять воспроизведением потока использовать CSS и SVG для обработки видео Полная версия этого шага находится в папке step-01. Немного HTML
Добавьте элемент video и элемент script в index.html в папку work.
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;video autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; И немного JavaScript
Добавьте следующее в main.js в вашей папке js:
'use strict'; // On this codelab, you will be streaming only video (video: true). const mediaStreamConstraints = { video: true, }; // Video element where stream will be placed. const localVideo = document.querySelector('video'); // Local stream that will be reproduced on the video. let localStream; // Handles success by adding the MediaStream to the video element. function gotLocalMediaStream(mediaStream) { localStream = mediaStream; localVideo.srcObject = mediaStream; } // Handles error by logging a message to the console with the error message. function handleLocalMediaStreamError(error) { console.log('navigator.getUserMedia error: ', error); } // Initializes media stream. navigator.mediaDevices.getUserMedia(mediaStreamConstraints) .then(gotLocalMediaStream).catch(handleLocalMediaStreamError); Все приведенные здесь примеры JavaScript используют ‘use strict’, для избежания частых ошибок в кодировании. Узнайте больше, что это означает в http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/
Попробуйте
Откройте index.html в вашем браузере и вы должны увидеть что-то подобное (с видом из вашей камеры, конечно!):
Как это работает
Следуя запросу getUserMedia(), браузер запрашивает у пользователя разрешение на доступ к своей камере (если это впервые, когда запрашивается доступ к камере для текущего источника). В случае успеха возвращается MediaStream, который может быть использован элементов мультимедиа через атрибут srcObject:
navigator.mediaDevices.getUserMedia(mediaStreamConstraints) .then(gotLocalMediaStream).catch(handleLocalMediaStreamError); function gotLocalMediaStream(mediaStream) { localVideo.srcObject = mediaStream; } Аргумент constraints позволяет указать, какой тип мультимедиа получать. В этом примере используется только видео, т.к. звук по умолчанию отключен:
const mediaStreamConstraints = { video: true, }; Вы можете использовать ограничения для дополнительных требований, таких как разрешение видео:
const hdConstraints = { video: { width: { min: 1280 }, height: { min: 720 } } } Спецификация MediaTrackConstraints перечисляет все возможные типы ограничений, хотя не все параметры поддерживаются во всех браузерах. Если запрошенное разрешение не поддерживается выбранной в данный момент камерой, getUserMedia() будет отклонен с ошибкой OverconstrainedError и пользователю даже не предложат предоставить разрешение на доступ к своей камере.
Демо-версию, демонстрирующую, как использовать ограничения для запроса различных разрешений, можно посмотреть по ссылке https://simpl.info/getusermedia/constraints/, а демо-версию с использованием ограничений для выбора камеры и микрофона – по этой ссылке https://simpl.info/getusermedia/sources/.
Если getUserMedia() сработал успешно, в качестве источника элемента video устанавливается видеопоток с веб-камеры:
function gotLocalMediaStream(mediaStream) { localVideo.srcObject = mediaStream; } Бонусные задания
Переданный getUserMedia() объект localStream находится в глобальной области видимости, поэтому вы можете проверить его через консоль браузера: откройте консоль в Chrome, введите stream и нажмите Return (для просмотра консоли в Chrome, нажмите Ctrl+Shift+J, или command+Option+J, если вы работаете на Mac). что возвращает localStream.getVideoTracks()? попробуйте сделать запрос localStream.getVideoTracks()[0].stop() Посмотрите на объект constraints: что произойдет, когда вы меняете его на {audio: true, video: true)? Какой размер у элемента video? Как можно получить естественный размер из JavaScript, в отличие от размера экрана? Используйте Chrome Dev Tools для проверки Попробуйте добавить CSS фильтры в элемент video. Например: video { filter: blur(4px) invert(1) opacity(0.5); } Попробуйте добавить SVG-фильтры. Например: video { filter: hue-rotate(180deg) saturate(200%); } Что вы узнали
На этом шаге вы узнали, как
получать видео с вашей веб-камеры устанавливать ограничения для мультимедиа как навести хаос в элементе video Полная версия этого шага находится в папке step-01.
Советы
не забывайте про атрибут autoplay в элемент video. Без него вы будете видеть только один кадр! есть гораздо больше ограничений для getUserMedia(). Посмотрите их по ссылке https://webrtc.github.io/samples/src/content/peerconnection/constraints/. Как видите, есть много интересных примеров c WebRTC на сайте. Лучшая практика
убедитесь, что ваш элемент video не переполняет его контейнер. Мы добавили width и max-width для установки соответствующего размера и максимального размера видео. Браузер будет рассчитывать высоту автоматически. video { max-width: 100%; width: 320px; } Следующий шаг
Вы получили видео, но как его транслировать? Узнайте на следующем шаге!
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-stream-to-cam/"},"https://romankurnovskii.com/ru/docs/python101/chapter12_introspection/":{title:"12. Интроспекция",tags:[],content:`Независимо от того, новичок ли вы в Python, используете ли вы его уже несколько лет или являетесь экспертом, умение использовать возможности интроспекции Python может помочь вам понять ваш код и тот новый пакет с ужасной документацией, который вы только что скачали. Интроспекция - это модное слово, которое означает наблюдение за собой и размышление о своих мыслях, чувствах и желаниях. В мире Python интроспекция - это нечто похожее. В данном случае интроспекция - это использование Python для изучения Python. В этой главе вы узнаете, как использовать Python, чтобы дать себе подсказку о коде, над которым вы работаете или пытаетесь изучить. Некоторые могут даже назвать это формой отладки.
Вот что мы рассмотрим:
type dir help Тип Python Вы можете не знать этого, но Python может быть вашим типом. Да, Python может сказать вам, какого типа у вас переменная или какой тип возвращается из функции. Это очень удобный инструмент. Давайте рассмотрим несколько примеров, чтобы все стало ясно:
\u0026gt;\u0026gt;\u0026gt; x = \u0026quot;test\u0026quot; \u0026gt;\u0026gt;\u0026gt; y = 7 \u0026gt;\u0026gt;\u0026gt; z = None \u0026gt;\u0026gt;\u0026gt; type(x) \u0026lt;class 'str'\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(y) \u0026lt;class 'int'\u0026gt; \u0026gt;\u0026gt;\u0026gt; type(z) \u0026lt;class 'NoneType'\u0026gt; Как вы видите, в Python есть ключевое слово type, которое позволяет определить, что есть что. В реальной жизни я использовал тип, чтобы понять, что происходит, когда данные моей базы данных повреждены или не соответствуют моим ожиданиям. Я просто добавляю пару строк и вывожу данные каждой строки вместе с ее типом. Это очень помогло мне, когда я был ошарашен каким-то глупым кодом, который я написал.
Python Dir Что такое dir? Это что-то, что вы говорите, когда кто-то говорит или делает что-то глупое? Не в этом контексте! Нет, здесь, на планете Python, ключевое слово dir используется для того, чтобы сообщить программисту, какие атрибуты и методы есть в переданном объекте. Если вы забыли передать объект, dir вернет список имен в текущей области видимости. Как обычно, это легче понять на нескольких примерах.
\u0026gt;\u0026gt;\u0026gt; dir(\u0026quot;test\u0026quot;) ['__add__', '__class__', '__contains__', '__delattr__', '__doc__', '__eq__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getslice__', '__gt__', '__hash__', '__init__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__str__', 'capitalize', 'center', 'count', 'decode', 'encode', 'endswith', 'expandtabs', 'find', 'index', 'isalnum', 'isalpha', 'isdigit', 'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'replace', 'rfind', 'rindex', 'rjust', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] Поскольку в Python все является объектом, мы можем передать строку в dir и узнать, какие методы у него есть. Довольно ловко, да? Теперь давайте попробуем сделать это с импортированным модулем:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; dir(sys) ['__displayhook__', '__doc__', '__egginsert', '__excepthook__', '__name__', '__plen', '__stderr__', '__stdin__', '__stdout__', '_getframe', 'api_version', 'argv', 'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright', 'displayhook', 'dllhandle', 'exc_clear', 'exc_info', 'exc_traceback', 'exc_type', 'exc_value', 'excepthook', 'exec_prefix', 'executable', 'exit', 'exitfunc', 'getcheckinterval', 'getdefaultencoding', 'getfilesystemencoding', 'getrecursionlimit', 'getrefcount', 'getwindowsversion', 'hexversion', 'maxint', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache', 'platform', 'prefix', 'setcheckinterval', 'setprofile', 'setrecursionlimit', 'settrace', 'stderr', 'stdin', 'stdout', 'version', 'version_info', 'warnoptions', 'winver']. Вот теперь это удобно! Если вы еще не поняли, функция dir очень удобна для тех пакетов сторонних разработчиков, которые вы скачали (или скоро скачаете) и которые практически не имеют документации. Как узнать, какие методы доступны в таких случаях? Ну, dir поможет вам разобраться в этом. Конечно, иногда документация находится в самом коде, что приводит нас к встроенной справочной утилите.
Python Help! Python поставляется с удобной утилитой помощи. Просто введите \u0026ldquo;help()\u0026rdquo; (без кавычек) в оболочке Python, и вы увидите следующие указания (версия Python может отличаться)
\u0026gt;\u0026gt;\u0026gt; help() Welcome to Python 3.9's help utility! If this is your first time using Python, you should definitely check out the tutorial on the Internet at https://docs.python.org/3.9/tutorial/. Enter the name of any module, keyword, or topic to get help on writing Python programs and using Python modules. To quit this help utility and return to the interpreter, just type \u0026quot;quit\u0026quot;. To get a list of available modules, keywords, symbols, or topics, type \u0026quot;modules\u0026quot;, \u0026quot;keywords\u0026quot;, \u0026quot;symbols\u0026quot;, or \u0026quot;topics\u0026quot;. Each module also comes with a one-line summary of what it does; to list the modules whose name or summary contain a given string such as \u0026quot;spam\u0026quot;, type \u0026quot;modules spam\u0026quot;. help\u0026gt; Обратите внимание, что теперь у вас есть подсказка \u0026ldquo;help\u0026gt;\u0026rdquo; вместо \u0026ldquo;\u0026raquo;\u0026gt;\u0026rdquo;. Когда вы находитесь в режиме справки, вы можете изучить различные модули, ключевые слова и темы, найденные в Python. Также обратите внимание, что при вводе слова \u0026ldquo;модули\u0026rdquo; вы увидите задержку, пока Python будет искать список в папке библиотеки. Если у вас установлено много сторонних модулей, это может занять довольно много времени, так что будьте готовы сделать себе мокко, пока ждете. Когда все будет готово, просто следуйте инструкциям и поиграйте с этим, и я думаю, вы поймете суть.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter12_introspection/"},"https://romankurnovskii.com/ru/docs/python101/02-part_ii/":{title:"Часть II - Стандартные модули",tags:[],content:`Во второй части вы получите сокращенное описание некоторых разделов стандартной библиотеки Python. Причина такого сокращения в том, что стандартная библиотека Python огромна! Поэтому этот раздел предназначен для того, чтобы вы познакомились с использованием модулей, поставляемых вместе с Python. Я расскажу о модулях, которые я чаще всего использую в своей повседневной работе, а также о тех, которые используют мои коллеги. Я думаю, что этот мини-тур подготовит вас к самостоятельной работе.
Что мы будем изучать:
csv ConfigParser logging os smtplib / email subprocess sys thread / queues time / datetime Далее мы научимся использовать ConfigParser, небольшой модуль, позволяющий читать и записывать файлы конфигурации. После этого мы рассмотрим logging. Модуль os может делать много интересных вещей, но мы постараемся сосредоточиться на тех, которые, как мне кажется, будут наиболее полезными. subprocess позволяет открывать другие процессы.
Модуль sys позволяет завершить программу, получить путь к Python, получить информацию о версии, перенаправить stdout и многое другое. Модуль thread позволяет создавать потоки в вашей программе. Мы не будем слишком углубляться в эту тему, так как она может быстро запутаться.
Модули time и datetime позволяют манипулировать датами и временем в Python, что имеет множество применений при разработке программ.
`,url:"https://romankurnovskii.com/ru/docs/python101/02-part_ii/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day12/":{title:"12. Golang - чтение данных и указатели",tags:["devops","golang"],content:`Получение данных с клавиуатуры Вчера (Днем 11-м) мы создали нашу первую программу Go, и данные, которые мы хотели получить от пользователя, были созданы как переменные в нашем коде. Теперь мы хотим спросить пользователя данные для ввода, чтобы дать переменной значение для конечного сообщения.
Получение пользовательских данных Прежде чем мы это сделаем, давайте еще раз взглянем на наше приложение и пройдемся по переменным, которые нам нужны в качестве теста, прежде чем получить этот пользовательский ввод.
Давайте теперь добавим новую переменную с именем TwitterName, вы можете найти этот новый код ниже, и если мы запустим этот код, это будет наш вывод.
package main import \u0026quot;fmt\u0026quot; func main() { challenge := \u0026quot;#90DaysOfDevOps\u0026quot; const daystotal = 90 fmt.Printf(\u0026quot;Welcome to %v\\n\u0026quot;, challenge) fmt.Printf(\u0026quot;This is a %v challenge\\n\u0026quot;, daystotal) var TwitterName string var DaysComplete int // ask user for their twitter handle TwitterName = \u0026quot;@MichaelCade1\u0026quot; DaysComplete = 12 fmt.Printf(\u0026quot;%v has completed %v days of the challenge\\n\u0026quot;, TwitterName, DaysComplete) fmt.Println(\u0026quot;Great work\u0026quot;) } Прежде чем мы это сделаем, давайте еще раз взглянем на наше приложение и пройдемся по переменным, которые нам нужны в качестве теста, прежде чем получить этот пользовательский ввод.
Вчера мы закончили с нашим кодом, выглядящим так:
package main import \u0026quot;fmt\u0026quot; func main() { var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; const daystotal = 90 var dayscomplete = 11 fmt.Printf(\u0026quot;Welcome to %v\\n\u0026quot;, challenge) fmt.Printf(\u0026quot;This is a %v challenge and you have completed %v days\\n\u0026quot;, daystotal, dayscomplete) fmt.Println(\u0026quot;Great work\u0026quot;) } Мы вручную определили в коде наши переменные и константы challenge, daystotal, dayscomplete.
Давайте теперь добавим новую переменную с именем TwitterName
У нас 12-й день, и нам нужно было бы менять dayscomplete каждый день и компилировать наш код каждый день, если бы он был жестко запрограммирован, что звучит не так уж здорово.
Получая пользовательский ввод, мы хотим получить значение, возможно, имя и количество завершенных дней. Для этого мы можем использовать другую функцию из пакета fmt.
Кратко о пакете fmt, различные функции для: форматированного ввода и вывода (I/O) (input and output)
Печать сообщений Собирать пользовательский ввод Записать в файл Это вместо того, чтобы присваивать значение переменной, мы хотим попросить пользователя ввести его.
fmt.Scan(\u0026amp;TwitterName) Обратите внимание, что мы также используем \u0026amp; перед переменной. Этот символ известен как указатель, который мы рассмотрим в следующем разделе.
В нашем коде вы можете видеть, что мы просим пользователя ввести две переменные, TwitterName и DaysCompleted
package main import \u0026quot;fmt\u0026quot; func main() { const DaysTotal int = 90 challenge := \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Printf(\u0026quot;Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\u0026quot;, challenge, DaysTotal) var TwitterName string var DaysCompleted uint // asking for user input fmt.Println(\u0026quot;Enter Your Twitter Handle: \u0026quot;) fmt.Scanln(\u0026amp;TwitterName) fmt.Println(\u0026quot;How many days have you completed?: \u0026quot;) fmt.Scanln(\u0026amp;DaysCompleted) fmt.Printf(\u0026quot;Thank you %v for taking part and completing %v days.\\n\u0026quot;, TwitterName, DaysCompleted) fmt.Println(\u0026quot;Good luck\u0026quot;) } Давайте теперь запустим нашу программу, и вы увидите, что у нас есть входные данные для обоих вышеперечисленных.
Хорошо, мы получили некоторый пользовательский ввод и напечатали сообщение, но как насчет того, чтобы заставить нашу программу сообщать нам, сколько дней у нас осталось в нашей задаче.
Для этого мы создали переменную с именем remainingDays, и мы жестко оценили ее в нашем коде как 90. Затем нам нужно изменить значение этого значения, чтобы распечатать remainingDays, когда мы получим пользовательский ввод DaysCompleted мы можем сделать это с помощью этого простого изменения переменной.
remainingDays = remainingDays - DaysCompleted Наша программа теперь выглядит вот так:
package main import \u0026quot;fmt\u0026quot; func main() { const DaysTotal int = 90 var remainingDays uint = 90 challenge := \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Printf(\u0026quot;Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\u0026quot;, challenge, DaysTotal) var TwitterName string var DaysCompleted uint // asking for user input fmt.Println(\u0026quot;Enter Your Twitter Handle: \u0026quot;) fmt.Scanln(\u0026amp;TwitterName) fmt.Println(\u0026quot;How many days have you completed?: \u0026quot;) fmt.Scanln(\u0026amp;DaysCompleted) // calculate remaining days remainingDays = remainingDays - DaysCompleted fmt.Printf(\u0026quot;Thank you %v for taking part and completing %v days.\\n\u0026quot;, TwitterName, DaysCompleted) fmt.Printf(\u0026quot;You have %v days remaining for the %v challenge\\n\u0026quot;, remainingDays, challenge) fmt.Println(\u0026quot;Good luck\u0026quot;) } Если мы теперь запустим эту программу, вы увидите, что простой расчет выполняется на основе пользовательского ввода и значения remainingDays
Что такое указатель? (Специальные переменные) Указатель — это (специальная) переменная, которая указывает на адрес памяти другой переменной.
Отличное объяснение этого можно найти здесь geeksforgeeks
package main import \u0026quot;fmt\u0026quot; func main() { var challenge = \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Println(challenge) fmt.Println(\u0026amp;challenge) } Ниже выполняется этот код.
Ресурсы Введение в Golang StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist Увидимся завтра.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day12/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-stream-with-rtcpeerconnection/":{title:"Потоковое видео с помощью RTCPeerConnection",tags:[],content:`Чему вы научитесь
На этом шаге вы узнаете, как:
Абстрагироваться от различий браузера с помощью оболочки WebRTC, adapter.js. Использовать RTCPeerConnection API для потоковой передачи видео. Управлять захватом и потоковой передачей мультимедиа. Полная версия этого шага находится в папке step-2.
Что такое RTCPeerConnection?
RTCPeerConnection - это API для выполнения WebRTC-запросов для потоковой передачи видео и аудио и обмена данными.
В этом примере устанавливается соединение между двумя объектами RTCPeerConnection (известными как узлы) на одной и той же странице.
Не очень практично, но зато полезно для понимания того, как работает RTCPeerConnection.
Добавление элементов video и кнопок управления
В index.html замените один видеоэлемент двумя видеоэлементами и тремя кнопками:
\u0026lt;video id=\u0026quot;localVideo\u0026quot; autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;video id=\u0026quot;remoteVideo\u0026quot; autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button id=\u0026quot;startButton\u0026quot;\u0026gt;Start\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;callButton\u0026quot;\u0026gt;Call\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;hangupButton\u0026quot;\u0026gt;Hang Up\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; Один видеоэлемент будет отображать поток из getUserMedia(), а другой будет показывать это же видео, но передаваемое через RTCPeerConnection (в реальном приложении один видеоэлемент будет отображать локальный поток, а другой – удаленный поток).
Добавьте adapter.js Добавьте ссылку на текущую версию adapter.js выше ссылки на main.js:
\u0026lt;script src=\u0026quot;https://webrtc.github.io/adapter/adapter-latest.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; adapter.js - это оболочка для изоляции приложений от изменений спецификаций и различий в префиксах. (Хотя на самом деле стандарты и протоколы, используемые для реализации WebRTC, очень стабильны, и существует всего несколько имен с префиксами.)
На этом этапе мы используем самую последнюю версию adapter.js, что хорошо для codelab, но не всегда хорошо для приложений. Здесь https://github.com/webrtc/adapter мы объясняем, как сделать так, чтоб у вашего приложения всегда был доступ к самой последней версии.
Для получения полной информации о взаимодействии сWebRTC, переходи по ссылке https://webrtc.github.io/webrtc-org/web-apis/interop/
Теперь index.html должен выглядеть так:
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;video id=\u0026quot;localVideo\u0026quot; autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;video id=\u0026quot;remoteVideo\u0026quot; autoplay playsinline\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;div\u0026gt; \u0026lt;button id=\u0026quot;startButton\u0026quot;\u0026gt;Start\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;callButton\u0026quot;\u0026gt;Call\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;hangupButton\u0026quot;\u0026gt;Hang Up\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;https://webrtc.github.io/adapter/adapter-latest.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Установите код RTCPeerConnection
Замените main.js в папке step-02.
Делать копи-паст в больших кусках кода в codelab – это так себе вариант, конечно. Но чтобы получить и запустить RTCPeerConnection, у нас нет других альтернатив, как провести вас через весь этот путь. Вам нужно научиться, как код работает в каждый момент.
Сделайте звонок Откройте index.html, нажмите кнопку Start, чтоб получить видео с вашей веб-камеры, и затем нажмите Call, чтобы установить одноранговое соединение. Вы должны увидеть одно и то же видео (с вашей веб-камеры) в обоих видео-элементах. Посмотрите консоль браузера, чтоб увидеть логи WebRTC.
Как это работает
В этом шаге будет много всего…
Если вы хотите пропустить объяснение ниже - ок. Вы все еще можете продолжить работу с codelab!
WebRTC использует API RTCPeerConnection для настройки соединения для потоковой передачи видео между клиентами WebRTC, известными как узлы. В этом примере два объекта RTCPeerConnection находятся на одной странице: pc1 и pc2. Это мало используется на практике, но зато хорошо демонстрирует, как работают API. Настройка вызова между WebRTC-узлами включает в себя три задачи:
Создать RTCPeerConnection для каждого конца вызова и на каждом конце добавить локальный поток из getUserMedia(). Получать и делиться сетевой информацией: потенциальные конечные точки подключения известны как ICE-кандидаты. Получать и делиться локальными и удаленными описаниями: метаданные о локальными мультимедиа в формате SDP. Представьте, что Алиса и Боб хотят использовать RTCPeerConnection для настройки видеочата. Сначала Алиса и Боб обмениваются информацией о сети. Выражение \u0026ldquo;finding candidates\u0026rdquo; относится к процессу поиска сетевых интерфейсов и портов с использованием ICE-фреймворк.
Алиса создает объект RTCPeerConnection с обработчиком onicecandidate (addEventListener(\u0026lsquo;icecandidate\u0026rsquo;)). Это соответствует следующему коду из main.js let localPeerConnection; localPeerConnection = new RTCPeerConnection(servers); localPeerConnection.addEventListener('icecandidate', handleConnection); localPeerConnection.addEventListener( 'iceconnectionstatechange', handleConnectionChange); Аргумент servers для RTCPeerConnection в этом примере не используется. Здесь вы можете указать STUN и TURN серверы. WebRTC разработан для работы с P2P, поэтому пользователи могут подключаться по самому прямому возможному маршруту. Однако WebRTC создан для работы в реальных сетях: клиентским приложениям необходимо проходить через шлюзы NAT (http://en.wikipedia.org/wiki/NAT_traversal) и брандмауэры, а P2P сеть нуждается в резервном варианте на случай сбоя прямого соединения. В рамках этого процесса, API WebRTC используют STUN-серверы для получения IP-адреса вашего компьютера и TURN-серверы для ретрансляции в случае сбоя P2P связи. Подробнее об этом - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/
Алиса вызывает getUserMedia() и добавляет переданные поток: navigator.mediaDevices.getUserMedia(mediaStreamConstraints). then(gotLocalMediaStream). catch(handleLocalMediaStreamError); function gotLocalMediaStream(mediaStream) { localVideo.srcObject = mediaStream; localStream = mediaStream; trace('Received local stream.'); callButton.disabled = false; // Enable call button. } localPeerConnection.addStream(localStream); trace('Added local stream to localPeerConnection.'); Обработчик onicecandidate из шага 1 вызывается, когда становятся доступными сетевые кандидаты. Алиса отправляет Бобу данные кандидата. В реальном приложении этот процесс (известный как сигналинг) осуществляется через службу обмена сообщениями – вы узнаете, как это сделать, позднее. Конечно, на этом этапе два объекта RTCPeerConnection находятся на одной странице и могут взаимодействовать напрямую без необходимости во внешних сообщениях. Когда Боб получает сообщение о кандидате от Алисы, он вызывает addIceCandidate(), чтобы добавить кандидата в описание удаленного узла: function handleConnection(event) { const peerConnection = event.target; const iceCandidate = event.candidate; if (iceCandidate) { const newIceCandidate = new RTCIceCandidate(iceCandidate); const otherPeer = getOtherPeer(peerConnection); otherPeer.addIceCandidate(newIceCandidate) .then(() =\u0026gt; { handleConnectionSuccess(peerConnection); }).catch((error) =\u0026gt; { handleConnectionFailure(peerConnection, error); }); trace(\`\${getPeerName(peerConnection)} ICE candidate:\\n\` + \`\${event.candidate.candidate}.\`); } } Узлам WebRTC также необходимо узнавать и обмениваться информацией о локальных и удаленных аудио- и видеоматериалах, такими как разрешение и возможности кодеков, и обмениваться ими. Сигналинг для обмена информацией о конфигурации мультимедиа осуществляется путем обмена большими двоичными объектами метаданных, известными как offer и answer, с использованием формата Session Description Protocol, известного как SDP (http://en.wikipedia.org/wiki/Session_Description_Protocol):
Алиса запускает метод RTCPeerConnectioncreateOffer(). Возвращенный промис обеспечивает RTCSessionDescription: Alice’s local session description: trace('localPeerConnection createOffer start.'); localPeerConnection.createOffer(offerOptions) .then(createdOffer).catch(setSessionDescriptionError); В случае успеха Алиса устанавливает локальное описание, используя setLocalDescription(), а затем отправляет это описание сеанса Бобу через сигналинг-канал. Боб принимает описание, отправленное ему Алисой, в качестве удаленного описания, используя setRemoteDescription(). Боб запускает метод RTCPeerConnection createAnswer(), передавая ему удаленное описание, которое он получил от Алисы, чтобы можно было создать локальный сеанс, совместимый с ее сеансом. Промис createAnswer() передает описание RTCSessionDescription: Боб устанавливает это как локальное описание и отправляет его Алисе. Когда Алиса получает описание сеанса Боба, она устанавливает его в качестве удаленного описания с помощью setRemoteDescription(). // Logs offer creation and sets peer connection session descriptions. function createdOffer(description) { trace(\`Offer from localPeerConnection:\\n\${description.sdp}\`); trace('localPeerConnection setLocalDescription start.'); localPeerConnection.setLocalDescription(description) .then(() =\u0026gt; { setLocalDescriptionSuccess(localPeerConnection); }).catch(setSessionDescriptionError); trace('remotePeerConnection setRemoteDescription start.'); remotePeerConnection.setRemoteDescription(description) .then(() =\u0026gt; { setRemoteDescriptionSuccess(remotePeerConnection); }).catch(setSessionDescriptionError); trace('remotePeerConnection createAnswer start.'); remotePeerConnection.createAnswer() .then(createdAnswer) .catch(setSessionDescriptionError); } // Logs answer to offer creation and sets peer connection session descriptions. function createdAnswer(description) { trace(\`Answer from remotePeerConnection:\\n\${description.sdp}.\`); trace('remotePeerConnection setLocalDescription start.'); remotePeerConnection.setLocalDescription(description) .then(() =\u0026gt; { setLocalDescriptionSuccess(remotePeerConnection); }).catch(setSessionDescriptionError); trace('localPeerConnection setRemoteDescription start.'); localPeerConnection.setRemoteDescription(description) .then(() =\u0026gt; { setRemoteDescriptionSuccess(localPeerConnection); }).catch(setSessionDescriptionError); } Пинг! Бонусные задания
Посмотрите chrome://webrtc-internals. Там отражены статы WebRTC и отлаженные данные (Полный список ссылок в Chrome – chrome://about). Сделайте разметку страницы через CSS: Расположите видео друг за другом Сделайте кнопки такой же ширины, но с большим размером текста Убедитесь, что макет работает на мобильных устройствах В консоли Chrome Dev Tools посмотрите localStream, localPeerConnection и remotePeerConnection. Из консоли, посмотрите на localPeerConnecionpc1.localDescription. Как выглядит формат SDP? Что вы узнали?
На этом шаге вы узнали, как
уйти от различий в браузерах через WebRTC оболочку adapter.js использовать RTCPeerConncetion API для потоковой передачи видео контролировать захват медиа и потоковую передачу данных делиться мультимедиа и сетевой информацией между узлами, чтоб разрешить вызов WebRTC. Полная версия этого шага находится в папке step-2. Советы
на этом шаге вам нужно столько всего освоить! Чтобы найти другие ресурсы, объясняющие более детально RTCPeerConnection, загляните на webrtc.org. Эта страница включает решения для JavaScript фреймворков – если вы хотите использовать WebRTC, но не хотите конфликтовать с API. Узнайте больше про оболочку adapter.js из https://github.com/webrtc/adapter Хотите посмотреть, как выглядит лучшее в мире приложение для видеочата? Посмотрите на AppRTC, каноническое приложение для звонков WebRTC: приложение (https://appr.tc/) и код (https://github.com/webrtc/apprtc) . Время настройки вызова составляет менее 500 мс! Лучшая практика
Для обеспечения надежности вашего кода в будущем используйте новые API-интерфейсы на основе промисов и включите совместимость с браузерами, которые их не поддерживают, используя adapter.js Следующий шаг
Этот шаг показывает, как использовать WebRTC для передачи видео между узлами – но эта codelab в том числе и о данных! В следующем шаге выясним, как передавать произвольные данные с помощью RTCDataChannel.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-stream-with-rtcpeerconnection/"},"https://romankurnovskii.com/ru/docs/python101/chapter13_csv/":{title:"13. Модуль csv",tags:[],content:`Модуль csv дает программисту Python возможность анализировать файлы CSV (Comma Separated Values – переменные, разделенные запятыми). CSV-файл - это текстовый файл, в котором каждая строка содержит несколько полей, разделенных запятыми или каким-либо другим разделителем. Каждую строку можно представить как ряд, а каждое поле - как столбец. Формат CSV не имеет стандарта, но они достаточно похожи, чтобы модуль csv мог читать подавляющее большинство CSV-файлов. Вы также можете записывать CSV-файлы с помощью модуля csv.
Чтение файла CSV Существует два способа чтения CSV-файла. Вы можете использовать функцию чтения модуля csv или класс DictReader. Мы рассмотрим оба метода. Но сначала нам нужно получить CSV-файл, чтобы было что разбирать. Существует множество сайтов, предоставляющих интересную информацию в формате CSV. Мы будем использовать сайт Всемирной организации здравоохранения (ВОЗ) для загрузки информации о туберкулезе. Вы можете перейти сюда, чтобы получить ее: http://www.who.int/tb/country/data/download/en/. Как только вы получите файл, мы будем готовы начать. Готовы? Тогда давайте посмотрим на код!
import csv def csv_reader(file_obj): \u0026quot;\u0026quot;\u0026quot; Read a csv file \u0026quot;\u0026quot;\u0026quot; reader = csv.reader(file_obj) for row in reader: print(\u0026quot; \u0026quot;.join(row)) if __name__ == \u0026quot;__main__\u0026quot;: csv_path = \u0026quot;TB_data_dictionary_2014-02-26.csv\u0026quot; with open(csv_path, \u0026quot;r\u0026quot;) as f_obj: csv_reader(f_obj) Давайте немного разберемся с этим. Во-первых, мы должны импортировать модуль csv. Затем мы создаем очень простую функцию csv_reader, которая получает доступ к объекту файла. Внутри функции мы передаем объект файла в функцию csv.reader, которая возвращает объект reader. Объект reader позволяет выполнять итерацию, как и обычный объект файла. Это позволяет нам итерировать каждую строку в объекте reader и выводить строку данных за вычетом запятых. Это работает по той причине, что каждый ряд является списком, и мы можем объединить все элементы в списке вместе, создав одну большую строку
Теперь давайте создадим собственный CSV-файл и передадим его в класс DictReader. Вот очень простой вариант:
first_name,last_name,address,city,state,zip_code Tyrese,Hirthe,1404 Turner Ville,Strackeport,NY,19106-8813 Jules,Dicki,2410 Estella Cape Suite 061,Lake Nickolasville,ME,00621-7435 Dedric,Medhurst,6912 Dayna Shoal,Stiedemannberg,SC,43259-2273 Сохраним его в файле с именем data.csv. Теперь мы готовы разобрать файл с помощью класса DictReader. Давайте попробуем это сделать:
import csv def csv_dict_reader(file_obj): \u0026quot;\u0026quot;\u0026quot; Read a CSV file using csv.DictReader \u0026quot;\u0026quot;\u0026quot; reader = csv.DictReader(file_obj, delimiter=',') for line in reader: print(line[\u0026quot;first_name\u0026quot;]), print(line[\u0026quot;last_name\u0026quot;]) if __name__ == \u0026quot;__main__\u0026quot;: with open(\u0026quot;data.csv\u0026quot;) as f_obj: csv_dict_reader(f_obj) В приведенном выше примере мы открываем файл и передаем объект файла в нашу функцию, как мы это делали раньше. Функция передает объект файла нашему классу DictReader. Мы сообщаем DictReader, что разделителем является запятая. На самом деле это не обязательно, так как код будет работать и без этого ключевого слова. Тем не менее, это хорошая идея, так как это позволяет пролить свет на то, что именно происходит внутри кода. Далее мы переходим к объекту reader и обнаруживаем, что каждая строка в объекте reader является словарем. Это позволяет очень просто распечатать определенные фрагменты строки.
Теперь мы готовы узнать, как записать файл csv на диск.
Запись файла CSV Модуль csv также имеет два метода, которые вы можете использовать для записи CSV-файла. Вы можете использовать функцию writer или класс DictWriter. Мы рассмотрим оба этих метода. Мы будем работать с функцией writer. Давайте рассмотрим простой пример:
import csv def csv_writer(data, path): \u0026quot;\u0026quot;\u0026quot; Write data to a CSV file path \u0026quot;\u0026quot;\u0026quot; with open(path, \u0026quot;w\u0026quot;, newline='') as csv_file: writer = csv.writer(csv_file, delimiter=',') for line in data: writer.writerow(line) if __name__ == \u0026quot;__main__\u0026quot;: data = [\u0026quot;first_name,last_name,city\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Tyrese,Hirthe,Strackeport\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Jules,Dicki,Lake Nickolasville\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Dedric,Medhurst,Stiedemannberg\u0026quot;.split(\u0026quot;,\u0026quot;) ] path = \u0026quot;output.csv\u0026quot; csv_writer(data, path) В приведенном выше коде мы создаем функцию csv_writer, которая принимает два аргумента: data и path. data - это список списков, который мы создаем в нижней части сценария. Мы используем сокращенную версию данных из предыдущего примера и разделяем строки на запятые. Это возвращает список. В итоге мы получаем вложенный список, который выглядит следующим образом:
[['first_name', 'last_name', 'city'], ['Tyrese', 'Hirthe', 'Strackeport'], ['Jules', 'Dicki', 'Lake Nickolasville'], ['Dedric', 'Medhurst', 'Stiedemannberg']] Функция csv_writer открывает путь, по которому мы проходим, и создает объект csv writer. Затем мы перебираем структуру вложенного списка и записываем каждую строку на диск. Обратите внимание, что при создании объекта writer мы указали, каким должен быть разделитель. Если вы хотите, чтобы разделителем была не запятая, а что-то другое, то это то место, где вы должны ее установить.
Теперь мы готовы узнать, как записать CSV-файл с помощью класса DictWriter! Мы собираемся использовать данные из предыдущей версии и преобразовать их в список словарей, которые мы можем скормить нашему голодному DictWriter. Давайте посмотрим:
import csv def csv_dict_writer(path, fieldnames, data): \u0026quot;\u0026quot;\u0026quot; Writes a CSV file using DictWriter \u0026quot;\u0026quot;\u0026quot; with open(path, \u0026quot;w\u0026quot;, newline='') as out_file: writer = csv.DictWriter(out_file, delimiter=',', fieldnames=fieldnames) writer.writeheader() for row in data: writer.writerow(row) if __name__ == \u0026quot;__main__\u0026quot;: data = [\u0026quot;first_name,last_name,city\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Tyrese,Hirthe,Strackeport\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Jules,Dicki,Lake Nickolasville\u0026quot;.split(\u0026quot;,\u0026quot;), \u0026quot;Dedric,Medhurst,Stiedemannberg\u0026quot;.split(\u0026quot;,\u0026quot;) ] my_list = [] fieldnames = data[0] for values in data[1:]: inner_dict = dict(zip(fieldnames, values)) my_list.append(inner_dict) path = \u0026quot;dict_output.csv\u0026quot; csv_dict_writer(path, fieldnames, my_list) Сначала мы начнем со второго раздела. Как вы видите, мы начинаем со структуры вложенного списка, которая была у нас раньше. Далее мы создаем пустой список и список, содержащий имена полей, который будет первым списком во вложенном списке. Помните, что списки основаны на нулях, поэтому первый элемент в списке начинается с нуля! Далее мы перебираем вложенные списки, начиная со второго элемента:
for values in data[1:]: inner_dict = dict(zip(fieldnames, values)) my_list.append(inner_dict) Внутри цикла for мы используем встроенные модули Python для создания словаря. Метод zip* возьмет два итератора (в данном случае списки) и превратит их в список кортежей. Вот пример:
zip(fieldnames, values) [('first_name', 'Dedric'), ('last_name', 'Medhurst'), ('city', 'Stiedemannberg')] Теперь, когда вы обернете этот вызов в dict, он превратит список кортежей в словарь. Наконец, мы добавляем словарь к списку. По окончании работы функции for вы получите структуру данных, которая будет выглядеть следующим образом:
**[{‘city’: ‘Strackeport’, ‘first_name’: ‘Tyrese’, ‘last_name’: ‘Hirthe’},** {‘city’: ‘Lake Nickolasville’, ‘first_name’: ‘Jules’, ‘last_name’: ‘Dicki’}, {‘city’: ‘Stiedemannberg’, ‘first_name’: ‘Dedric’, ‘last_name’: ‘Medhurst’}] В конце второй сессии мы вызываем нашу функцию csv_dict_writer и передаем все необходимые аргументы. Внутри функции мы создаем экземпляр DictWriter и передаем ему объект файла, значение разделителя и список имен полей. Затем мы записываем имена полей на диск и в цикле просматриваем данные по одной строке за раз, записывая их на диск. Класс DictWriter также поддерживает метод writerows, который мы могли бы использовать вместо цикла. Функция csv.writer также поддерживает это сделать.
Возможно, вам будет интересно узнать, что с помощью модуля csv можно также создавать диалекты. Это позволит вам указывать модулю csv, как именно читать или писать файл в очень простой форме. Если вам нужно что-то подобное из-за странно отформатированного файла от клиента, то вы найдете эту функциональность бесценной.
Подведение итогов Теперь вы знаете, как использовать модуль csv для чтения и записи CSV-файлов. Существует множество веб-сайтов, которые размещают свои данные в этом формате, и он часто используется в деловом мире. В следующей главе мы начнем изучать модуль ConfigParser.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter13_csv/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day13/":{title:"13. Go - подключение Twitter API",tags:["devops","golang"],content:`Твитните о своем прогрессе с нашим новым приложением В последний день изучения этого языка программирования мы только коснулись его основ, но я думаю, что это начало.
За последние несколько дней мы взяли небольшую идею для приложения и добавили функциональность, в этой статье я хочу воспользоваться преимуществами тех пакетов, которые мы упомянули, и создать функциональность для нашего приложения, чтобы не только дать вам обновление вашего прогресса на экране, но также отправьте твит с подробностями задачи и вашим статусом.
Добавление возможности твитить свой прогресс Первое, что нам нужно сделать, это настроить доступ API разработчика к Twitter, чтобы это работало.
Перейдите на Платформу разработчиков Twitter и войдите в систему, используя свой идентификатор Twitter и данные. Оказавшись внутри, вы должны увидеть что-то вроде приведенного ниже без приложения, которое я уже создал.
Здесь вы также можете запросить дополнительный доступ. Это может занять некоторое время, но для меня это было очень быстро.
Затем мы должны выбрать «Projects \u0026amp; Apps» и создать наше приложение. Ограничения зависят от доступа к вашей учетной записи, при этом у вас должно быть только одно приложение и один проект, а с повышенными правами у вас может быть 3 приложения.
Дайте вашему приложению имя
Затем вам будут предоставлены эти токены API, важно сохранить их в безопасном месте. (С тех пор я удалил это приложение) Они понадобятся нам позже с нашим приложением Go.
Теперь у нас создано наше приложение (мне пришлось изменить имя моего приложения, так как то, что на скриншоте выше, уже было сделано, эти имена должны быть уникальными)
Ключи, которые мы собрали ранее, известны как наши потребительские ключи, и нам также понадобятся наш токен доступа и секреты. Мы можем собрать эту информацию, используя вкладку «Ключи и токены».
Хорошо, на данный момент мы закончили работу с порталом для разработчиков Twitter. Убедитесь, что вы сохранили свои ключи, потому что они понадобятся нам позже.
Перейти Twitter бот Помните код, который мы запускаем в нашем приложении?
package main import \u0026quot;fmt\u0026quot; func main() { const DaysTotal int = 90 var remainingDays uint = 90 challenge := \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Printf(\u0026quot;Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\u0026quot;, challenge, DaysTotal) var TwitterName string var DaysCompleted uint // asking for user input fmt.Println(\u0026quot;Enter Your Twitter Handle: \u0026quot;) fmt.Scanln(\u0026amp;TwitterName) fmt.Println(\u0026quot;How many days have you completed?: \u0026quot;) fmt.Scanln(\u0026amp;DaysCompleted) // calculate remaining days remainingDays = remainingDays - DaysCompleted fmt.Printf(\u0026quot;Thank you %v for taking part and completing %v days.\\n\u0026quot;, TwitterName, DaysCompleted) fmt.Printf(\u0026quot;You have %v days remaining for the %v challenge\\n\u0026quot;, remainingDays, challenge) fmt.Println(\u0026quot;Good luck\u0026quot;) } Теперь нам нужно подумать о коде для отправки нашего вывода или сообщения в Twitter в виде твита. Мы будем использовать go-twitter. Это клиентская библиотека Go для Twitter API.
Чтобы проверить это, прежде чем помещать это в наше основное приложение, я создал новый каталог в нашей папке src с именем go-twitter-bot, запустил go mod init github.com/michaelcade/go-twitter-bot в папке который затем создал файл go.mod, а затем мы можем начать писать наш новый main.go и протестировать его.
Теперь нам нужны те ключи, токены и секреты, которые мы собрали на портале разработчиков Twitter. Мы собираемся установить их в наших переменных среды. Это будет зависеть от ОС, которую вы используете:
Windows
set CONSUMER_KEY set CONSUMER_SECRET set ACCESS_TOKEN set ACCESS_TOKEN_SECRET Linux / macOS
export CONSUMER_KEY export CONSUMER_SECRET export ACCESS_TOKEN export ACCESS_TOKEN_SECRET At this stage, you can take a look at day13_example2 at the code but you will see here that we are using a struct to define our keys, secrets and tokens.
We then have a func to parse those credentials and make that connection to the Twitter API
Then based on the success we will then send a tweet.
На этом этапе вы можете взглянуть на следующий код
package main import ( // other imports \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/dghubble/go-twitter/twitter\u0026quot; \u0026quot;github.com/dghubble/oauth1\u0026quot; ) // Credentials stores all of our access/consumer tokens // and secret keys needed for authentication against // the twitter REST API. type Credentials struct { ConsumerKey string ConsumerSecret string AccessToken string AccessTokenSecret string } // getClient is a helper function that will return a twitter client // that we can subsequently use to send tweets, or to stream new tweets // this will take in a pointer to a Credential struct which will contain // everything needed to authenticate and return a pointer to a twitter Client // or an error func getClient(creds *Credentials) (*twitter.Client, error) { // Pass in your consumer key (API Key) and your Consumer Secret (API Secret) config := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret) // Pass in your Access Token and your Access Token Secret token := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret) httpClient := config.Client(oauth1.NoContext, token) client := twitter.NewClient(httpClient) // Verify Credentials verifyParams := \u0026amp;twitter.AccountVerifyParams{ SkipStatus: twitter.Bool(true), IncludeEmail: twitter.Bool(true), } // we can retrieve the user and verify if the credentials // we have used successfully allow us to log in! user, _, err := client.Accounts.VerifyCredentials(verifyParams) if err != nil { return nil, err } log.Printf(\u0026quot;User's ACCOUNT:\\n%+v\\n\u0026quot;, user) return client, nil } func main() { fmt.Println(\u0026quot;Go-Twitter Bot v0.01\u0026quot;) creds := Credentials{ AccessToken: os.Getenv(\u0026quot;ACCESS_TOKEN\u0026quot;), AccessTokenSecret: os.Getenv(\u0026quot;ACCESS_TOKEN_SECRET\u0026quot;), ConsumerKey: os.Getenv(\u0026quot;CONSUMER_KEY\u0026quot;), ConsumerSecret: os.Getenv(\u0026quot;CONSUMER_SECRET\u0026quot;), } client, err := getClient(\u0026amp;creds) if err != nil { log.Println(\u0026quot;Error getting Twitter Client\u0026quot;) log.Println(err) } tweet, resp, err := client.Statuses.Update(\u0026quot;A Test Tweet from the future, testing a #90DaysOfDevOps Program that tweets, tweet tweet\u0026quot;, nil) if err != nil { log.Println(err) } log.Printf(\u0026quot;%+v\\n\u0026quot;, resp) log.Printf(\u0026quot;%+v\\n\u0026quot;, tweet) } Здесь вы увидите, что мы используем структуру для определения наших ключей, секретов и токенов.
Затем у нас есть func, чтобы проанализировать эти учетные данные и установить это соединение с API Twitter.
Затем, в зависимости от успеха, мы отправим твит.
Код выше либо выдаст вам ошибку в зависимости от того, что происходит, либо будет выполнен успешно, и вам будет отправлен твит с сообщением, указанным в коде.
Соединение двух вместе - Go-Twitter-Bot + наше приложение Теперь нам нужно объединить эти два файла в наш main.go. Я уверен, что кто-то кричит, что есть лучший способ сделать это, и, пожалуйста, прокомментируйте это, поскольку вы можете иметь более одного файла .go в одном файле. project это может иметь смысл, но это работает.
Так выглядит итоговый рзультат:
package main import ( // other imports \u0026quot;fmt\u0026quot; \u0026quot;log\u0026quot; \u0026quot;os\u0026quot; \u0026quot;github.com/dghubble/go-twitter/twitter\u0026quot; \u0026quot;github.com/dghubble/oauth1\u0026quot; ) // Credentials stores all of our access/consumer tokens // and secret keys needed for authentication against // the twitter REST API. type Credentials struct { ConsumerKey string ConsumerSecret string AccessToken string AccessTokenSecret string } // getClient is a helper function that will return a twitter client // that we can subsequently use to send tweets, or to stream new tweets // this will take in a pointer to a Credential struct which will contain // everything needed to authenticate and return a pointer to a twitter Client // or an error func getClient(creds *Credentials) (*twitter.Client, error) { // Pass in your consumer key (API Key) and your Consumer Secret (API Secret) config := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret) // Pass in your Access Token and your Access Token Secret token := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret) httpClient := config.Client(oauth1.NoContext, token) client := twitter.NewClient(httpClient) // Verify Credentials verifyParams := \u0026amp;twitter.AccountVerifyParams{ SkipStatus: twitter.Bool(true), IncludeEmail: twitter.Bool(true), } // we can retrieve the user and verify if the credentials // we have used successfully allow us to log in! user, _, err := client.Accounts.VerifyCredentials(verifyParams) if err != nil { return nil, err } log.Printf(\u0026quot;User's ACCOUNT:\\n%+v\\n\u0026quot;, user) return client, nil } func main() { creds := Credentials{ AccessToken: os.Getenv(\u0026quot;ACCESS_TOKEN\u0026quot;), AccessTokenSecret: os.Getenv(\u0026quot;ACCESS_TOKEN_SECRET\u0026quot;), ConsumerKey: os.Getenv(\u0026quot;CONSUMER_KEY\u0026quot;), ConsumerSecret: os.Getenv(\u0026quot;CONSUMER_SECRET\u0026quot;), } { const DaysTotal int = 90 var remainingDays uint = 90 challenge := \u0026quot;#90DaysOfDevOps\u0026quot; fmt.Printf(\u0026quot;Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\u0026quot;, challenge, DaysTotal) var TwitterName string var DaysCompleted uint // asking for user input fmt.Println(\u0026quot;Enter Your Twitter Handle: \u0026quot;) fmt.Scanln(\u0026amp;TwitterName) fmt.Println(\u0026quot;How many days have you completed?: \u0026quot;) fmt.Scanln(\u0026amp;DaysCompleted) // calculate remaining days remainingDays = remainingDays - DaysCompleted //fmt.Printf(\u0026quot;Thank you %v for taking part and completing %v days.\\n\u0026quot;, TwitterName, DaysCompleted) //fmt.Printf(\u0026quot;You have %v days remaining for the %v challenge\\n\u0026quot;, remainingDays, challenge) // fmt.Println(\u0026quot;Good luck\u0026quot;) client, err := getClient(\u0026amp;creds) if err != nil { log.Println(\u0026quot;Error getting Twitter Client, this is expected if you did not supply your Twitter API tokens\u0026quot;) log.Println(err) } message := fmt.Sprintf(\u0026quot;Hey I am %v I have been doing the %v for %v days and I have %v Days left\u0026quot;, TwitterName, challenge, DaysCompleted, remainingDays) tweet, resp, err := client.Statuses.Update(message, nil) if err != nil { log.Println(err) } log.Printf(\u0026quot;%+v\\n\u0026quot;, resp) log.Printf(\u0026quot;%+v\\n\u0026quot;, tweet) } } Результатом этого должен быть твит, но если вы не указали свои переменные среды, вы должны получить сообщение об ошибке, подобное приведенному ниже.
После того, как вы исправите это или решите не проходить аутентификацию в Twitter, вы можете использовать код, с которым мы закончили вчера. Вывод терминала в случае успеха будет выглядеть примерно так:
Полученный твит должен выглядеть примерно так:
Как скомпилировать для нескольких ОС Далее я хочу затронуть вопрос: «Как компилировать для нескольких операционных систем?» Отличительной особенностью Go является то, что он может легко компилироваться для многих различных операционных систем. Вы можете получить полный список, выполнив следующую команду:
go tool dist list Использование наших команд go build до сих пор было замечательным, и оно будет использовать переменные среды GOOS и GOARCH, чтобы определить хост-компьютер и то, для чего должна быть собрана сборка. Но мы также можем создавать другие двоичные файлы, используя приведенный ниже код в качестве примера.
GOARCH=amd64 GOOS=darwin go build -o \${BINARY_NAME}_0.1_darwin main.go GOARCH=amd64 GOOS=linux go build -o \${BINARY_NAME}_0.1_linux main.go GOARCH=amd64 GOOS=windows go build -o \${BINARY_NAME}_0.1_windows main.go GOARCH=arm64 GOOS=linux go build -o \${BINARY_NAME}_0.1_linux_arm64 main.go GOARCH=arm64 GOOS=darwin go build -o \${BINARY_NAME}_0.1_darwin_arm64 main.go Это даст вам двоичные файлы в вашем каталоге для всех вышеперечисленных платформ. Затем вы можете взять это и создать make-файл для создания этих двоичных файлов всякий раз, когда вы добавляете новые функции и функции в свой код.
Файл: makefile
BINARY_NAME=90DaysOfDevOps build: GOARCH=amd64 GOOS=darwin go build -o \${BINARY_NAME}_0.2_darwin main.go GOARCH=amd64 GOOS=linux go build -o \${BINARY_NAME}_0.2_linux main.go GOARCH=amd64 GOOS=windows go build -o \${BINARY_NAME}_0.2_windows main.go GOARCH=arm64 GOOS=linux go build -o \${BINARY_NAME}_0.2_linux_arm64 main.go GOARCH=arm64 GOOS=darwin go build -o \${BINARY_NAME}_0.2_darwin_arm64 main.go run: ./\${BINARY_NAME} build_and_run: build run clean: go clean rm \${BINARY_NAME}-darwin rm \${BINARY_NAME}-linux rm \${BINARY_NAME}-windows Источники StackOverflow 2021 Developer Survey Why we are choosing Golang to learn Jake Wright - Learn Go in 12 minutes Techworld with Nana - Golang full course - 3 hours 24 mins NOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins FreeCodeCamp - Learn Go Programming - Golang Tutorial for Beginners Hitesh Choudhary - Complete playlist A great repo full of all things DevOps \u0026amp; exercises GoByExample - Example based learning go.dev/tour/list go.dev/learn На этом блок \u0026ldquo;язык программирования\u0026rdquo;. Так много всего, что можно охватить, и я надеюсь, что вы смогли продолжить изучение вышеизложенного и понять некоторые другие аспекты языка программирования Go.
Затем мы сосредоточимся на Linux и некоторых основах, которые мы все должны знать.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day13/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-rtcdatachannel-exchange-data/":{title:"Использование RTCDataChannel для обмена данными",tags:[],content:`Чему вы научитесь
как обмениваться данными между узлами WebRTC Полная версия этого шага находится в папке step-03.
Обновите свой HTML
На этом шаге вы будете использовать WebRTC каналы данных для отправки текста между двумя textarea элементами на одной странице. Это опять не сильно применимо на практике, но зато демонстрирует, как WebRTC можно использовать для обмена данными, а также для потоковых видео.
Удалите элементы video и button из index.html и замените их следующим HTML-кодом:
\u0026lt;textarea id=\u0026quot;dataChannelSend\u0026quot; disabled placeholder=\u0026quot;Press Start, enter some text, then press Send.\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;textarea id=\u0026quot;dataChannelReceive\u0026quot; disabled\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;div id=\u0026quot;buttons\u0026quot;\u0026gt; \u0026lt;button id=\u0026quot;startButton\u0026quot;\u0026gt;Start\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;sendButton\u0026quot;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;closeButton\u0026quot;\u0026gt;Stop\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; Одна текстовая область будет предназначена для ввода текста, другая будет отображать текст в потоковом режиме между узлами. Теперь index.html должен выглядеть так:
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;textarea id=\u0026quot;dataChannelSend\u0026quot; disabled placeholder=\u0026quot;Press Start, enter some text, then press Send.\u0026quot;\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;textarea id=\u0026quot;dataChannelReceive\u0026quot; disabled\u0026gt;\u0026lt;/textarea\u0026gt; \u0026lt;div id=\u0026quot;buttons\u0026quot;\u0026gt; \u0026lt;button id=\u0026quot;startButton\u0026quot;\u0026gt;Start\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;sendButton\u0026quot;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026quot;closeButton\u0026quot;\u0026gt;Stop\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;https://webrtc.github.io/adapter/adapter-latest.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Обновите свой JavaScript
Замените main.js содержимым из step-03/js/main.js.
Как и на предыдущем шаге, делать копи-паст на больших кусках кода – не идеальный вариант развития событий в codelab (как и с RTCPeerConnection). Но альтернатив у нас нет.
Протестируйте потоковые данные между узлами: откройте index.html, нажмите Start для установки соединения между узлами, введите какой-то текст в textarea слева, затем нажмите на Send, чтобы передать текст через каналы данных WebRTC.
Как это работает Этот код использует RTCPeerConnection и RTCDataChannel для обмена текстовыми сообщениями Большая часть кода на этом шаге такая же, как и в примере RTCPeerConnection. Функции sendData() и createConnection() содержат большую часть нового кода:
function createConnection() { dataChannelSend.placeholder = ''; var servers = null; pcConstraint = null; dataConstraint = null; trace('Using SCTP based data channels'); // For SCTP, reliable and ordered delivery is true by default. // Add localConnection to global scope to make it visible // from the browser console. window.localConnection = localConnection = new RTCPeerConnection(servers, pcConstraint); trace('Created local peer connection object localConnection'); sendChannel = localConnection.createDataChannel('sendDataChannel', dataConstraint); trace('Created send data channel'); localConnection.onicecandidate = iceCallback1; sendChannel.onopen = onSendChannelStateChange; sendChannel.onclose = onSendChannelStateChange; // Add remoteConnection to global scope to make it visible // from the browser console. window.remoteConnection = remoteConnection = new RTCPeerConnection(servers, pcConstraint); trace('Created remote peer connection object remoteConnection'); remoteConnection.onicecandidate = iceCallback2; remoteConnection.ondatachannel = receiveChannelCallback; localConnection.createOffer().then( gotDescription1, onCreateSessionDescriptionError ); startButton.disabled = true; closeButton.disabled = false; } function sendData() { var data = dataChannelSend.value; sendChannel.send(data); trace('Sent Data: ' + data); } Синтаксис в RTCDataChannel намеренно похож на WebSocket, с методом send() событием message.
Обратите внимание на использование dataConstraint. Каналы передачи данных могут быть настроены для обеспечения различных типов обмена данными — например, отправляемые данные могут быть в приоритете над над производительностью. Более подробную информацию о возможностях можно найти на https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createDataChannel .
Три типа ограничений Это сбивает с толку!
Различные типы параметров настройки вызовов WebRTC часто называются «ограничениями».
Узнайте больше об ограничениях и возможностях:
RTCPeerConnection https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/RTCPeerConnection RTCDataChannel https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createDataChannel getUserMedia() https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia Бонусные задания
с SCTP-протоколом, используемым каналами передачи данных WebRTC, надежная и упорядоченная доставка данных включена по умолчанию. Когда может понадобиться RTCDataChannel для обеспечения надежной доставки даных, а когда производительность может быть важнее – даже если это означает потерю каких-то данных? Используйте CSS для улучшения макета страницы и добавьте атрибут placeholder в текстовую область dataChannelReceive. Протестируйте страницу на мобильном устройстве. Что вы узнали? На этом шаге вы узнали, как
устанавливать соединение между двумя узлами WebRTC обмениваться текстовыми данными между узлами Полная версия этого шага находится в папке step-03.
Узнайте больше
Каналы передачи данных WebRTC (написано пару лет назад, но все еще стоит прочитать) - http://www.html5rocks.com/en/tutorials/webrtc/datachannels/ Почему SCTP был выбран для канала передачи данных WebRTC? - https://bloggeek.me/sctp-data-channel/ Следующий шаг Вы узнали, как обмениваться данными между узлами на одной и той же странице, но как вы собираетесь это делать между разными устройствами? Сначала вам необходимо настроить сигналинг-канал для обмена сообщениями метаданных. Как – узнайте на следующем шаге!
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-rtcdatachannel-exchange-data/"},"https://romankurnovskii.com/ru/docs/python101/chapter14_config_parser/":{title:"14. Модуль configparser",tags:[],content:`Файлы конфигурации используются как пользователями, так и программистами. Обычно они используются для хранения настроек вашего приложения или даже настроек вашей операционной системы. Основная библиотека Python включает модуль configparser, который можно использовать для создания конфигурационных файлов и взаимодействия с ними. В этой главе мы потратим несколько минут на изучение его работы.
Создание конфигурационного файла Создать конфигурационный файл с помощью configparser очень просто. Давайте создадим код для демонстрации:
import configparser def createConfig(path): \u0026quot;\u0026quot;\u0026quot; Create a config file \u0026quot;\u0026quot;\u0026quot; config = configparser.ConfigParser() config.add_section(\u0026quot;Settings\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font\u0026quot;, \u0026quot;Courier\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_size\u0026quot;, \u0026quot;10\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_style\u0026quot;, \u0026quot;Normal\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_info\u0026quot;, \u0026quot;You are using %(font)s at %(font_size)s pt\u0026quot;) with open(path, \u0026quot;w\u0026quot;) as config_file: config.write(config_file) if __name__ == \u0026quot;__main__\u0026quot;: path = \u0026quot;settings.ini\u0026quot; createConfig(path) Данный код создает файл config с одной секцией, под названием Settings, которая будет содержать наши опции: font, font_size, font_style и font_info. . Также обратите внимание, что в Python 3 нам нужно указать, что мы записываем файл только в режиме записи, т.е. \u0026ldquo;w\u0026rdquo;. Еще в Python 2 для записи в двоичном режиме нужно было использовать \u0026ldquo;wb\u0026rdquo;.
Как читать, обновлять и удалять опции Теперь мы готовы к тому, что бы научиться чтению файла config, обновлять его опции и даже удалять их. В этом случае проще научиться, написав код! Просто добавьте следующую функцию в код, который вы написали выше.
import configparser import os def crudConfig(path): \u0026quot;\u0026quot;\u0026quot; Create, read, update, delete config \u0026quot;\u0026quot;\u0026quot; if not os.path.exists(path): createConfig(path) config = configparser.ConfigParser() config.read(path) # read some values from the config font = config.get(\u0026quot;Settings\u0026quot;, \u0026quot;font\u0026quot;) font_size = config.get(\u0026quot;Settings\u0026quot;, \u0026quot;font_size\u0026quot;) # change a value in the config config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_size\u0026quot;, \u0026quot;12\u0026quot;) # delete a value from the config config.remove_option(\u0026quot;Settings\u0026quot;, \u0026quot;font_style\u0026quot;) # write changes back to the config file with open(path, \u0026quot;w\u0026quot;) as config_file: config.write(config_file) if __name__ == \u0026quot;__main__\u0026quot;: path = \u0026quot;settings.ini\u0026quot; crudConfig(path) Этот код сначала проверяет, существует ли путь к файлу конфигурации. Если нет, то для его создания используется функция createConfig, которую мы создали ранее. Затем мы создаем объект ConfigParser и передаем ему путь к файлу конфигурации для чтения. Чтобы прочитать опцию в конфигурационном файле, мы вызываем метод get нашего объекта ConfigParser, передавая ему имя секции и имя опции. Это вернет значение опции. Если вы хотите изменить значение параметра, используйте метод set, в котором передайте имя секции, имя опции и новое значение. Наконец, для удаления параметра можно использовать метод remove_option.
В нашем примере мы изменяем значение font_size на 12 и полностью удаляем опцию font_style. Затем мы записываем изменения обратно на диск.
Однако это не совсем удачный пример, так как вам не стоит иметь функцию, которая делает всё таким образом. Поэтому давайте разделим ее на несколько функций:
import configparser import os def create_config(path): \u0026quot;\u0026quot;\u0026quot; Create a config file \u0026quot;\u0026quot;\u0026quot; config = configparser.ConfigParser() config.add_section(\u0026quot;Settings\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font\u0026quot;, \u0026quot;Courier\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_size\u0026quot;, \u0026quot;10\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_style\u0026quot;, \u0026quot;Normal\u0026quot;) config.set(\u0026quot;Settings\u0026quot;, \u0026quot;font_info\u0026quot;, \u0026quot;You are using %(font)s at %(font_size)s pt\u0026quot;) with open(path, \u0026quot;w\u0026quot;) as config_file: config.write(config_file) def get_config(path): \u0026quot;\u0026quot;\u0026quot; Returns the config object \u0026quot;\u0026quot;\u0026quot; if not os.path.exists(path): create_config(path) config = configparser.ConfigParser() config.read(path) return config def get_setting(path, section, setting): \u0026quot;\u0026quot;\u0026quot; Print out a setting \u0026quot;\u0026quot;\u0026quot; config = get_config(path) value = config.get(section, setting) msg = \u0026quot;{section} {setting} is {value}\u0026quot;.format( section=section, setting=setting, value=value) print(msg) return value def update_setting(path, section, setting, value): \u0026quot;\u0026quot;\u0026quot; Update a setting \u0026quot;\u0026quot;\u0026quot; config = get_config(path) config.set(section, setting, value) with open(path, \u0026quot;w\u0026quot;) as config_file: config.write(config_file) def delete_setting(path, section, setting): \u0026quot;\u0026quot;\u0026quot; Delete a setting \u0026quot;\u0026quot;\u0026quot; config = get_config(path) config.remove_option(section, setting) with open(path, \u0026quot;w\u0026quot;) as config_file: config.write(config_file) if __name__ == \u0026quot;__main__\u0026quot;: path = \u0026quot;settings.ini\u0026quot; font = get_setting(path, 'Settings', 'font') font_size = get_setting(path, 'Settings', 'font_size') update_setting(path, \u0026quot;Settings\u0026quot;, \u0026quot;font_size\u0026quot;, \u0026quot;12\u0026quot;) delete_setting(path, \u0026quot;Settings\u0026quot;, \u0026quot;font_style\u0026quot;) Этот пример выглядит более организованно, по сравнению с первым. Я пошел настолько далеко, что назвал функции в соответствии с PEP8. Каждая функция должна быть самоочевидной и самодостаточной. Вместо того чтобы помещать всю логику в одну функцию, мы разделили ее на несколько функций, а затем продемонстрировали их функциональность в нижнем выражении if. Теперь вы можете импортировать модуль и использовать его самостоятельно.
Обратите внимание на то, что в этом примере есть сложная секция, так что вам, возможно, захочется усовершенствовать этот пример в дальнейшем, чтобы сделать его более универсальным.
Как использовать интерполяцию Модуль configparser также позволяет интерполяцию, что означает, что вы можете использовать некоторые опции для создания другой опции. Мы фактически делаем это с опцией font_info, поскольку ее значение основано на опциях font и font_size. Мы можем изменить интерполированное значение с помощью словаря Python. Давайте вкратце продемонстрируем оба этих случая.
import configparser import os def interpolationDemo(path): if not os.path.exists(path): createConfig(path) config = configparser.ConfigParser() config.read(path) print(config.get(\u0026quot;Settings\u0026quot;, \u0026quot;font_info\u0026quot;)) print(config.get(\u0026quot;Settings\u0026quot;, \u0026quot;font_info\u0026quot;, vars={\u0026quot;font\u0026quot;: \u0026quot;Arial\u0026quot;, \u0026quot;font_size\u0026quot;: \u0026quot;100\u0026quot;})) if __name__ == \u0026quot;__main__\u0026quot;: path = \u0026quot;settings.ini\u0026quot; interpolationDemo(path) Если вы запустите этот код, вы увидите примерно следующий результат:
Вы используете Courier размером 12 pt Вы используете шрифт Arial размером 100 пт Подведение итогов На данном этапе вы должны знать достаточно о возможностях configparser, чтобы использовать его в своих собственных проектах. Есть еще один проект под названием ConfigObj, который не является частью Python, и с которым вы, возможно, захотите ознакомиться. ConfigObj более гибкий и имеет больше возможностей, чем configparser. Но если вы находитесь в затруднительном положении или ваша организация не разрешает использование пакетов сторонних разработчиков, то configparser, вероятно, подойдет.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter14_config_parser/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day14/":{title:"14. DevOps и Linux",tags:["devops","linux","virtualbox","vagrant"],content:`Общая картина: DevOps и Linux Linux и DevOps имеют очень схожие культуры и взгляды; оба ориентированы на настройку и масштабируемость. Оба эти аспекта Linux имеют особое значение для DevOps.
Многие технологии начинаются с Linux, особенно если они связаны с разработкой программного обеспечения или управлением инфраструктурой.
Кроме того, многие проекты с открытым исходным кодом, особенно инструменты DevOps, с самого начала разрабатывались для работы в Linux.
С точки зрения DevOps или фактически с точки зрения какой-либо операционной роли вы столкнетесь с Linux, я бы сказал, в основном. Есть место для WinOps, но большую часть времени вы будете администрировать и развертывать серверы Linux.
Я использую Linux ежедневно в течение нескольких лет, но мой настольный компьютер всегда был либо macOS, либо Windows. Однако, когда я перешел на роль Cloud Native, в которой я сейчас нахожусь, я сделал решительный шаг, чтобы убедиться, что мой ноутбук полностью основан на Linux и является моим ежедневным драйвером, в то время как мне по-прежнему нужна была Windows для рабочих приложений и многих моих аудио и видеоаппаратура не работает в Linux Я заставлял себя постоянно работать на рабочем столе Linux, чтобы лучше понять многие вещи, которые мы собираемся затронуть в течение следующих 7 дней.
Начало Я не предлагаю вам делать то же самое, что и я, в любом случае, поскольку есть более простые варианты и менее разрушительные, но я скажу, что этот полный рабочий день заставит вас быстрее научиться тому, как заставить все работать в Linux.
В течение большей части этих 7 дней я фактически собираюсь развернуть виртуальную машину в Virtual Box на моей машине с Windows. Я также собираюсь развернуть настольную версию дистрибутива Linux, в то время как многие серверы Linux, которыми вы будете администрировать, скорее всего, будут серверами без графического интерфейса и полностью основанными на оболочке. Однако, как я сказал в начале, многие инструменты, которые мы рассмотрели в течение всех этих 90 дней, начинались с Linux, я также настоятельно рекомендую вам погрузиться в работу этого рабочего стола Linux для этого обучения.
В оставшейся части этого поста мы сосредоточимся на настройке и запуске виртуальной машины Ubuntu Desktop в нашей среде Virtual Box. Теперь мы можем просто загрузить Virtual Box и получить последний Ubuntu ISO с сайтов, на которые даны ссылки, и продолжить сборку. нашу среду рабочего стола, но это не было бы очень DevOps с нашей стороны, не так ли?
Еще одна веская причина использовать большинство дистрибутивов Linux заключается в том, что они бесплатны и имеют открытый исходный код. Мы также выбираем Ubuntu, поскольку это, вероятно, наиболее широко используемый дистрибутив, не думая о мобильных устройствах и корпоративных серверах RedHat Enterprise. Я могу ошибаться, но с CentOS и ее историей я уверен, что Ubuntu занимает первое место в списке, и это очень просто.
HashiCorp Vagrant Vagrant — это утилита CLI, которая управляет жизненным циклом ваших виртуальных машин. Мы можем использовать vagrant для запуска и отключения виртуальных машин на разных платформах, включая vSphere, Hyper-v, Virtual Box, а также Docker. У него есть другие провайдеры, но мы будем придерживаться того, что здесь мы используем Virtual Box, так что все готово.
Vagrant — свободное и открытое программное обеспечение для создания и конфигурирования виртуальной среды разработки. Является обёрткой для программного обеспечения виртуализации, например VirtualBox, и средств управления конфигурациями, таких как Chef, Salt и Puppet.
Первое, что нам нужно сделать, это установить Vagrant на нашу машину, когда вы перейдете на страницу загрузок, вы увидите все операционные системы, перечисленные на ваш выбор. HashiCorp Vagrant Я использую Windows, поэтому я взял двоичный файл для своей системы и установил его в свою систему.
Далее нам также нужно установить Virtual Box. Опять же, это также может быть установлено на многих разных операционных системах.
Файл VAGRANTFILE VAGRANTFILE описывает тип машины, которую мы хотим развернуть. Он также определяет, как мы хотим, чтобы конфигурация и подготовка этой машины выглядели.
Когда дело доходит до их сохранения и организации ваших VAGRANTFILE, я стараюсь помещать их в отдельные папки в своем рабочем пространстве. Ниже вы можете увидеть, как это выглядит в моей системе. Надеюсь, после этого вы поиграете с Vagrant и увидите легкость запуска разных систем, это также отлично подходит для этой кроличьей норы, известной как скачок дистрибутива для Linux Desktops.
Давайте взглянем на этот VAGRANTFILE и посмотрим, что мы строим.
Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;chenhan/ubuntu-desktop-20.04\u0026quot; config.vm.provider :virtualbox do |v| v.memory = 8096 v.cpus = 4 v.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--vram\u0026quot;, \u0026quot;128mb\u0026quot;] end end Это очень простой VAGRANTFILE. В целом, мы говорим, что нам нужна конкретная «сборка». Сборка, возможно, является либо общедоступным образом, либо частной сборкой системы, которую вы ищете. Вы можете найти длинный список здесь, в общедоступном каталоге Vagrant
Далее мы говорим, что хотим использовать определенного провайдера, в данном случае это «VirtualBox», а затем мы хотим определить память нашей машины как «8 ГБ, а количество процессоров — как «4». Мой опыт также говорит мне, что вы можете также добавить следующую строку, если у вас возникли проблемы с отображением. Это установит видеопамять на то, что вы хотите, я бы увеличил ее до 128 МБ, но зависит от вашей системы.
v.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--vram\u0026quot;, \u0026quot;\u0026quot;] Инициализация нашего рабочего стола Linux Теперь мы готовы запустить нашу первую машину в терминале нашего ПК. В моем случае я использую PowerShell на своем компьютере с Windows, перейдите в папку своих проектов и там, где вы найдете свой VAGRANTFILE. Оказавшись там, вы можете ввести команду vagrant up, и если все правильно, вы увидите что-то вроде того, что показано ниже.
Еще одна вещь, которую следует добавить, это то, что сеть будет настроена на NAT на вашей виртуальной машине, на данном этапе нам действительно не нужно знать о NAT, и я планирую провести целую сессию в следующем разделе о сети. Но знайте, что это просто кнопка, когда дело доходит до включения машины в вашу домашнюю сеть, это также сетевой режим по умолчанию в Virtual Box. Вы можете узнать больше в документации Virtual Box
Как только vagrant up завершен, мы можем использовать vagrant ssh, чтобы перейти прямо в терминал нашей новой виртуальной машины.
Именно здесь мы будем проводить большую часть наших исследований в течение следующих нескольких дней, но я также хочу погрузиться в некоторые настройки для вашей рабочей станции разработчика, которые я сделал, и это значительно упрощает вашу жизнь при использовании этого в качестве ежедневного драйвера, и, конечно же, а ты реально в DevOps разве что у тебя крутой нестандартный терминал?
Но просто для подтверждения в Virtual Box вы должны увидеть приглашение для входа в систему при выборе виртуальной машины.
О, и если вы зашли так далеко и спрашивали: «ЧТО ТАКОЕ ИМЯ ПОЛЬЗОВАТЕЛЯ И ПАРОЛЬ?»
Username = vagrant Password = vagrant Завтра мы рассмотрим некоторые команды и то, что они делают. Терминал станет местом, где все произойдет.
Ресурсы Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need be a hacker!) Vargant tutorial Как я уже упоминал, далее мы рассмотрим команды, которые мы можем использовать ежедневно в наших средах Linux.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day14/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-setup-signaling-service/":{title:"Настройка службы сигналинга для обмена сообщениями",tags:[],content:`Чему вы научитесь
На этом шаге вы узнаете, как:
Использовать npm для установки взаимосвязей, как указано в package.json Запускать сервер Node.js и использовать node-static для обслуживания статических файлов. Настраивать службу обмена сообщениями на Node.js через Socket.IO . Использовать это для создания ‘комнат\u0026quot; и обмена сообщениями. Полная версия этого шага находится в папке step-04.
Концепции
Чтобы установить и поддерживать вызов WebRTC, клиенты WebRTC (узлы) должны обмениваться метаданными:
Информация о кандидате (сети). сообщения offer и answer, содержащие информацию о медиа, например, о разрешении и кодеках. Другими словами, обмен метаданными требуется до P2P потоковой передачи аудио, видео или данных. Этот процесс называется сигналингом. На предыдущих этапах объекты RTCPeerConnection отправителя и получателя находились на одной странице, поэтому \u0026ldquo;сигналинг\u0026rdquo; - это просто вопрос передачи метаданных между объектами.
В реальном приложении отправитель и получатель RTCPeerConnections запущены на веб-страницах на разных устройствах, и вам нужен способ для обмена метаданными.
Для этого используется signaling-server: сервер, который может передавать сообщения между клиентами WebRTC (узлами). Фактически сообщения представляют собой обычный текст: строковые объекты JavaScript.
Обязательное условие: установить Node.js
Для выполнения следующих шагов этой codelab (папки step-04 до step-06) вам необходимо запустить сервер на локальном хосте с помощью Node.js . Вы можете скачать и установить Node.js по этой ссылке (https://nodejs.org/en/download/) или через предпочтительный для вас менеджер пакетов (https://nodejs.org/en/download/package-manager/). После установки вы сможете импортировать зависимости, необходимые для следующих шагов (запуск npm install), а также запустить небольшой локальный сервер для выполнения codelab (запуск node index.js). Эти команды будут указаны позже, когда они потребуются.
О приложении
WebRTC использует клиентский JavaScript API, но для использования в реальных приложениях также требуется сигналинг-сервер (обмена сообщениями), а также серверы STUN и TURN. Вы можете узнать больше здесь - https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/. На этом шаге вы создадите простой Node.js сигналинг-сервер, использующий Socket.IO Node js модуль и библиотеку JavaScript для обмена сообщениями. Опыт работы с Node.js и Socket.IO будет полезным, но не решающим; компоненты обмена сообщениями очень просты.
Выбор правильного сигналинг-сервера В этой кодовой лаборатории используется Socket.IO для сигналинг-сервера. Дизайн Socket.IO упрощает создание службы для обмена сообщениями. и Socket.IO подходит для изучения сигналинга WebRTC благодаря встроенной концепции ‘комнат\u0026quot;. Однако для производственного сервиса есть альтернативы получше. Смотрите, как выбрать сигналинг-протокол для вашего следующего проекта WebRTC - https://bloggeek.me/siganling-protocol-webrtc/
В этом примере сервер (Node.js приложение) реализовано в index.js, и клиент, который работает на нем (веб-приложение), реализован в index.html. Node.js приложение на этом этапе имеет две задачи. Во-первых, он действует как ретранслятор сообщений:
socket.on('message', function (message) { log('Got message: ', message); socket.broadcast.emit('message', message); }); Во-вторых, он управляет «комнатами» видеочата WebRTC:
if (numClients === 0) { socket.join(room); socket.emit('created', room, socket.id); } else if (numClients === 1) { socket.join(room); socket.emit('joined', room, socket.id); io.sockets.in(room).emit('ready'); } else { // max two clients socket.emit('full', room); } Наше простое приложение WebRTC позволит максимум двум узлам совместно использовать комнату.
HTML и JavaScript
Обновите index.html. Теперь страница должна выглядеть примерно так:
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;script src=\u0026quot;/socket.io/socket.io.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; На этом шаге вы ничего не увидите на странице: все протоколирование выполняется в консоли браузера. (Чтобы просмотреть консоль в Chrome, нажмите Ctrl-Shift-J или Command-Option-J, если работаете на Mac.) Заменить js/main.js следующим файлом:
'use strict'; var isInitiator; window.room = prompt(\u0026quot;Enter room name:\u0026quot;); var socket = io.connect(); if (room !== \u0026quot;\u0026quot;) { console.log('Message from client: Asking to join room ' + room); socket.emit('create or join', room); } socket.on('created', function(room, clientId) { isInitiator = true; }); socket.on('full', function(room) { console.log('Message from client: Room ' + room + ' is full :^('); }); socket.on('ipaddr', function(ipaddr) { console.log('Message from client: Server IP address is ' + ipaddr); }); socket.on('joined', function(room, clientId) { isInitiator = false; }); socket.on('log', function(array) { console.log.apply(console, array); }); Настройте Socket.IO для запуска Node.js В HTML-файле вы, возможно, видели, что используете Socket.IO файл:
\u0026lt;script src=\u0026quot;/socket.io/socket.io.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; На верхнем уровне вашей папки work создайте файл с именем package.json со следующим содержимым:
{ \u0026quot;name\u0026quot;: \u0026quot;webrtc-codelab\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;0.0.1\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;WebRTC codelab\u0026quot;, \u0026quot;dependencies\u0026quot;: { \u0026quot;node-static\u0026quot;: \u0026quot;^0.7.10\u0026quot;, \u0026quot;socket.io\u0026quot;: \u0026quot;^1.2.0\u0026quot; } } Это манифест приложения, который сообщает диспетчеру пакетов узлов (npm), какие зависимости проекта следует установить.
Чтобы установить зависимости (например, /socket.io/socket.io.js), выполните следующие действия из терминала командной строки в вашей папке work: npm install
Вы должны увидеть журнал установки, который заканчивается примерно так:
Как вы видите, npm установил зависимости, определенные в package.json.
Создайте новый файл index.js на верхнем уровне вашей папки work (не в папке js) и добавьте следующий код:
'use strict'; var os = require('os'); var nodeStatic = require('node-static'); var http = require('http'); var socketIO = require('socket.io'); var fileServer = new(nodeStatic.Server)(); var app = http.createServer(function(req, res) { fileServer.serve(req, res); }).listen(8080); var io = socketIO.listen(app); io.sockets.on('connection', function(socket) { // convenience function to log server messages on the client function log() { var array = ['Message from server:']; array.push.apply(array, arguments); socket.emit('log', array); } socket.on('message', function(message) { log('Client said: ', message); // for a real app, would be room-only (not broadcast) socket.broadcast.emit('message', message); }); socket.on('create or join', function(room) { log('Received request to create or join room ' + room); var clientsInRoom = io.sockets.adapter.rooms[room]; var numClients = clientsInRoom ? Object.keys(clientsInRoom.sockets).length : 0; log('Room ' + room + ' now has ' + numClients + ' client(s)'); if (numClients === 0) { socket.join(room); log('Client ID ' + socket.id + ' created room ' + room); socket.emit('created', room, socket.id); } else if (numClients === 1) { log('Client ID ' + socket.id + ' joined room ' + room); io.sockets.in(room).emit('join', room); socket.join(room); socket.emit('joined', room, socket.id); io.sockets.in(room).emit('ready'); } else { // max two clients socket.emit('full', room); } }); socket.on('ipaddr', function() { var ifaces = os.networkInterfaces(); for (var dev in ifaces) { ifaces[dev].forEach(function(details) { if (details.family === 'IPv4' \u0026amp;\u0026amp; details.address !== '127.0.0.1') { socket.emit('ipaddr', details.address); } }); } }); }); Из терминала командной строки выполните следующую команду в папке work: node index.js
В браузере откройте localhost:8080.
Каждый раз, когда вы открываете этот URL-адрес, вам будет предложено ввести название комнаты. Чтобы присоединиться к одной и той же комнате, каждый раз выбирайте одно и то же имя комнаты, например, «foo».
Откройте новую вкладку и снова откройте localhost: 8080. Выберите то же самое название комнаты.
Откройте localhost:8080 в третьей вкладке или окне. Выберите то же название комнаты еще раз.
Проверьте консоль на каждой из вкладок: вы должны увидеть логи из JavaScript выше.
Бонусные задания
Какие альтернативные механизмы обмена сообщениями могут быть возможны? С какими проблемами вы можете столкнуться при использовании «чистого» WebSocket? Какие проблемы могут быть связаны с масштабированием этого приложения? Можете ли вы разработать метод для тестирования тысяч или миллионов одновременных запросов на номер? Это приложение использует запрос JavaScript для получения названия комнаты. Разработайте способ получения названия комнаты из URL. Например, localhost:8080/foo будет указывать имя комнаты foo. Что вы узнали
На этом шаге вы узнали, как:
Использовать npm для установки зависимостей проекта, как указано в package.json Запускать Node.js сервер для обмена системных файлов. Настраивать службу обмена сообщениями на Node.js через socket.io . Использовать это для создания ‘комнат\u0026quot; и обмена сообщениями. Полная версия этого шага находится в папке step-04. Узнайте больше
Пример socket.io chat - https://github.com/rauchg/chat-example WebRTC в реальном мире: STUN, TURN и сигналинг - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/ Термин \u0026ldquo;signaling\u0026rdquo; в WebRTC - https://www.webrtc-experiment.com/docs/WebRTC-Signaling-Concepts.html Следующий шаг Узнайте, как исполь зовать сигналинг, чтобы позволить двум пользователям установить одноранговое соединение.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-setup-signaling-service/"},"https://romankurnovskii.com/ru/docs/python101/chapter15_logging/":{title:"15. Логирование",tags:[],content:`Глава 15 - Логирование Python предоставляет очень мощную библиотеку протоколирования в своей стандартной библиотеке. Многие программисты используют операторы печати для отладки (в том числе и я), но вы также можете использовать для этого протоколирование. Использование лога также более чистый метод, если вы не хотите просматривать весь свой код, чтобы удалить все операторы print. В данном разделе мы рассмотрим следующее:
Создание простого логгера Как вести журнал из нескольких модулей Форматирование лога Конфигурация лога К концу этой главы вы должны быть в состоянии уверенно создавать собственные логи для своих приложений. Давайте приступим!
Создание простого логгера Создать лог с помощью модуля logging легко и просто. Проще всего посмотреть на кусок кода, а затем объяснить его, так что вот вам код для чтения:
import logging # add filemode=\u0026quot;w\u0026quot; to overwrite logging.basicConfig(filename=\u0026quot;sample.log\u0026quot;, level=logging.INFO) logging.debug(\u0026quot;This is a debug message\u0026quot;) logging.info(\u0026quot;Informational message\u0026quot;) logging.error(\u0026quot;An error has happened!\u0026quot;) Как и следовало ожидать, чтобы получить доступ к модулю logging, его необходимо сначала импортировать. Самый простой способ создать лог - использовать функцию basicConfig модуля logging и передать ему несколько аргументов с ключевыми словами. Она принимает следующие аргументы: filename, filemode, format, datefmt, level и stream. В нашем примере мы передаем ей имя файла и уровень протоколирования, который мы установили на INFO. Существует пять уровней протоколирования (в порядке возрастания): DEBUG, INFO, WARNING, ERROR и CRITICAL. По умолчанию, если вы запустите этот код несколько раз, он будет добавляться в лог, если он уже существует. Если вы предпочитаете, чтобы ваш логгер перезаписывал лог, то передайте filemode=\u0026ldquo;w\u0026rdquo;, как указано в комментарии в коде. Говоря о выполнении кода, вот что вы получите, если выполните его один раз:
INFO:root:Informational message ERROR:root:An error has happened! Обратите внимание, что отладочное сообщение отсутствует в выводе. Это потому, что мы установили уровень INFO, поэтому наш логгер будет вести лог только в том случае, если это сообщение INFO, WARNING, ERROR или CRITICAL. Часть root означает, что это сообщение поступает от корневого или главного логгера. В следующем разделе мы рассмотрим, как изменить это значение, чтобы оно было более описательным. Если вы не используете basicConfig, то модуль протоколирования будет выводить сообщения в консоль / stdout.
Модуль протоколирования также может записывать некоторые исключения в файл или в любое другое место, куда вы настроите его. Вот пример:
import logging logging.basicConfig(filename=\u0026quot;sample.log\u0026quot;, level=logging.INFO) log = logging.getLogger(\u0026quot;ex\u0026quot;) try: raise RuntimeError except RuntimeError: log.exception(\u0026quot;Error!\u0026quot;) Давайте немного разложим это по полочкам. Здесь мы используем метод getLogger модуля logging, чтобы вернуть объект logger с именем ex. Это удобно, когда в одном приложении используется несколько логгеров, так как позволяет определить, какие сообщения поступили от каждого из них. Этот пример заставит возникнуть RuntimeError, выявит ошибку и запишет весь трассировочный откат в файл, что может быть очень удобно при отладке.
Логирование из нескольких модулей (а также форматирование!) Чем больше вы кодите, тем чаще вы создаете набор пользовательских модулей, которые работают вместе. Если вы хотите, чтобы запись велась в одном месте, значит, вы обратились по адресу. Мы рассмотрим простой способ, а затем покажем более сложный метод, который также является и более настраиваемым. Вот один простой способ:
import logging import otherMod def main(): \u0026quot;\u0026quot;\u0026quot; The main entry point of the application \u0026quot;\u0026quot;\u0026quot; logging.basicConfig(filename=\u0026quot;mySnake.log\u0026quot;, level=logging.INFO) logging.info(\u0026quot;Program started\u0026quot;) result = otherMod.add(7, 8) logging.info(\u0026quot;Done!\u0026quot;) if __name__ == \u0026quot;__main__\u0026quot;: main() Здесь мы импортируем logging и модуль нашего собственного создания (\u0026ldquo;otherMod\u0026rdquo;). Затем мы создаем наш файл журнала, как делали раньше. Другой модуль выглядит следующим образом:
# otherMod.py import logging def add(x, y): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; logging.info(\u0026quot;added %s and %s to get %s\u0026quot; % (x, y, x+y)) return x+y Если вы запустите основной код, вы должны получить лог со следующим содержанием:
INFO:root:Program started INFO:root:added 7 and 8 to get 15 INFO:root:Done! Заметили проблему в этом? Вы не можете однозначно сказать, откуда приходят сообщения. При этом, чем больше модулей пишут в лог, тем сложнее становится картина. Так что нам нужно это исправить. Таким образом, мы приходим к более сложному способу создание логгера. Давайте взглянем на другой способ создания:
import logging import otherMod2 def main(): \u0026quot;\u0026quot;\u0026quot; The main entry point of the application \u0026quot;\u0026quot;\u0026quot; logger = logging.getLogger(\u0026quot;exampleApp\u0026quot;) logger.setLevel(logging.INFO) # create the logging file handler fh = logging.FileHandler(\u0026quot;new_snake.log\u0026quot;) formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') fh.setFormatter(formatter) # add handler to logger object logger.addHandler(fh) logger.info(\u0026quot;Program started\u0026quot;) result = otherMod2.add(7, 8) logger.info(\u0026quot;Done!\u0026quot;) if __name__ == \u0026quot;__main__\u0026quot;: main() Здесь мы создаем экземпляр логгера с именем \u0026ldquo;exampleApp\u0026rdquo;. Мы устанавливаем его уровень протоколирования, создали объект обработчика лог-файла, и объект форматирование. Обработчик файла должен установить объект форматирование в качестве своего форматтера, после чего обработчик файла должен быть добавлен в регистратор логгера.. Остальной код в main в основном такой же. Только обратите внимание, что вместо \u0026ldquo;logging.info\u0026rdquo; будет \u0026ldquo;logger.info\u0026rdquo; или как бы вы ни назвали свою переменную logger. Вот обновленный код OtherMod2:
# otherMod2.py import logging module_logger = logging.getLogger(\u0026quot;exampleApp.otherMod2\u0026quot;) def add(x, y): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; logger = logging.getLogger(\u0026quot;exampleApp.otherMod2.add\u0026quot;) logger.info(\u0026quot;added %s and %s to get %s\u0026quot; % (x, y, x+y)) return x+y Обратите внимание, что здесь у нас определены два логгера. Мы ничего не делаем с module_logger в данном случае, но используем другой. Если вы запустите основной скрипт, вы должны увидеть следующий вывод в вашем файле:
2022-12-22 154:21:30,592 - exampleApp - INFO - Program started 2022-12-22 154:21:30,592 - exampleApp.otherMod2.add - INFO - added 7 and 8 to get 15 2022-12-22 154:21:30,592 - exampleApp - INFO - Done Вы заметите, что все ссылки на root были удалены. Вместо этого он использует наш объект Formatter, который говорит, что мы должны получить человекочитаемое время, имя логгера, уровень логгирования и затем сообщение. На самом деле эти параметры известны как атрибуты LogRecord. Полный список атрибутов LogRecord см. в документации, так как их слишком много, чтобы перечислять их здесь.
Конфигурация логов для работы Модуль logging может быть настроен тремя различными способами. Вы можете настроить его с помощью методов (логгеров, форматоров, обработчиков), как мы делали ранее в этой статье; вы можете использовать файл конфигурации и передать его в fileConfig(); или вы можете создать словарь информации о конфигурации и передать его в функцию dictConfig(). Давайте сначала создадим файл конфигурации, а затем рассмотрим, как выполнить его с помощью Python. Вот пример файла конфигурации:
[loggers] keys=root,exampleApp [handlers] keys=fileHandler, consoleHandler [formatters] keys=myFormatter [logger_root] level=CRITICAL handlers=consoleHandler [logger_exampleApp] level=INFO handlers=fileHandler qualname=exampleApp [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=myFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler formatter=myFormatter args=(\u0026quot;config.log\u0026quot;,) [formatter_myFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s datefmt= Вы заметите, что у нас указаны два регистратора: root и exampleApp. По какой-то причине \u0026ldquo;root\u0026rdquo; является обязательным. Если вы не включите его, Python выдаст ошибку ValueError из функции config.py\u0026rsquo;s _install_loggers, которая является частью модуля logging. Если вы установите обработчик root в fileHandler, то в итоге вы удвоите вывод журнала, поэтому, чтобы этого не произошло, вместо этого мы отправляем его на консоль. Внимательно изучите этот пример. Вам понадобится секция для каждого ключа в первых трех секциях. Теперь давайте посмотрим, как мы загружаем их в коде:
# log_with_config.py import logging import logging.config import otherMod2 def main(): \u0026quot;\u0026quot;\u0026quot; Based on http://docs.python.org/howto/logging.html#configuring-logging \u0026quot;\u0026quot;\u0026quot; logging.config.fileConfig('logging.conf') logger = logging.getLogger(\u0026quot;exampleApp\u0026quot;) logger.info(\u0026quot;Program started\u0026quot;) result = otherMod2.add(7, 8) logger.info(\u0026quot;Done!\u0026quot;) if __name__ == \u0026quot;__main__\u0026quot;: main() Как вы видите, все, что вам нужно сделать, это передать путь к файлу конфигурации в logging.config.fileConfig. Вы также заметите, что нам больше не нужен весь этот код настройки, поскольку все это находится в файле конфигурации. Также мы можем просто импортировать модуль otherMod2 без изменений. В любом случае, если вы выполните вышеописанное, в конечном итоге в вашем файле журнала должно появиться следующее:
2022-12-22 18:23:33,338 - exampleApp - INFO - Program started 2022-12-22 18:23:33,338 - exampleApp.otherMod2.add - INFO - added 7 and 8 to get 15 2022-12-22 18:23:33,338 - exampleApp - INFO - Done! Как вы уже догадались, он очень похож на другой пример. Теперь мы перейдем к другому методу config. Метод конфигурирования по словарю (dictConfig) был добавлен только в Python 2.7. В документации не сказано прямо, как это работает. На самом деле, примеры в документации почему-то показывают YAML. В любом случае, вот рабочий код, который вы можете посмотреть:
# log_with_config.py import logging import logging.config import otherMod2 def main(): \u0026quot;\u0026quot;\u0026quot; Based on http://docs.python.org/howto/logging.html#configuring-logging \u0026quot;\u0026quot;\u0026quot; dictLogConfig = { \u0026quot;version\u0026quot;:1, \u0026quot;handlers\u0026quot;:{ \u0026quot;fileHandler\u0026quot;:{ \u0026quot;class\u0026quot;:\u0026quot;logging.FileHandler\u0026quot;, \u0026quot;formatter\u0026quot;:\u0026quot;myFormatter\u0026quot;, \u0026quot;filename\u0026quot;:\u0026quot;config2.log\u0026quot; } }, \u0026quot;loggers\u0026quot;:{ \u0026quot;exampleApp\u0026quot;:{ \u0026quot;handlers\u0026quot;:[\u0026quot;fileHandler\u0026quot;], \u0026quot;level\u0026quot;:\u0026quot;INFO\u0026quot;, } }, \u0026quot;formatters\u0026quot;:{ \u0026quot;myFormatter\u0026quot;:{ \u0026quot;format\u0026quot;:\u0026quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026quot; } } } logging.config.dictConfig(dictLogConfig) logger = logging.getLogger(\u0026quot;exampleApp\u0026quot;) logger.info(\u0026quot;Program started\u0026quot;) result = otherMod2.add(7, 8) logger.info(\u0026quot;Done!\u0026quot;) if __name__ == \u0026quot;__main__\u0026quot;: main() Если вы выполните этот код, вы получите тот же результат, что и в предыдущем методе. Обратите внимание, что вам больше не нужен логгер root, когда вы используете словарь конфигурации.
Подведение итогов На данном этапе вы должны знать, как начать использовать логгеры и как настроить их несколькими различными способами. Вы также должны получить знания о том, как изменять вывод с помощью объекта Formatter. Модуль протоколирования очень удобен для поиска и устранения неисправностей в вашем приложении. Обязательно уделите некоторое время практике работы с этим модулем перед написанием большого приложения.
В следующей главе мы рассмотрим, как использовать модуль os.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter15_logging/"},"https://romankurnovskii.com/ru/docs/python101/chapter20_sys/":{title:"20. Модуль sys",tags:[],content:`Модуль sys предоставляет специфические для системы параметры и функции. Мы сузим наше исследование до следующих:
sys.argv sys.executable sys.exit sys.modules sys.path sys.platform sys.stdin/stdout/stderr sys.argv Значение sys.argv - это список аргументов командной строки Python, которые были переданы сценарию Python. Первый аргумент, argv[0] - это имя самого сценария Python. В зависимости от платформы, на которой вы работаете, первый аргумент может содержать полный путь к скрипту или только имя файла. Для получения дополнительной информации следует изучить документацию.
Давайте попробуем выполнить несколько примеров, чтобы ознакомиться с этим небольшим инструментом:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.argv [''] Если вы выполните это в интерпретаторе, то получите список с пустой строкой. Давайте создадим файл с именем \u0026ldquo;sysargv.py\u0026rdquo; со следующим содержимым:
# sysargv.py import sys print(sys.argv) Теперь запустите код в IDLE. Вы должны увидеть, что он выведет список с одним элементом, содержащим путь к вашему скрипту. Давайте попробуем передать скрипту несколько аргументов. Откройте терминал / консоль и измените директорию (используйте команду \u0026ldquo;cd\u0026rdquo;) на ту, где находится ваш скрипт. Затем запустите что-то вроде этого:
Вы заметите, что он выводит на экран следующее:
['sysargv.py', '-v', 'somefile.py'] Первый аргумент - это имя написанного нами сценария. Следующие два аргумента в списке - это аргументы, которые мы передали нашему скрипту в командной строке.
sys.executable Значение sys.executable - это абсолютный путь к интерпретатору Python. Это полезно, когда вы используете чужую машину и вам нужно знать, где установлен Python. На некоторых системах эта команда будет неудачной и вернет пустую строку или None. Вот как ее использовать:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.executable 'C:\\\\Python27\\\\pythonw.exe' ##sys.exit
Функция sys.exit() позволяет разработчику выйти из Python. Функция exit принимает необязательный аргумент, обычно целое число, которое выдает статус завершения. Ноль считается \u0026ldquo;успешным завершением\u0026rdquo;. Обязательно проверьте, имеет ли ваша операционная система какие-либо специальные значения для статусов выхода, чтобы вы могли следовать им в своем приложении. Обратите внимание, что когда вы вызываете exit, он вызывает исключение SystemExit, что позволяет функциям очистки работать в в конечных пунктах блоков try / except.
Давайте рассмотрим, как вызвать exit:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.exit(0) Traceback (most recent call last): File \u0026quot;\u0026lt;pyshell#5\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; sys.exit(0) SystemExit: 0 Когда вы запустите этот код в IDLE, вы увидите, что возникла ошибка SystemExit. Давайте создадим несколько сценариев, чтобы проверить это. Сначала нужно создать главный сценарий, программу, которая будет вызывать другой сценарий Python. Назовем его \u0026ldquo;call_exit.py\u0026rdquo;. Поместите в него следующий код:
# call_exit.py import subprocess code = subprocess.call([\u0026quot;python.exe\u0026quot;, \u0026quot;exit.py\u0026quot;]) print(code) Теперь создайте еще один Python-скрипт под названием \u0026ldquo;exit.py\u0026rdquo; и сохраните его в той же папке. Поместите в него следующий код:
import sys sys.exit(0) Теперь давайте попробуем запустить этот код:
На скриншоте выше видно, что написанный нами сценарий exit вернул ноль, поэтому он был успешно запущен. Вы также узнали, как вызвать другой сценарий Python из самого Python!
sys.path Значение path модуля sys - это список строк, который определяет путь поиска модулей. По сути, это указывает Python, в каких местах искать модуль, когда он пытается его импортировать. Согласно документации Python, sys.path инициализируется из переменной окружения PYTHONPATH, плюс по умолчанию, зависящему от установки. Давайте попробуем:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; print(sys.path) ['', 'C:\\\\Python27\\\\Lib\\\\idlelib', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\setuptools-0.9.5-py2.7.egg', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\pip-1.3.1-py2.7.egg', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\sphinx-1.2b3-py2.7.egg', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\docutils-0.11-py2.7.egg', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\pygments-1.6-py2.7.egg', 'C:\\\\Windows\\\\system32\\\\python27.zip', ' C:\\\\Python27\\\\DLLs', 'C:\\\\Python27\\\\lib', 'C:\\\\Python27\\\\lib\\\\plat-win', 'C:\\\\Python27\\\\lib\\\\lib-tk', 'C:\\\\Python27', 'C:\\\\Python27\\\\lib\\\\site-packages', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\PIL', 'C:\\\\Python27\\\\lib\\\\site-packages\\\\wx-2.9.4-msw'] Данная функция может быть весьма полезной во время отладки причины, по которой модуль не импортируется. Вы также можете изменить путь. Так как данная функция является путем, мы можем добавлять или удалять путь из неё. Давайте взглянем на то, как добавлять путь:
\u0026gt;\u0026gt;\u0026gt; sys.path.append(\u0026quot;/path/to/my/module\u0026quot;) Я оставлю удаление пути в качестве упражнения для читателя.
sys.platform Значение sys.platform - это идентификатор платформы. Вы можете использовать его для добавления модулей для конкретной платформы в sys.path, импорта различных модулей в зависимости от платформы или запуска различных частей кода. Давайте посмотрим:
\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.platform 'win32' Это говорит нам о том, что Python запущен на машине Windows. Вот пример того, как мы можем использовать эту информацию:
\u0026gt;\u0026gt;\u0026gt; os = sys.platform \u0026gt;\u0026gt;\u0026gt; if os == \u0026quot;win32\u0026quot;: # use Window-related code here import _winreg elif os.startswith('linux'): # do something Linux specific import subprocess subprocess.Popen([\u0026quot;ls, -l\u0026quot;]) Приведенный выше код показывает, как мы можем проверить, используем ли мы определенную операционную систему. Если мы находимся в Windows, мы получим информацию из реестра Window с помощью модуля Python под названием _winreg. Если мы находимся в Linux, мы можем выполнить команду ls, чтобы получить информацию о каталоге, в котором мы находимся.
sys.stdin / stdout / stderr Параметры stdin, stdout и stderr соответствуют файловым объектам, которые соответствуют стандартным потокам ввода, вывода и ошибок интерпретатора, соответственно. stdin используется для всех входных данных интерпретатора, кроме скриптов, в то время как stdout используется для вывода операторов ** print** и expression. Основная причина, по которой я упоминаю об этом, заключается в том, что иногда вам потребуется перенаправить stdout или stderr или оба потока в файл, например, в журнал или на какой-либо дисплей в созданном вами пользовательском графическом интерфейсе. Вы также можете перенаправить stdin, но я редко видел, чтобы это делалось.
Подведение итогов В модуле sys есть много других значений и методов. Обязательно посмотрите его в документации Python, раздел 27.1. Вы многому научились в этой главе. Теперь вы знаете, как выйти из программы Python, как получить информацию о платформе, как работать с аргументами, передаваемыми из командной строки, и многое другое. В следующей главе мы узнаем о потоках Python!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter20_sys/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day15/":{title:"15. Команды Linux в DevOps",tags:["devops","linux"],content:`Команды Linux для DevOps Я упомянул вчера, что мы собираемся провести много времени в терминале с некоторыми командами, чтобы что-то сделать.
Я также упомянул, что с нашей виртуальной машиной, подготовленной с помощью vagrant, мы можем использовать vagrant ssh и получить доступ к нашей машине. Вам нужно будет находиться в том же каталоге, из которого мы его предоставили.
Для SSH нам не понадобятся имя пользователя и пароль, они понадобятся нам только в том случае, если решим войти в консоль Virtual Box.
Вот где мы хотим быть, как показано ниже:
Команды Очевидно, что я не могу охватить здесь все команды. Есть тонны документации, которые охватывают их, но также, если вы находитесь в своем терминале, и вам просто нужно понять параметры конкретной команды, у нас есть команда man, сокращенная от manual. Мы можем использовать это, чтобы просмотреть каждую из команд, которые мы коснемся в этом посте, чтобы узнать больше вариантов для каждой из них. Мы можем запустить man man, который поможет вам со страницами руководства. Чтобы выйти из справочных страниц, вы должны нажать q для выхода.
Примеры:
man ls man whoami ... sudo Если вы знакомы с Windows и щелкаете правой кнопкой мыши по запустить от имени администратора, мы можем думать о sudo как об этом. Когда вы запускаете команду с помощью этой команды, вы будете запускать ее как «root», она запросит у вас пароль перед запуском команды.
Для разовых работ, таких как установка приложений или служб, вам может понадобиться эта команда sudo, но что, если у вас есть несколько задач, и вы хотите какое-то время пожить как sudo? Здесь вы можете снова использовать sudo su так же, как sudo, после ввода вам будет предложено ввести пароль root. В тестовой виртуальной машине, такой как наша, это нормально, но мне было бы очень сложно работать как «root» в течение длительного времени, могут произойти плохие вещи. Чтобы выйти из этого возвышенного положения, вы просто набираете «exit».
Я ловлю себя на том, что все время использую clear. Команда clear делает именно то, о чем говорит: она очищает экран от всех предыдущих команд, помещая курсор наверх и предоставляя вам красивое чистое рабочее пространство. Windows, это «cls» в .mdprompt.
Давайте теперь посмотрим на некоторые команды, с помощью которых мы можем создавать вещи в нашей системе, а затем визуализировать их в нашем терминале. Прежде всего, у нас есть mkdir, это позволит нам создать папку в нашей системе. С помощью следующей команды мы можем создать папку в нашем домашнем каталоге с именем Day15 mkdir Day15
С помощью cd это позволяет нам изменить каталог, поэтому для перехода в наш вновь созданный каталог мы можем сделать это с помощью вкладки cd Day15, которая также может использоваться для автозаполнения доступного каталога. Если мы хотим вернуться к тому, с чего начали, мы можем использовать cd ..
rmdir позволяет нам удалить каталог, если мы запустим rmdir Day15, тогда папка будет удалена (обратите внимание, что это будет работать, только если у вас ничего нет в папке)
Я уверен, что все мы делали это, когда мы переходили в глубины нашей файловой системы в каталог и не знали, где мы находимся. pwd дает нам распечатку рабочего каталога, pwd, насколько это похоже на пароль, означает печать рабочего каталога.
Мы знаем, как создавать папки и каталоги, но как мы создаем файлы? Мы можем создавать файлы с помощью команды «touch», если бы мы запускали «touch Day15», это создало бы файл. Игнорируйте mkdir, мы еще увидим это позже.
ls Я могу поставить на это свой дом, вы будете использовать эту команду так много раз, что она выведет список всех файлов и папок в текущем каталоге. Давайте посмотрим, сможем ли мы увидеть тот файл, который мы только что создали.
Как мы можем найти файлы в нашей системе Linux? locate позволит нам искать в нашей файловой системе. Если мы используем locate Day15, он сообщит о местонахождении файла. Бонусом является то, что если вы знаете, что файл существует, но вы получаете пустой результат, запустите sudo updatedb, который проиндексирует все файлы в файловой системе, а затем снова запустите locate. Если у вас нет locate, вы можете установить его с помощью этой команды sudo apt install mlocate
Как насчет перемещения файлов из одного места в другое? mv позволит вам перемещать ваши файлы. Пример mv Day15 90DaysOfDevOps переместит ваш файл в папку 90DaysOfDevOps.
Мы переместили наш файл, но что, если мы хотим переименовать его сейчас во что-то другое? Мы можем сделать это снова с помощью команды mv. Мы можем просто использовать mv Day15 day15, чтобы перейти к верхнему регистру, или мы могли бы использовать mv day15 AnotherDay, чтобы полностью изменить его, теперь используйте ls для проверки файла.
Хватит, теперь давайте избавимся (удалим) от нашего файла и, возможно, даже от нашего каталога, если он у нас есть. rm просто rm AnotherDay удалит наш файл. Мы также будем использовать rm -R, который будет рекурсивно работать через папку или местоположение. Мы также можем использовать rm -R -f, чтобы принудительно удалить все эти файлы. Спойлер, если вы запустите rm -R -f /, добавьте к нему sudo, и вы можете попрощаться со своей системой \u0026hellip;.!
Мы рассмотрели перемещение файлов, но что, если я просто хочу скопировать файлы из одной папки в другую, просто скажу, что это очень похоже на команду mv, но мы используем cp, чтобы теперь мы могли сказать cp Day15 Desktop
Мы создали папки и файлы, но на самом деле мы не поместили никакого содержимого в нашу папку, мы можем добавить содержимое несколькими способами, но самый простой способ - это echo, мы также можем использовать echo, чтобы распечатать много вещей в нашей папке. терминал, я лично часто использую эхо для вывода системных переменных, чтобы узнать, установлены они или нет. мы можем использовать echo \u0026quot;Hello #90DaysOfDevOps\u0026quot; \u0026gt; Day15, и это добавит это в наш файл. Мы также можем добавить к нашему файлу, используя echo \u0026quot;Commands are fun!\u0026quot; \u0026gt;\u0026gt; День15
Еще одна из тех команд, которые вы будете часто использовать! кошка сокращение от конкатенации. Мы можем использовать cat Day15, чтобы увидеть содержимое внутри файла. Отлично подходит для быстрого чтения этих файлов конфигурации.
Если у вас есть длинный сложный файл конфигурации, и вы хотите или вам нужно найти что-то быстрое в этом файле, а не читать каждую строку, тогда grep вам в помощь, это позволит нам искать в вашем файле определенное слово, используя cat Day15 | grep \u0026quot;#90DaysOfDevOps\u0026quot;
Если вы похожи на меня и часто используете эту команду clear, то вы можете пропустить некоторые из ранее запущенных команд, мы можем использовать «историю», чтобы узнать все те команды, которые мы запускали ранее. history -c удалит историю.
Когда вы запускаете history и хотите выбрать конкретную команду, вы можете использовать !3, чтобы выбрать 3-ю команду в списке.
Вы также можете использовать history | grep \u0026quot;Команда\u0026quot; для поиска чего-то определенного.
На серверах для отслеживания времени выполнения команды может быть полезно добавлять дату и время к каждой команде в файле истории.
Следующая системная переменная управляет этим поведением:
HISTTIMEFORMAT=\u0026quot;%d-%m-%Y %T \u0026quot; Вы можете легко добавить ее в свой bash_profile:
echo 'export HISTTIMEFORMAT=\u0026quot;%d-%m-%Y %T \u0026quot;' \u0026gt;\u0026gt; ~/.bash_profile Можем увеличить размер файла для хранения истории:
echo 'export HISTSIZE=100000' \u0026gt;\u0026gt; ~/.bash_profile echo 'export HISTFILESIZE=10000000' \u0026gt;\u0026gt; ~/.bash_profile Нужно сменить пароль? passwd позволит нам изменить наш пароль. Обратите внимание, что когда вы добавляете свой пароль таким образом, когда он скрыт, он не будет отображаться в history, однако, если ваша команда имеет -p ПАРОЛЬ, тогда он будет виден в вашей history.
Мы также можем добавить новых пользователей в нашу систему, мы можем сделать это с помощью useradd, мы должны добавить пользователя с помощью нашей команды sudo, мы можем добавить нового пользователя с помощью sudo useradd NewUser
Для повторного создания группы требуется sudo, и мы можем использовать sudo groupadd DevOps, тогда, если мы хотим добавить нашего нового пользователя в эту группу, мы можем сделать это, запустив sudo usermod -a -G DevOps -a is add а -G это имя группы.
Как добавить пользователей в группу sudo? Это было бы очень редким случаем но для того, чтобы сделать это, выполним: usermod -a -G sudo NewUser
Права / Permissions read, write and execute - — это права доступа ко всем нашим файлам и папкам в нашей системе Linux.
Полный список:
0 = None --- 1 = Execute only --X 2 = Write only -W- 3 = Write \u0026amp; Exectute -WX 4 = Read Only R-- 5 = Read \u0026amp; Execute R-X 6 = Read \u0026amp; Write RW- 7 = Read, Write \u0026amp; Execute RWX Вы также увидите «777» или «775», и они представляют те же числа, что и в приведенном выше списке, но каждый из них представляет User - Group - Everyone*
Давайте посмотрим на наш файл. ls -al Day15 вы можете увидеть 3 группы, упомянутые выше, пользователь и группа могут читать и изменять (write), но все остальыне только читать (read).
Мы можем изменить это с помощью chmod, вы можете сделать это, если вы также создаете двоичные файлы в своих системах, и вам нужно дать возможность запускать эти двоичные файлы. chmod 750 Day15 теперь запустите ls -la Day15, если вы хотите запустить это для всей папки, вы можете использовать -R, чтобы сделать это рекурсивно.
Как насчет смены владельца файла? Мы можем использовать «chown» для этой операции, если мы хотим изменить владельца нашего «Day15» с пользователя «vagrant» на «NewUser», мы можем запустить «sudo chown NewUser Day15» снова, можно использовать «-R».
Команда, с которой вы столкнетесь, это awk, где она реально используется, когда у вас есть выходные данные, из которых вам нужны только определенные данные. например, запуская who, мы получаем строки с информацией, но, возможно, нам нужны только имена. Мы можем запустить кто | awk '{print $1}', чтобы получить только список этого первого столбца.
Если вы хотите читать потоки данных из стандартного ввода, то генерирует и выполняет командные строки; это означает, что он может принимать вывод команды и передавать его в качестве аргумента другой команды. xargs — полезный инструмент для этого случая использования. Если, например, мне нужен список всех учетных записей пользователей Linux в системе, которую я могу запустить. cut -d: -f1 \u0026lt; /etc/passwd и получите длинный список, который мы видим ниже.
Если я хочу заархивировать этот список, я могу сделать это, используя xargs в команде вроде этой cut -d: -f1 \u0026lt; /etc/passwd | sort | xargs
Я также не упомянул команду cut, которая позволяет нам удалять разделы из каждой строки файла. Его можно использовать для вырезания частей строки по положению байта, символу и полю. Команда cut -d \u0026quot; \u0026quot; -f 2 list.txt позволяет нам удалить первую букву, которая у нас есть, и просто отобразить наши числа. Есть так много комбинаций, которые можно использовать здесь с этой командой, я уверен, что потратил слишком много времени, пытаясь использовать эту команду, когда я мог бы быстрее извлечь данные вручную.
Также обратите внимание, если вы вводите команду, и вы больше не довольны ею, и вы хотите начать снова, просто нажмите Ctrl + c, и это отменит эту строку и начнет все заново.
Ресурсы Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need be a hacker!) Это уже довольно большой список, но я могу с уверенностью сказать, что я использую все эти команды в своей повседневной жизни, будь то администрирование серверов Linux или мой рабочий стол Linux, это очень легко, когда вы находитесь в Windows или macOS для навигации по пользовательскому интерфейсу, но в Linux Servers их нет, все делается через терминал.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day15/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-peer-signaling-combine/":{title:"Соединение однорангового соединения и сигналинга",tags:[],content:`Чему вы научитесь
На этом шаге вы узнаете, как:
Запускать службу сигнализации WebRTC с помощью Socket.IO на Node.js Использовать эту службу для обмена метаданными WebRTC между узлами. Полная версия этого шага находится в папке step-05. Поменяйте HTML и JavaScript
Замените содержимое index.html следующим:
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026quot;videos\u0026quot;\u0026gt; \u0026lt;video id=\u0026quot;localVideo\u0026quot; autoplay muted\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;video id=\u0026quot;remoteVideo\u0026quot; autoplay\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;/socket.io/socket.io.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://webrtc.github.io/adapter/adapter-latest.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Замените js/main.js содержимым из step-05/js/main.js.
Запустите Node.js сервер
Если вы не отслеживаете эту codelab из своей папки work, вам может потребоваться установить зависимости для папки step-05 или вашей текущей рабочей папки. Выполните следующую команду из своей рабочей папки: npm install
После установки, если ваш Node.js сервер не запущен, запустите его, вызвав следующую команду в папке work: node index.js
Убедитесь, что вы используете версию index.js из предыдущего шага, который реализует Socket.IO. Для получения дополнительной информации о Node и Socket.IO, посмотрите раздел \u0026ldquo;Set up a signaling service to exchange messages\u0026rdquo;. В вашем браузере откройте localhost:8080.
Снова откройте localhost: 8080 в новой вкладке или окне. Один видеоэлемент будет отображать локальный поток из getUserMedia(), а другой будет показывать \u0026ldquo;удаленное\u0026rdquo; видео, передаваемое через RTCPeerConnection.
Вам необходимо перезапускать Node.js сервер каждый раз, когда вы закрываете клиентскую вкладку или окно. Посмотрите логи в консоли браузера.
Бонусные задания
Это приложение поддерживает только видеочат один на один. Как вы можете изменить дизайн, чтобы несколько человек могли посещать одну и ту же комнату видеочата? В примере строго задано имя комнаты foo. Каков наилучший способ включить другие имена комнат? Как пользователям обмениваться названием комнаты? Попробуйте создать альтернативу для обмена именами комнат. Как вы могли бы изменить приложение Что вы узнали
На этом шаге вы узнали, как:
Запускать сигналинг-службу WebRTC с помощью Socket.IO через Node.js . Использовать эту службу для обмена метаданными WebRTC между узлами. Полная версия этого шага находится в папке step-05. Советы
Статистика WebRTC и данные отладки доступны в chrome:// webrtc-internals. test.webrtc.org может использоваться для проверки ваших локальных настроек и тестирования камеры и микрофона. Если у вас возникли странные проблемы с кэшированием, попробуйте следующее: Выполните принудительную перезагрузку обновление, удерживая нажатой клавишу ctrl и нажав кнопку Reload Перезапустите браузер Запустите npm cache clean из командной строки. Далее
Узнайте, как делать снимки, получать изображения и делиться ими между удаленными узлами.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-peer-signaling-combine/"},"https://romankurnovskii.com/ru/docs/python101/chapter16_os/":{title:"16. Модуль os",tags:[],content:"Модуль os имеет множество применений. Мы не будем рассматривать все его возможности. Вместо этого мы получим обзор его возможностей, а также рассмотрим один из его подмодулей, известный как os.path. В частности, мы рассмотрим следующее:\nos.name os.environ os.chdir() os.getcwd() os.getenv() os.putenv() os.mkdir() os.makedirs() os.remove() os.rename() os.rmdir() os.startfile() os.walk() os.path Кажется, что этого очень много, но существует примерно в десять раз больше других действий, которые может выполнять модуль os. В этой главе мы просто дадим вам небольшое представление о том, что доступно. Чтобы использовать любой из методов, упомянутых в этом разделе, вам нужно импортировать модуль os, как показано ниже:\nimport os Давайте начнем изучать, как использовать этот модуль!\nos.name Модуль os имеет как вызываемые функции, так и обычные значения. В случае с os.name это просто значение. Когда вы обращаетесь к os.name, вы получите информацию о том, на какой платформе вы работаете. Вы увидите одно из следующих значений: \u0026lsquo;posix\u0026rsquo;, \u0026rsquo;nt\u0026rsquo;, \u0026lsquo;os2\u0026rsquo;, \u0026lsquo;ce\u0026rsquo;, \u0026lsquo;java\u0026rsquo;, \u0026lsquo;riscos\u0026rsquo;. Давайте посмотрим, что мы получим при запуске на Windows 7:\n\u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; os.name 'nt' Это говорит нам о том, что наш экземпляр Python запущен на компьютере под управлением Windows. Откуда мы это знаем? Потому что Microsoft начала называть свою операционную систему NT много лет назад. Например, Windows 7 также известна как Windows NT 6.1.\nos.environ, os.getenv() и os.putenv() Значение os.environ известно как объект mapping, который возвращает словарь переменных окружения пользователя. Вы можете этого не знать, но каждый раз, когда вы используете свой компьютер, устанавливаются некоторые переменные окружения. Они могут дать вам ценную информацию, например, количество процессоров, тип ЦП, имя компьютера и т.д. Давайте посмотрим, что мы можем узнать о нашей машине:\n\u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; os.environ {'ALLUSERSPROFILE': 'C:\\\\ProgramData', 'APPDATA': 'C:\\\\Users\\\\mike\\\\AppData\\\\Roaming', 'CLASSPATH': '.;C:\\\\Program Files\\\\QuickTime\\\\QTSystem\\\\QTJava.zip', 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files', 'COMPUTERNAME': 'MIKE-PC', 'COMSPEC': 'C:\\\\Windows\\\\system32\\\\cmd.exe', 'FP_NO_HOST_CHECK': 'NO', 'HOMEDRIVE': 'C:', 'HOMEPATH': '\\\\Users\\\\mike', 'LOCALAPPDATA': 'C:\\\\Users\\\\mike\\\\AppData\\\\Local', 'LOGONSERVER': '\\\\\\\\MIKE-PC', 'NUMBER_OF_PROCESSORS': '2', 'OS': 'Windows_NT', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC', 'PROCESSOR_ARCHITECTURE': 'x86', 'PROCESSOR_IDENTIFIER': 'x86 Family 6 Model 15 Stepping 13, GenuineIntel', 'PROCESSOR_LEVEL': '6', 'PROGRAMDATA': 'C:\\\\ProgramData', 'PROGRAMFILES': 'C:\\\\Program Files', 'PSMODULEPATH': 'C:\\\\Windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\', 'PUBLIC': 'C:\\\\Users\\\\Public', 'PYTHONIOENCODING': 'cp437', 'QTJAVA': 'C:\\\\Program Files\\\\QuickTime\\\\QTSystem\\\\QTJava.zip', 'SESSIONNAME': 'Console', 'SYSTEMDRIVE': 'C:', 'SYSTEMROOT': 'C:\\\\Windows', 'TEMP': 'C:\\\\Users\\\\mike\\\\AppData\\\\Local\\\\Temp', 'TMP': 'C:\\\\Users\\\\mike\\\\AppData\\\\Local\\\\Temp', 'USERDOMAIN': 'mike-PC', 'USERNAME': 'mike', 'USERPROFILE': 'C:\\\\Users\\\\mike', 'VBOX_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\', 'VS90COMNTOOLS': 'C:\\\\Program Files\\\\Microsoft Visual Studio 9.0\\\\Common7\\\\Tools\\\\', 'WINDIR': 'C:\\\\Windows', 'WINDOWS_TRACING_FLAGS': '3', 'WINDOWS_TRACING_LOGFILE': 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log', 'WINGDB_ACTIVE': '1', 'WINGDB_PYTHON': 'c:\\\\python27\\\\python.exe', 'WINGDB_SPAWNCOOKIE': 'rvlxwsGdD7SHYIJm'} Ваш результат не будет таким же, как у меня, поскольку конфигурация ПК у всех немного отличается, но вы увидите нечто похожее. Как вы могли заметить, это возвращает словарь. Значит, вы можете получить доступ к переменным окружения, используя обычные методы работы со словарями. Вот пример:\n\u0026gt;\u0026gt;\u0026gt; print(os.environ[\u0026quot;TMP\u0026quot;]) 'C:\\\\Users\\\\mike\\\\AppData\\\\Local\\\\Temp' Вы также можете использовать функцию os.getenv для доступа к этой переменной среды:\n\u0026gt;\u0026gt;\u0026gt; os.getenv(\u0026quot;TMP\u0026quot;) 'C:\\\\Users\\\\mike\\\\AppData\\\\Local\\\\Temp' Преимущество использования os.getenv() вместо словаря os.environ заключается в том, что если вы попытаетесь получить доступ к несуществующей переменной окружения, функция getenv просто вернет None. Если бы вы сделали то же самое с os.environ, то получили бы ошибку. Давайте попробуем это сделать, чтобы вы могли увидеть, что произойдет:\n\u0026gt;\u0026gt;\u0026gt; os.environ[\u0026quot;TMP2\u0026quot;] Traceback (most recent call last): File \u0026quot;\u0026lt;pyshell#1\u0026gt;\u0026quot;, line 1, in \u0026lt;module\u0026gt; os.environ[\u0026quot;TMP2\u0026quot;] File \u0026quot;C:\\Python27\\lib\\os.py\u0026quot;, line 423, in __getitem__ return self.data[key.upper()] KeyError: 'TMP2' \u0026gt;\u0026gt;\u0026gt; print(os.getenv(\u0026quot;TMP2\u0026quot;)) None os.chdir() и os.getcwd() Функция os.chdir позволяет нам изменить каталог, в котором в данный момент запущена наша сессия Python. Если вы хотите узнать, в каком каталоге вы сейчас находитесь, то вызовите os.getcwd(). Давайте попробуем использовать обе функции:\n\u0026gt;\u0026gt;\u0026gt; os.getcwd() 'C:\\\\Python27' \u0026gt;\u0026gt;\u0026gt; os.chdir(r\u0026quot;c:\\Users\\mike\\Documents\u0026quot;) \u0026gt;\u0026gt;\u0026gt; os.getcwd() 'c:\\\\Users\\\\mike\\\\Documents' Код выше показывает нам, что мы начали работу в каталоге Python по умолчанию, когда мы запускаем этот код в IDLE. Затем мы меняем папки с помощью os.chdir(). Наконец, мы вызываем os.getcwd() во второй раз, чтобы убедиться, что мы успешно перешли в папку.\nos.mkdir() и os.makedirs() Возможно, вы уже догадались, но два метода, рассматриваемые в этом разделе, используются для создания каталогов. Первый из них - os.mkdir(), который позволяет нам создать одну папку. Давайте попробуем это сделать:\n\u0026gt;\u0026gt;\u0026gt; os.mkdir(\u0026quot;test\u0026quot;) \u0026gt;\u0026gt;\u0026gt; path = r'C:\\Users\\mike\\Documents\\pytest' \u0026gt;\u0026gt;\u0026gt; os.mkdir(path Первая строка кода создаст папку с именем test в текущем каталоге. Вы можете использовать методы из предыдущего раздела, чтобы выяснить, куда вы только что запустили свой код, если забыли. Во втором примере путь присваивается переменной, а затем мы передаем путь в os.mkdir(). Это позволяет вам создать папку в любом месте вашей системы, на которое у вас есть права.\nФункция os.makedirs() создаст все промежуточные папки в пути, если они еще не существуют. В принципе, это означает, что вы можете создать путь, содержащий вложенные папки. Я часто так делаю, когда создаю файл журнала, который находится в датированной структуре папок, например, Год/Месяц/День. Давайте рассмотрим пример:\n\u0026gt;\u0026gt;\u0026gt; path = r'C:\\Users\\mike\\Documents\\pytest\\2014\\02\\19' \u0026gt;\u0026gt;\u0026gt; os.makedirs(path) Что здесь произошло? Этот код просто создал кучу папок! Если в вашей системе все еще была папка pytest, то она просто добавила папку 2014 с другой папкой внутри, которая также содержала папку. Попробуйте сделать это сами, используя правильный путь в вашей системе.\nos.remove() и os.rmdir() Функции os.remove() и os.rmdir() используются для удаления файлов и каталогов соответственно. Давайте рассмотрим пример работы os.remove():\n\u0026gt;\u0026gt;\u0026gt; os.remove(\u0026quot;test.txt\u0026quot;) Этот фрагмент кода попытается удалить файл с именем test.txt из текущего рабочего каталога. Если он не сможет найти файл, вы, скорее всего, получите какую-либо ошибку. Вы также получите ошибку, если файл используется (т.е. заблокирован) или у вас нет разрешения на удаление файла. Возможно, вы также захотите проверить os.unlink, который делает то же самое. Термин unlink - это традиционное название этой процедуры в Unix.\nТеперь давайте рассмотрим пример os.rmdir():\n\u0026gt;\u0026gt;\u0026gt; os.rmdir(\u0026quot;pytest\u0026quot;) Приведенный выше код попытается удалить каталог с именем pytest из текущего рабочего каталога. Если попытка успешна, вы увидите, что каталог больше не существует. Если каталог не существует, у вас нет разрешения на его удаление или каталог не пуст, будет выдана ошибка. Возможно, вы также захотите взглянуть на os.removedirs(), которая может рекурсивно удалять вложенные пустые каталоги.\nos.rename(src, dst) Функция os.rename() переименовывает файл или папку. Давайте рассмотрим пример переименования файла:\n\u0026gt;\u0026gt;\u0026gt; os.rename(\u0026quot;test.txt\u0026quot;, \u0026quot;pytest.txt\u0026quot;) В этом примере мы говорим os.rename переименовать файл с именем test.txt в pytest.txt. Это происходит в нашем текущем рабочем каталоге. Вы увидите ошибку, если попытаетесь переименовать несуществующий файл или если у вас нет соответствующего разрешения на переименование файла.\nСуществует также функция os.renames, которая рекурсивно переименовывает каталог или файл.\nos.startfile() Метод os.startfile() позволяет нам \u0026ldquo;запустить\u0026rdquo; файл с помощью связанной с ним программы. Другими словами, мы можем открыть файл с помощью связанной с ним программы, как если бы вы дважды щелкнули по PDF-файлу, и он открылся бы в Adobe Reader. Давайте попробуем!\n\u0026gt;\u0026gt;\u0026gt; os.startfile(r'C:\\Users\\mike\\Documents\\labels.pdf') В приведенном выше примере я передаю os.startfile полный путь, который указывает ему открыть файл под названием labels.pdf. На моей машине это приведет к открытию PDF в Adobe Reader. Вы должны попробовать открыть свои собственные PDF, MP3 и фотографии с помощью этого метода, чтобы увидеть, как он работает.\nos.walk() Метод os.walk() дает нам способ итерации по пути корневого уровня. Это означает, что мы можем передать путь в эту функцию и получить доступ ко всем его подкаталогам и файлам. Давайте воспользуемся одной из папок Python, которые у нас есть под рукой, чтобы протестировать эту функцию. Мы будем использовать: C:\\Python27\\Tools\n\u0026gt;\u0026gt;\u0026gt; path = r'C:\\Python27\\Tools' \u0026gt;\u0026gt;\u0026gt; for root, dirs, files in os.walk(path): print(root) C:\\Python27\\Tools C:\\Python27\\Tools\\i18n C:\\Python27\\Tools\\pynche C:\\Python27\\Tools\\pynche\\X C:\\Python27\\Tools\\Scripts C:\\Python27\\Tools\\versioncheck C:\\Python27\\Tools\\webchecker При желании можно также перебирать dirs и files. Вот один из способов сделать это:\n\u0026gt;\u0026gt;\u0026gt; for root, dirs, files in os.walk(path): print(root) for _dir in dirs: print(_dir) for _file in files: print(_file) Этот фрагмент кода выведет много информации, поэтому я не буду показывать его вывод здесь, но не стесняйтесь попробовать. Теперь мы готовы к изучению работы с путями!\nos.path Подмодуль os.path модуля os имеет множество замечательных функций, встроенных в него. Мы рассмотрим следующие функции:\n- basename - exists - isdir и isfile - join - split В этом подмодуле есть много других функций. Вы можете прочитать о них в документации Python, раздел 10.1.\nos.path.basename Функция basename возвращает только имя файла пути. Вот пример:\n\u0026gt;\u0026gt;\u0026gt; os.path.basename(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') 'ChipViewer.py' Это полезно, когда мне нужно использовать имя файла для именования какого-либо связанного файла, например, файла лога. Это часто случается, когда я обрабатываю файл данных.\nos.path.dirname Функция dirname возвращает только часть пути, связанную с каталогом. Это легче понять, если мы посмотрим на некоторый код:\n\u0026gt;\u0026gt;\u0026gt; os.path.dirname(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') 'C:\\\\Python27\\\\Tools\\\\pynche' В этом примере мы просто получаем путь к каталогу. Это также полезно, когда вы хотите сохранить другие файлы рядом с обрабатываемым файлом, например, вышеупомянутый файл лога.\nos.path.exists Функция exists сообщит вам, существует ли путь или нет. Все, что вам нужно сделать, это передать ей путь. Давайте посмотрим:\n\u0026gt;\u0026gt;\u0026gt; os.path.exists(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') True \u0026gt;\u0026gt;\u0026gt; os.path.exists(r'C:\\Python27\\Tools\\pynche\\fake.py') False В первом примере мы передаем функции exists реальный путь, и она возвращает True, что означает, что путь существует. Во втором примере мы передали ей плохой путь, и она сообщила нам, что путь не существует, вернув False.\nos.path.isdir / os.path.isfile Методы isdir и isfile тесно связаны с методом exists тем, что они также проверяют существование. Однако isdir проверяет только, является ли путь каталогом, а isfile - только, является ли путь файлом. Если вы хотите проверить, существует ли путь независимо от того, является ли он файлом или каталогом, то вам нужно использовать метод exists. В любом случае, давайте рассмотрим несколько примеров:\n\u0026gt;\u0026gt;\u0026gt; os.path.isfile(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') True \u0026gt;\u0026gt;\u0026gt; os.path.isdir(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') False \u0026gt;\u0026gt;\u0026gt; os.path.isdir(r'C:\\Python27\\Tools\\pynche') True \u0026gt;\u0026gt;\u0026gt; os.path.isfile(r'C:\\Python27\\Tools\\pynche') False Уделите немного времени изучению этого набора примеров. В первом примере мы передаем путь к файлу и проверяем, действительно ли этот путь является файлом. Затем во втором примере тот же путь проверяется на то, является ли он каталогом. Вы можете сами посмотреть, что из этого получилось. Затем в последних двух примерах мы немного изменили ситуацию, передав путь к каталогу тем же двум функциям. Эти примеры демонстрируют, как работают эти две функции.\nos.path.join Метод join дает вам возможность соединить один или несколько компонентов пути вместе с помощью соответствующего разделителя. Например, в Windows разделителем является обратная косая черта, а в Linux - прямая косая черта. Вот как это работает:\n\u0026gt;\u0026gt;\u0026gt; os.path.join(r'C:\\Python27\\Tools\\pynche', 'ChipViewer.py') 'C:\\\\Python27\\\\Tools\\\\pynche\\\\ChipViewer.py' В этом примере мы объединили путь к каталогу и путь к файлу, чтобы получить полностью квалифицированный путь. Обратите внимание, что метод join не проверяет, существует ли результат!\nos.path.split Метод split разбивает путь на кортеж, содержащий каталог и файл. Давайте посмотрим:\n\u0026gt;\u0026gt;\u0026gt; os.path.split(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') ('C:\\\\Python27\\\\Tools\\\\pynche', 'ChipViewer.py') Этот пример показывает, что происходит, когда мы указываем путь к файлу. Посмотрим, что произойдет, если в конце пути не будет имени файла:\n\u0026gt;\u0026gt;\u0026gt; os.path.split(r'C:\\Python27\\Tools\\pynche') ('C:\\\\Python27\\\\Tools', 'pynche') Как вы можете видеть, он взял путь и разделил его таким образом, что последняя вложенная папка стала вторым элементом кортежа, а остальная часть пути - первым элементом.\nДля нашего последнего примера я подумал, что вы захотите увидеть типичный случай использования split:\n\u0026gt;\u0026gt;\u0026gt; dirname, fname = os.path.split(r'C:\\Python27\\Tools\\pynche\\ChipViewer.py') \u0026gt;\u0026gt;\u0026gt; dirname 'C:\\\\Python27\\\\Tools\\\\pynche' \u0026gt;\u0026gt;\u0026gt; fname 'ChipViewer.py' Здесь показано, как выполнить множественное присваивание. Когда вы разделяете путь, он возвращает двухэлементный кортеж. Поскольку у нас две переменные слева, первый элемент кортежа присваивается первой переменной, а второй элемент - второй переменной.\nПодведение итогов К этому моменту вы должны быть хорошо знакомы с модулем oss. В этой главе вы узнали следующее:\nкак работать с переменными окружения изменять каталоги и определять текущий рабочий каталог создавать и удалять папки и файлы переименовывать файлы/папки запускать файл и связанное с ним приложение перемещаться по каталогу работать с путями В модуле os есть множество других функций, которые здесь не рассматриваются. Обязательно прочитайте документацию, чтобы узнать, что еще вы можете делать. В следующей главе мы познакомимся с модулями email и smtplib.\n",url:"https://romankurnovskii.com/ru/docs/python101/chapter16_os/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day16/":{title:"16. Управление системой, файловой системой и хранилищем в Linux",tags:["devops","linux","файловая ситема linux"],content:`Управление системой, файловой системой и хранилищем в Linux К этому времени мы кратко рассмотрели Linux и DevOps, а затем мы настроили нашу лабораторную среду с помощью vagant 14-й день), а затем коснулись небольшой части команд, которые будут в вашем ежедневном набор инструментов во время использования терминала - (День 15).
Сегодня мы рассмотрим три ключевые области обслуживания систем Linux с помощью обновлений, установки программного обеспечения. Поймем для чего используются системные папки, а также рассмотрим хранилище.
Управление Ubuntu и программным обеспечением Первое, что мы собираемся рассмотреть, это то, как мы обновляем нашу операционную систему. Большинству из вас этот процесс знаком в ОС Windows и macOS, он немного отличается на рабочем столе и сервере Linux.
Мы рассмотрим диспетчер пакетов apt - утилита, которую мы собираемся использовать на нашей виртуальной машине Ubuntu для обновлений и установки программного обеспечения.
Как правило, по крайней мере на рабочих станциях разработчиков, мы запускаем эту команду, чтобы убедиться, что у нас есть последние доступные обновления из центральных репозиториев перед установкой любого программного обеспечения.
sudo apt-get update
Теперь у нас есть обновленная виртуальная машина Ubuntu с установленными последними обновлениями ОС. Теперь мы хотим установить здесь некоторое программное обеспечение. Давайте выберем figlet — программу, генерирующую текстовые баннеры. Если мы введем «figlet» в наш терминал, вы увидите, что приложение не установлен в нашей системе.
Однако из вышеизложенного вы увидите, что утилита apt предлагает нам некотоыре опции установки apt install ... , которые мы можем попробовать. Это потому, что в репозиториях по умолчанию есть программа figlet. Давайте попробуем sudo apt install figlet Теперь мы можем использовать наше приложение figlet Если мы хотим удалить эту или любую из наших установок программного обеспечения, мы также можем сделать это с помощью менеджера пакетов «apt». sudo apt remove figlet
Существуют сторонние репозитории, которые мы также можем добавить в нашу систему, те, к которым у нас есть доступ из коробки, являются репозиториями Ubuntu по умолчанию.
Если бы, например, мы хотели установить vagrant на нашу виртуальную машину Ubuntu, мы не смогли бы сделать это прямо сейчас, и вы можете увидеть это ниже в первой введенной команде. Затем мы добавляем ключ к репозиторию HashiCorp, а затем добавляем репозиторий в нашу систему.
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026quot;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026quot; Как только мы добавим репозиторий HashiCorp, мы можем запустить sudo apt install vagrant и установить vagrant в нашей системе.
Существует много вариантов, когда дело доходит до установки программного обеспечения, различных вариантов менеджеров пакетов, встроенных в Ubuntu, мы также могли бы использовать сохраненные темплейты (snapshots) для установки нашего программного обеспечения.
Надеюсь, это даст вам представление о том, как управлять установками ОС и программного обеспечения в Linux.
Файловая система Linux состоит из файлов конфигурации, и если вы хотите что-то изменить, вы меняете эти файлы конфигурации.
В Windows у вас есть диск C:, и это то, что мы считаем корнем. В Linux у нас есть /, где мы собираемся найти важные папки в нашей системе Linux.
/bin - Сокращенно от binary, папка bin — это место, где в основном находятся наши двоичные файлы, которые нужны вашей системе, исполняемые файлы и инструменты. /boot - Все файлы, необходимые вашей системе для загрузки. Как загрузиться и с какого диска загрузиться. /dev - Вы можете найти информацию об устройстве здесь, здесь вы найдете указатели на ваши диски sda, которые будут вашим основным диском ОС. /etc - Вероятно, это самая важная папка в вашей системе Linux, где находится большинство ваших файлов конфигурации. /home - здесь вы найдете свои пользовательские папки и файлы. У нас есть пользовательская папка vagrant. В ней вы найдете папки \u0026ldquo;Documents\u0026rdquo; и «Desktop», с которыми мы работали для раздела команд. /lib - Мы упомянули, что /bin — это место, где находятся наши бинарные и исполняемые файлы, а /lib — это место, где вы найдете разделяемые библиотеки для них. /media - Съемные носители. Флешки, диски и тд). /mnt - Mount. Это временная точка монтирования. Подробнее мы расскажем в следующем разделе о хранении данных. /opt - Дополнительные пакеты программного обеспечения. Вы заметите, что здесь хранится некоторое программное обеспечение для vagrant и virtual box. /proc - Информация о ядре (kernel) и процессе (process), аналогичная /dev /root - Домашняя папка для root. Чтобы получить доступ, вам нужно войти в эту папку с помощью sudo. /run - Каталог, содержащий PID файлы процессов, похожий на /var/run, но в отличие от него, он размещен в TMPFS, а поэтому после перезагрузки все файлы теряются. Сохраняет состояния текущих процессов /sbin - System binaries. Так же как и /bin, содержит двоичные исполняемые файлы, которые доступны на ранних этапах загрузки, когда не примонтирован каталог /usr. Но здесь находятся программы, которые можно выполнять только с правами суперпользователя. Это разные утилиты для обслуживания системы. Например, iptables, reboot, fdisk, ifconfig,swapon и т д. /tmp - Содержит временные файлы, созданные системой, любыми программами или пользователями /usr - User Aplications. Если бы мы, как обычный пользователь, установили пакеты программного обеспечения, они обычно устанавливались бы в папку /usr/bin. Здесь находятся исполняемые файлы, исходники программ, различные ресурсы приложений, картинки, музыка и документация
/usr/bin - Содержит исполняемые файлы различных программ, которые не нужны на первых этапах загрузки системы, например, музыкальные плееры, графические редакторы, браузеры и т.д.
/var - Variable. Переменные файлы. Наши приложения устанавливаются в папку bin. Нам нужно где-то хранить все файлы журналов, это /var. Здесь содержатся файлы системных журналов, различные кеши, базы данных и так далее /var/log - Logs. Здесь содержатся большинство файлов логов всех программ, установленных в операционной системе. У многих программ есть свои подкаталоги в этой папке, например, /var/log/apache - логи веб-сервера, /var/log/squid - файлы журналов кеширующего сервера squid. Если в системе что-либо сломалось, скорее всего, ответы вы найдете здесь.
/var/run - Содержит файлы с PID процессов, которые могут быть использованы, для взаимодействия между программами. В отличие от каталога /run данные сохраняются после перезагрузки.
/sys - System. Информация о системе. Назначение каталогов Linux из этой папки - получение информации о системе непосредственно от ядра. Это еще одна файловая система организуемая ядром и позволяющая просматривать и изменить многие параметры работы системы, например, работу swap, контролировать вентиляторы и многое другое.
Хранение Когда мы подходим к системе Linux или любой другой системе, мы можем захотеть узнать о доступных дисках и о том, сколько свободного места у нас есть на этих дисках. Следующие несколько команд помогут нам идентифицировать, использовать и управлять хранилищем.
lsblk Список заблокированных устройств. «sda» — это наш физический диск, а затем «sda1, sda2, sda3» — наши разделы на этом диске. df дает нам немного больше информации об этих разделах, сколько всего, используется и доступно. Здесь вы можете использовать и другие флаги. Я обычно использую df -h, чтобы дать нам \u0026ldquo;человеческий (понятный\u0026rdquo; (human) вывод данных. Если вы добавляли новый диск в свою систему, и это то же самое в Windows, вам нужно было бы отформатировать диск в управлении дисками, в терминале Linux вы можете сделать это с помощью sudo mkfs -t ext4 /dev/sdb с sdb, относящимся к нашему недавно добавленному диску.
Затем нам нужно будет смонтировать наш недавно отформатированный диск, чтобы его можно было использовать. Мы сделали бы это в нашей ранее упомянутой папке /mnt и создали бы там каталог с sudo mkdir NewDisk, а затем использовали бы sudo mount /dev/sdb newdisk для монтирования диска в это место.
Также возможно, что вам нужно будет безопасно отключить хранилище из вашей системы, а не просто вытащить его из конфигурации. Мы можем сделать это с помощью sudo umount /dev/sdb.
Если вы не хотите размонтировать этот диск и собираетесь использовать этот диск для базы данных или какого-либо другого варианта постоянного использования, тогда вы хотите, чтобы он был там при перезагрузке системы. Чтобы это произошло, нам нужно добавить этот диск в наш файл конфигурации /etc/fstab, чтобы он сохранялся, если вы этого не сделаете, его нельзя будет использовать при перезагрузке машины, и вам придется вручную выполнить описанное выше. процесс. Данные по-прежнему будут на диске, но они не будут автоматически монтироваться, пока вы не добавите конфигурацию в этот файл.
После того, как вы отредактировали файл конфигурации fstab, вы можете проверить свою работу с помощью sudo mount -a, если ошибок нет, тогда ваши изменения теперь будут сохраняться при перезапусках.
Мы расскажем, как вы будете редактировать файл с помощью текстового редактора в будущем сеансе.
Ресурсы Структура файловой системы Linux Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need to be a hacker!) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day16/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-take-photo/":{title:"Сделайте фото и отправьте его через канал данных",tags:[],content:`Чему вы научитесь
На этом шаге вы узнаете, как:
Делать снимок и получать из него данные, используя элемент canvas. Обмениваться изображениями с удаленным пользователем. Полная версия этого шага находится в папке step-06.
Как это работает
Ранее вы узнали, как обмениваться текстовыми сообщениями с помощью RTCDataChannel.
Этот шаг позволяет обмениваться целыми файлами: в этом примере - фотографиями, снятыми с помощью getUserMedia().
Основные части этого шага заключаются в следующем:
Установите канал передачи данных. Обратите внимание, что на этом шаге вы не добавляете никаких медиапотоков к одноранговому соединению. Захватите видеопоток пользователя с веб-камеры с помощью getUserMedia(): var video = document.getElementById('video'); function grabWebCamVideo() { console.log('Getting user media (video) ...'); navigator.mediaDevices.getUserMedia({ video: true }) .then(gotStream) .catch(function(e) { alert('getUserMedia() error: ' + e.name); }); } Когда пользователь нажимает кнопку Snap, получает снимок (видеокадр) из видеопотока и отображает его в элементе canvas: var photo = document.getElementById('photo'); var photoContext = photo.getContext('2d'); function snapPhoto() { photoContext.drawImage(video, 0, 0, photo.width, photo.height); show(photo, sendBtn); } Когда пользователь нажимает кнопку Send, преобразуйте изображение в байты и отправьте их по каналу передачи данных: function sendPhoto() { // Split data channel message in chunks of this byte length. var CHUNK_LEN = 64000; var img = photoContext.getImageData(0, 0, photoContextW, photoContextH), len = img.data.byteLength, n = len / CHUNK_LEN | 0; console.log('Sending a total of ' + len + ' byte(s)'); dataChannel.send(len); // split the photo and send in chunks of about 64KB for (var i = 0; i \u0026lt; n; i++) { var start = i * CHUNK_LEN, end = (i + 1) * CHUNK_LEN; console.log(start + ' - ' + (end - 1)); dataChannel.send(img.data.subarray(start, end)); } // send the reminder, if any if (len % CHUNK_LEN) { console.log('last ' + len % CHUNK_LEN + ' byte(s)'); dataChannel.send(img.data.subarray(n * CHUNK_LEN)); } } Принимающая сторона преобразует байты сообщений канала передачи данных обратно в изображение и отображает изображение пользователю: function receiveDataChromeFactory() { var buf, count; return function onmessage(event) { if (typeof event.data === 'string') { buf = window.buf = new Uint8ClampedArray(parseInt(event.data)); count = 0; console.log('Expecting a total of ' + buf.byteLength + ' bytes'); return; } var data = new Uint8ClampedArray(event.data); buf.set(data, count); count += data.byteLength; console.log('count: ' + count); if (count === buf.byteLength) { // we're done: all data chunks have been received console.log('Done. Rendering photo.'); renderPhoto(buf); } }; } function renderPhoto(data) { var canvas = document.createElement('canvas'); canvas.width = photoContextW; canvas.height = photoContextH; canvas.classList.add('incomingPhoto'); // trail is the element holding the incoming images trail.insertBefore(canvas, trail.firstChild); var context = canvas.getContext('2d'); var img = context.createImageData(photoContextW, photoContextH); img.data.set(data); context.putImageData(img, 0, 0); } Получите код
Замените содержимое вашей папки work содержимым из step-06. Ваш файл index.html в папке work теперь должен выглядеть следующим образом :
\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Realtime communication with WebRTC\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;/css/main.css\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Realtime communication with WebRTC\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt; \u0026lt;span\u0026gt;Room URL: \u0026lt;/span\u0026gt;\u0026lt;span id=\u0026quot;url\u0026quot;\u0026gt;...\u0026lt;/span\u0026gt; \u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026quot;videoCanvas\u0026quot;\u0026gt; \u0026lt;video id=\u0026quot;camera\u0026quot; autoplay\u0026gt;\u0026lt;/video\u0026gt; \u0026lt;canvas id=\u0026quot;photo\u0026quot;\u0026gt;\u0026lt;/canvas\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;buttons\u0026quot;\u0026gt; \u0026lt;button id=\u0026quot;snap\u0026quot;\u0026gt;Snap\u0026lt;/button\u0026gt;\u0026lt;span\u0026gt; then \u0026lt;/span\u0026gt;\u0026lt;button id=\u0026quot;send\u0026quot;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;span\u0026gt; or \u0026lt;/span\u0026gt; \u0026lt;button id=\u0026quot;snapAndSend\u0026quot;\u0026gt;Snap \u0026amp;amp; Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;incoming\u0026quot;\u0026gt; \u0026lt;h2\u0026gt;Incoming photos\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026quot;trail\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script src=\u0026quot;/socket.io/socket.io.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://webrtc.github.io/adapter/adapter-latest.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;js/main.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Если вы не отслеживаете эту codelab из своей папки work, вам может потребоваться установить зависимости для папки step-06 или вашей текущей рабочей папки. Просто запустите следующую команду из своей рабочей папки:
npm install После установки, если ваш Node.js сервер не запущен, запустите его, вызвав следующую команду из вашей папки work: node index.js
Убедитесь, что вы используете версию index.js, который реализует Socket.IO, и не забудьте перезапустить ваш сервер Node.js, если вы собираетесь что-то менять. Для большей информации на Node и Socket.IO, загляните в раздел «Set up a signaling service to exchange messages».
При необходимости нажмите на кнопку Allow, чтобы разрешить приложению использовать вашу веб-камеру.
Приложение создаст случайный ID комнаты, и добавьте этот ID в URL. Откройте URL из адресной стройки в новой вкладке или окне браузера.
Нажмите кнопку Snap\u0026amp;Send и затем посмотрите входящую область в другой вкладке внизу страницы. Приложение переносит фотографии между вкладками.
Вы должны увидеть что-то типа этого:
Бонусные задания:
Как вы можете изменить код, чтобы сделать возможным совместное использование файлов любого типа? Узнайте больше
The MediaStream Image Capture API (https://www.chromestatus.com/features/4843864737185792): API для фотосъемки и управления камерами — скоро появится в браузере! API MediaRecorder для записи аудио и видео: демо-примеры (https://webrtc.github.io/samples/src/content/getusermedia/record/) и документация (https://www.chromestatus.com/features/5929649028726784) Что вы узнали
Как делать фото и получать из нее данные с помощью элемента canvas. Как обмениваться этими данными с удаленным пользователем. Полная версия этого шага находится в папке step-06.
`,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-take-photo/"},"https://romankurnovskii.com/ru/docs/python101/chapter17_smtplib/":{title:"17. Модуль email / smtplib",tags:[],content:`Python предоставляет пару действительно хороших модулей, с помощью которых мы можем создавать электронные письма. Это модули email и smtplib. Вместо того чтобы рассматривать различные методы этих двух модулей, мы потратим некоторое время на изучение того, как на самом деле использовать эти модули. В частности, мы рассмотрим следующее:
Основы работы с почтой Как выполнять рассылку несколько адресов одновременно Как отправлять письма, используя строки TO, CC и BCC Как создавать содержимое и тело письма при помощи модуля email Давайте начнем!
Основы работы с электронной почтой - Как отправить письмо с помощью smtplib Модуль smtplib очень интуитивно понятен в использовании. Давайте напишем небольшой пример, показывающий, как отправить письмо. Сохраните следующий код в файл на жестком диске:
import smtplib HOST = \u0026quot;mySMTP.server.com\u0026quot; SUBJECT = \u0026quot;Test email from Python\u0026quot; TO = \u0026quot;mike@someAddress.org\u0026quot; FROM = \u0026quot;python@mydomain.com\u0026quot; text = \u0026quot;Python 3.4 rules them all!\u0026quot; BODY = \u0026quot;\\r\\n\u0026quot;.join(( \u0026quot;From: %s\u0026quot; % FROM, \u0026quot;To: %s\u0026quot; % TO, \u0026quot;Subject: %s\u0026quot; % SUBJECT , \u0026quot;\u0026quot;, text )) server = smtplib.SMTP(HOST) server.sendmail(FROM, [TO], BODY) server.quit() Мы импортировали два модуля, smtplib и модуль string. Две трети этого кода используется для настройки электронной почты. Большинство переменных не требуют пояснений, поэтому мы сосредоточимся только на переменной BODY. Здесь мы используем модуль string для объединения всех предыдущих переменных в одну строку, где каждая строка заканчивается (\u0026quot;/r\u0026quot;), а новая начинается с (\u0026quot;/n\u0026quot;).После вывода BODY вы получите следующую картину:
'From: python@mydomain.com\\r\\nTo: mike@mydomain.com\\r\\nSubject: Test email from Python\\r\\n\\r\\nblah blah blah' После этого мы устанавливаем соединение сервера с нашим хостом, а затем вызываем метод sendmail модуля smtplib для отправки письма. Затем мы отсоединяемся от сервера. Вы заметите, что в этом коде нет имени пользователя или пароля. Если ваш сервер требует аутентификации, то вам нужно будет добавить следующий код:
server.login(username, password) Эта часть должна быть добавлена сразу после создания объекта сервер. Как правило, вам, скорее всего, захочется добавить этот код в функцию и вызвать его с теми или иными параметрами. Или же вам может потребоваться использовать часть этой информации в файл config. Попробуем добавить этот код в функцию.
import smtplib def send_email(host, subject, to_addr, from_addr, body_text): \u0026quot;\u0026quot;\u0026quot; Send an email \u0026quot;\u0026quot;\u0026quot; BODY = \u0026quot;\\r\\n\u0026quot;.join(( \u0026quot;From: %s\u0026quot; % from_addr, \u0026quot;To: %s\u0026quot; % to_addr, \u0026quot;Subject: %s\u0026quot; % subject , \u0026quot;\u0026quot;, body_text )) server = smtplib.SMTP(host) server.sendmail(from_addr, [to_addr], BODY) server.quit() if __name__ == \u0026quot;__main__\u0026quot;: host = \u0026quot;mySMTP.server.com\u0026quot; subject = \u0026quot;Test email from Python\u0026quot; to_addr = \u0026quot;mike@someAddress.org\u0026quot; from_addr = \u0026quot;python@mydomain.com\u0026quot; body_text = \u0026quot;Python rules them all!\u0026quot; send_email(host, subject, to_addr, from_addr, body_text) Теперь вы можете увидеть, насколько мал фактический код, просто взглянув на саму функцию. Это 13 строк! И мы могли бы сделать его короче, если бы не помещали каждый элемент в BODY на отдельной строке, но это было бы не так читабельно.Сейчас мы создадим файл config, чтобы сберечь информацию сервера и адреса from. Зачем? В работе, которой я занимаюсь, мы можем использовать разные почтовые серверы для отправки электронной почты, или если почтовый сервер будет обновлен и его имя изменится, то нам нужно будет изменить только файл config, а не код. То же самое может произойти и с адресом from, если наша компания будет куплена и объединена с другой.
Давайте посмотрим на файл конфигурации (сохраните его как email.ini):
[smtp] server = some.server.com from_addr = python@mydomain.com Это очень простой файл config. В нем у нас есть секция с надписью smtp, в которой у нас есть два элемента: server и from_addr. Мы будем использовать configObj для чтения этого файла и превращения его в словарь Python. Вот обновленная версия кода (сохраните ее как smtp_config.py):
import os import smtplib import sys from configparser import ConfigParser def send_email(subject, to_addr, body_text): \u0026quot;\u0026quot;\u0026quot; Send an email \u0026quot;\u0026quot;\u0026quot; base_path = os.path.dirname(os.path.abspath(__file__)) config_path = os.path.join(base_path, \u0026quot;email.ini\u0026quot;) if os.path.exists(config_path): cfg = ConfigParser() cfg.read(config_path) else: print(\u0026quot;Config not found! Exiting!\u0026quot;) sys.exit(1) host = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;server\u0026quot;) from_addr = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;from_addr\u0026quot;) BODY = \u0026quot;\\r\\n\u0026quot;.join(( \u0026quot;From: %s\u0026quot; % from_addr, \u0026quot;To: %s\u0026quot; % to_addr, \u0026quot;Subject: %s\u0026quot; % subject , \u0026quot;\u0026quot;, body_text )) server = smtplib.SMTP(host) server.sendmail(from_addr, [to_addr], BODY) server.quit() if __name__ == \u0026quot;__main__\u0026quot;: subject = \u0026quot;Test email from Python\u0026quot; to_addr = \u0026quot;mike@someAddress.org\u0026quot; body_text = \u0026quot;Python rules them all!\u0026quot; send_email(subject, to_addr, body_text) Мы добавили небольшую проверку в этот код. Сначала мы хотим получить путь, по которому находится сам скрипт, который представляет собой base_path. Затем мы объединяем этот путь с именем файла, чтобы получить полный путь к файлу конфигурации. Затем мы проверяем существование этого файла. Если он есть, мы создаем ConfigParser, а если его нет, то выводим сообщение и выходим из сценария. На всякий случай следует добавить обработчик исключений вокруг вызова ConfigParser.read(), поскольку файл может существовать, но быть поврежденным или у нас может не быть разрешения на его открытие, что вызовет исключение. Это будет небольшой проект, который вы можете выполнить самостоятельно. В любом случае, предположим, что все идет хорошо и объект ConfigParser успешно создан. Теперь мы можем извлечь информацию о хосте и from_addr, используя обычный синтаксис ConfigParser.
Теперь мы готовы узнать, как отправлять несколько писем одновременно!
Отправка нескольких писем одновременно Давайте немного изменим наш последний пример и отправим несколько писем!
import os import smtplib import sys from configparser import ConfigParser def send_email(subject, body_text, emails): \u0026quot;\u0026quot;\u0026quot; Send an email \u0026quot;\u0026quot;\u0026quot; base_path = os.path.dirname(os.path.abspath(__file__)) config_path = os.path.join(base_path, \u0026quot;email.ini\u0026quot;) if os.path.exists(config_path): cfg = ConfigParser() cfg.read(config_path) else: print(\u0026quot;Config not found! Exiting!\u0026quot;) sys.exit(1) host = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;server\u0026quot;) from_addr = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;from_addr\u0026quot;) BODY = \u0026quot;\\r\\n\u0026quot;.join(( \u0026quot;From: %s\u0026quot; % from_addr, \u0026quot;To: %s\u0026quot; % ', '.join(emails), \u0026quot;Subject: %s\u0026quot; % subject , \u0026quot;\u0026quot;, body_text )) server = smtplib.SMTP(host) server.sendmail(from_addr, emails, BODY) server.quit() if __name__ == \u0026quot;__main__\u0026quot;: emails = [\u0026quot;mike@someAddress.org\u0026quot;, \u0026quot;someone@gmail.com\u0026quot;] subject = \u0026quot;Test email from Python\u0026quot; body_text = \u0026quot;Python rules them all!\u0026quot; send_email(subject, body_text, emails) Вы заметите, что в этом примере мы удалили параметр to_addr и добавили параметр emails, который представляет собой список адресов электронной почты. Чтобы это работало, нам нужно создать строку, разделенную запятыми, в части To: в BODY, а также передать список адресов электронной почты методу sendmail. Таким образом, для создания простой строки, разделенной запятыми, мы делаем следующее: ‘, ‘.join(emails). Просто, да?
Отправка электронной почты с использованием строк TO, CC и BCC Теперь нам осталось выяснить, как отправить письмо, используя поля CC и BCC. Давайте создадим новую версию этого кода, которая будет поддерживать эту функциональность!
import os import smtplib import sys from configparser import ConfigParser def send_email(subject, body_text, to_emails, cc_emails, bcc_emails): \u0026quot;\u0026quot;\u0026quot; Send an email \u0026quot;\u0026quot;\u0026quot; base_path = os.path.dirname(os.path.abspath(__file__)) config_path = os.path.join(base_path, \u0026quot;email.ini\u0026quot;) if os.path.exists(config_path): cfg = ConfigParser() cfg.read(config_path) else: print(\u0026quot;Config not found! Exiting!\u0026quot;) sys.exit(1) host = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;server\u0026quot;) from_addr = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;from_addr\u0026quot;) BODY = \u0026quot;\\r\\n\u0026quot;.join(( \u0026quot;From: %s\u0026quot; % from_addr, \u0026quot;To: %s\u0026quot; % ', '.join(to_emails), \u0026quot;CC: %s\u0026quot; % ', '.join(cc_emails), \u0026quot;BCC: %s\u0026quot; % ', '.join(bcc_emails), \u0026quot;Subject: %s\u0026quot; % subject , \u0026quot;\u0026quot;, body_text )) emails = to_emails + cc_emails + bcc_emails server = smtplib.SMTP(host) server.sendmail(from_addr, emails, BODY) server.quit() if __name__ == \u0026quot;__main__\u0026quot;: emails = [\u0026quot;mike@somewhere.org\u0026quot;] cc_emails = [\u0026quot;someone@gmail.com\u0026quot;] bcc_emails = [\u0026quot;schmuck@newtel.net\u0026quot;] subject = \u0026quot;Test email from Python\u0026quot; body_text = \u0026quot;Python rules them all!\u0026quot; send_email(subject, body_text, emails, cc_emails, bcc_emails) В этом коде мы передаем 3 списка, каждый из которых содержит по одному адресу электронной почты. Мы создаем поля CC и BCC точно так же, как и раньше, но нам также нужно объединить три списка в один, чтобы мы могли передать объединенный список в метод sendmail(). На форумах вроде StackOverflow обсуждалось, что некоторые почтовые клиенты могут обрабатывать поле BCC странным образом, что позволяет получателю видеть список BCC в заголовках письма. Я не могу подтвердить такое поведение, но знаю, что Gmail успешно удаляет информацию BCC из заголовка письма.
Теперь мы готовы перейти к использованию модуля электронной почты Python!
Добавление вложения/тела письма с помощью модуля email Теперь мы возьмем все то, чему мы научились в предыдущих разделах и смешаем их вместе с модулем email, чтобы отправлять письма с прикрепленными файлами. Модуль email позволяет прикреплять вложения к письму очень просто. Вот код
import os import smtplib import sys from configparser import ConfigParser from email import encoders from email.mime.text import MIMEText from email.mime.base import MIMEBase from email.mime.multipart import MIMEMultipart from email.utils import formatdate #---------------------------------------------------------------------- def send_email_with_attachment(subject, body_text, to_emails, cc_emails, bcc_emails, file_to_attach): \u0026quot;\u0026quot;\u0026quot; Send an email with an attachment \u0026quot;\u0026quot;\u0026quot; base_path = os.path.dirname(os.path.abspath(__file__)) config_path = os.path.join(base_path, \u0026quot;email.ini\u0026quot;) header = 'Content-Disposition', 'attachment; filename=\u0026quot;%s\u0026quot;' % file_to_attach # get the config if os.path.exists(config_path): cfg = ConfigParser() cfg.read(config_path) else: print(\u0026quot;Config not found! Exiting!\u0026quot;) sys.exit(1) # extract server and from_addr from config host = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;server\u0026quot;) from_addr = cfg.get(\u0026quot;smtp\u0026quot;, \u0026quot;from_addr\u0026quot;) # create the message msg = MIMEMultipart() msg[\u0026quot;From\u0026quot;] = from_addr msg[\u0026quot;Subject\u0026quot;] = subject msg[\u0026quot;Date\u0026quot;] = formatdate(localtime=True) if body_text: msg.attach( MIMEText(body_text) ) msg[\u0026quot;To\u0026quot;] = ', '.join(to_emails) msg[\u0026quot;cc\u0026quot;] = ', '.join(cc_emails) attachment = MIMEBase('application', \u0026quot;octet-stream\u0026quot;) try: with open(file_to_attach, \u0026quot;rb\u0026quot;) as fh: data = fh.read() attachment.set_payload( data ) encoders.encode_base64(attachment) attachment.add_header(*header) msg.attach(attachment) except IOError: msg = \u0026quot;Error opening attachment file %s\u0026quot; % file_to_attach print(msg) sys.exit(1) emails = to_emails + cc_emails server = smtplib.SMTP(host) server.sendmail(from_addr, emails, msg.as_string()) server.quit() if __name__ == \u0026quot;__main__\u0026quot;: emails = [\u0026quot;mike@someAddress.org\u0026quot;, \u0026quot;nedry@jp.net\u0026quot;] cc_emails = [\u0026quot;someone@gmail.com\u0026quot;] bcc_emails = [\u0026quot;anonymous@circe.org\u0026quot;] subject = \u0026quot;Test email with attachment from Python\u0026quot; body_text = \u0026quot;This email contains an attachment!\u0026quot; path = \u0026quot;/path/to/some/file\u0026quot; send_email_with_attachment(subject, body_text, emails, cc_emails, bcc_emails, path) Здесь мы переименовали нашу функцию и добавили новый аргумент, file_to_attach. Нам также нужно добавить заголовок и создать объект MIMEMultipart. Заголовок может быть создан в любое время перед тем, как мы добавим вложение. Мы добавляем элементы в объект MIMEMultipart (msg), как ключи в словарь. Обратите внимание, что для вставки правильно отформатированной даты мы должны использовать метод formatdate модуля email. Чтобы добавить тело сообщения, нам нужно создать экземпляр MIMEText. Если вы внимательны, вы увидите, что мы не добавили информацию BCC, но вы можете легко сделать это, следуя условиям написанного выше кода. Далее мы добавляем вложение. Мы обернем его в обработчик исключений и используем оператор with, чтобы извлечь файл и поместить его в наш объект MIMEBase. Наконец, мы добавляем его в переменную msg и отправляем его. Обратите внимание, что в методе sendmail мы должны преобразовать msg в строку.
Подведение итогов Теперь вы знаете, как отправлять электронные письма с помощью Python. Тем, кто любит мини-проекты, стоит вернуться назад и добавить дополнительную обработку ошибок в части кода server.sendmail на случай, если во время процесса произойдет что-то странное, например, SMTPAuthenticationError или SMTPConnectError. Мы можем также увеличить обработку ошибок во время прикрепления файла к телу письма, чтобы уловить другие ошибки. Наконец, мы можем взять списки различных писем, чтобы создать один нормальный, в котором отсутствуют дубликаты адресов. Это очень важно, если мы читаем список электронных адресов из файла.
Также обратите внимание, что наш адрес from является поддельным. Мы можем подделывать письма при помощи Python и других языков программирование, однако это крайне неэтично, а в некоторых странах еще и нелегально. Вы были предупреждены! Используйте свои знания с умом и пользуйтесь Python для удовольствия и прибыли!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter17_smtplib/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day17/":{title:"17. Текстовые редакторы Nano/Vim",tags:["devops","vim","nano"],content:`Текстовые редакторы nano и vim Большинство систем Linux - сервера, и у них не будет графического интерфейса. Я также упомянул в прошлой статье, что Linux в основном состоит из файлов конфигурации, и для внесения изменений вам потребуется иметь возможность редактировать эти файлы конфигурации, чтобы изменить что-либо в системе.
Существует множество вариантов, но я думаю, что мы должны рассмотреть, вероятно, два наиболее распространенных текстовых редактора терминала. Я использовал оба этих редактора, и для меня «nano» — это удобная кнопка, когда дело доходит до быстрых изменений, но у «vim» такой широкий набор возможностей.
nano Доступна не во всех системах. Отлично для начала. Если вы запустите nano 90DaysOfDevOps.txt, мы создадим новый файл, в котором ничего не будет, здесь мы можем добавить наш текст, и в окне внизу есть инструкции о том, что мы хотим сделать с этим файлом.
Мы можем нажать control x + enter, а затем запустить ls, теперь вы можете увидеть наш новый текстовый файл.
Можно запустить cat для этого файла, чтобы прочитать наш файл. Затем мы можем использовать тот же nano 90DaysOfDevOps.txt, чтобы добавить дополнительный текст или изменить ваш файл.
Для меня nano очень удобен, когда дело доходит до внесения небольших изменений в файлы конфигурации.
vim Возможно, самый распространенный текстовый редактор.
В значительной степени поддерживается в каждом дистрибутиве Linux. Невероятно мощный! Скорее всего, вы найдете полный 7-часовой курс, посвященный только vim. Мы можем перейти в vim с помощью команды vim или, если мы хотим отредактировать наш новый текстовый файл, мы могли бы запустить vim 90DaysOfDevOps.txt, но сначала вы увидите отсутствие меню справки внизу.
Первый вопрос может быть «Как мне выйти из vim?» это будет escape, и если мы не внесли никаких изменений, то это будет :q
Вы начинаете в обычном «normal» режиме, есть и другие режимы «command, normal, visual, insert», если мы хотим добавить текст, нам нужно будет переключиться с «normal» на «insert», нам нужно нажать «i», если вы добавили какой-то текст и хотели бы сохранить эти изменения, тогда вы нажмете escape, а затем :wq
Вы можете подтвердить это с помощью команды cat, чтобы убедиться, что вы сохранили эти изменения.
В vim есть несколько крутых быстрых функций, которые позволяют очень быстро выполнять простые задачи, если вы знаете ярлыки, что само по себе является лекцией. Допустим, мы добавили список повторяющихся слов, и теперь нам нужно его изменить, может быть, это файл конфигурации, и мы повторяем сетевое имя, и теперь это изменилось, и мы хотим быстро изменить это. Я использую слово \u0026ldquo;Day\u0026rdquo; в этом примере.
Теперь мы хотим заменить это слово на 90DaysOfDevOps, мы можем сделать это, нажав «esc» и набрав «:%s/Day/90DaysOfDevOps». В результате, когда вы нажимаете Enter, слово day заменяется на 90DaysOfDevOps.
Копировать и вставить стало для меня большим открытием. Копия не копия, а дерьмо. мы можем скопировать, используя yy на клавиатуре в обычном режиме. p вставьте в ту же строку, P вставьте в новую строку.
Вы также можете удалить эти строки, выбрав количество строк, которые вы хотите удалить, а затем dd
Также, вероятно, вам понадобится время для поиска файла, теперь мы можем использовать grep, как упоминалось в предыдущем сеансе, но мы также можем использовать vim. мы можем использовать /word, и это найдет первое совпадение, для перехода к следующему вы будете использовать клавишу n и так далее.
Для vim это даже не касается поверхности, самый большой совет, который я могу дать, — взяться за руки и использовать vim везде, где это возможно.
Обычный вопрос на собеседовании: какой ваш любимый текстовый редактор в Linux, и я хотел бы убедиться, что у вас есть хотя бы эти знания об обоих, чтобы вы могли ответить: «Нано» — это нормально, потому что это просто. По крайней мере, вы показываете компетентность в понимании того, что такое текстовый редактор. Но потренируйтесь с ними, чтобы стать более опытным.
Еще один указатель для навигации в vim, мы можем использовать «H, J, K, L», а также наши клавиши со стрелками.
Ресурсы Vim Cheat Sheet Vim in 100 Seconds Vim tutorial Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need to be a hacker!) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day17/"},"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-results/":{title:"Выводы",tags:[],content:`Вы создали приложение для потоковой передачи видео в реальном времени и обмена данными!
Что вы узнали
В этой codelab вы узнали, как:
Получать видео с вашей веб-камеры. стримить видео с помощью RTCPeerConnection. Стримить данные с помощью RTCDataChannel. Настраивать сигналинг-службу для обмена сообщениями. Комбинировать одноранговое соединение и сигналинг. Сделать снимок и поделиться им через канал передачи данных. Следующие шаги
Посмотрите на код и архитектуру канонического приложения AppRTC для чата WebRTC – приложение (https://appr.tc/), код (https://github.com/webrtc/apprtc) Попробуйте реальные примеры (http://webrtc.github.io/samples) из github.com/webrtc/samples. Узнать больше
Ряд ресурсов для начала работы с WebRTC доступен на https://webrtc.org/ `,url:"https://romankurnovskii.com/ru/docs/webrtc/practice/practice-results/"},"https://romankurnovskii.com/ru/docs/python101/chapter18_sqlite/":{title:"18. Модуль sqlite",tags:[],content:`SQLite - это самодостаточный, безсерверный, безконфигурационный транзакционный движок базы данных SQL. Python получил модуль sqlite3 еще в версии 2.5, что означает, что вы можете создавать базы данных SQLite с любым текущим Python без загрузки дополнительных зависимостей. Mozilla использует базы данных SQLite в своем популярном браузере Firefox для хранения закладок и другой различной информации. В этой главе вы узнаете следующее:
- Как создать базу данных SQLite - Как вставить данные в таблицу - Как редактировать данные - Как удалить данные - Основные SQL-запросы Другими словами, вместо того, чтобы рассказывать о модуле sqlite3 по кусочкам, мы рассмотрим, как его реально использовать.
Если вы хотите осмотреть свою базу данных визуально, вы можете использовать плагин SQLite Manager для Firefox (просто найдите его в Google) или, если вам нравится командная строка, вы можете использовать оболочку командной строки SQLite.
Как создать базу данных и вставить некоторые данные Создать базу данных в SQLite очень просто, но для этого нужно знать немного SQL. Вот код, который создаст базу данных для хранения музыкальных альбомов:
import sqlite3 conn = sqlite3.connect(\u0026quot;mydatabase.db\u0026quot;) # or use :memory: to put it in RAM cursor = conn.cursor() # create a table cursor.execute(\u0026quot;\u0026quot;\u0026quot;CREATE TABLE albums (title text, artist text, release_date text, publisher text, media_type text) \u0026quot;\u0026quot;\u0026quot;) Сначала мы должны импортировать модуль sqlite3 и создать соединение с базой данных. Вы можете передать ему путь к файлу, имя файла или просто использовать специальную строку \u0026ldquo;:memory:\u0026rdquo; для создания базы данных в памяти. В нашем случае мы создали ее на диске в файле под названием mydatabase.db. Далее мы создаем объект курсора, который позволяет взаимодействовать с базой данных и добавлять записи, среди прочего. Здесь мы используем синтаксис SQL для создания таблицы с именем albums с 5 текстовыми полями: title, artist, release_date, publisher и media_type. SQLite поддерживает только пять типов данных: null, integer, real, text и blob. Давайте построим этот код и вставим некоторые данные в нашу новую таблицу!
Примечание: Если вы выполните команду CREATE TABLE, а база данных уже существует, вы получите сообщение об ошибке.
# insert some data cursor.execute(\u0026quot;\u0026quot;\u0026quot;INSERT INTO albums VALUES ('Glow', 'Andy Hunter', '7/24/2012', 'Xplore Records', 'MP3')\u0026quot;\u0026quot;\u0026quot; ) # save data to database conn.commit() # insert multiple records using the more secure \u0026quot;?\u0026quot; method albums = [('Exodus', 'Andy Hunter', '7/9/2002', 'Sparrow Records', 'CD'), ('Until We Have Faces', 'Red', '2/1/2011', 'Essential Records', 'CD'), ('The End is Where We Begin', 'Thousand Foot Krutch', '4/17/2012', 'TFKmusic', 'CD'), ('The Good Life', 'Trip Lee', '4/10/2012', 'Reach Records', 'CD')] cursor.executemany(\u0026quot;INSERT INTO albums VALUES (?,?,?,?,?)\u0026quot;, albums) conn.commit() Здесь мы используем команду INSERT INTO SQL для вставки записи в нашу базу данных. Обратите внимание, что каждый элемент должен быть заключен в одинарные кавычки. Это может усложниться, если вам нужно вставить строки, содержащие одинарные кавычки. В любом случае, чтобы сохранить запись в базе данных, мы должны создать ее. Следующий фрагмент кода показывает, как добавить сразу несколько записей с помощью метода executemany курсора. Обратите внимание, что мы используем вопросительные знаки (?) вместо подстановки строк (%s) для вставки значений. Обратите внимание, что использование строки замещения НЕ безопасно, так как может стать причиной появления атаки инъекций SQL . Метод вопросительных знаков намного лучше, а использование SQLAlchemy еще лучше, потому что он делает все экранирование за вас, и вам не придется возиться с преобразованием встроенных одинарных кавычек в то, что примет SQLite.
Обновление и удаление записей Возможность обновления записей в базе данных является ключевым условием сохранения точности данных. Если вы не можете обновлять записи, то ваши данные быстро устареют и станут бесполезными. Иногда вам также необходимо удалять строки из данных. В этом разделе мы рассмотрим обе эти темы. Для начала, давайте выполним обновление!
import sqlite3 conn = sqlite3.connect(\u0026quot;mydatabase.db\u0026quot;) cursor = conn.cursor() sql = \u0026quot;\u0026quot;\u0026quot; UPDATE albums SET artist = 'John Doe' WHERE artist = 'Andy Hunter' \u0026quot;\u0026quot;\u0026quot; cursor.execute(sql) conn.commit() Здесь мы используем команду SQL UPDATE для обновления таблицы альбомов. Вы можете использовать SET для изменения поля, поэтому в данном случае мы изменим поле artist на John Doe в любой записи WHERE, поле artist установлено на Andy Hunter. Разве это не просто? Обратите внимание, что если вы не зафиксируете изменения, то ваши изменения не будут записаны в базу данных. Команда DELETE почти так же проста. Давайте проверим это!
import sqlite3 conn = sqlite3.connect(\u0026quot;mydatabase.db\u0026quot;) cursor = conn.cursor() sql = \u0026quot;\u0026quot;\u0026quot; DELETE FROM albums WHERE artist = 'John Doe' \u0026quot;\u0026quot;\u0026quot; cursor.execute(sql) conn.commit() Удаление даже проще, чем обновление. SQL состоит всего из 2 строк! В данном случае все, что нам нужно было сделать, это указать SQLite, из какой таблицы удалять (альбомы) и какие записи удалять с помощью предложения WHERE. Таким образом, он искал все записи, у которых в поле artist было указано \u0026ldquo;John Doe\u0026rdquo;, и удалял их.
Основные запросы в SQLite Запросы в SQLite практически не отличаются от тех, которые вы используете для других баз данных, таких как MySQL или Postgres. Вы просто используете обычный синтаксис SQL для выполнения запросов, а затем заставляете объект курсора выполнить SQL. Вот несколько примеров:
import sqlite3 conn = sqlite3.connect(\u0026quot;mydatabase.db\u0026quot;) #conn.row_factory = sqlite3.Row cursor = conn.cursor() sql = \u0026quot;SELECT * FROM albums WHERE artist=?\u0026quot; cursor.execute(sql, [(\u0026quot;Red\u0026quot;)]) print(cursor.fetchall()) # or use fetchone() print(\u0026quot;\\nHere's a listing of all the records in the table:\\n\u0026quot;) for row in cursor.execute(\u0026quot;SELECT rowid, * FROM albums ORDER BY artist\u0026quot;): print(row) print(\u0026quot;\\nResults from a LIKE query:\\n\u0026quot;) sql = \u0026quot;\u0026quot;\u0026quot; SELECT * FROM albums WHERE title LIKE 'The%'\u0026quot;\u0026quot;\u0026quot; cursor.execute(sql) print(cursor.fetchall()) Первый запрос, который мы выполняем, это SELECT *, что означает, что мы хотим выбрать все записи, которые соответствуют имени исполнителя, которое мы вводим, в данном случае это \u0026ldquo;Red\u0026rdquo;. Далее мы выполняем SQL и используем fetchall(), чтобы вернуть все результаты. Вы также можете использовать fetchone(), чтобы получить первый результат. Вы также заметите, что здесь есть закомментированная секция, связанная с таинственным row_factory. Если вы уберете эту строку из комментария, результаты будут возвращаться в виде объектов Row, которые подобны словарям Python и дают вам доступ к полям строки, как словарь. Однако вы не можете выполнять присваивание элементов с объектом Row.
Второй запрос очень похож на первый, но он возвращает все записи в базе данных и упорядочивает результаты по имени исполнителя в порядке возрастания. Здесь также показано, как мы можем перебирать результаты в цикле. Последний запрос показывает, как использовать команду LIKE в SQL для поиска частичных фраз. В данном случае мы выполняем поиск по всей таблице названий, которые начинаются со слова \u0026ldquo;The\u0026rdquo;. Знак процента (%) является оператором подстановки.
Подведение итогов Теперь вы знаете, как использовать Python для создания базы данных SQLite. Вы можете создавать, обновлять и удалять записи, а также выполнять запросы к базе данных.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter18_sqlite/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day18/":{title:"18. Web Сервер и SSH",tags:["devops","linux","ssh"],content:`SSH Как мы уже упоминали, вы, скорее всего, будете управлять множеством удаленных серверов Linux, поэтому вам необходимо убедиться, что ваше подключение к этим удаленным серверам безопасно. В этом разделе мы хотим рассказать о некоторых основах SSH (Secure Shell), которые должен знать каждый, и которые помогут вам с этим безопасным туннелем к вашим удаленным системам.
Настройка соединения по SSH Передача файлов Создайте свой закрытый ключ Введение в SSH Безопасная оболочка (Secure Shell) Сетевой протокол (Networking Protocol) Обеспечивает безопасную связь Может защитить любой сетевой сервис Обычно используется для удаленного доступа из командной строки В нашей среде, если вы следили за нами, мы уже использовали SSH, но все это было настроено и автоматизировано с помощью нашей конфигурации vagrant, поэтому нам нужно было только запустить vagrant ssh, и мы получили доступ к нашей удаленной виртуальной машине.
Если бы наша удаленная машина не находилась в той же системе, что и наша рабочая станция, и находилась бы в удаленном месте, возможно, в облачной системе или в центре обработки данных, к которому мы могли бы получить доступ только через Интернет, нам потребовался бы безопасный способ, чтобы получить доступ к системе для управления ею.
SSH обеспечивает безопасный туннель между клиентом и сервером, поэтому злоумышленники ничего не могут перехватить.
На сервере есть служба SSH на стороне сервера, которая всегда работает и прослушивает определенный TCP-порт (22).
Если мы используем наш клиент для подключения с правильными учетными данными или ключом SSH, мы получаем доступ к этому серверу.
Добавление bridged network adapter в нашу систему Чтобы мы могли использовать SSH с нашей виртуальной машиной, нам нужно добавить сетевой адаптер на нашу машину.
Выключите виртуальную машину, щелкните ее правой кнопкой мыши в Virtual Box и выберите настройки. В новом окне выберите сеть.
Теперь снова включите вашу машину, и теперь у вас будет IP-адрес на вашей локальной машине. Вы можете подтвердить это с помощью команды ip addr.
Проверка работы SSH-сервера Мы знаем, что SSH уже настроен на нашей машине, поскольку мы использовали его с vagrant, но мы можем удостовериться, что сервер бежит, запустив
sudo systemctl status ssh
Если в вашей системе нет SSH-сервера, вы можете установить его, введя эту команду sudo apt install openssh-server
Затем вы хотите убедиться, что наш SSH разрешен и брандмауэр работает. Мы можем сделать это с помощью sudo ufw allow ssh. Это не требуется в нашей конфигурации, поскольку мы автоматизировали это с помощью нашего vagrant.
Удаленный доступ — пароль SSH Теперь, когда наш SSH-сервер прослушивает порт 22 для любых входящих запросов на подключение, и мы добавили \u0026ldquo;мост\u0026rdquo; (bridged networking), мы можем использовать putty или SSH-клиент на нашей локальной машине для подключения к нашей системе с помощью SSH.
Затем нажмите «Открыть», если вы впервые подключаетесь к этой системе через этот IP-адрес, вы получите это предупреждение. Мы знаем, что это наша система, поэтому вы можете выбрать «yes».
Затем нам будет предложено ввести имя пользователя (vagrant) и пароль (пароль по умолчанию — vagrant). Ниже вы увидите, что теперь мы используем наш SSH-клиент (Putty) для подключения к нашей машине с использованием имени пользователя и пароля.
На этом этапе мы подключаемся к нашей виртуальной машине с нашего удаленного клиента и можем выполнять наши команды в нашей системе.
Удаленный доступ — ключ SSH Вышеупомянутый простой способ получить доступ к вашим системам, однако, по-прежнему зависит от имени пользователя и пароля, и если какой-либо злоумышленник получит доступ к этой информации, а также к общедоступному адресу или IP-адресу вашей системы, это может быть легко скомпрометировано. Здесь предпочтительны SSH-ключи.
Ключи SSH означают, что мы предоставляем пару ключей, чтобы и клиент, и сервер знали, что это доверенное устройство.
Создать ключ несложно. На нашем локальном компьютере (Windows) мы можем выполнить следующую команду: если у вас установлен ssh-клиент в любой системе, я полагаю, что эта же команда будет работать?
ssh-keygen -t ed25519
Я не буду вдаваться в подробности того, что такое ed25519 и что означает здесь, но вы можете воспользоваться поиском, если хотите узнать больше о криптографии
На данный момент у нас есть созданный ключ SSH, хранящийся в C:\\Users\\micha/.ssh/
Но чтобы связать это с нашей виртуальной машиной Linux, нам нужно скопировать ключ. Мы можем сделать это, используя ssh-copy-id vagrant@192.168.169.135.
Я использовал PowerShell для создания своих ключей на моем клиенте Windows, но здесь нет доступного ssh-copy-id. Есть способы, которыми вы можете сделать это в Windows, и небольшой поиск в Интернете найдет вам альтернативу, но я просто использую git bash на своем компьютере с Windows, чтобы сделать копию.
Теперь мы можем вернуться к Powershell, чтобы проверить, что наше соединение теперь работает с нашими ключами SSH, и пароль не требуется.
ssh vagrant@192.168.169.135
При необходимости мы могли бы защититься, используя кодовую фразу. Мы также могли бы сделать еще один шаг, заявив, что пароли вообще не нужны, что означает, что будут разрешены только пары ключей через SSH. Вы можете сделать это в следующем файле конфигурации.
sudo nano /etc/ssh/sshd_config
здесь есть строка с PasswordAuthentication yes, она будет закомментирована #, вы должны раскомментировать и изменить yes на no. Затем вам нужно будет перезагрузить службу SSH с помощью «sudo systemctl reload sshd».
Настройка веб-сервера Не имеет прямого отношения к тому, что мы только что сделали с SSH выше, но я хотел рассмотрть, поскольку это снова еще одна задача, которая может показаться вам немного сложной, но на самом деле этого не должно быть.
У нас есть виртуальная машина с Linux, и на данном этапе мы хотим добавить веб-сервер apache к нашей виртуальной машине, чтобы мы могли разместить на нем простой веб-сайт, который обслуживает мою домашнюю сеть. Обратите внимание, что эта веб-страница не будет доступна из Интернета, это можно сделать, но здесь это не рассматривается.
Вы также можете увидеть, что это называется стеком LAMP.
Linux Operating System Apache Web Server mySQL database PHP Apache2 Apache2 — это HTTP-сервер с открытым исходным кодом. Мы можем установить apache2 с помощью следующей команды.
sudo apt-get install apache2
Чтобы убедиться, что apache2 установлен правильно, мы можем запустить sudo service apache2 restart.
Затем, используя сетевой адрес моста из пошагового руководства по SSH, откройте браузер и перейдите по этому адресу. Мой http://192.168.169.135/
mySQL MySQL — это база данных, в которой мы будем хранить данные для нашего простого веб-сайта. Чтобы установить MySQL, мы должны использовать следующую команду sudo apt-get install mysql-server
PHP PHP — это серверный язык (server-side scripting language), мы будем использовать его для взаимодействия с базой данных MySQL. Окончательная установка заключается в установке PHP и зависимостей с помощью sudo apt-get install php libapache2-mod-php php-mysql.
Первое изменение конфигурации, которое мы хотим внести в apache из коробки, — это использование index.html, и вместо этого мы хотим использовать index.php.
Мы будем использовать sudo nano /etc/apache2/mods-enabled/dir.conf и переместим index.php в первый элемент списка.
Перезапустите службу apache2 sudo systemctl restart apache2
Теперь давайте подтвердим, что наша система правильно настроена для PHP. Создайте следующий файл с помощью этой команды, это откроет пустой файл в nano.
sudo nano /var/www/html/90Days.php
затем скопируйте следующее и используйте Ctrl + x, чтобы выйти и сохранить файл.
\u0026lt;?php phpinfo(); ?\u0026gt; Теперь снова перейдите к IP-адресу виртуальной машины Linux с дополнительным 90Days.php в конце URL-адреса. http://192.168.169.135/90Days.php вы должны увидеть что-то похожее на показанное ниже, если PHP настроен правильно.
Установка WordPress Я просмотрел тьюториал, чтобы установить WordPress в наш стек LAMP, некоторые команды показаны ниже, если они не показаны правильно в пошаговом руководстве [How to install wordpress on Ubuntu with LAMP](https://blog.ssdnodes.com/blog/ как установить-wordpress-на-ubuntu-18-04-с-лампой-учебник/)
sudo mysql -u root -p
CREATE DATABASE wordpressdb;
CREATE USER 'admin-user'@'localhost' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES ON wordpressdb.* TO 'admin-user'@'localhost';
FLUSH PRIVILEGES;
EXIT;
sudo apt install php-curl php-gd php-mbstring php-xml php-xmlrpc php-soap php-intl php-zip
sudo systemctl restart apache2
cd /var/www
sudo curl -O https://wordpress.org/latest.tar.gz
sudo tar -xvf latest.tar.gz
sudo rm latest.tar.gz
На данный момент вы находитесь на шаге 4 в связанной статье, вам нужно будет выполнить шаги, чтобы убедиться, что для каталога WordPress установлены все правильные разрешения.
Поскольку это только внутреннее действие, вам не нужно «генерировать ключи безопасности» на этом шаге. Перейдите к шагу 5, который меняет конфигурацию Apache на WordPress.
Затем, если все настроено правильно, вы сможете получить доступ через свой внутренний сетевой адрес и запустить установку WordPress.
Ресурсы Client SSH GUI - Remmina The Beginner\u0026rsquo;s guide to SSH Vim in 100 Seconds Vim tutorial Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need to be a hacker!) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day18/"},"https://romankurnovskii.com/ru/docs/python101/chapter19_subprocess/":{title:"19. Модуль subprocess",tags:[],content:`Модуль subprocess дает разработчику возможность запускать процессы или программы из Python. Другими словами, вы можете запускать приложения и передавать им аргументы с помощью модуля subprocess. Модуль subprocess был добавлен в Python 2.4, чтобы заменить модули os, состоящие из os.popen, os.spawn и os.system, а также заменить popen2 и старый модуль commands. Мы рассмотрим следующие аспекты модуля subprocess:
функция вызова класс Popen как взаимодействовать с порожденным процессом. Давайте начнем!
Функция вызова call Модуль subprocess предоставляет функцию call. Эта функция позволяет вам вызвать другую программу, дождаться завершения команды и затем вернуть код возврата. Она принимает один или несколько аргументов, а также следующие ключевые аргументы (с их значениями по умолчанию): stdin=None, stdout=None, stderr=None, shell=False.
Рассмотрим простой пример:
\u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; subprocess.call(\u0026quot;notepad.exe\u0026quot;) 0 Если вы запустите эту программу на машине под управлением Windows, вы увидите, что открылся блокнот. Вы заметите, что IDLE ждет, пока вы закроете Блокнот, а затем возвращает код ноль (0). Это означает, что программа завершилась успешно. Если вы получаете что-либо, кроме нуля, то это обычно означает, что произошла какая-то ошибка.
Обычно, когда вы вызываете эту функцию, вы хотите присвоить полученный код возврата переменной, чтобы вы могли проверить, соответствует ли результат ожиданию. Давайте сделаем это:
\u0026gt;\u0026gt;\u0026gt; code = subprocess.call(\u0026quot;notepad.exe\u0026quot;) \u0026gt;\u0026gt;\u0026gt; if code == 0: print(\u0026quot;Success!\u0026quot;) else: print(\u0026quot;Error!\u0026quot;) Success! Если вы запустите этот код, вы увидите, что он выводит на экран сообщение Success!. Метод call также принимает аргументы для передачи в выполняемую программу. Давайте посмотрим, как это работает:
\u0026gt;\u0026gt;\u0026gt; code = subprocess.call([\u0026quot;ping\u0026quot;, \u0026quot;www.yahoo.com\u0026quot;]) Pinging ds-any-fp3-real.wa1.b.yahoo.com [98.139.180.149] with 32 bytes of data: Reply from 98.139.180.149: bytes=32 time=66ms TTL=45 Reply from 98.139.180.149: bytes=32 time=81ms TTL=45 Reply from 98.139.180.149: bytes=32 time=81ms TTL=45 Reply from 98.139.180.149: bytes=32 time=69ms TTL=45 Ping statistics for 98.139.180.149: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 66ms, Maximum = 81ms, Average = 74ms \u0026gt;\u0026gt;\u0026gt; code 0 Обратите внимание, что в этом примере мы передаем список аргументов. Первый элемент в списке - это программа, которую мы хотим вызвать. Все остальное в списке - это аргументы, которые мы хотим передать этой программе. Итак, в этом примере мы выполняем команду ping для сайта Yahoo. Вы заметите, что код возврата равен нулю, так что все завершилось успешно.
Вы также можете выполнить программу с помощью оболочки операционной системы. Это добавляет уровень абстракции к процессу и повышает вероятность возникновения проблем с безопасностью. Вот официальное предупреждение документации Python по этому вопросу:
Выполнение необработанных вводных данных из ненадежного источника делает программу уязвимой для внедрений в оболочку, что является серьезным недостатком безопасности, который может привести к произвольному исполнению команд. По этой причине, применение shell=True настоятельно не рекомендуется в случаях, когда командная строка создана во внешнем входе.
Обычная рекомендация - не использовать его, если внешний процесс или человек может изменить аргументы вызова. Если вы сами что-то жестко кодируете, то это не так важно.
Класс Popen Класс Popen выполняет дочернюю программу в новом процессе. В отличие от метода call, он не ждет завершения вызванного процесса, если вы не попросите его об этом с помощью метода wait. Давайте попробуем запустить Notepad через Popen и посмотрим, в чем разница:
\u0026gt;\u0026gt;\u0026gt; program = \u0026quot;notepad.exe\u0026quot; \u0026gt;\u0026gt;\u0026gt; subprocess.Popen(program) \u0026lt;subprocess.Popen object at 0x01EE0430\u0026gt; Здесь мы создаем переменную program и присваиваем ей значение \u0026ldquo;notepad.exe\u0026rdquo;. Затем мы передаем ее классу Popen. Когда вы запустите эту программу, вы увидите, что она немедленно возвращает объект subprocess.Popen и вызванное приложение выполняется. Давайте заставим Popen ждать завершения работы программы:
\u0026gt;\u0026gt;\u0026gt; program = \u0026quot;notepad.exe\u0026quot; \u0026gt;\u0026gt;\u0026gt; process = subprocess.Popen(program) \u0026gt;\u0026gt;\u0026gt; code = process.wait() \u0026gt;\u0026gt;\u0026gt; print(code) 0 Когда вы делаете это в IDLE, блокнот всплывает и может оказаться перед сеансом IDLE. Просто уберите блокнот с дороги, но не закрывайте его! Вам нужно указать вашему процессу подождать, пока вы не получите код возврата. После того, как вы впишете эту строку, закройте блокнот и впишите код. Или вы можете просто поместить весь этот код в сохраненный файл Python и запустить его.
Обратите внимание на то, что использование метода wait может вызвать блокировку дочернего процесса при использовании команды stdout/stderr=PIPE, когда процесс генерирует достаточно выходных данных для блокировки pipe Для облегчения этой ситуации можно использовать метод communicate. Мы рассмотрим этот метод в следующем разделе.
Теперь давайте попробуем запустить Popen, используя несколько аргументов:
\u0026gt;\u0026gt;\u0026gt; subprocess.Popen([\u0026quot;ls\u0026quot;, \u0026quot;-l\u0026quot;]) \u0026lt;subprocess.Popen object at 0xb7451001\u0026gt; Если вы запустите этот код в Linux, вы увидите возникшее сообщение объекта Popen, затем перечень разрешений и содержимого той или иной папки, которую вы используете. Вы можете использовать аргумент shell совместно с Popen, но с теми же предостережениями, что и с методом call.
Обучение общению Существует несколько способов взаимодействия с процессом, который вы вызвали. Мы остановимся на том, как использовать метод communicate модуля subprocess. Давайте посмотрим:
args = [\u0026quot;ping\u0026quot;, \u0026quot;www.yahoo.com\u0026quot;] process = subprocess.Popen(args, stdout=subprocess.PIPE) data = process.communicate() print(data) В этом примере кода мы создаем переменную args для хранения нашего списка аргументов. Затем мы перенаправляем стандартный выход (stdout) на наш subprocess, чтобы мы могли общаться с ним. Сам метод communicate позволяет нам общаться с процессом, который мы только что создали. На самом деле мы можем передавать процессу входные данные с помощью этого метода. Но в нашем примере, мы используем этот метод для чтения stdout. Обратите внимание на то, что когда вы запускаете этот код, цель которого – поддержка связи, он будет дожидаться финиша процесса, После чего выдаст кортеж, состоящий из двух элементов, которые являются содержимым stdout и stderr. Вот результат:
('Pinging ds-any-fp3-real.wa1.b.yahoo.com [98.139.180.149] with 32 bytes of data: Reply from 98.139.180.149: bytes=32 time=139ms TTL=45 Reply from 98.139.180.149: bytes=32 time=162ms TTL=45 Reply from 98.139.180.149: bytes=32 time=164ms TTL=45 Reply from 98.139.180.149: bytes=32 time=110ms TTL=45 Ping statistics for 98.139.180.149: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 110ms, Maximum = 164ms, Average = 143ms ', None) Это довольно некрасиво. Давайте заставим его вывести результат в более читабельном формате?
import subprocess args = [\u0026quot;ping\u0026quot;, \u0026quot;www.yahoo.com\u0026quot;] process = subprocess.Popen(args, stdout=subprocess.PIPE) data = process.communicate() for line in data: print(line) Если вы запустите этот код, на экране должно появиться нечто похожее на следующее:
Pinging ds-any-fp3-real.wa1.b.yahoo.com [98.139.180.149] with 32 bytes of data: Reply from 98.139.180.149: bytes=32 time=67ms TTL=45 Reply from 98.139.180.149: bytes=32 time=68ms TTL=45 Reply from 98.139.180.149: bytes=32 time=70ms TTL=45 Reply from 98.139.180.149: bytes=32 time=69ms TTL=45 Ping statistics for 98.139.180.149: Packets: Sent = 4, Received = 4, Lost = 0 (0% loss), Approximate round trip times in milli-seconds: Minimum = 67ms, Maximum = 70ms, Average = 68ms None Последняя строка, в которой написано \u0026ldquo;None\u0026rdquo;, является результатом stderr, что означает, что ошибок не было.
Подведение итогов На данный момент у вас есть знания, позволяющие эффективно использовать модуль subprocess. Вы можете открыть процесс двумя различными способами, знаете, как дождаться кода возврата, и знаете, как взаимодействовать с созданным вами дочерним процессом.
В следующей главе мы рассмотрим модуль sys.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter19_subprocess/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day19/":{title:"19. Автоматизация задачи с помощью bash-скриптов",tags:["devops","bash"],content:`Автоматизация задачи с помощью bash-скриптов Оболочка, которую мы собираемся использовать сегодня, — это bash, но мы рассмотрим другую оболочку завтра, когда будем углубляться в ZSH.
BASH - Bourne Again Shell («возрождённый» shell)
Мы могли бы почти посвятить целую секцию из 7 дней написанию сценариев оболочки, как и языкам программирования. Bash дает нам возможность работать вместе с другими инструментами автоматизации для достижения цели.
Я до сих пор разговариваю со многими людьми, которые настроили несколько сложных сценариев оболочки, чтобы что-то произошло, и они полагаются на этот сценарий для некоторых из наиболее важных вещей в бизнесе, я не говорю, что нам нужно понимать сценарии оболочки/bash. для этой цели это не путь. Но мы должны изучить сценарии оболочки/bash, чтобы работать вместе с нашими инструментами автоматизации и для специальных задач.
Одним из примеров, который мы использовали, может быть VAGRANTFILE, который мы использовали для создания нашей виртуальной машины, мы могли бы обернуть его в простой сценарий bash, который удалял и обновлял его каждый понедельник утром, чтобы у нас была свежая копия нашей виртуальной машины Linux. каждую неделю мы могли бы также добавлять весь программный стек, который нам нужен, на указанную машину с Linux и так далее с помощью одного сценария bash.
Я думаю, что еще одна вещь, которую я, по крайней мере, слышу, это то, что практические вопросы по скриптам становятся все более и более очевидными во всех интервью.
Начало Как и в случае со многим, что мы рассмотрим за все эти 90 дней, единственный реальный способ научиться — это делать. Практический опыт поможет впитать все это в вашу мышечную память.
Прежде всего, нам понадобится текстовый редактор. В День 17 мы рассказали, наверное, о двух самых распространенных текстовых редакторах и немного о том, как их использовать.
Давайте приступим прямо к делу и создадим наш первый сценарий оболочки.
touch 90DaysOfDevOps.sh - создает файл 90DaysOfDevOps.sh
За ним следует nano 90DaysOfDevOps.sh, это откроет наш новый пустой сценарий оболочки в nano. Опять же, вы можете выбрать другой текстовый редактор.
Первая строка всех скриптов bash должна выглядеть примерно так: #!/usr/bin/bash, это путь к вашему двоичному файлу bash.
Однако вы должны проверить это в терминале, запустив which bash, если вы не используете Ubuntu, вы также можете попробовать whereis bash из терминала.
Однако вы можете увидеть другие пути, перечисленные в уже созданных сценариях оболочки, которые могут включать:
#!/bin/bash #!/usr/bin/env bash В следующей строке нашего скрипта я хотел бы добавить комментарий и добавить цель скрипта или хотя бы какую-то информацию обо мне. Вы можете сделать это, используя #. Это позволяет нам комментировать определенные строки в нашем коде и предоставлять описания того, что будут делать следующие команды. Я считаю, что чем больше заметок, тем лучше для пользователя, особенно если вы делитесь этим.
Иногда я использую figlet, программу, которую мы установили ранее в разделе Linux, для создания аски-арта, чтобы начать что-то в наших скриптах.
Все команды, которые мы использовали ранее в этом разделе Linux (День 15) можно использовать здесь как простую команду для тестирования нашего скрипта.
Давайте добавим в наш скрипт простой блок кода.
mkdir 90DaysOfDevOps cd 90DaysOfDevOps touch Day19 ls Затем вы можете сохранить это и выйти из текстового редактора. Если мы запустим наш скрипт с ./90DaysOfDevOps.sh, вы должны получить сообщение об отказе в разрешении. Вы можете проверить права доступа к этому файлу с помощью команды ls -la, и вы увидите, что у нас нет прав на выполнение этого файла.
Мы можем изменить это, используя chmod +x 90DaysOfDevOps.sh, и тогда вы увидите x, означающий, что теперь мы можем запустить (execute) наш скрипт.
Теперь мы можем снова запустить наш скрипт, используя ./90DaysOfDevOps.sh после того, как запуск скрипта создал новый каталог, перешел в этот каталог, а затем создал новый файл.
Довольно простые вещи, но вы можете начать понимать, как это можно использовать для вызова других инструментов, как часть способов сделать вашу жизнь проще и автоматизировать вещи.
Переменные, условные операторы Большая часть этого раздела на самом деле является повторением того, что мы рассмотрели, когда изучали Golang, но я думаю, что нам стоит углубиться в это снова.
Переменные Переменные позволяют нам один раз определить конкретный повторяющийся термин, который используется в потенциально сложном сценарии.
Чтобы добавить переменную, вы просто добавляете ее вот так на чистую строку в вашем скрипте.
challenge=\u0026quot;90DaysOfDevOps\u0026quot;
Таким образом, когда и где мы используем $challenge в нашем коде, если мы изменим переменную, это будет отражено повсюду.
Если мы сейчас запустим наш скрипт sh, вы увидите распечатку, которая была добавлена к нашему скрипту.
Мы также можем запросить пользовательский ввод, который может установить наши переменные, используя следующее:
echo \u0026quot;Enter your name\u0026quot; read name Затем это определило бы ввод как переменную $name. Затем мы могли бы использовать это позже.
Условные операторы Может быть, мы хотим узнать, кто участвует в нашем марафоне \u0026ldquo;90 дней\u0026rdquo; и сколько дней они прошли, мы можем определить это, используя условные выражения if if-else else-if, это то, что мы определили ниже в нашем скрипте. .
#!/bin/bash # ___ ___ ____ ___ __ ____ ___ # / _ \\ / _ \\| _ \\ __ _ _ _ ___ / _ \\ / _| _ \\ _____ __/ _ \\ _ __ ___ #| (_) | | | | | | |/ _\` | | | / __| | | | |_| | | |/ _ \\ \\ / / | | | '_ \\/ __| # \\__, | |_| | |_| | (_| | |_| \\__ \\ |_| | _| |_| | __/\\ V /| |_| | |_) \\__ \\ # /_/ \\___/|____/ \\__,_|\\__, |___/\\___/|_| |____/ \\___| \\_/ \\___/| .__/|___/ # |___/ |_| # # This script is to demonstrate bash scripting! # Variables to be defined ChallengeName=#90DaysOfDevOps TotalDays=90 # User Input echo \u0026quot;Enter Your Name\u0026quot; read name echo \u0026quot;Welcome $name to $ChallengeName\u0026quot; echo \u0026quot;How Many Days of the $ChallengeName challenge have you completed?\u0026quot; read DaysCompleted if [ $DaysCompleted -eq 90 ] then echo \u0026quot;You have finished, well done\u0026quot; elif [ $DaysCompleted -lt 90 ] then echo \u0026quot;Keep going you are doing great\u0026quot; else echo \u0026quot;You have entered the wrong amount of days\u0026quot; fi Вы также можете видеть из вышеприведенного, что мы проводим некоторые сравнения или сверяем значения друг с другом, чтобы перейти к следующему этапу. У нас есть разные варианты, которые стоит отметить.
eq - if the two values are equal will return TRUE ne - if the two values are not equal will return TRUE gt - if the first value is greater than the second value will return TRUE ge - if the first value is greater than or equal to the second value will return TRUE lt - if the first value is less than the second value will return TRUE le - if the first value is less than or equal to the second value will return TRUE Мы также можем использовать сценарии bash для получения информации о файлах и папках, это называется условиями файлов.
-d file True if the file is a directory -e file True if the file exists -f file True if the provided string is a file g file True if the group id is set on a file -r file True if the file is readable -s file True if the file has a non-zero size FILE=\u0026quot;90DaysOfDevOps.txt\u0026quot; if [ -f \u0026quot;$FILE\u0026quot; ] then echo \u0026quot;$FILE is a file\u0026quot; else echo \u0026quot;$FILE is not a file\u0026quot; fi При условии, что этот файл все еще находится в нашем каталоге, мы должны вернуть первую команду echo. Но если мы удалим этот файл, мы должны получить вторую команду echo.
Надеюсь, вы увидите, как это можно использовать для экономии времени при поиске в системе определенных элементов.
Я нашел этот удивительный репозиторий на GitHub, в котором, кажется, бесконечное количество скриптов DevOps Bash Tools
Пример Scenario: У нас есть наша компания под названием «90DaysOfDevOps», и мы работаем некоторое время, и теперь пришло время расширить команду с 1 человека до гораздо большего в ближайшие недели. Я пока единственный, кто знает процесс адаптации, поэтому мы хотим чтобы уменьшить это узкое место, автоматизировав некоторые из этих задач.
Requirements:
Пользователь может быть передан в качестве аргумента командной строки. Пользователь создается с именем аргумента командной строки. Пароль может быть проанализирован как аргумент командной строки. Пароль установлен для пользователя Отображается сообщение об успешном создании учетной записи. Давайте начнем с создания нашего сценария оболочки с помощью touch create_user.sh.
Прежде чем мы двинемся дальше, давайте также создадим этот исполняемый файл, используя chmod +x create_user.sh
затем мы можем использовать nano create_user.sh, чтобы начать редактирование нашего скрипта для сценария, который мы установили.
Мы можем взглянуть на первое требование «Пользователь может быть передан в качестве аргумента командной строки», мы можем использовать следующее
#! /usr/bin/bash #A user can be passed in as a command line argument echo \u0026quot;$1\u0026quot; Идем далее и запускаем ./create_user.sh Michael, замените Michael своим именем при запуске скрипта. Далее мы можем выполнить второе требование: «Пользователь создается с именем аргумента командной строки», это можно сделать с помощью команды useradd. Опция -m предназначена для создания домашнего каталога пользователя как /home/username.
#! /usr/bin/bash #A user can be passed in as a command line argument echo \u0026quot;$1 user account being created.\u0026quot; #A user is created with the name of command line argument sudo useradd -m \u0026quot;$1\u0026quot; Предупреждение: если вы не укажете имя учетной записи пользователя, произойдет ошибка, поскольку мы не заполнили переменную $1
Затем мы можем проверить, была ли создана эта учетная запись с помощью команды awk -F: '{print $1}' /etc/passwd.
More about awk linux command
Наше следующее требование: «Пароль может быть проанализирован как аргумент командной строки». Во-первых, мы никогда не собираемся делать это в продакшене, нам нужно проработать список требований в лаборатории, чтобы понять.
#! /usr/bin/bash #A user can be passed in as a command line argument echo \u0026quot;$1 user account being created.\u0026quot; #A user is created with the name of command line argument sudo useradd -m \u0026quot;$1\u0026quot; #A password can be parsed in as a command line argument. sudo chpasswd \u0026lt;\u0026lt;\u0026lt; \u0026quot;$1\u0026quot;:\u0026quot;$2\u0026quot; Если мы затем запустим этот скрипт с двумя параметрами ./create_user.sh пароль 90DaysOfDevOps
На изображении ниже вы можете видеть, что мы выполнили наш скрипт, он создал нашего пользователя и пароль, а затем мы вручную перешли к этому пользователю и подтвердили это с помощью команды whoami.
Последнее требование: «Отображается сообщение об успешном создании учетной записи». На самом деле у нас уже есть это в верхней строке нашего кода, и мы можем видеть на снимке экрана выше, что у нас есть «созданная учетная запись пользователя 90DaysOfDevOps». Это осталось от нашего тестирования с параметром $1.
Теперь этот сценарий можно использовать для быстрого подключения и настройки новых пользователей в наших системах Linux. Но, может быть, вместо того, чтобы некоторым историческим людям приходилось работать с этим, а затем получать новые имена пользователей или пароли для других людей, мы могли бы добавить некоторый пользовательский ввод, который мы ранее рассмотрели ранее, для захвата наших переменных.
#! /usr/bin/bash echo \u0026quot;What is your intended username?\u0026quot; read username echo \u0026quot;What is your password\u0026quot; read password #A user can be passed in as a command line argument echo \u0026quot;$username user account being created.\u0026quot; #A user is created with the name of command line argument sudo useradd -m $username #A password can be parsed in as a command line argument. sudo chpasswd \u0026lt;\u0026lt;\u0026lt; $username:$password Шаги стали более интерактивными,
Просто чтобы закончить это, возможно, мы хотим вывести успешный вывод, чтобы сказать, что наша новая учетная запись пользователя завершена.
Одна вещь, которую я заметил, это то, что мы отображаем пароль на нашем входе, мы можем скрыть это, используя флаг -s в строке кода read -s password
Если вы хотите удалить пользователя, которого вы создали для лабораторных целей, вы можете сделать это с помощью sudo userdel test_user
Еще раз, я не говорю, что это будет то, что вы будете создавать в своей повседневной жизни, но я думал, что это то, что подчеркнет гибкость того, для чего вы можете использовать сценарии оболочки.
Подумайте о любых повторяющихся задачах, которые вы выполняете каждый день, неделю или месяц, и о том, как вы могли бы лучше автоматизировать это. Первым вариантом, вероятно, будет использование сценария bash, прежде чем переходить к более сложной территории.
Я создал очень простой bash-файл, который помогает мне развернуть кластер Kubernetes с помощью minikube на моем локальном компьютере вместе со службами данных и Kasten K10, чтобы продемонстрировать требования и нужды, связанные с управлением данными. Project Pace. Но я не счел уместным поднимать вопрос здесь, поскольку мы еще не рассмотрели Kubernetes.
Ресурсы Bash in 100 seconds Bash script with practical examples - Full Course Client SSH GUI - Remmina The Beginner\u0026rsquo;s guide to SSH Vim in 100 Seconds Vim tutorial Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need to be a hacker!) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day19/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day20/":{title:"20. Настройка рабочей среды DevOps",tags:["devops"],content:`Настройка рабочей среды Не путать с тем, как мы настраиваем серверы Linux таким образом. Я хочу продемонстрировать возможности выбора и гибкость, которые у нас есть при настройке настольного компьютера Linux.
Я использую рабочий стол Linux уже почти год, и я настроил его именно так, как я хочу с точки зрения внешнего вида. Используя нашу виртуальную машину Ubuntu в Virtual Box, мы можем выполнить некоторые настройки, которые я сделал для своего ежедневного драйвера.
Я собрал видео на YouTube, показывающее остальные, так как некоторые люди могли бы лучше следовать за ним:
Из коробки наша система будет выглядеть примерно так:
Мы также можем увидеть нашу оболочку bash по умолчанию: Многое из этого сводится к точечным файлам, которые мы рассмотрим в этой заключительной статье Linux из этой серии.
dotfiles Сначала я хочу покопаться в dotfiles, я сказал в предыдущий день, что Linux состоит из файлов конфигурации. Эти файлы представляют собой файлы конфигурации для вашей системы Linux и приложений.
Я также добавлю, что dotfiles используются не только для настройки и придания красивого вида вашему рабочему столу, но и для изменения и конфигурации dotfile, которые помогут вам повысить производительность.
Как я уже упоминал, многие программы хранят свои конфигурации в этих точечных файлах. Эти файлы помогают управлять функциональностью.
Каждый файл начинается с . Вы, наверное, догадались, откуда взялось название?
До сих пор мы использовали bash в качестве нашей оболочки, что означает, что у вас будут .bashrc и .bash_profile в нашей домашней папке. Ниже вы можете увидеть несколько точечных файлов, которые есть в нашей системе.
Мы собираемся изменить нашу оболочку, поэтому позже мы увидим новый точечный файл конфигурации .zshrc.
Но теперь вы знаете, если мы ссылаемся на точечные файлы, вы знаете, что это файлы конфигурации. Мы можем использовать их для добавления псевдонимов в нашу командную строку, а также путей к различным местоположениям. Некоторые люди публикуют свои точечные файлы, чтобы они были общедоступными. Вы найдете мой здесь, на моем GitHub MichaelCade/dotfiles, здесь вы найдете мой пользовательский файл .zshrc, мой предпочтительный терминал - терминатор, который также имеет некоторые файлы конфигурации в папке, а затем также некоторые параметры фона.
ZSH Как я упоминал во время наших взаимодействий, до сих пор мы использовали оболочку bash по умолчанию с Ubuntu. ZSH очень похож, но имеет некоторые преимущества перед bash.
Zsh имеет такие функции, как интерактивное завершение с помощью табуляции, автоматический поиск файлов, интеграция с регулярными выражениями, расширенное сокращение для определения области действия команды и богатый движок тем.
Мы можем использовать наш менеджер пакетов «apt», чтобы установить zsh в нашей системе. Давайте продолжим и запустим sudo apt install zsh с нашего терминала bash. Я собираюсь сделать это из консоли виртуальной машины, а не через SSH.
Когда команда установки завершена, вы можете запустить zsh внутри вашего терминала, это запустит сценарий настройки оболочки.
Я выбрал «1» на вопрос выше, и теперь у нас есть еще несколько вариантов. Из этого меню видно, что мы можем внести некоторые готовые изменения, чтобы настроить ZSH в соответствии с нашими потребностями.
Если вы выходите из мастера с 0, а затем используете ls -la | grep .zshrc вы должны увидеть, что у нас есть новый файл конфигурации.
Теперь мы хотим сделать zsh нашей оболочкой по умолчанию каждый раз, когда мы открываем наш терминал, мы можем сделать это, выполнив следующую команду, чтобы изменить нашу оболочку chsh -s $(which zsh), нам затем нужно выйти из системы и снова войти в нее для грядут изменения.
Когда вы снова войдете в систему и откроете терминал, он должен выглядеть примерно так. Мы также можем подтвердить, что наша оболочка теперь изменена, запустив which $SHELL
Обычно я выполняю этот шаг на каждом рабочем столе Ubuntu, который я запускаю, и в целом, не заходя дальше, обнаруживаю, что оболочка zsh немного быстрее, чем bash.
OhMyZSH Далее мы хотим немного улучшить внешний вид, а также добавить некоторые функции, которые помогут нам перемещаться по терминалу.
OhMyZSH — это бесплатная платформа с открытым исходным кодом для управления вашей конфигурацией zsh. Существует множество плагинов, тем и других вещей, которые просто делают взаимодействие с оболочкой zsh намного приятнее.
Вы можете узнать больше о ohmyzsh
Давайте установим Oh My ZSH, у нас есть несколько вариантов с curl, wget или fetch, у нас есть первые два, доступные в нашей системе, но я начну с curl.
sh -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026quot;
Когда вы запустите приведенную выше команду, вы должны увидеть вывод, как показано ниже. Теперь мы можем перейти к добавлению темы для нашего опыта, в комплекте с Oh My ZSH более 100, но я выбираю для всех своих приложений, и все это тема Дракулы.
Я также хочу добавить, что эти два плагина являются обязательными при использовании Oh My ZSH.
git clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions
git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting
nano ~/.zshrc
отредактируйте plugins, чтобы включить plugins=(git zsh-autosuggestions zsh-syntax-highlighting)
Расширения Gnome Я также использую расширения Gnome, и в частности список ниже
Gnome extensions
- Caffeine - CPU Power Manager - Dash to Dock - Desktop Icons - User Themes Установка программ Краткий список программ, которые я устанавливаю на машину с помощью apt
- VSCode - azure-cli - containerd.io - docker - docker-ce - google-cloud-sdk - insomnia - packer - terminator - terraform - vagrant тема Dracula Этот сайт - единственная тема, которую я использую в данный момент. Выглядит четким, чистым и все выглядит отлично. Dracula Theme
По ссылке выше можем поискать zsh на сайте и найдем как минимум два варианта.
Следуйте приведенным инструкциям, чтобы выполнить установку вручную или с помощью git. Затем вам нужно будет, наконец, отредактировать файл конфигурации .zshrc, как показано ниже.
Далее нам понадобится тема Gnome Terminal Dracula со всеми инструкциями
На самом деле мне потребовалось бы много времени, чтобы задокументировать каждый шаг, поэтому я создал пошаговое видео процесса. (Нажмите на изображение ниже)
Если вы дочитали до этого момента, значит, мы закончили наш раздел Linux в #90DaysOfDevOps. Я снова открыт для отзывов и дополнений к ресурсам здесь.
Я также подумал, что было проще показать вам многие шаги с помощью видео, чем записывать их здесь, что вы думаете об этом? У меня есть цель вернуться к этим дням и, где это возможно, создать видео-пошаговые руководства, чтобы добавить и, возможно, лучше объяснить и показать некоторые вещи, которые мы рассмотрели. Что вы думаете?
Ресурсы Bash in 100 seconds Bash script with practical examples - Full Course Client SSH GUI - Remmina The Beginner\u0026rsquo;s guide to SSH Vim in 100 Seconds Vim tutorial Learn the Linux Fundamentals - Part 1 Linux for hackers (don\u0026rsquo;t worry you don\u0026rsquo;t need to be a hacker!) Завтра мы начинаем наши 7 дней погружения в сетевое взаимодействие, мы будем стараться получить базовые знания и понимание сетевого взаимодействия (Networking) в DevOps.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day20/"},"https://romankurnovskii.com/ru/docs/python101/chapter21_thread/":{title:"21. Модуль потоков Thread",tags:[],content:`В Python есть несколько различных конструкций параллелизма, таких как threading, queues и мультипроцессинг. Раньше модуль threading был основным способом реализации multiprocessing. Несколько лет назад в набор стандартных библиотек Python был добавлен модуль multiprocessing. Эта глава будет посвящена использованию потоков и очередей.
Использование потоков Мы начнем с простого примера, который просто демонстрирует работу потоков. Мы создадим подкласс класса Thread и заставим его выводить свое имя в stdout. Приступим к кодированию!
import random import time from threading import Thread class MyThread(Thread): \u0026quot;\u0026quot;\u0026quot; A threading example \u0026quot;\u0026quot;\u0026quot; def __init__(self, name): \u0026quot;\u0026quot;\u0026quot;Initialize the thread\u0026quot;\u0026quot;\u0026quot; Thread.__init__(self) self.name = name def run(self): \u0026quot;\u0026quot;\u0026quot;Run the thread\u0026quot;\u0026quot;\u0026quot; amount = random.randint(3, 15) time.sleep(amount) msg = \u0026quot;%s is running\u0026quot; % self.name print(msg) def create_threads(): \u0026quot;\u0026quot;\u0026quot; Create a group of threads \u0026quot;\u0026quot;\u0026quot; for i in range(5): name = \u0026quot;Thread #%s\u0026quot; % (i+1) my_thread = MyThread(name) my_thread.start() if __name__ == \u0026quot;__main__\u0026quot;: create_threads() В приведенном выше коде мы импортируем модуль Python random, модуль time и импортируем класс Thread из модуля threading. Далее мы создаем подкласс Thread и переопределяем его метод init, чтобы он принимал аргумент, который мы обозначим как name. Чтобы запустить поток, нужно вызвать его метод start(). Когда вы запускаете поток, автоматически вызывается метод run. Мы переопределили метод run потока, чтобы заставить его выбрать случайный промежуток времени для сна. Приведенный здесь пример random.randint заставит Python случайным образом выбрать число от 3 до 15. Затем мы заставим поток спать то количество секунд, которое мы только что случайно выбрали, чтобы имитировать, что он действительно что-то делает. Наконец, мы выводим имя потока, чтобы дать пользователю знать, что поток завершен.
Функция create_threads создаст 5 потоков, присвоив каждому из них уникальное имя. Если вы запустите этот код, вы должны увидеть что-то вроде этого:
Thread #2 is running Thread #3 is running Thread #1 is running Thread #4 is running Thread #5 is running Порядок вывода будет отличаться каждый раз. Попробуйте выполнить код несколько раз, чтобы увидеть смену порядка. Теперь давайте напишем что-нибудь более практичное!
Написание потокового загрузчика Предыдущий пример был не очень полезен, кроме как в качестве инструмента для объяснения работы потоков. Поэтому в этом примере мы создадим класс Thread, который сможет загружать файлы из Интернета. Налоговая служба США имеет множество PDF-форм, которые граждане используют для уплаты налогов. Мы будем использовать этот бесплатный ресурс для нашей демонстрации. Вот код:
import os import urllib2 from threading import Thread class DownloadThread(Thread): \u0026quot;\u0026quot;\u0026quot; A threading example that can download a file \u0026quot;\u0026quot;\u0026quot; def __init__(self, url, name): \u0026quot;\u0026quot;\u0026quot;Initialize the thread\u0026quot;\u0026quot;\u0026quot; Thread.__init__(self) self.name = name self.url = url def run(self): \u0026quot;\u0026quot;\u0026quot;Run the thread\u0026quot;\u0026quot;\u0026quot; handle = urllib2.urlopen(self.url) fname = os.path.basename(self.url) with open(fname, \u0026quot;wb\u0026quot;) as f_handler: while True: chunk = handle.read(1024) if not chunk: break f_handler.write(chunk) msg = \u0026quot;%s has finished downloading %s!\u0026quot; % (self.name, self.url) print(msg) def main(urls): \u0026quot;\u0026quot;\u0026quot; Run the program \u0026quot;\u0026quot;\u0026quot; for item, url in enumerate(urls): name = \u0026quot;Thread %s\u0026quot; % (item+1) thread = DownloadThread(url, name) thread.start() if __name__ == \u0026quot;__main__\u0026quot;: urls = [\u0026quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026quot;] main(urls) По сути, это полная переработка первого сценария. В нем мы импортируем модули os и urllib2, а также модуль threading. Мы будем использовать urllib2 для выполнения фактической загрузки внутри класса потока. Модуль os используется для извлечения имени загружаемого файла, чтобы мы могли использовать его для создания файла с таким же именем на нашей машине. В классе DownloadThread мы настраиваем init для приема url и имени потока. В методе run мы открываем url, извлекаем имя файла и затем используем это имя для именования / создания файла на диске. Затем мы используем цикл while для загрузки файла по килобайту за раз и записи его на диск. После завершения сохранения файла мы выводим имя потока и url, который закончил загрузку.
Версия кода для Python 3 немного отличается. Вы должны импортировать urllib вместо urllib2 и использовать urllib.request.urlopen вместо urllib2.urlopen. Вот код, чтобы вы могли увидеть разницу:
# Python 3 version import os import urllib.request from threading import Thread class DownloadThread(Thread): \u0026quot;\u0026quot;\u0026quot; A threading example that can download a file \u0026quot;\u0026quot;\u0026quot; def __init__(self, url, name): \u0026quot;\u0026quot;\u0026quot;Initialize the thread\u0026quot;\u0026quot;\u0026quot; Thread.__init__(self) self.name = name self.url = url def run(self): \u0026quot;\u0026quot;\u0026quot;Run the thread\u0026quot;\u0026quot;\u0026quot; handle = urllib.request.urlopen(self.url) fname = os.path.basename(self.url) with open(fname, \u0026quot;wb\u0026quot;) as f_handler: while True: chunk = handle.read(1024) if not chunk: break f_handler.write(chunk) msg = \u0026quot;%s has finished downloading %s!\u0026quot; % (self.name, self.url) print(msg) def main(urls): \u0026quot;\u0026quot;\u0026quot; Run the program \u0026quot;\u0026quot;\u0026quot; for item, url in enumerate(urls): name = \u0026quot;Thread %s\u0026quot; % (item+1) thread = DownloadThread(url, name) thread.start() if __name__ == \u0026quot;__main__\u0026quot;: urls = [\u0026quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026quot;] main(urls) Использование Queues Очередь(Queues Python) может быть использована для стековых реализаций «пришел первым – ушел первым» (first-in-first-out (FIFO)) или же «пришел последним – ушел последним» (last-in-last-out (LILO)) , если вы используете их правильно. В этом разделе мы смешаем потоки и создадим простой сценарий загрузки файлов, чтобы продемонстрировать, как работают очереди в случаях, когда нам нужен параллелизм.
Чтобы объяснить, как работают Queues, мы перепишем сценарий загрузки из предыдущего раздела, чтобы использовать Queues. Давайте начнем!
import os import threading import urllib.request from queue import Queue class Downloader(threading.Thread): \u0026quot;\u0026quot;\u0026quot;Threaded File Downloader\u0026quot;\u0026quot;\u0026quot; def __init__(self, queue): \u0026quot;\u0026quot;\u0026quot;Initialize the thread\u0026quot;\u0026quot;\u0026quot; threading.Thread.__init__(self) self.queue = queue def run(self): \u0026quot;\u0026quot;\u0026quot;Run the thread\u0026quot;\u0026quot;\u0026quot; while True: # gets the url from the queue url = self.queue.get() # download the file self.download_file(url) # send a signal to the queue that the job is done self.queue.task_done() def download_file(self, url): \u0026quot;\u0026quot;\u0026quot;Download the file\u0026quot;\u0026quot;\u0026quot; handle = urllib.request.urlopen(url) fname = os.path.basename(url) with open(fname, \u0026quot;wb\u0026quot;) as f: while True: chunk = handle.read(1024) if not chunk: break f.write(chunk) def main(urls): \u0026quot;\u0026quot;\u0026quot; Run the program \u0026quot;\u0026quot;\u0026quot; queue = Queue() # create a thread pool and give them a queue for i in range(5): t = Downloader(queue) t.setDaemon(True) t.start() # give the queue some data for url in urls: queue.put(url) # wait for the queue to finish queue.join() if __name__ == \u0026quot;__main__\u0026quot;: urls = [\u0026quot;http://www.irs.gov/pub/irs-pdf/f1040.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040a.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040ez.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040es.pdf\u0026quot;, \u0026quot;http://www.irs.gov/pub/irs-pdf/f1040sb.pdf\u0026quot;] main(urls) Давайте немного разложим это по полочкам. Прежде всего, нам нужно посмотреть на определение главной функции, чтобы увидеть, как все это работает. Здесь мы видим, что она принимает список url адресов. Затем функция main создает экземпляр очереди, который она передает 5 демонизированным потокам. Основная разница между демонизированным и недемонизированным потоком в том, что вам нужно отслеживать недемонизированные потоки и закрывать их вручную, в то время как поток «демон» нужно только запустить и забыть о нем. Когда ваше приложение закроется, закроется и поток. Далее мы загрузили очередь (при помощи метода put) вместе с переданными url.
Наконец, мы говорим очереди ждать, пока потоки выполнят свою обработку, используя метод join. В классе загрузки у нас есть строка self.queue.get(), которая блокирует очередь до тех пор, пока ей не будет что вернуть. Это означает, что потоки просто сидят без дела, ожидая, пока очередь что-нибудь получит. Это также означает, что для того, чтобы поток мог получить что-то из очереди, он должен вызвать метод очереди get. Таким образом, по мере добавления или помещения элементов в очередь пул потоков будет забирать или получать элементы и обрабатывать их. Это также известно как dequeing. Когда все элементы в очереди обработаны, сценарий завершает работу и выходит из программы. На моей машине он загружает все 5 документов менее чем за секунду.
Подведение итогов Теперь вы знаете, как использовать потоки и очереди как в теории, так и на практике. Потоки особенно полезны, когда вы создаете пользовательский интерфейс и хотите сохранить его работоспособность. Без потоков пользовательский интерфейс стал бы неотзывчивым и завис бы во время загрузки большого файла или выполнения большого запроса к базе данных. Чтобы этого не произошло, вы выполняете длительные процессы в потоках, а затем возвращаетесь к интерфейсу, когда закончите.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter21_thread/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day21/":{title:"21. DevOps настройка сети",tags:["devops"],content:`Общая картина: DevOps и Сеть Добро пожаловать в День 21! Мы собираемся заняться сетевыми технологиями в течение следующих 7 дней. Сеть и DevOps являются всеобъемлющей темой, но нам также необходимо изучить некоторые основы сетевых технологий.
В конечном счете, как мы уже говорили ранее, DevOps — это культура и изменение процессов в ваших организациях. Как мы уже говорили, это могут быть виртуальные машины, контейнеры, Kubernetes, но это также может быть и сеть. Если мы используем эти принципы DevOps для нашей инфраструктуры, которая чтобы включить сеть более точно с точки зрения DevOps, вам также необходимо знать о сети, а также о различных топологиях, сетевых инструментах и ​​стеках, которые у нас есть.
Я бы сказал, что наши сетевые устройства должны быть настроены с использованием инфраструктуры как кода, и все должно быть автоматизировано, как и наши виртуальные машины, но для этого мы должны хорошо понимать, что мы автоматизируем.
Что такое NetDevOps | Сетевой DevOps? Вы также можете услышать термины Network DevOps или NetDevOps. Возможно, вы уже являетесь сетевым инженером (network engineer) и хорошо разбираетесь в сетевых компонентах в инфраструктуре, вы понимаете элементы, используемые в сети, такие как DHCP, DNS, NAT и т. д. и т. д. У вас также будет хорошее понимание аппаратных или программно-определяемых сетей. опции, коммутаторы, маршрутизаторы и т.д. и т.п.
Но если вы не сетевой инженер, то нам, вероятно, необходимо получить базовые знания по всем направлениям в некоторых из этих областей, чтобы мы могли понять конечную цель Network DevOps.
Но в отношении этих терминов мы можем думать о NetDevOps или Network DevOps как о применении принципов и практик DevOps к сети, применении инструментов управления версиями и автоматизации к созданию, тестированию, мониторингу и развертыванию сети.
Если мы думаем о сетевой DevOps как о необходимости автоматизации, мы упоминали ранее о том, что DevOps разрушает разрозненность между командами. Если сетевые команды не перейдут на аналогичную модель и процесс, они станут узким местом или даже полным провалом.
Использование принципов автоматизации подготовки, настройки, тестирования, контроля версий и развертывания — отличное начало. Автоматизация в целом обеспечит скорость развертывания, стабильность сетевой инфраструктуры и последовательное улучшение, а также процесс, который будет совместно использоваться в нескольких средах после их тестирования. Например, полностью протестированная сетевая политика, которая была полностью протестирована в одной среде, может быть быстро использована в другом месте из-за характера этого в коде, а не в процессе, созданном вручную, как это могло быть раньше. Действительно хорошую точку зрения и схему этого мышления можно найти здесь. Сетевой DevOps
Networking - основы Давайте для начала забудем о стороне DevOps, и теперь нам нужно очень кратко взглянуть на некоторые основы работы в сети.
Сетевые устройства Host — это любые устройства, которые отправляют или получают трафик. IP Address \u0026ldquo;определение\u0026rdquo; каждого хоста. (адрес)
Network — (Сеть) это то, что транспортирует трафик между хостами. Если бы у нас не было сетей, было бы много ручного перемещения данных! Логическая группа хостов, для которых требуется аналогичное подключение. Switches (Коммутаторы) облегчают связь внутри сети. Коммутатор пересылает пакеты данных между хостами. Коммутатор отправляет пакеты напрямую хостам.
Сеть: группа хостов, которым требуется одинаковое подключение. Хосты в сети используют одно и то же пространство IP-адресов. Маршрутизатор (Router) облегчает связь между сетями. Если мы сказали ранее, что коммутатор следит за связью внутри сети, маршрутизатор позволяет нам объединить эти сети или, по крайней мере, предоставить им доступ друг к другу, если это разрешено. Маршрутизатор может обеспечить точку контроля трафика (безопасность, фильтрация, перенаправление). Все больше и больше коммутаторов теперь также предоставляют некоторые из этих функций.
Маршрутизаторы узнают, к каким сетям они подключены. Это известно как маршруты, таблица маршрутизации — это все сети, о которых знает маршрутизатор.
Маршрутизатор имеет IP-адрес в сетях, к которым он подключен. Этот IP-адрес также будет использоваться каждым хостом за пределами их локальной сети, также известной как шлюз.
Маршрутизаторы также создают иерархию в сетях, о которой я упоминал ранее.
Коммутаторы и маршрутизаторы (Switches vs Routers ) Маршрутизация – это процесс перемещения данных между сетями.
Маршрутизатор — это устройство, основной задачей которого является маршрутизация. Коммутация — это процесс перемещения данных в сети.
Коммутатор — это устройство, основное назначение которого — коммутация. Это во многом базовый обзор устройств, поскольку мы знаем, что существует множество различных сетевых устройств, таких как:
Access Points Firewalls Load Balancers Layer 3 Switches IDS / IPS Proxies Virtual Switches Virtual Routers Хотя все эти устройства будут выполнять маршрутизацию и/или коммутацию.
В течение следующих нескольких дней мы собираемся узнать немного больше об этом списке.
OSI Model Network Protocols DNS (Domain Name System) NAT DHCP Subnets Ресурсы Computer Networking full course
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day21/"},"https://romankurnovskii.com/ru/docs/python101/chapter21-1_asyncio/":{title:"21-1. Модуль asyncio",tags:[],content:` Асинхронное программирование — это концепция программирования, при применении которой запуск длительных операций происходит без ожидания их завершения и не блокирует дальнейшее выполнение программы.
Корутина: корутины — это более общая форма подпрограмм. Подпрограммы имеют одну точку входа и одну точку выхода. А корутины поддерживают множество точек входа, выхода и возобновления их выполнения.
Python-модуль asyncio позволяет заниматься асинхронным программированием с применением конкурентного выполнения кода, основанного на корутинах. Хотя этот модуль имеется в Python уже много лет, он остаётся одним из самых интересных механизмов языка. Но asyncio, при этом, можно назвать ещё и одним из модулей, которые вызывают больше всего недоразумений. Дело в том, что начинающим разработчикам бывает трудно приступить к использованию asyncio.
# определение корутины async def custom_coro(): # ... # создание объекта корутины coro = custom_coro() # приостановить выполнение кода и запланировать выполнение целевого объекта await custom_coro() # обход асинхронного итератора async for item in async_iterator: print(item) Ресурсы https://habr.com/ru/company/wunderfund/blog/700474/ `,url:"https://romankurnovskii.com/ru/docs/python101/chapter21-1_asyncio/"},"https://romankurnovskii.com/ru/docs/python101/chapter22_time/":{title:"22. Работа с датами и временем",tags:[],content:`Python предоставляет разработчику несколько инструментов для работы с датами и временем. В этой главе мы рассмотрим модули datetime и time. Мы изучим, как они работают, и некоторые распространенные способы их использования. Начнем с модуля datetime!
Модуль datetime Мы познакомимся со следующими классами модуля datetime:
datetime.date datetime.timedelta datetime.datetime Они будут охватывать большинство случаев, когда вам понадобится использовать дату и объект datetime в Python. Существует также класс tzinfo для работы с часовыми поясами, который мы не будем рассматривать. Не стесняйтесь заглянуть в документацию Python для получения дополнительной информации об этом классе.
datetime.date Python может представлять даты несколькими различными способами. Сначала мы рассмотрим формат datetime.date, поскольку он является одним из самых простых объектов даты.
\u0026gt;\u0026gt;\u0026gt; datetime.date(2012, 13, 14) Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; builtins.ValueError: month must be in 1..12 \u0026gt;\u0026gt;\u0026gt; datetime.date(2012, 12, 14) datetime.date(2012, 12, 14) Этот код показывает, как создать простой объект даты. Класс date принимает три аргумента: год, месяц и день. Если вы передадите ему недопустимое значение, вы увидите ошибку ValueError, как показано выше. В противном случае будет возвращен объект datetime.date. Давайте рассмотрим другой пример:
\u0026gt;\u0026gt;\u0026gt; import datetime \u0026gt;\u0026gt;\u0026gt; d = datetime.date(2012, 12, 14) \u0026gt;\u0026gt;\u0026gt; d.year 2012 \u0026gt;\u0026gt;\u0026gt; d.day 14 \u0026gt;\u0026gt;\u0026gt; d.month 12 Здесь мы присваиваем объект date переменной d. Теперь мы можем обращаться к различным компонентам даты по имени, например, d.year или d.month. Теперь давайте узнаем, какой сегодня день:
\u0026gt;\u0026gt;\u0026gt; datetime.date.today() datetime.date(2014, 3, 5) Это может быть полезно, когда вам нужно записать, какой сегодня день. Или, возможно, вам нужно произвести вычисления на основе сегодняшнего дня. В любом случае, это удобный метод.
datetime.datetime Объект datetime.datetime содержит всю информацию из datetime.date плюс объект datetime.time. Давайте создадим пару примеров, чтобы лучше понять разницу между этим объектом и объектом datetime.date.
\u0026gt;\u0026gt;\u0026gt; datetime.datetime(2014, 3, 5) datetime.datetime(2014, 3, 5, 0, 0) \u0026gt;\u0026gt;\u0026gt; datetime.datetime(2014, 3, 5, 12, 30, 10) datetime.datetime(2014, 3, 5, 12, 30, 10) \u0026gt;\u0026gt;\u0026gt; d = datetime.datetime(2014, 3, 5, 12, 30, 10) \u0026gt;\u0026gt;\u0026gt; d.year 2014 \u0026gt;\u0026gt;\u0026gt; d.second 10 \u0026gt;\u0026gt;\u0026gt; d.hour 12 Здесь мы видим, что datetime.datetime принимает несколько дополнительных аргументов: год, месяц, день, час, минуту и секунду. Он также позволяет указывать микросекунды и информацию о часовом поясе. При работе с базами данных вы часто будете использовать подобные объекты. Чаще всего вам нужно преобразовать формат даты или времени Python в формат даты или временной метки SQL. Сегодняшний день можно узнать с помощью datetime.datetime, используя два различных метода:
\u0026gt;\u0026gt;\u0026gt; datetime.datetime.today() datetime.datetime(2014, 3, 5, 17, 56, 10, 737000) \u0026gt;\u0026gt;\u0026gt; datetime.datetime.now() datetime.datetime(2014, 3, 5, 17, 56, 15, 418000) В модуле datetime есть еще один метод, о котором вам следует знать, - strftime. Этот метод позволяет разработчику создать строку, которая представляет время в более удобном для восприятия формате. В документации по Python, раздел 8.1.7, есть целая таблица вариантов форматирования, с которой вам следует ознакомиться. Мы рассмотрим пару примеров, чтобы показать вам возможности этого метода:
\u0026gt;\u0026gt;\u0026gt; datetime.datetime.today().strftime(\u0026quot;%Y%m%d\u0026quot;) '20140305' \u0026gt;\u0026gt;\u0026gt; today = datetime.datetime.today() \u0026gt;\u0026gt;\u0026gt; today.strftime(\u0026quot;%m/%d/%Y\u0026quot;) '03/05/2014' \u0026gt;\u0026gt;\u0026gt; today.strftime(\u0026quot;%Y-%m-%d-%H.%M.%S\u0026quot;) '2014-03-05-17.59.53' Первый пример является своего рода хаком. Он показывает, как преобразовать сегодняшний объект datetime в строку, соответствующую формату YYYYMMDD (год, месяц, день). Второй пример лучше. Здесь мы присваиваем сегодняшний объект datetime переменной под названием today, а затем пробуем две различные операции форматирования строки. Первая добавляет прямые косые черты между элементами datetime, а также переставляет их так, чтобы получились месяц, день, год. Последний пример создает своеобразную временную метку, которая имеет довольно типичный формат: YYYY-MM-DD.HH.MM.SS. Если вы хотите перейти к двузначному году, вы можете заменить %Y на %y.
datetime.timedelta Объект datetime.timedelta представляет длительность времени. Другими словами, это разница между двумя датами или временем. Давайте рассмотрим простой пример:
\u0026gt;\u0026gt;\u0026gt; now = datetime.datetime.now() \u0026gt;\u0026gt;\u0026gt; now datetime.datetime(2014, 3, 5, 18, 13, 51, 230000) \u0026gt;\u0026gt;\u0026gt; then = datetime.datetime(2014, 2, 26) \u0026gt;\u0026gt;\u0026gt; delta = now - then \u0026gt;\u0026gt;\u0026gt; type(delta) \u0026lt;type 'datetime.timedelta'\u0026gt; \u0026gt;\u0026gt;\u0026gt; delta.days 7 \u0026gt;\u0026gt;\u0026gt; delta.seconds 65631 Здесь мы создаем два объекта datetime. Один для сегодняшнего дня, а другой - для недели назад. Затем мы берем разницу между ними. Это возвращает объект timedelta, который мы можем использовать, чтобы узнать количество дней или секунд между двумя датами. Если вам нужно узнать количество часов или минут между двумя датами, вам придется прибегнуть к математическим вычислениям. Вот один из способов сделать это:
\u0026gt;\u0026gt;\u0026gt; seconds = delta.total_seconds() \u0026gt;\u0026gt;\u0026gt; hours = seconds // 3600 \u0026gt;\u0026gt;\u0026gt; hours 186.0 \u0026gt;\u0026gt;\u0026gt; minutes = (seconds % 3600) // 60 \u0026gt;\u0026gt;\u0026gt; minutes 13.0 Это говорит нам о том, что в неделе 186 часов и 13 минут. Обратите внимание, что в качестве оператора деления мы используем двойную прямую косую черту. Это известно как floor division.
Теперь мы готовы двигаться дальше и узнать немного о модуле time!
Модуль time Модуль time предоставляет разработчику Python доступ к различным функциям, связанным со временем. Модуль времени основан на так называемой эпохе - точке, с которой начинается отсчет времени. Для Unix-систем эпохой является 1970 год. Чтобы узнать, какая эпоха в вашей системе, попробуйте выполнить следующее:
\u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; time.gmtime(0) time.struct_time(tm_year=1970, tm_mon=1, tm_mday=1, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=3, tm_yday=1, tm_isdst=0) Я запустил эту программу на Windows 7, и она тоже считает, что время началось в 1970 году. В любом случае, в этом разделе мы будем изучать следующие функции, связанные со временем:
time.ctime time.sleep time.strftime time.time Давайте начнем!
time.ctime Функция time.ctime преобразует время в секундах начиная с эпохи в строку, показывающую местное время. Если вы не передадите ей ничего, то будет возвращено текущее время. Давайте опробуем пару примеров:
\u0026gt;\u0026gt;\u0026gt; import time \u0026gt;\u0026gt;\u0026gt; time.ctime() 'Thu Mar 06 07:28:48 2014' \u0026gt;\u0026gt;\u0026gt; time.ctime(1384112639) 'Sun Nov 10 13:43:59 2013' Здесь показаны результаты вызова ctime , как со случайным набором секунд, начиная с эпохи, так и с пустным значением. Я видел, как подобные вещи используются, когда кто-то сохраняет дату в виде секунд от эпохи, а затем хочет преобразовать ее во что-то понятное человеку. Немного проще, сохранить большое целое число (или длинное) в базу данных, чем страдать над ним, форматируя объект datetime в какой-либо объект даты, который принимает база данных. Конечно, это также имеет свой недостаток: вам, возможно, нужно будет преобразовать целое число или значение с плавающей запятой обратно в строку.
time.sleep Функция time.sleep дает разработчику возможность приостановить выполнение вашего сценария на заданное количество секунд. Это как добавление паузы в вашу программу.Я нашел этот класс особенно полезным, когда мне нужно было подождать несколько секунд, пока закроется файл, или база данных закончит выполнять свою задачу. Давайте взглянем на пример. Откройте новое окно в IDLE и сохраните следующий код:
import time for x in range(5): time.sleep(2) print(\u0026quot;Slept for 2 seconds\u0026quot;) Теперь запустите код в IDLE. Это можно сделать, перейдя в меню Run, а затем выбрав пункт меню Run module. После этого вы увидите, как модуль выведет фразу Slept for 2 seconds пять раз с двухсекундной паузой между каждой печатью. Это действительно так просто в использовании!
##time.strftime
Модуль time имеет функцию strftime, которая работает практически так же, как и версия datetime. Разница в основном в том, что она принимает на вход: кортеж или объект struct_time, подобный тем, которые возвращаются при вызове time.gmtime() или time.localtime(). Вот небольшой пример:
\u0026gt;\u0026gt;\u0026gt; time.strftime(\u0026quot;%Y-%m-%d-%H.%M.%S\u0026quot;, time.localtime()) '2014-03-06-20.35.56 Этот код очень похож на код временной метки, который мы создали в части datetime этой главы. Я думаю, что метод datetime немного более интуитивен, поскольку вы просто создаете объект datetime.datetime и затем вызываете его метод strftime с нужным вам форматом. В модуле time вам нужно передать формат плюс кортеж времени. Вы сами решаете, какой из этих вариантов наиболее удобен для вас.
time.time Функция time.time возвращает время в секундах от эпохи в виде числа с плавающей точкой. Давайте посмотрим:
\u0026gt;\u0026gt;\u0026gt; time.time() 1394199262.318 Весьма просто. Это можно использовать, когда вы хотите сохранить текущее время в базу данных, но не хотите возиться с преобразованием его в метод datetime базы данных. Вы также можете вспомнить, что метод ctime принимает время в секундах, поэтому мы можем использовать time.time, чтобы получить количество секунд, которое нужно передать в ctime, вот так:
\u0026gt;\u0026gt;\u0026gt; time.ctime(time.time()) 'Fri Mar 07 07:36:38 2014' Если вы немного покопаетесь в документации к модулю времени или просто немного поэкспериментируете с ним, вы, вероятно, найдете еще несколько вариантов использования этой функции.
Подведение итогов На данном этапе вы должны знать, как работать с датами и временем с помощью стандартных модулей Python. Python дает вам много возможностей, когда дело доходит до работы с датами. Эти модули пригодятся вам, если вам понадобится создать приложение, которое будет отслеживать назначенные встречи или должно работать в определенные дни. Они также полезны при работе с базами данных.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter22_time/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day22/":{title:"22. Открытая сетевая модель OSI",tags:["devops"],content:`Модель OSI — 7 уровней Общая цель сети как отрасли состоит в том, чтобы позволить двум хостам обмениваться данными. Если я хочу передать данные от одного хоста к другому хосту, мне нужно будет что-то подключить к этому хосту, перейти к другому хосту, подключить его к первому хосту.
Сеть позволяет нам автоматизировать это, позволяя хосту автоматически обмениваться данными по сети, и для этого эти хосты должны следовать набору правил.
Это ничем не отличается от любого другого языка. У английского есть набор правил, которым должны следовать два носителя английского языка. У испанского есть свой собственный набор правил.
Правила организации сети разделены на семь разных уровней, и эти уровни известны как модель OSI.
Введение в модель OSI Модель OSI (модель взаимодействия открытых систем)/(Open Systems Interconnection Model) — это структура, используемая для описания функций сетевой системы. Модель OSI характеризует вычислительные функции в виде универсального набора правил и требований для обеспечения функциональной совместимости между различными продуктами и программным обеспечением. В эталонной модели OSI обмен данными между вычислительной системой разделен на семь различных уровней абстракции: физический, канальный, сетевой, транспортный, сеансовый, презентационный и прикладной (Physical, Data Link, Network, Transport, Session, Presentation, Application). Физический Уровень 1 в модели OSI, известный как физический, предполагает возможность передачи данных с одного хоста на другой с помощью средств, будь то физический кабель или мы также можем рассмотреть Wi-Fi на этом уровне. Мы также можем увидеть здесь более устаревшее оборудование вокруг концентраторов и повторителей для передачи данных с одного хоста на другой. Канал передачи данных Уровень 2, канал передачи данных обеспечивает передачу данных от узла к узлу, где данные упакованы в кадры. Существует также уровень исправления ошибок, которые могли возникнуть на физическом уровне. Здесь мы также вводим или впервые видим MAC-адреса.
Здесь мы видим первое упоминание о коммутаторах, о которых мы рассказали в первый день нашей работы с сетью День 21 Сеть Вы, вероятно, слышали термин «коммутаторы уровня 3» или «коммутаторы уровня 2». В нашей модели OSI уровень 3. Цель сети — прямая(end to end) доставка, именно здесь мы видим наши IP-адреса, также упомянутые в обзоре первого дня.
Маршрутизаторы и хосты существуют на уровне 3, помните, что маршрутизатор — это возможность маршрутизации между несколькими сетями. Все, что имеет IP, может считаться уровнем 3. Так зачем же нам нужны схемы адресации как на уровне 2, так и на уровне 3? (MAC-адреса и IP-адреса)
Если мы подумаем о передаче данных с одного хоста на другой, каждый хост имеет IP-адрес, но между ними есть несколько коммутаторов и маршрутизаторов. Каждое из устройств имеет этот MAC-адрес уровня 2.
MAC-адрес уровня 2 будет передаваться только от хоста к коммутатору/маршрутизатору, он ориентирован на переходы, где IP-адреса уровня 3 будут оставаться с этим пакетом данных, пока он не достигнет своего конечного хоста. (Концы с концами)
IP-адреса — уровень 3 = сквозная доставка
MAC-адреса — уровень 2 = доставка между переходами
Теперь есть сетевой протокол, который мы рассмотрим, но не сегодня, называемый ARP (протокол разрешения адресов), который связывает наши адреса Layer3 и Layer2.
Транспорт Предоставление услуг между услугами, уровень 4 предназначен для различения потоков данных. Точно так же, как уровни 3 и 2 имели свои схемы адресации, на уровне 4 у нас есть порты.
Сессия, Презентация, Приложение Различие между слоями 5, 6, 7 немного расплывчато
Стоит взглянуть на IP-модель TCP, чтобы получить более свежее представление.
Давайте теперь попробуем объяснить, что на самом деле происходит, когда хосты общаются друг с другом, используя этот сетевой стек. На одном хосте есть приложение, которое будет генерировать данные, предназначенные для отправки на другой хост.
Исходный хост будет проходить так называемый процесс инкапсуляции. Эти данные будут сначала отправлены на уровень 4.
Уровень 4 добавит заголовок к этим данным, что может облегчить задачу уровня 4, которая заключается в доставке услуг. Это будет порт, использующий либо TCP, либо UDP. Он также будет включать исходный порт и порт назначения.
Это также может быть известно как сегмент (данные и порт).
Этот сегмент будет передан по стеку osi на уровень 3, сетевой уровень, сетевой уровень добавит к этим данным еще один заголовок. Этот заголовок будет способствовать цели уровня 3, который является сквозной доставкой, что означает, что в этом заголовке у вас будет IP-адрес источника и IP-адрес назначения, заголовок плюс данные также могут называться пакетом.
Затем уровень 3 возьмет этот пакет и передаст его уровню 2, уровень 2 еще раз добавит еще один заголовок к этим данным для достижения цели уровня 2 по доставке переходов, что означает, что этот заголовок будет включать в себя MAC-адреса источника и получателя. Это называется кадром, когда у вас есть заголовок и данные уровня 2.
Затем этот кадр преобразуется в единицы и нули и отправляется по физическому кабелю уровня 1 или Wi-Fi.
Выше я упомянул названия для каждого уровня заголовка и данных, но решил нарисовать и это.
Очевидно, что приложение, отправляющее данные, отправляется куда-то, поэтому получение происходит в обратном порядке, чтобы получить эту резервную копию в стеке и на принимающем хосте. Ресурсы Computer Networking full course Practical Networking `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day22/"},"https://romankurnovskii.com/ru/docs/python101/chapter23_xml/":{title:"23. Модуль xml",tags:[],content:`Python имеет встроенные возможности разбора XML, доступ к которым можно получить с помощью модуля xml. В этой статье мы сосредоточимся на двух подмодулях модуля xml:
minidom ElementTree . Мы начнем с minidom просто потому, что этот метод раньше был де-факто методом разбора XML. Затем мы рассмотрим, как вместо него использовать ElementTree.
Работа с minidom Для начала необходимо разобрать XML. Взгляните на следующий короткий пример XML:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Это довольно типичный XML, и на самом деле он довольно интуитивно понятен для чтения. В будущем вы, скорее всего, столкнетесь с более сложным примером XML, так что в нашем случае мы работаем с очень удобным материалом. В любом случае, сохраните описанный выше код XML под следующим названием: appt.xml
Давайте потратим некоторое время на ознакомление с тем, как разобрать этот файл с помощью модуля Python minidom. Это довольно длинный кусок кода, так что приготовьтесь.
import xml.dom.minidom import urllib.request class ApptParser(object): def __init__(self, url, flag='url'): self.list = [] self.appt_list = [] self.flag = flag self.rem_value = 0 xml = self.getXml(url) self.handleXml(xml) def getXml(self, url): try: print(url) f = urllib.request.urlopen(url) except: f = url doc = xml.dom.minidom.parse(f) node = doc.documentElement if node.nodeType == xml.dom.Node.ELEMENT_NODE: print('Element name: %s' % node.nodeName) for (name, value) in node.attributes.items(): print(' Attr -- Name: %s Value: %s' % (name, value)) return node def handleXml(self, xml): rem = xml.getElementsByTagName('zAppointments') appointments = xml.getElementsByTagName(\u0026quot;appointment\u0026quot;) self.handleAppts(appointments) def getElement(self, element): return self.getText(element.childNodes) def handleAppts(self, appts): for appt in appts: self.handleAppt(appt) self.list = [] def handleAppt(self, appt): begin = self.getElement(appt.getElementsByTagName(\u0026quot;begin\u0026quot;)[0]) duration = self.getElement(appt.getElementsByTagName(\u0026quot;duration\u0026quot;)[0]) subject = self.getElement(appt.getElementsByTagName(\u0026quot;subject\u0026quot;)[0]) location = self.getElement(appt.getElementsByTagName(\u0026quot;location\u0026quot;)[0]) uid = self.getElement(appt.getElementsByTagName(\u0026quot;uid\u0026quot;)[0]) self.list.append(begin) self.list.append(duration) self.list.append(subject) self.list.append(location) self.list.append(uid) if self.flag == 'file': try: state = self.getElement(appt.getElementsByTagName(\u0026quot;state\u0026quot;)[0]) self.list.append(state) alarm = self.getElement(appt.getElementsByTagName(\u0026quot;alarmTime\u0026quot;)[0]) self.list.append(alarm) except Exception as e: print(e) self.appt_list.append(self.list) def getText(self, nodelist): rc = \u0026quot;\u0026quot; for node in nodelist: if node.nodeType == node.TEXT_NODE: rc = rc + node.data return rc if __name__ == \u0026quot;__main__\u0026quot;: appt = ApptParser(\u0026quot;appt.xml\u0026quot;) print(appt.appt_list) Этот код основан на примере из документации Python, и я должен признать, что моя мутация этого кода кажется мне немного уродливой. Давайте немного разберем этот код. Параметр url, который вы видите в классе ApptParser, может быть либо url, либо файлом. В методе getXml мы используем обработчик исключений, чтобы попытаться открыть url. Если он выдает ошибку, то мы считаем, что url - это путь к файлу. Далее мы используем метод minidom\u0026rsquo;s parse для разбора XML. Затем мы извлекаем узел из XML. Мы проигнорируем условие, поскольку оно не имеет значения для данного обсуждения. Наконец, мы возвращаем объект node.
Технически, node - это XML, и мы передаем его методу handleXml. Чтобы получить все экземпляры назначений в XML, мы делаем следующее:
xml.getElementsByTagName(\u0026quot;appointment\u0026quot;). Затем мы передаем эту информацию в метод handleAppts. Это большой объем информации. Возможно, было бы неплохо немного отрефакторить этот код, чтобы вместо передачи информации он просто устанавливал переменные класса, а затем вызывал следующий метод без каких-либо аргументов. Я оставлю это в качестве упражнения для читателя. В любом случае, все, что делает метод handleAppts, это перебирает каждую встречу и вызывает метод handleAppt, чтобы извлечь из нее некоторую дополнительную информацию, добавить данные в список и добавить этот список в другой список. Идея заключалась в том, чтобы в итоге получить список списков, содержащих все необходимые данные о моих встречах.
Вы заметите, что метод handleAppt вызывает метод getElement, который вызывает метод getText. Технически, вы можете пропустить вызов getElement и просто вызвать getText напрямую. С другой стороны, вам может потребоваться дополнительная обработка getElement для преобразования текста в другой тип перед возвращением его обратно. Например, вы можете захотеть преобразовать числа в целые числа, плавающие или объекты decimal.Decimal.
Давайте попробуем еще один пример с minidom, прежде чем двигаться дальше. Мы будем использовать пример XML с сайта Microsoft MSDN: http://msdn.microsoft.com/en-us/library/ms762271%28VS.85%29.aspx. Сохраните следующий XML как example.xml
\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;catalog\u0026gt; \u0026lt;book id=\u0026quot;bk101\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Gambardella, Matthew\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;XML Developer's Guide\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Computer\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;44.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-10-01\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;An in-depth look at creating applications with XML.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026quot;bk102\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Ralls, Kim\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Midnight Rain\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Fantasy\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-16\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026quot;bk103\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Corets, Eva\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Maeve Ascendant\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Fantasy\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-11-17\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/catalog\u0026gt; В данном примере мы просто разберем XML, извлечем названия книг и выведем их в stdout. Вот код:
import xml.dom.minidom as minidom def getTitles(xml): \u0026quot;\u0026quot;\u0026quot; Print out all titles found in xml \u0026quot;\u0026quot;\u0026quot; doc = minidom.parse(xml) node = doc.documentElement books = doc.getElementsByTagName(\u0026quot;book\u0026quot;) titles = [] for book in books: titleObj = book.getElementsByTagName(\u0026quot;title\u0026quot;)[0] titles.append(titleObj) for title in titles: nodes = title.childNodes for node in nodes: if node.nodeType == node.TEXT_NODE: print(node.data) if __name__ == \u0026quot;__main__\u0026quot;: document = 'example.xml' getTitles(document) Этот код - всего лишь одна короткая функция, принимающая один аргумент - XML-файл. Мы импортируем модуль minidom и даем ему такое же имя, чтобы на него было легче ссылаться. Затем мы разбираем XML. Первые две строки в функции практически такие же, как и в предыдущем примере. Мы используем метод getElementsByTagName для захвата нужных нам частей XML, затем перебираем результаты и извлекаем из них названия книг. На самом деле извлекаются объекты заголовков, поэтому нам нужно также выполнить итерацию и извлечь обычный текст, для чего мы используем вложенный цикл for.
Теперь давайте уделим немного времени на то, чтобы опробовать другой подмодуль модуля xml под названием ElementTree.
Парсинг с помощью ElementTree В этом разделе вы узнаете, как создать XML-файл, отредактировать XML и разобрать XML с помощью ElementTree. Для сравнения мы будем использовать тот же XML, который мы использовали в предыдущем разделе, чтобы проиллюстрировать различия между использованием minidom и ElementTree. Вот исходный XML:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Давайте начнем с изучение того, как создать этот фрагмент XML программно с помощью Python!
Как создать XML с помощью ElementTree Создать XML с помощью ElementTree очень просто. В этом разделе мы попытаемся создать приведенный выше XML с помощью Python. Вот код:
import xml.etree.ElementTree as xml def createXML(filename): \u0026quot;\u0026quot;\u0026quot; Create an example XML file \u0026quot;\u0026quot;\u0026quot; root = xml.Element(\u0026quot;zAppointments\u0026quot;) appt = xml.Element(\u0026quot;appointment\u0026quot;) root.append(appt) # add appointment children begin = xml.SubElement(appt, \u0026quot;begin\u0026quot;) begin.text = \u0026quot;1181251680\u0026quot; uid = xml.SubElement(appt, \u0026quot;uid\u0026quot;) uid.text = \u0026quot;040000008200E000\u0026quot; alarmTime = xml.SubElement(appt, \u0026quot;alarmTime\u0026quot;) alarmTime.text = \u0026quot;1181572063\u0026quot; state = xml.SubElement(appt, \u0026quot;state\u0026quot;) location = xml.SubElement(appt, \u0026quot;location\u0026quot;) duration = xml.SubElement(appt, \u0026quot;duration\u0026quot;) duration.text = \u0026quot;1800\u0026quot; subject = xml.SubElement(appt, \u0026quot;subject\u0026quot;) tree = xml.ElementTree(root) with open(filename, \u0026quot;w\u0026quot;) as fh: tree.write(fh) if __name__ == \u0026quot;__main__\u0026quot;: createXML(\u0026quot;appt.xml\u0026quot;) Если вы запустите этот код, вы должны получить что-то вроде нижеизложенного (возможно, все в одной строке):
\u0026lt;zAppointments\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state /\u0026gt; \u0026lt;location /\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject /\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Это довольно близко к оригиналу и, безусловно, является действительным XML. Хотя это не совсем то же самое, но достаточно близко. Давайте уделим немного времени коду и убедимся, что мы его поняли. Сначала мы создаем корневой элемент с помощью функции ElementTree\u0026rsquo;s Element. Затем мы создаем элемент назначения и добавляем его к корню. Затем мы создаем подэлементы, передавая объект элемента назначения (appt) в SubElement вместе с именем, например \u0026ldquo;begin\u0026rdquo;. Затем для каждого подэлемента мы устанавливаем свойство text, чтобы придать ему значение. В конце сценария мы создаем ElementTree и используем его для записи XML в файл.
Теперь мы готовы узнать, как редактировать этот файл!
Как редактировать XML с помощью ElementTree Редактировать XML с помощью ElementTree также просто. Однако, чтобы сделать все немного интереснее, мы добавим в XML еще один блок назначений:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181253977\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;sdlkjlkadhdakhdfd\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181588888\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;TX\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;Dallas\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Теперь давайте напишем код, чтобы изменить значение каждого из тегов begin с секунд с момента эпохи на что-то более удобочитаемое. Для этого мы воспользуемся модулем time Python:
import time import xml.etree.cElementTree as ET def editXML(filename): \u0026quot;\u0026quot;\u0026quot; Edit an example XML file \u0026quot;\u0026quot;\u0026quot; tree = ET.ElementTree(file=filename) root = tree.getroot() for begin_time in root.iter(\u0026quot;begin\u0026quot;): begin_time.text = time.ctime(int(begin_time.text)) tree = ET.ElementTree(root) with open(\u0026quot;updated.xml\u0026quot;, \u0026quot;w\u0026quot;) as f: tree.write(f) if __name__ == \u0026quot;__main__\u0026quot;: editXML(\u0026quot;original_appt.xml\u0026quot;) Здесь мы создаем объект ElementTree (дерево) и извлекаем из него root. Затем мы используем метод iter() ElementTree, чтобы найти все теги с меткой \u0026ldquo;begin\u0026rdquo;. Обратите внимание, что метод iter() был добавлен в Python 2.7. В нашем цикле for мы устанавливаем свойство text каждого элемента в более удобочитаемый для человека формат времени с помощью time.ctime(). Обратите внимание, что при передаче строки в ctime нам пришлось преобразовать ее в целое число. Результат должен выглядеть примерно следующим образом:
\u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;Thu Jun 07 16:28:00 2007\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state /\u0026gt; \u0026lt;location /\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;Thu Jun 07 17:06:17 2007\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;sdlkjlkadhdakhdfd\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181588888\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;TX\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;Dallas\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Вы также можете использовать методы find() или findall() ElementTree для поиска определенных тегов в вашем XML. Метод find() просто найдет первый экземпляр, тогда как findall() найдет все теги с указанной меткой. Эти методы полезны для редактирования или для разбора, о чем мы поговорим в следующей теме!
Как разобрать XML с помощью ElementTree Теперь мы научимся выполнять базовый разбор с помощью ElementTree. Сначала мы прочитаем код, а затем пройдемся по нему шаг за шагом, чтобы понять его. Обратите внимание, что этот код основан на первоначальном примере, но он должен работать и на втором.
import xml.etree.cElementTree as ET def parseXML(xml_file): \u0026quot;\u0026quot;\u0026quot; Parse XML with ElementTree \u0026quot;\u0026quot;\u0026quot; tree = ET.ElementTree(file=xml_file) print(tree.getroot()) root = tree.getroot() print(\u0026quot;tag=%s, attrib=%s\u0026quot; % (root.tag, root.attrib)) for child in root: print(child.tag, child.attrib) if child.tag == \u0026quot;appointment\u0026quot;: for step_child in child: print(step_child.tag) # iterate over the entire tree print(\u0026quot;-\u0026quot; * 40) print(\u0026quot;Iterating using a tree iterator\u0026quot;) print(\u0026quot;-\u0026quot; * 40) iter_ = tree.getiterator() for elem in iter_: print(elem.tag) # get the information via the children! print(\u0026quot;-\u0026quot; * 40) print(\u0026quot;Iterating using getchildren()\u0026quot;) print(\u0026quot;-\u0026quot; * 40) appointments = root.getchildren() for appointment in appointments: appt_children = appointment.getchildren() for appt_child in appt_children: print(\u0026quot;%s=%s\u0026quot; % (appt_child.tag, appt_child.text)) if __name__ == \u0026quot;__main__\u0026quot;: parseXML(\u0026quot;appt.xml\u0026quot;) Вы уже должны понять, что и как работает, но в этом примере и предыдущем мы импортируем cElementTree вместо обычного ElementTree. Разница между этими двумя в том, что cElementTree основан на С, а не на Python, так что он намного быстрее. В любом случае, мы снова создаем объект ElementTree и извлекаем root из него. Обратите внимание на то, что мы выводим root, его тег и атрибуты. Далее мы покажем несколько способов итерации тегов. Первый цикл просто итерирует XML, дочку за дочкой. Таким образом выведется только дочерний код (назначение) с наивысшим уровнем , так что мы добавили оператор if, чтобы проверить дочерний код и выполнить его итерацию.
Далее мы возьмем итератор из самого объекта дерева и выполним итерацию таким образом. Вы получите ту же информацию, но без лишних шагов в первом примере. Третий метод использует функцию getchildren() корня. Здесь снова нужен внутренний цикл, чтобы перебрать все дочерние элементы внутри каждого тега назначения. Последний пример использует метод iter() корня, чтобы просто перебрать все теги, которые соответствуют строке \u0026ldquo;begin\u0026rdquo;.
Как упоминалось в предыдущем разделе, вы также можете использовать find() или findall(), чтобы помочь вам найти определенные теги или наборы тегов соответственно. Также обратите внимание, что каждый объект Element имеет свойство tag и свойство text, которые можно использовать для получения точной информации.
Подведение итогов Теперь вы знаете, как использовать minidom для разбора XML. Вы также узнали, как использовать ElementTree для создания, редактирования и разбора XML. Существуют и другие библиотеки вне Python, которые предоставляют дополнительные методы работы с XML. Убедитесь в том, что вы пользуетесь понятным вам инструментом, так как данный вопрос может быть очень сложным и непонятным, если пытаться решить его неправильным инструментом.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter23_xml/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day23/":{title:"23. Протоколы сети",tags:["devops"],content:`Протоколы сети Набор правил и сообщений, образующих стандарт.
ARP - Address Resolution Protocol - протокол разрешения адресов Если вы хотите по-настоящему разобраться в ARP, вы можете прочитать Internet Standard здесь RFC 826
ARP соединяет IP-адреса с фиксированными физическими адресами машин, также известными как MAC-адреса, в сети уровня 2.
FTP - File Transfer Protocol - протокол передачи файлов Позволяет передавать файлы из источника в место назначения. Как правило, этот процесс аутентифицируется, но при настройке можно использовать анонимный доступ. Теперь вы будете чаще видеть FTPS, который обеспечивает подключение SSL/TLS к FTP-серверам от клиента для повышения безопасности. Этот протокол можно найти на прикладном уровне модели OSI.
SMTP - Simple Mail Transfer Protocol - протокол передачи почты Почтовые серверы, используемые для передачи электронной почты, используют SMTP для отправки и получения почтовых сообщений. Вы по-прежнему обнаружите, что даже с Microsoft 365 протокол SMTP используется для той же цели.
HTTP - Hyper Text Transfer Protocol - Протокол передачи гипертекста HTTP является основой Интернета и просмотра контента. Дает нам возможность легко получить доступ к нашим любимым веб-сайтам. HTTP по-прежнему широко используется, но HTTPS используется или должен использоваться на большинстве ваших любимых сайтов.
SSL - Secure Sockets Layer | TLS - Transport Layer Security - Уровень защищенных сокетов | TLS — безопасность транспортного уровня TLS заменил SSL, TLS — это криптографический протокол, который обеспечивает безопасность связи по сети. Его можно найти в почте, мессенджерах и других приложениях, но чаще всего он используется для защиты HTTPS.
HTTPS - HTTP secured with SSL/TLS - HTTP, защищенный с помощью SSL/TLS Расширение HTTP, используемое для безопасной связи по сети, HTTPS шифруется с помощью TLS, как упоминалось выше. Основное внимание здесь уделялось обеспечению аутентификации, конфиденциальности и целостности при обмене данными между хостами.
DNS - Domain Name System - система доменных имен DNS используется для сопоставления удобных для человека доменных имен, например, все мы знаем google.com, но если вы откроете браузер и введете 8.8.8.8 вы получите Google в том виде, в каком мы его знаем. Однако удачи вам в попытках запомнить все IP-адреса всех ваших веб-сайтов, на некоторых из них мы даже используем Google для поиска информации.
Именно здесь в дело вступает DNS, он гарантирует доступность хостов, служб и других ресурсов.
На всех хостах, если им требуется подключение к Интернету, они должны иметь DNS, чтобы иметь возможность разрешать эти доменные имена. DNS — это область, на изучение которой вы можете потратить дни и годы. Я бы также сказал по опыту, что DNS в основном является распространенной причиной всех ошибок, когда речь идет о сети. Однако не уверен, что сетевой инженер согласится с этим.
DHCP - Dynamic Host Configuration Protocol - Протокол динамического конфигурирования сервера Мы много обсуждали протоколы, необходимые для работы наших хостов, будь то доступ в Интернет или передача файлов между собой.
На каждом хосте нам нужны 4 вещи, чтобы он мог выполнять обе эти задачи.
IP Address Subnet Mask Default Gateway DNS Мы рассмотрели IP-адрес, являющийся уникальным адресом для вашего хоста в сети, в которой он находится, мы можем думать об этом как о нашем домашнем номере.
Маску подсети мы скоро рассмотрим, но вы можете думать об этом как о почтовом индексе или почтовом индексе.
Шлюз по умолчанию — это IP-адрес нашего маршрутизатора, как правило, в нашей сети, предоставляющий нам возможность подключения уровня 3. Вы могли бы думать об этом как о единственной дороге, которая позволяет нам покинуть нашу улицу.
Затем у нас есть DNS, как мы только что рассмотрели, чтобы помочь нам преобразовать сложные общедоступные IP-адреса в более подходящие и запоминающиеся доменные имена. Может быть, мы можем думать об этом как о гигантском сортировочном офисе, чтобы убедиться, что мы получаем правильный пост.
Как я уже сказал, каждому хосту требуются эти 4 вещи, если у вас 1000 или 10 000 хостов, вам потребуется очень много времени, чтобы определить каждый из них по отдельности. Здесь в дело вступает DHCP, который позволяет вам определить область действия вашей сети, а затем этот протокол будет распространяться на все доступные хосты в вашей сети.
Другой пример: вы идете в кафе, берете кофе и садитесь за свой ноутбук, или ваш телефон позволяет назвать это вашим хостом. Вы подключаете свой хост к Wi-Fi в кофейне, и вы получаете доступ к Интернету, сообщения и почта начинают пинговаться, и вы можете просматривать веб-страницы и социальные сети. Когда вы подключались к Wi-Fi в кофейне, ваша машина получала DHCP-адрес либо от выделенного DHCP-сервера, либо, скорее всего, от маршрутизатора, который также обрабатывает DHCP.
Subnetting - Подсети Подсеть — это логическое подразделение IP-сети.
Подсети разбивают большие сети на более мелкие, более управляемые сети, которые работают более эффективно.
Каждая подсеть является логическим подразделением большей сети. Подключенные устройства с достаточным количеством подсетей имеют общий идентификатор IP-адреса, что позволяет им взаимодействовать друг с другом.
Маршрутизаторы управляют связью между подсетями.
Размер подсети зависит от требований к подключению и используемой сетевой технологии.
Организация несет ответственность за определение своего количества и размера подсетей в пределах адресного пространства. доступны, и детали остаются локальными для этой организации. Подсети также могут быть сегментированы на еще более мелкие подсети для таких вещей, как соединения «точка-точка», или для подсетей, поддерживающих несколько устройств.
Среди прочих преимуществ сегментация крупных сети в подсети включает IP-адрес перераспределение и уменьшает перегрузку сети, оптимизацию, сетевую связь и эффективность.
Подсети также могут повысить безопасность сети. Если часть сети скомпрометирована, ее можно поместить в карантин, что затруднит перемещение злоумышленников по более крупной сети.
Ресурсы Computer Networking full course Practical Networking `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day23/"},"https://romankurnovskii.com/ru/docs/python101/chapter24_debugging/":{title:"24. Отладчик Python",tags:[],content:`Python поставляется с собственным модулем отладчика, который называется pdb. Этот модуль предоставляет интерактивный отладчик исходного кода для ваших программ на Python. Вы можете устанавливать брейкпоинты, просматривать код, изучать кадры стека и многое другое. Мы рассмотрим следующие аспекты этого модуля:
Как запустить отладчик Переход по коду Установка точек останова Давайте начнем с создания небольшого фрагмента кода, чтобы попробовать отладку. Вот глупый пример:
# debug_test.py def doubler(a): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; result = a*2 print(result) return result def main(): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; for i in range(1,10): doubler(i) if __name__ == \u0026quot;__main__\u0026quot;: main() Теперь давайте узнаем, как запустить отладчик на этом фрагменте кода.
Как запустить отладчик Вы можете запустить отладчик тремя различными способами. Первый - просто импортировать его и вставить pdb.set_trace() в свой код, чтобы запустить отладчик. Вы можете импортировать отладчик в IDLE и заставить его запустить ваш модуль. Или можете вызвать отладчик в командной строке. В этом разделе мы сосредоточимся на двух последних методах. Мы начнем с использования отладчика в интерпретаторе (IDLE). Откройте терминал (окно командной строки) и перейдите к месту, где вы сохранили приведенный выше пример кода. Затем запустите Python. Теперь сделайте следующее:
\u0026gt;\u0026gt;\u0026gt; import debug_test \u0026gt;\u0026gt;\u0026gt; import pdb \u0026gt;\u0026gt;\u0026gt; pdb.run('debug_test.main()') \u0026gt; \u0026lt;string\u0026gt;(1)\u0026lt;module\u0026gt;() (Pdb) continue 2 4 6 8 10 12 14 16 18 Здесь мы импортируем наш модуль и pdb. Затем мы выполняем метод pdb run и говорим ему вызвать метод main нашего модуля. Это вызывает подсказку отладчика. Здесь мы набрали continue, чтобы сказать ему, что нужно продолжить выполнение сценария. Вы также можете набрать букву c в качестве сокращения для continue. При наборе continue отладчик продолжит выполнение до тех пор, пока не достигнет точки останова или пока сценарий не завершится.
Другой способ запустить отладчик - выполнить следующую команду в терминале:
python -m pdb debug_test.py Если вы запустите его таким образом, вы увидите немного другой результат:
-\u0026gt; def doubler(a): (Pdb) c 2 4 6 8 10 12 14 16 18 The program finished and will be restarted Вы заметите, что в этом примере мы использовали c вместо continue. Вы также заметите, что отладчик перезапускается в конце. Это сохраняет состояние отладчика (например, точки останова) и может быть более полезным, чем остановка отладчика. Иногда вам потребуется просмотреть код несколько раз, чтобы понять, что в нем не так.
Давайте копнем немного глубже и узнаем, как пройтись по коду.
Пошаговый просмотр кода Если вы хотите просмотреть код по одной строке за раз, вы можете использовать команду step (или просто \u0026ldquo;s\u0026rdquo;). Вот сессия для вашего удовольствия:
C:\\Users\\mike\u0026gt;cd c:\\py101 c:\\py101\u0026gt;python -m pdb debug_test.py \u0026gt; c:\\py101\\debug_test.py(4)\u0026lt;module\u0026gt;() -\u0026gt; def doubler(a): (Pdb) step \u0026gt; c:\\py101\\debug_test.py(11)\u0026lt;module\u0026gt;() -\u0026gt; def main(): (Pdb) s \u0026gt; c:\\py101\\debug_test.py(16)\u0026lt;module\u0026gt;() -\u0026gt; if __name__ == \u0026quot;__main__\u0026quot;: (Pdb) s \u0026gt; c:\\py101\\debug_test.py(17)\u0026lt;module\u0026gt;() -\u0026gt; main() (Pdb) s --Call-- \u0026gt; c:\\py101\\debug_test.py(11)main() -\u0026gt; def main(): (Pdb) next \u0026gt; c:\\py101\\debug_test.py(13)main() -\u0026gt; for i in range(1,10): (Pdb) s \u0026gt; c:\\py101\\debug_test.py(14)main() -\u0026gt; doubler(i) (Pdb) Здесь мы запускаем отладчик и говорим ему, что нужно войти в код. Он начинает сверху и проходит через первые два определения функций. Затем он доходит до условной и обнаруживает, что должен выполнить функцию main. Мы переходим в главную функцию, а затем используем команду next. Команда next выполнит вызываемую функцию, если встретит ее без перехода в нее. Если вы хотите сделать шаг в вызываемую функцию, то достаточно использовать команду step.
Когда вы видите строку типа \u0026gt; c:py101debug_test.py(13)main(), обратите внимание на число в круглых скобках. Это число - номер текущей строки в коде.
Вы можете использовать команду args (или a) для вывода текущего списка аргументов на экран. Еще одна удобная команда - jump (или j), за которой следует пробел и номер строки, на которую вы хотите \u0026ldquo;перепрыгнуть\u0026rdquo;. Это дает вам возможность пропустить кучу монотонных шагов, чтобы добраться до нужной строки. Это приводит нас к изучению точек останова!
Настройка точек останова Точка останова - это строка в коде, где вы хотите приостановить выполнение. Вы можете установить точку останова, вызвав команду break (или b), за которой следует пробел и номер строки, на которой вы хотите прерваться. Вы также можете дополнить номер строки именем файла и двоеточием, чтобы указать breakpoint в другом файле. Команда break также позволяет установить точку останова с помощью аргумента function. Существует также команда tbreak, которая устанавливает временную точку останова, которая автоматически удаляется при ее достижении.
Вот пример:
c:\\py101\u0026gt;python -m pdb debug_test.py \u0026gt; c:\\py101\\debug_test.py(4)\u0026lt;module\u0026gt;() -\u0026gt; def doubler(a): (Pdb) break 6 Breakpoint 1 at c:\\py101\\debug_test.py:6 (Pdb) c \u0026gt; c:\\py101\\debug_test.py(6)doubler() -\u0026gt; result = a*2 Мы запускаем отладчик, а затем говорим ему установить breakpoint на строке 6. Затем мы продолжаем, и он останавливается на строке 6, как и должно быть. Сейчас самое время проверить список аргументов, чтобы убедиться, что он соответствует вашим ожиданиям. Попробуйте это сделать, набрав args сейчас. Затем выполните еще одно continue и еще один args, чтобы посмотреть, как изменился результат.
Подведение итогов Существует множество других команд, которые вы можете использовать в отладчике. Я рекомендую прочитать документацию, чтобы узнать о других командах. Однако на данном этапе вы должны уметь эффективно использовать отладчик для отладки собственного кода.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter24_debugging/"},"https://romankurnovskii.com/ru/docs/python101/03-part_iii/":{title:"Часть III - Промежуточные вопросы и ответы",tags:[],content:`В третьей части вы узнаете о некоторых внутренних компонентах Python, которые многие относят к владению Python среднего уровня. Вы перешли от молока и готовы к мясу! В этой части мы рассмотрим следующие темы:
Отладка Декораторы Оператор лямбда Профилирование кода Тестирование В первой главе этого раздела вы познакомитесь с модулем отладки Python, pdb, и узнаете, как использовать его для отладки кода. Следующая глава посвящена декораторам. Вы узнаете о том, как их создавать, и о некоторых декораторах, встроенных в Python. В третьей главе мы рассмотрим оператор лямбда, который, по сути, создает однострочную анонимную функцию. Это немного странно, но весело! В четвертой главе речь пойдет о том, как профилировать свой код. Эта дисциплина дает вам возможность найти возможные узкие места в вашем коде, чтобы вы знали, на чем сосредоточиться для оптимизации кода. Последняя глава этого раздела посвящена тестированию кода. В ней вы узнаете, как тестировать свой код с помощью нескольких встроенных модулей Python.
Я думаю, что вы найдете этот раздел очень полезным для продолжения вашего обучения Python.
`,url:"https://romankurnovskii.com/ru/docs/python101/03-part_iii/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day24/":{title:"24. Автоматизация сети",tags:["devops","Ansible"],content:`Автоматизация сети Основы сетевой автоматизации Основные задачи для сетевой автоматизации
Тестирование устройств и проверка конфигурации; Инициализация развернутых физических устройств и сервисов, а также развертывание и инициализация виртуальных устройств; Сбор сетевых данных, относящихся к устройствам, системам, программному обеспечению, топологии сети, трафику и сервисам в реальном времени; Анализ данных, в том числе упреждающая аналитика ИИ и машинного обучения, для обеспечения информации о текущем и будущем поведении сети; Проверка соответствия конфигурации требованиям для обеспечения правильной работы всех сетевых устройств и сервисов; Обновление программного обеспечения, включая откат программного обеспечения при необходимости; Замкнутая коррекция проблем с сетью, включая поиск и устранение неисправностей, а также исправление сложных и трудновыявляемых сбоев; Подробный анализ отчетов, панелей наблюдения, оповещений и предупреждений; Реализация требований безопасности; Мониторинг сети и ее сервисов для поддержания уровня обслуживания и удовлетворенности клиентов Процесс внедрения автоматизации специфичен для каждого бизнеса. Когда дело доходит до развертывания автоматизации, не существует универсального решения. Способность определить и использовать подход, который лучше всего подходит для вашей организации, имеет решающее значение для продвижения к поддержке и созданию более гибкой среды для пользователей. (Мы обсуждали что-то подобное в самом начале в отношении всего DevOps, изменения культуры и автоматизированного процесса, который это приносит)
Чтобы разобраться во всем, вам нужно будет определить, как задача или процесс, которые вы пытаетесь автоматизировать, будут улучшать опыт конечного пользователя или ценность для бизнеса, следуя пошаговому систематическому подходу.
«Если не знаешь, куда идешь, любая дорога приведет тебя туда».
Имея структуру проекта, которую вы пытаетесь достичь, зная, какова ваша конечная цель, а затем шаг за шагом работая над достижением этой цели, измеряйте успех автоматизации на различных этапах на основе бизнес-результатов.
Создавайте концепции, моделируя существующие приложения. Нет необходимости разрабатывать концепции автоматизации в пузыре, потому что их нужно применять к вашему приложению, вашему сервису, вашей инфраструктуре, поэтому начните создавать концепции и моделировать их вокруг вашей существующей инфраструктуры, вы повторно существующие приложения.
Подход к автоматизации сети Мы должны определить задачи и выполнить обнаружение запросов на изменение сети, чтобы у вас были наиболее распространенные проблемы и проблемы, решение которых нужно автоматизировать.
Составьте список всех запросов на изменение и рабочих процессов, которые в настоящее время обрабатываются вручную. Определить наиболее распространенные, трудоемкие и подверженные ошибкам действия. Приоритизируйте запросы, используя бизнес-ориентированный подход. Это основа построения процесса автоматизации, что нужно автоматизировать, а что нет. Затем мы должны разделить задачи и проанализировать, как разные сетевые функции работают и взаимодействуют друг с другом.
Команда инфраструктуры/сети получает заявки на изменения на нескольких уровнях для развертывания приложений. На основе сетевых сервисов разделить их на разные области и понять, как они взаимодействуют друг с другом. Оптимизация приложений ADC (контроллер доставки приложений) (Application Delivery Controller) Межсетевой экран DDI (DNS, DHCP, IPAM и т. д.) Маршрутизация Другие Определите различные зависимости, чтобы устранить деловые и культурные различия и обеспечить сотрудничество между командами. Повторно используемые политики, определение и упрощение повторно используемых сервисных задач, процессов и ввода/вывода.
Определить предложения для различных услуг, процессов и ввода/вывода. Упрощение процесса развертывания сократит время выхода на рынок как новых, так и существующих рабочих нагрузок. Когда у вас есть стандартный процесс, его можно упорядочить и согласовать с отдельными запросами для многопоточного подхода и доставки. Объедините политики со специфическими для бизнеса действиями. Как внедрение этой политики помогает бизнесу? Экономит время? Экономит деньги? Обеспечивает лучший бизнес-результат?
Убедитесь, что сервисные задачи совместимы. Свяжите добавочные сервисные задачи, чтобы они соответствовали созданию бизнес-сервисов. Обеспечьте гибкость связывания и повторного связывания сервисных задач по запросу. Разверните возможности самообслуживания и проложите путь к повышению операционной эффективности. Разрешить несколько наборов технологических навыков продолжать вносить свой вклад в надзор и соответствие. Управляйте политиками и процессами, добавляя и улучшая их, сохраняя при этом доступность и обслуживание.
Начните с малого, автоматизировав существующие задачи. Ознакомьтесь с процессом автоматизации, чтобы вы могли определить другие области, которые могут выиграть от автоматизации. повторяйте свои инициативы по автоматизации, постепенно добавляя гибкость при сохранении требуемой доступности. Использование поэтапного подхода прокладывает путь к успеху! Оркестрируйте сетевой сервис!
Для быстрой доставки приложений требуется автоматизация процесса развертывания. Создание гибкой сервисной среды требует управления различными элементами в рамках набора технологических навыков. Подготовьтесь к комплексной оркестровке, обеспечивающей контроль над автоматизацией и порядком развертывания. Инструменты автоматизации сети Хорошей новостью здесь является то, что по большей части инструменты, которые мы используем для автоматизации сети, как правило, те же, что мы будем использовать для других областей автоматизации.
Опреационная система. Большую часть своего обучения я сосредоточился на использовании инструментов под Linux. Но почти все инструменты, которых мы коснемся, кросплатформенные.
Интегрированная среда разработки (IDE). Опять же, здесь особо нечего сказать, кроме всего прочего, я бы предложил Visual Studio Code в качестве вашей IDE, основываясь на обширных подключаемых модулях, доступных для стольких разных языков.
Управление конфигурацией. Мы еще не добрались до раздела «Управление конфигурацией», но совершенно очевидно, что Ansible является фаворитом в этой области для управления и автоматизации конфигураций. Ansible написан на Python, но вам не нужно знать Python. Link to Ansible Network Modules
Мы также коснемся Ansible Tower в разделе управления конфигурацией, рассматривая его как внешний интерфейс с графическим интерфейсом (GUI) для Ansible.
CI/CD. Мы рассмотрим больше концепций и инструментов, связанных с этим, но важно хотя бы упомянуть здесь, поскольку это охватывает не только сеть, но и все предоставление услуг и платформ.
В частности, Jenkins предоставляет или кажется популярным инструментом для сетевой автоматизации.
Отслеживает репозиторий git на наличие изменений, а затем инициирует их. Контроль версий (Version Control). Углубимся в это позже.
Git обеспечивает контроль версий вашего кода на локальном устройстве - Кроссплатформенность GitHub, GitLab, BitBucket и т. д. — это онлайн-сайты, на которых вы определяете свои репозитории и загружаете свой код. Language | Scripting. Что-то, что мы здесь не рассмотрели, это Python как язык, я решил вместо этого погрузиться в Go как язык программирования, исходя из моих обстоятельств, я бы сказал, что это был тесный контакт между Golang и Python и Python, кажется, Победитель в категории «Сетевая автоматизация».
Здесь стоит упомянуть Nornir, фреймворк автоматизации, написанный на Python. Кажется, что это берет на себя роль Ansible, но особенно в отношении сетевой автоматизации. Документация Nornir Анализ API. Postman — отличный инструмент для анализа RESTful API. Помогает создавать, тестировать и изменять API.
POST \u0026raquo;\u0026gt; Для создания объектов ресурсов. GET \u0026raquo;\u0026gt; Для получения ресурсов. PUT \u0026raquo;\u0026gt; Для создания или замены ресурсов. PATCH \u0026raquo;\u0026gt; Для создания или обновления объекта ресурсов. Delete \u0026raquo;\u0026gt; Чтобы удалить ресурс Postman tool Download
Еще инструменты Cisco NSO (Network Services Orchestrator)
NetYCE - Simplify Network Automation
Network Test Automation
В течение следующих 3 дней я планирую более подробно изучить некоторые вещи, которые мы рассмотрели, и поработать над Python и сетевой автоматизацией.
До сих пор мы далеко не охватили все сетевые темы, но хотели сделать это достаточно широким, чтобы следовать за ним и продолжать учиться на ресурсах, которые я добавляю ниже.
Ресурсы 3 Necessary Skills for Network Automation Computer Networking full course Practical Networking Python Network Automation `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day24/"},"https://romankurnovskii.com/ru/docs/python101/chapter25_decorators/":{title:"25. Декораторы",tags:[],content:`Декораторы в Python - это действительно здорово, но поначалу их может быть трудно понять. Декоратор в Python - это функция, которая принимает в качестве аргумента другую функцию. Декоратор обычно изменяет или улучшает функцию, которую он принял, и возвращает измененную функцию. Это означает, что при вызове декорированной функции вы получите функцию, которая может быть немного другой, иметь дополнительные возможности по сравнению с базовым определением. Но давайте вернемся немного назад. Возможно, нам следует рассмотреть основной строительный блок декоратора, а именно функцию.
Простая функция Функция - это блок кода, который начинается с ключевого слова def в Python, за которым следует фактическое имя функции. Функция может принимать от нуля и более аргументов, ключевые аргументы или сочетание этих аргументов. Функция всегда выдает результат. Если вы не укажете, что она должна возвращать, вернется None. Вот очень простая функция, которая просто возвращает строку:
def a_function(): \u0026quot;\u0026quot;\u0026quot;A pretty useless function\u0026quot;\u0026quot;\u0026quot; return \u0026quot;1+1\u0026quot; if __name__ == \u0026quot;__main__\u0026quot;: value = a_function() print(value) Все, что мы делаем в приведенном выше коде, - это вызываем функцию и печатаем возвращаемое значение. Давайте создадим еще одну функцию:
def another_function(func): \u0026quot;\u0026quot;\u0026quot; A function that accepts another function \u0026quot;\u0026quot;\u0026quot; def other_func(): val = \u0026quot;The result of %s is %s\u0026quot; % (func(), eval(func()) ) return val return other_func Эта функция принимает один аргумент, и этот аргумент должен быть функцией или вызываемой. На самом деле, ее следует вызывать только с помощью ранее определенной функции. Вы заметите, что внутри этой функции есть вложенная функция, которую мы называем other_func. Она берет результат переданной ей функции, оценивает его и создает строку, рассказывающую о том, что она сделала, которую затем возвращает. Давайте посмотрим на полную версию кода:
def another_function(func): \u0026quot;\u0026quot;\u0026quot; A function that accepts another function \u0026quot;\u0026quot;\u0026quot; def other_func(): val = \u0026quot;The result of %s is %s\u0026quot; % (func(), eval(func()) ) return val return other_func def a_function(): \u0026quot;\u0026quot;\u0026quot;A pretty useless function\u0026quot;\u0026quot;\u0026quot; return \u0026quot;1+1\u0026quot; if __name__ == \u0026quot;__main__\u0026quot;: value = a_function() print(value) decorator = another_function(a_function) print(decorator()) Вот как работает декоратор. Мы создаем одну функцию, а затем передаем ее во вторую функцию. Вторая функция является функцией декоратора. Декоратор изменяет или улучшает переданную ему функцию и возвращает модификацию. Если вы запустите этот код, вы должны увидеть следующее в качестве вывода в stdout:
1+1 The result of 1+1 is 2 Давайте немного изменим код, чтобы превратить another_function в декоратор:
def another_function(func): \u0026quot;\u0026quot;\u0026quot; A function that accepts another function \u0026quot;\u0026quot;\u0026quot; def other_func(): val = \u0026quot;The result of %s is %s\u0026quot; % (func(), eval(func()) ) return val return other_func @another_function def a_function(): \u0026quot;\u0026quot;\u0026quot;A pretty useless function\u0026quot;\u0026quot;\u0026quot; return \u0026quot;1+1\u0026quot; if __name__ == \u0026quot;__main__\u0026quot;: value = a_function() print(value) Вы заметите, что в Python декоратор начинается с символа @, за которым следует имя функции, которую мы собираемся \u0026ldquo;декорировать\u0026rdquo;. Чтобы применить декоратор, достаточно поместить его в строку перед определением функции. Теперь, когда мы вызываем функцию a_function, она будет декорирована, и мы получим следующий результат:
The result of 1+1 is 2 Давайте создадим декоратор, который действительно делает что-то полезное.
Создание декоратора логирования Иногда вы захотите создать лог того, что делает функция. В большинстве случаев вы, вероятно, будете вести журнал в самой функции. Иногда вы можете захотеть сделать это на уровне функции, чтобы получить представление о потоке программы или, возможно, для выполнения некоторых бизнес-правил, например, аудита. Вот небольшой декоратор, который мы можем использовать для записи имени любой функции и того, что она возвращает:
import logging def log(func): \u0026quot;\u0026quot;\u0026quot; Log what function is called \u0026quot;\u0026quot;\u0026quot; def wrap_log(*args, **kwargs): name = func.__name__ logger = logging.getLogger(name) logger.setLevel(logging.INFO) # add file handler fh = logging.FileHandler(\u0026quot;%s.log\u0026quot; % name) fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s' formatter = logging.Formatter(fmt) fh.setFormatter(formatter) logger.addHandler(fh) logger.info(\u0026quot;Running function: %s\u0026quot; % name) result = func(*args, **kwargs) logger.info(\u0026quot;Result: %s\u0026quot; % result) return func return wrap_log @log def double_function(a): \u0026quot;\u0026quot;\u0026quot; Double the input parameter \u0026quot;\u0026quot;\u0026quot; return a*2 if __name__ == \u0026quot;__main__\u0026quot;: value = double_function(2) Этот небольшой скрипт имеет функцию log, которая принимает функцию в качестве единственного аргумента. Она создаст объект logger и имя файла журнала на основе имени функции. Затем функция log будет записывать, какая функция была вызвана и что функция вернула, если таковая имеется.
Встроенные декораторы Python поставляется с несколькими встроенными декораторами. Основных три:
@classmethod @staticmethod @property Также декораторы есть в различных частях стандартной библиотеки Python. Примером может служить functools.wraps.
@classmethod и @staticmethod Я никогда не использовал их сам, поэтому провел небольшое исследование. Декоратор @classmethod может быть вызван с экземпляром класса или непосредственно самим классом в качестве первого аргумента. Согласно документации Python: ОВ соответствии с документацией Python: он может быть вызван как в классе (например, C.f()), или в экземпляре (например, C().f()). Экземпляр игнорируется, за исключением его класса. Если метод класса вызван для выведенного класса, то объект выведенного класса передается в качестве подразумеваемого первого аргумента. Основной случай использования декоратора @classmethod, который я обнаружил в своем исследовании, - это альтернативный конструктор или вспомогательный метод для инициализации.
Декоратор @staticmethod - это просто функция внутри класса. Вы можете вызывать его как с инстанцированием класса, так и без него. Типичный случай использования - когда у вас есть функция, которая, по вашему мнению, связана с классом. По большей части это стилистический выбор.
Возможно, вам поможет пример кода, показывающий, как работают эти два декоратора:
class DecoratorTest(object): \u0026quot;\u0026quot;\u0026quot; Test regular method vs @classmethod vs @staticmethod \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; pass def doubler(self, x): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; print(\u0026quot;running doubler\u0026quot;) return x*2 @classmethod def class_tripler(klass, x): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; print(\u0026quot;running tripler: %s\u0026quot; % klass) return x*3 @staticmethod def static_quad(x): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; print(\u0026quot;running quad\u0026quot;) return x*4 if __name__ == \u0026quot;__main__\u0026quot;: decor = DecoratorTest() print(decor.doubler(5)) print(decor.class_tripler(3)) print(DecoratorTest.class_tripler(3)) print(DecoratorTest.static_quad(2)) print(decor.static_quad(3)) print(decor.doubler) print(decor.class_tripler) print(decor.static_quad) Этот пример демонстрирует, что вы можете одинаково вызывать обычный метод и оба декорированных метода. Вы заметите, что декорированные функции @classmethod и @staticmethod можно вызывать непосредственно из класса или из экземпляра класса. Если вы попытаетесь вызвать обычную функцию из класса (например, DecoratorTest.doubler(2)), вы получите TypeError. Вы также заметите, что последний оператор печати показывает, что decor.static_quad возвращает обычную функцию, а не связанный метод.
@contextmanager https://docs.python.org/3/library/contextlib.html В Python есть механизм менеджмента контекста, который поможет вам правильно управлять ресурсами.
На примере оператора with:
with open(\u0026quot;test.txt\u0026quot;,'w') as f: f.write(\u0026quot;Yang is writing!\u0026quot;) Как показано в приведенном выше коде, мы можем открыть файл с помощью оператора with, чтобы он был закрыт автоматически после записи. Нам не нужно явно вызывать функцию f.close(), чтобы закрыть файл.
Иногда нам нужно определить индивидуальный менеджер контекста для каких-то особых требований.
Например, следующий код реализует простой настраиваемый контекстный менеджер, который может выводить соответствующую информацию при открытии или закрытии файла.
from contextlib import contextmanager @contextmanager def file_manager(filename, mode): print(\u0026quot;The file is opening...\u0026quot;) file = open(filename,mode) yield file print(\u0026quot;The file is closing...\u0026quot;) file.close() with file_manager('test.txt', 'w') as f: f.write('Yang is writing!') # The file is opening... # The file is closing... @property Настраиваем геттеры и сеттеры для классов
В Python есть небольшое понятие, называемое property, которое может выполнять несколько полезных функций. Рассмотрим, как сделать следующее:
Преобразовать методы класса в атрибуты, доступные только для чтения. Реализовать сеттеры и геттеры. Один из самых простых способов использования свойства - использовать его в качестве декоратора метода. Это позволяет превратить метод класса в атрибут класса. Я нахожу это полезным, когда мне нужно сделать какую-то комбинацию значений. Другие находят это полезным для написания методов преобразования, к которым они хотят иметь доступ как к методам. Давайте рассмотрим простой пример:
class Person(object): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, first_name, last_name): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.first_name = first_name self.last_name = last_name @property def full_name(self): \u0026quot;\u0026quot;\u0026quot; Return the full name \u0026quot;\u0026quot;\u0026quot; return \u0026quot;%s %s\u0026quot; % (self.first_name, self.last_name) В приведенном выше коде мы создаем два атрибута или свойства класса: self.first_name и self.last_name. Затем мы создаем метод full_name, к которому прикреплен декоратор @property. Это позволяет нам сделать следующее в сессии интерпретатора:
\u0026gt;\u0026gt;\u0026gt; person = Person(\u0026quot;Иван\u0026quot;, \u0026quot;Иванов\u0026quot;) \u0026gt;\u0026gt;\u0026gt; person.full_name 'Иван Иванов' \u0026gt;\u0026gt;\u0026gt; person.first_name 'Иван' \u0026gt;\u0026gt;\u0026gt; person.full_name = \u0026quot;Александр Иванович\u0026quot; Traceback (most recent call last): File \u0026quot;\u0026lt;string\u0026gt;\u0026quot;, line 1, in \u0026lt;fragment\u0026gt; AttributeError: can't set attribute Как видите, поскольку мы превратили метод в свойство, мы можем обращаться к нему, используя обычную точечную нотацию. Однако, если мы попытаемся установить свойство на что-то другое, мы вызовем ошибку AttributeError. Единственный способ изменить свойство full_name - сделать это косвенно:
\u0026gt;\u0026gt;\u0026gt; person.first_name = \u0026quot;Иван\u0026quot; \u0026gt;\u0026gt;\u0026gt; person.full_name 'Иван Иванов' Это несколько ограничивает возможности, поэтому давайте рассмотрим другой пример, где мы можем создать свойство, которое позволит нам устанавливать его.
Замена сеттеров и геттеров с помощью @property Давайте представим, что у нас есть старый код, который написал кто-то, кто не очень хорошо понимал Python. Если вы похожи на меня, вы уже сталкивались с подобным кодом:
from decimal import Decimal class Fees(object): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self._fee = None def get_fee(self): \u0026quot;\u0026quot;\u0026quot; Return the current fee \u0026quot;\u0026quot;\u0026quot; return self._fee def set_fee(self, value): \u0026quot;\u0026quot;\u0026quot; Set the fee \u0026quot;\u0026quot;\u0026quot; if isinstance(value, str): self._fee = Decimal(value) elif isinstance(value, Decimal): self._fee = value Чтобы использовать этот класс, мы должны использовать сеттеры и геттеры, которые определены:
\u0026gt;\u0026gt;\u0026gt; f = Fees() \u0026gt;\u0026gt;\u0026gt; f.set_fee(\u0026quot;1\u0026quot;) \u0026gt;\u0026gt;\u0026gt; f.get_fee() Decimal('1') Если вы хотите добавить в этот код обычный доступ к атрибутам в точечной нотации, не ломая все приложения, которые зависят от этого куска кода, вы можете изменить его очень просто, добавив свойство:
from decimal import Decimal class Fees(object): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self._fee = None def get_fee(self): \u0026quot;\u0026quot;\u0026quot; Return the current fee \u0026quot;\u0026quot;\u0026quot; return self._fee def set_fee(self, value): \u0026quot;\u0026quot;\u0026quot; Set the fee \u0026quot;\u0026quot;\u0026quot; if isinstance(value, str): self._fee = Decimal(value) elif isinstance(value, Decimal): self._fee = value fee = property(get_fee, set_fee) Мы добавили одну строку в конец этого кода. Теперь мы можем делать что-то вроде этого:
\u0026gt;\u0026gt;\u0026gt; f = Fees() \u0026gt;\u0026gt;\u0026gt; f.set_fee(\u0026quot;1\u0026quot;) \u0026gt;\u0026gt;\u0026gt; f.fee Decimal('1') \u0026gt;\u0026gt;\u0026gt; f.fee = \u0026quot;2\u0026quot; \u0026gt;\u0026gt;\u0026gt; f.get_fee() Decimal('2') Как вы можете видеть, когда мы используем property таким образом, это позволяет свойству fee устанавливать и получать значение самостоятельно, не нарушая унаследованный код. Давайте перепишем этот код с использованием декоратора property и посмотрим, сможем ли мы заставить его разрешить установку.
from decimal import Decimal class Fees(object): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self._fee = None @property def fee(self): \u0026quot;\u0026quot;\u0026quot; The fee property - the getter \u0026quot;\u0026quot;\u0026quot; return self._fee @fee.setter def fee(self, value): \u0026quot;\u0026quot;\u0026quot; The setter of the fee property \u0026quot;\u0026quot;\u0026quot; if isinstance(value, str): self._fee = Decimal(value) elif isinstance(value, Decimal): self._fee = value if __name__ == \u0026quot;__main__\u0026quot;: f = Fees() Приведенный выше код демонстрирует, как создать \u0026ldquo;сеттер\u0026rdquo; для свойства fee. Это можно сделать, украсив второй метод, который также называется fee, декоратором @fee.setter. Сеттер вызывается, когда вы делаете что-то вроде этого:
\u0026gt;\u0026gt;\u0026gt; f = Fees() \u0026gt;\u0026gt;\u0026gt; f.fee = \u0026quot;1\u0026quot; Если вы посмотрите на сигнатуру для property, то в качестве \u0026ldquo;аргументов\u0026rdquo; в ней указаны fget, fset, fdel и doc. Вы можете создать другой декорированный метод с тем же именем, чтобы он соответствовал функции удаления, используя @fee.deleter, если вы хотите перехватить команду del для атрибута.
@lru_cache https://docs.python.org/3/library/functools.html Ускоряем программы кэшированием
Этот декоратор можно использовать для кэширования результатов функции, так что последующие вызовы функции с теми же аргументами не будут выполняться снова.
Это особенно полезно для функций, которые требуют больших вычислительных затрат или часто вызываются с одними и теми же аргументами.
from functools import lru_cache import time @lru_cache(maxsize=None) def fibonacci(n): if n \u0026lt; 2: return n return fibonacci(n - 1) + fibonacci(n - 2) start_time = time.perf_counter() print(fibonacci(30)) end_time = time.perf_counter() print(f\u0026quot;The execution time: {end_time - start_time:.8f} seconds\u0026quot;) # The execution time: 0.00002990 seconds Декоратор @lru_cache имеет параметр maxsize, который определяет максимальное количество результатов для хранения в кэше. Когда кэш заполнен и необходимо сохранить новый результат, наименее использованный результат вытесняется из кэша, чтобы освободить место для нового. Это называется стратегией наименее использованного результата (LRU).
По умолчанию maxsize установлен на 128. Если для maxsize установлено значение None, функция LRU отключена, и кэш может неограниченно увеличиваться.
Ресурсы https://habr.com/ru/post/709280/ `,url:"https://romankurnovskii.com/ru/docs/python101/chapter25_decorators/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day25/":{title:"25. Автоматизация сети с помощью Python",tags:["devops"],content:`Python для автоматизации сети Python — это стандартный язык, используемый для автоматизированных сетевых операций.
Хотя это не только автоматизация сети, кажется, что оно везде, когда вы ищете ресурсы, и, как упоминалось ранее, если это не Python, то обычно это Ansible, который также написан на Python.
Я думаю, что уже упоминал об этом, но в разделе «Изучение языка программирования» я выбрал Golang, а не Python, по причинам, связанным с тем, что моя компания разрабатывает Go, так что это было хорошей причиной для меня, чтобы учиться, но если это не так, тогда Python взял бы это время.
Удобочитаемость и простота использования. Кажется, что Python просто имеет смысл. Похоже, что в коде нет требований к {} для начального и конечного блоков. Соедините это с сильной IDE, такой как VS Code, у вас будет довольно легкий старт, если вы хотите запустить какой-либо код Python. Pycharm может быть еще одной IDE, о которой стоит упомянуть.
Библиотеки. Расширяемость Python - это настоящая золотая жила, я упоминал ранее, что это не только для сетевой автоматизации, но на самом деле существует множество библиотек для всех видов устройств и конфигураций. Вы можете увидеть огромное количество здесь PyPi Если вы хотите загрузить библиотеку на свою рабочую станцию, вы используете инструмент под названием «pip», чтобы подключиться к PyPI и загрузить его локально. Сетевые поставщики, такие как Cisco, Juniper и Arista, разработали библиотеки для облегчения доступа к своим устройствам.
Мощный и эффективный - Помните, во времена Go я прошел сценарий \u0026ldquo;Hello World\u0026rdquo;, и мы прошли, кажется, 6 строк кода? В Питоне это print('hello world') Сложите все вышеперечисленные пункты вместе, и должно быть легко понять, почему Python обычно упоминается как инструмент де-факто при работе над автоматизацией.
Я думаю, важно отметить, что, возможно, несколько лет назад существовали сценарии, которые могли взаимодействовать с вашими сетевыми устройствами, чтобы, возможно, автоматизировать резервное копирование конфигурации или собирать журналы и другую информацию о ваших устройствах. Автоматизация, о которой мы здесь говорим, немного отличается, потому что общий сетевой ландшафт также изменился, чтобы лучше соответствовать этому образу мышления и обеспечить большую автоматизацию.
Программно-определяемая сеть (Software-Defined Network). Контроллеры SDN несут ответственность за доставку конфигурации уровня управления на все устройства в сети, что означает только единую точку контакта для любых изменений в сети, больше не требуется telnet или SSH для доступа к каждому устройству, а также полагаются на люди, чтобы сделать это, что имеет повторяющийся шанс отказа или неправильной конфигурации.
Оркестрация высокого уровня (High-Level Orchestration ). Поднимитесь на уровень выше этих контроллеров SDN, и это позволит оркестровать уровни обслуживания, а затем интегрировать этот уровень оркестровки в выбранные вами платформы, VMware, Kubernetes, общедоступные облака и т. д.
Управление на основе политик (Policy-based management) - Что вы хотите иметь? Какое желаемое состояние? Вы описываете это, и в системе есть все детали, как это понять, чтобы стать желаемым состоянием.
Настройка рабочей среды Не у всех есть доступ к физическим маршрутизаторам, коммутаторам и другим сетевым устройствам.
Я хотел дать нам возможность ознакомиться с некоторыми из ранее упомянутых инструментов, а также получить практические навыки и научиться автоматизировать настройку наших сетей.
Когда дело доходит до вариантов, есть несколько, из которых мы можем выбрать.
GNS3 VM Eve-ng Unimus Мы построим нашу среду, используя Eve-ng, как упоминалось ранее, вы можете использовать физическое устройство, но, честно говоря, виртуальная среда означает, что у нас может быть среда-песочница. для тестирования множества различных сценариев. Кроме того, может быть интересна возможность играть с различными устройствами и топологиями.
Мы собираемся делать все на EVE-NG с изданием сообщества.
Начало Издание сообщества поставляется в форматах ISO и OVF для загрузки.
Мы будем использовать загрузку в формате OVF, но в случае с ISO есть возможность сборки на «голом железе» без использования гипервизора.
Для нашего пошагового руководства мы будем использовать VMware Workstation, поскольку у меня есть лицензия через мой vExpert, но вы в равной степени можете использовать VMware Player или любой другой вариант, упомянутый в документации. К сожалению, мы не можем использовать нашу ранее созданную среду в Virtual box!
Здесь также у меня возникла проблема с GNS3 с Virtual Box, хотя он и поддерживается.
Download VMware Workstation Player - FREE
VMware Workstation PRO. Есть бесплатный пробный период.
Установка на VMware Workstation PRO Теперь у нас загружено и установлено программное обеспечение hypervisor, а также загружен файл EVE-NG OVF. Теперь мы готовы к настройке. Откройте VMware Workstation, а затем выберите file -\u0026gt; open.
Когда вы загружаете образ EVE-NG OVF, он будет находиться в сжатом файле. Извлеките содержимое в свою папку, чтобы оно выглядело так. Перейдите в папку, в которую вы загрузили образ EVE-NG OVF, и начните импорт. Дайте ему узнаваемое имя и сохраните виртуальную машину где-нибудь в вашей системе.
Когда процесс импорта завершится, увеличьте количество процессоров до 4 и объем выделенной памяти до 8 ГБ. (Это должно быть после импорта с последней версией, если нет, то отредактируйте настройки ВМ)
Также убедитесь, что установлен флажок Virtualise Intel VT-x/EPT или AMD-V/RVI. Этот параметр указывает рабочей станции VMware передавать флаги виртуализации гостевой ОС (вложенная виртуализация). Это была проблема, с которой я столкнулся с GNS3 с Virtual Box, хотя мой процессор это позволяет.
Включение и доступ Примечание и кроличья нора: помните, я упоминал, что это не будет работать с VirtualBox! Ну да, была такая же проблема с VMware Workstation и EVE-NG, но это не вина платформы виртуализации!
У меня есть WSL2, работающий на моей машине с Windows, и это, похоже, лишает возможности запускать что-либо, вложенное в вашу среду. Я смущен тем, почему виртуальная машина Ubuntu работает, поскольку она, кажется, устраняет аспект виртуализации Intel VT-d ЦП при использовании WSL2.
Чтобы решить эту проблему, мы можем запустить следующую команду на нашем компьютере с Windows и перезагрузить систему, обратите внимание, что, пока она отключена, вы не сможете использовать WSL2.
bcdedit /set hypervisorlaunchtype off
Если вы хотите вернуться и использовать WSL2, вам нужно будет запустить эту команду и перезагрузиться.
bcdedit /set hypervisorlaunchtype auto
Обе эти команды нужно запускать от имени администратора!
Хорошо, вернемся к шоу. Теперь у вас должна быть включенная машина в VMware Workstation, и у вас должно появиться приглашение, похожее на это.
Данные для входа:
username = root password = eve
Затем вас попросят снова ввести пароль root, который позже будет использоваться для SSH-соединения с хостом. Затем мы можем изменить имя хоста.
Затем мы определяем доменное имя DNS, я использовал имя ниже, но я не уверен, нужно ли будет его изменить позже.
Затем мы настраиваем сеть, я выбираю статический, чтобы указанный IP-адрес оставался постоянным после перезагрузки. На последнем шаге укажите статический IP-адрес из сети, доступной с вашей рабочей станции. Здесь есть несколько дополнительных шагов, где вам нужно будет указать маску подсети для вашей сети, шлюз по умолчанию и DNS. После завершения он перезагрузится, когда будет выполнено резервное копирование, вы можете взять свой статический IP-адрес и ввести его в свой браузер. Имя пользователя по умолчанию для графического интерфейса — «admin», пароль — «eve», а имя пользователя по умолчанию для SSH — «root» и пароль — «eve», но это было бы изменено, если бы вы изменили его во время установки. Я выбрал HTML5 для консоли вместо нативной, так как это откроет новую вкладку в вашем браузере, когда вы будете перемещаться по разным консолям.
Далее мы собираемся:
Установить клиентский пакет EVE-NG Загрузить некоторые сетевые образы в EVE-NG. Построить топологию сети Добавить \u0026ldquo;ноды\u0026rdquo; (машины/хосты, Nodes) Соединить ноды между собой Начнем создавать скрипты Python Посмотрим на telnetlib, Netmiko, Paramiko и Pexpect Ресурсы Free Course: Introduction to EVE-NG EVE-NG - Creating your first lab 3 Necessary Skills for Network Automation Computer Networking full course Practical Networking Python Network Automation `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day25/"},"https://romankurnovskii.com/ru/docs/python101/chapter26_lambda/":{title:"26. Лямбда",tags:[],content:`Оператор лямбда в Python - это анонимная или несвязанная функция, причем довольно ограниченная. Давайте рассмотрим несколько типичных примеров и посмотрим, сможем ли мы найти для нее применение. Примеры, которые обычно встречаются при изучении лямбды, - это что-то вроде скучной функции удвоения. Чтобы не быть голословным, наш простой пример покажет, как найти квадратный корень. Сначала мы покажем обычную функцию, а затем ее лямбда-эквивалент:
import math def sqroot(x): \u0026quot;\u0026quot;\u0026quot; Finds the square root of the number passed in \u0026quot;\u0026quot;\u0026quot; return math.sqrt(x) square_rt = lambda x: math.sqrt(x) Если вы попробуете каждую из этих функций, то в итоге получите плавающее число. Вот несколько примеров:
\u0026gt;\u0026gt;\u0026gt; sqroot(49) 7.0 \u0026gt;\u0026gt;\u0026gt; square_rt(64) 8.0 Довольно ловко, верно? Но где мы можем использовать лямбду в реальной жизни? Может быть, в программе-калькуляторе? Ну, это может сработать, но это довольно ограниченное применение для встроенного модуля Python! Одна из основных частей Python, к которой регулярно применяются примеры лямбд, - это обратные вызовы Tkinter. Tkinter - это набор инструментов для создания графических интерфейсов, который входит в состав Python.
Tkinter + lambda Мы начнем с Tkinter, поскольку он входит в стандартный пакет Python. Вот очень простой сценарий с тремя кнопками, две из которых привязаны к обработчику событий с помощью лямбды:
import Tkinter as tk class App: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; frame = tk.Frame(parent) frame.pack() btn22 = tk.Button(frame, text=\u0026quot;22\u0026quot;, command=lambda: self.printNum(22)) btn22.pack(side=tk.LEFT) btn44 = tk.Button(frame, text=\u0026quot;44\u0026quot;, command=lambda: self.printNum(44)) btn44.pack(side=tk.LEFT) quitBtn = tk.Button(frame, text=\u0026quot;QUIT\u0026quot;, fg=\u0026quot;red\u0026quot;, command=frame.quit) quitBtn.pack(side=tk.LEFT) def printNum(self, num): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; print(\u0026quot;You pressed the %s button\u0026quot; % num) if __name__ == \u0026quot;__main__\u0026quot;: root = tk.Tk() app = App(root) root.mainloop() Обратите внимание на переменные btn22 и btn44. Именно здесь происходит действие. Мы создаем экземпляр tk.Button и одним махом привязываем его к нашему методу printNum. Наша лямбда присваивается параметру команды кнопки. Это значит, что мы создаем одноразовую функцию для команды, по аналогии с кнопкой выхода, где мы вызываем метод выхода из фрейма. Разница в том, что отдельная лямбда – это метод, который вызывает другой метод и передает ему целое число. В методе printNum, мы пишем в stdout о том, какая кнопка была нажата, пользуясь информацией, которая была передана функцией lambda. Улавливаете? Если да, то мы продолжим. Если нет – перечитайте данный параграф столько раз, сколько нужно для того, чтобы эта информация усвоилась, или пока не сойдете с ума, в любом случае, что-то должно произойти первым.
Подведение итогов Оператор лямбда используется и во многих других проектах. Если набрать в Google название проекта Python и лямбда, можно найти множество живого кода. Например, набрав в поиске \u0026ldquo;django lambda\u0026rdquo;, вы узнаете, что в django есть фабрика modelformset, использующая лямбды. Плагин Elixir для SqlAlchemy также использует лямбды. Будьте внимательны, и вы удивитесь, как часто вы будете натыкаться на этот удобный механизм создания функций.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter26_lambda/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day26/":{title:"26. Развертывание виртуальной лаборатории EVE-NG в домашних условиях",tags:["devops"],content:`Создание нашей лаборатории Мы собираемся продолжить настройку нашей эмулируемой сети с помощью EVE-NG, а затем, надеюсь, развернуть несколько устройств и начать думать о том, как мы можем автоматизировать настройку этих устройств. В День 25 мы рассказали об установке EVE-NG на нашу машину с помощью VMware Workstation.
Установка клиента EVE-NG Существует также клиентский пакет, который позволяет нам выбирать, какое приложение используется при подключении к устройствам по SSH. Он также настроит Wireshark для захвата пакетов между ссылками. Вы можете установить клиентский пакет для своей ОС (Windows, macOS, Linux).
EVE-NG Client Download
Подсказка: если вы используете Linux в качестве клиента, то есть этот клиентский пакет.
Установка проста: next, next и я бы посоветовал оставить значения по умолчанию.
Получение сетевых образов Этот шаг непростой, я просмотрел несколько видеороликов, на которые я дам ссылки в конце, которые ссылаются на некоторые ресурсы и загрузки для нашего маршрутизатора и переключают изображения, рассказывая нам, как и куда их загрузить.
Важно отметить, что я использую все в образовательных целях. Я бы предложил загрузить официальные образы от сетевых поставщиков.
Blog \u0026amp; Links to YouTube videos
How To Add Cisco VIRL vIOS image to Eve-ng
В целом шаги здесь немного сложны и могли бы быть намного проще, но приведенные выше блоги и видео показывают процесс добавления изображений в вашу коробку EVE-NG.
Я использовал FileZilla для передачи qcow2 на виртуальную машину через SFTP.
Для нашей лаборатории нам нужны Cisco vIOS L2 (коммутаторы) и Cisco vIOS (маршрутизатор).
Создаем лабораторию Внутри веб-интерфейса EVE-NG мы собираемся создать нашу новую топологию сети. У нас будет четыре коммутатора и один маршрутизатор, который будет нашим шлюзом во внешние сети.
Node IP Address Router 10.10.88.110 Switch1 10.10.88.111 Switch2 10.10.88.112 Switch3 10.10.88.113 Switch4 10.10.88.114 Добавление наших узлов в EVE-NG Когда вы впервые войдете в EVE-NG, вы увидите экран, как показано ниже, мы хотим начать с создания нашей первой лаборатории.
Дайте вашей лаборатории имя, а остальные поля являются необязательными.
Затем увидим пустой экран, чтобы начать создание вашей сети. Щелкните правой кнопкой мыши на своем холсте и выберите \u0026lsquo;add node\u0026rsquo;.
Далее появляется длинный список опций. Если вы следовали вышеизложенному, у вас будут два синих, показанных ниже, а остальные будут серыми и недоступными для выбора.
Мы хотим добавить следующее в нашу лабораторию:
1 x Cisco vIOS Router 4 x Cisco vIOS Switch Соединяем наши ноды Теперь нам нужно добавить возможность подключения между нашими маршрутизаторами и коммутаторами. Мы можем сделать это довольно легко, наведя курсор на устройство и увидев значок подключения, как показано ниже, а затем подключив его к устройству, к которому мы хотим подключиться.
Когда вы закончите подключение своей среды, вы также можете добавить способ определения физических границ или местоположений с помощью прямоугольников или кругов, которые также можно найти в контекстном меню. Вы также можете добавить текст, который полезен, когда мы хотим определить наши имена или IP-адреса в наших лабораториях.
Я пошел дальше и сделал свою лабораторию такой, как показано ниже. You will also notice that the lab above is all powered off, we can start our lab by selecting everything and right-clicking and selecting start selected.
Как только мы запустим нашу лабораторию, вы сможете подключаться к консоли на каждом устройстве, и вы заметите, что на этом этапе они довольно тупые без настройки. Мы можем добавить некоторую конфигурацию к каждому узлу, скопировав или создав свою собственную в каждом терминале.
Я оставлю свою конфигурацию в сетевой папке репозитория для справки.
Node Configuration Router R1 Switch1 SW1 Switch2 SW2 Switch3 SW3 Switch4 SW4 Ресурсы Free Course: Introduction to EVE-NG EVE-NG - Creating your first lab 3 Necessary Skills for Network Automation Computer Networking full course Practical Networking Python Network Automation Большинство примеров, которые я использую здесь, поскольку я не сетевой инженер, взяты из этой обширной книги, которая не является бесплатной, но я использую некоторые примеры оттуда, чтобы помочь понять автоматизацию сети.
Hands-On Enterprise Automation with Python (Book) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day26/"},"https://romankurnovskii.com/ru/docs/python101/chapter27_profiling/":{title:"27. Профилирование кода",tags:[],content:`Профилирование кода - это попытка найти узкие места в вашем коде. Профилирование должно выявить, какие части вашего кода занимают больше всего времени на выполнение. Узнав это, вы можете посмотреть на эти части кода и попытаться найти способы их оптимизации. Python содержит три встроенных профайлера: cProfile, profile и hotshot. Согласно документации Python, hotshot \u0026ldquo;больше не поддерживается и может быть отменен в будущей версии Python\u0026rdquo;. Модуль profile - это чистый модуль Python, но он добавляет много накладных расходов в профилируемые программы. Поэтому мы сосредоточимся на cProfile, который имеет интерфейс, имитирующий модуль profile.
Профилирование кода с помощью cProfile Профилирование кода с помощью cProfile достаточно просто. Все, что вам нужно сделать, это импортировать модуль и вызвать его функцию run. Давайте рассмотрим простой пример:
\u0026gt;\u0026gt;\u0026gt; import hashlib \u0026gt;\u0026gt;\u0026gt; import cProfile \u0026gt;\u0026gt;\u0026gt; cProfile.run(\u0026quot;hashlib.md5(b'abcdefghijkl').digest()\u0026quot;) 4 function calls in 0.000 CPU seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.000 0.000 \u0026lt;string\u0026gt;:1(\u0026lt;module\u0026gt;) 1 0.000 0.000 0.000 0.000 {_hashlib.openssl_md5} 1 0.000 0.000 0.000 0.000 {method 'digest' of '_hashlib.HASH' objects} 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} Здесь мы импортируем модуль hashlib и используем cProfile для профилирования создания хэша MD5. Первая строка показывает, что было 4 вызова функций. Следующая строка говорит нам о том, как упорядочены результаты. Согласно документации, стандартное имя относится к крайнему правому столбцу. Здесь есть несколько столбцов.
ncalls - количество выполненных вызовов. tottime - общее время, проведенное в данной функции. percall - это отношение tottime к ncalls. cumtime - суммарное время, проведенное в данной и всех подфункциях. Это работает также и с рекурсивными функциями! Второй столбец percall - это коэффициент cumtime, деленный на примитивные вызовы filename:lineno(function) предоставляет соответствующие данные каждой функции Примитивный вызов - это вызов, который не был вызван с помощью рекурсии.
Это не очень интересный пример, поскольку здесь нет очевидных узких мест. Давайте создадим кусок кода с некоторыми встроенными узкими местами и посмотрим, обнаружит ли их профайлер.
import time def fast(): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; print(\u0026quot;I run fast!\u0026quot;) def slow(): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; time.sleep(3) print(\u0026quot;I run slow!\u0026quot;) def medium(): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; time.sleep(0.5) print(\u0026quot;I run a little slowly...\u0026quot;) def main(): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; fast() slow() medium() if __name__ == '__main__': main() В этом примере мы создаем четыре функции. Первые три выполняются с разной скоростью. Функция fast будет выполняться с нормальной скоростью; функция medium будет выполняться примерно полсекунды, а slow* функции потребуется около 3 секунд. Функция main вызывает остальные три. Теперь давайте запустим cProfile против этой маленькой глупой программы:
\u0026gt;\u0026gt;\u0026gt; import cProfile \u0026gt;\u0026gt;\u0026gt; import ptest \u0026gt;\u0026gt;\u0026gt; cProfile.run('ptest.main()') I run fast! I run slow! I run a little slowly... 8 function calls in 3.500 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 3.500 3.500 \u0026lt;string\u0026gt;:1(\u0026lt;module\u0026gt;) 1 0.000 0.000 0.500 0.500 ptest.py:15(medium) 1 0.000 0.000 3.500 3.500 ptest.py:21(main) 1 0.000 0.000 0.000 0.000 ptest.py:4(fast) 1 0.000 0.000 3.000 3.000 ptest.py:9(slow) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} 2 3.499 1.750 3.499 1.750 {time.sleep} На этот раз мы видим, что программе потребовалось 3,5 секунды для выполнения. Если вы изучите результаты, то увидите, что cProfile определил slow функцию как занявшую 3 секунды. Это самое большое узкое место после main функции. Обычно, когда вы обнаруживаете такое узкое место, вы пытаетесь найти более быстрый способ выполнения кода или, возможно, решаете, что время выполнения является приемлемым. В данном примере мы знаем, что лучший способ ускорить работу функции - удалить вызов time.sleep или, по крайней мере, уменьшить длительность сна.
Вы также можете вызвать cProfile в командной строке, а не использовать его в интерпретаторе. Вот один из способов сделать это:
python -m cProfile ptest.py Это позволит запустить cProfile против вашего сценария точно так же, как мы делали это раньше. Но что если вы хотите сохранить результаты работы профайлера? Это легко сделать с помощью cProfile! Все, что вам нужно сделать, это передать ему команду -o, за которой следует имя (или путь) выходного файла. Вот пример:
python -m cProfile -o output.txt ptest.py К сожалению, файл, который он выводит, не совсем удобен для чтения. Если вы хотите прочитать этот файл, то вам нужно использовать модуль Python pstats. Вы можете использовать pstats для форматирования вывода различными способами. Вот код, который показывает, как получить результат, похожий на тот, что мы видели до сих пор:
\u0026gt;\u0026gt;\u0026gt; import pstats \u0026gt;\u0026gt;\u0026gt; p = pstats.Stats(\u0026quot;output.txt\u0026quot;) \u0026gt;\u0026gt;\u0026gt; p.strip_dirs().sort_stats(-1).print_stats() Thu Mar 20 18:32:16 2014 output.txt 8 function calls in 3.501 seconds Ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 3.501 3.501 ptest.py:1(\u0026lt;module\u0026gt;) 1 0.001 0.001 0.500 0.500 ptest.py:15(medium) 1 0.000 0.000 3.501 3.501 ptest.py:21(main) 1 0.001 0.001 0.001 0.001 ptest.py:4(fast) 1 0.001 0.001 3.000 3.000 ptest.py:9(slow) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} 2 3.499 1.750 3.499 1.750 {time.sleep} \u0026lt;pstats.Stats instance at 0x017C9030\u0026gt; Вызов strip_dirs удалит из вывода все пути к модулям, а вызов sort_stats выполнит сортировку, которую мы привыкли видеть. В документации по cProfile есть множество действительно интересных примеров, показывающих различные способы извлечения информации с помощью модуля pstats.
Подведение итогов На данном этапе вы должны уметь использовать модуль cProfile для диагностики причин медленной работы вашего кода. Возможно, вы также захотите взглянуть на модуль Python timeit. Он позволяет засекать время на небольших участках кода, если вы не хотите разбираться со сложностями, связанными с профилированием. Есть также несколько других модулей сторонних разработчиков, которые хорошо подходят для профилирования, например, проекты line_profiler и memory_profiler.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter27_profiling/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day27/":{title:"27. Работа с сетью в Python",tags:["devops"],content:`Практическое знакомство с Python и сетью В этом заключительном разделе основ работы с сетью мы рассмотрим некоторые задачи и инструменты автоматизации с помощью нашей лабораторной среды, созданной День 26
Мы будем использовать туннель SSH для подключения к нашим устройствам с нашего клиента по сравнению с telnet. Туннель SSH, созданный между клиентом и устройством, зашифрован. Мы также рассмотрели SSH в разделе Linux в День 18
Доступ к нашей виртуальной эмулируемой среде Чтобы мы могли взаимодействовать с нашими коммутаторами, нам либо нужна рабочая станция внутри сети EVE-NG, и вы можете развернуть там Linux-систему с установленным Python для выполнения вашей автоматизации (Ресурс для настройки Linux внутри EVE-NG) или можно сделать как я и определить облако для доступа со своей рабочей станции.
Для этого мы щелкнули правой кнопкой мыши на нашем холсте и выбрали сеть, а затем выбрали \u0026ldquo;Management(Cloud0)\u0026rdquo;, чтобы подключиться к нашей домашней сети.
Однако внутри этой сети у нас ничего нет, поэтому нам нужно добавить соединения из новой сети на каждое из наших устройств. Я вошел в систему на каждом из наших устройств и выполнил следующие команды для интерфейсов, применимых к тому месту, где появляется облако.
enable config t int gi0/0 ip add dhcp no sh exit exit sh ip int br Последний шаг дает нам адрес DHCP из нашей домашней сети. Список сетей моего устройства выглядит следующим образом:
Node IP Address Home Network IP Router 10.10.88.110 192.168.169.115 Switch1 10.10.88.111 192.168.169.178 Switch2 10.10.88.112 192.168.169.193 Switch3 10.10.88.113 192.168.169.125 Switch4 10.10.88.114 192.168.169.197 SSH к сетевому устройству Имея все вышеперечисленное, мы теперь можем подключаться к нашим устройствам в нашей домашней сети, используя нашу рабочую станцию. Я использую Putty, но также имею доступ к другим терминалам, таким как git bash, которые дают мне возможность подключаться к нашим устройствам по SSH.
Ниже вы можете видеть, что у нас есть SSH-соединение с нашим маршрутизатором. (Р1)
Использование Python для сбора информации с наших устройств Первый пример того, как мы можем использовать Python, — это сбор информации со всех наших устройств, и, в частности, я хочу иметь возможность подключаться к каждому из них и запускать простую команду, чтобы предоставить мне конфигурацию и настройки интерфейса. Я сохранил этот скрипт:
#!/usr/bin/env python from netmiko import ConnectHandler from getpass import getpass #password = getpass() R1 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.115\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW1 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.178\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW2 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.193\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW3 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.125\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW4 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.197\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } command = \u0026quot;show ip int brief\u0026quot; for device in (R1, SW1, SW2, SW3, SW4): net_connect = ConnectHandler(**device) print(net_connect.find_prompt()) print(net_connect.send_command(command)) net_connect.disconnect() Теперь, когда я запускаю это, я вижу каждую конфигурацию порта на всех моих устройствах.
Это может быть удобно, если у вас много разных устройств, создайте этот один скрипт, чтобы вы могли централизованно контролировать и быстро понимать все конфигурации в одном месте.
Использование Python для настройки наших устройств Вышеупомянутое полезно, но как насчет использования Python для настройки наших устройств, в нашем сценарии у нас есть транковый порт между \u0026lsquo;SW1\u0026rsquo; и \u0026lsquo;SW2\u0026rsquo;, снова представьте, если бы это нужно было сделать на многих из тех же коммутаторов, которые мы хотим автоматизировать, и не нужно вручную подключаться к каждому коммутатору, чтобы внести изменения в конфигурацию.
Для этого мы можем использовать следующий скрипт. Это подключится через SSH и выполнит это изменение на нашем \u0026lsquo;SW1\u0026rsquo;, которое также изменится на \u0026lsquo;SW2\u0026rsquo;.
from netmiko import ConnectHandler SW2 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.193\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, \u0026quot;secret\u0026quot;: \u0026quot;access123\u0026quot;, } core_sw_config = [\u0026quot;int range gig0/1 - 2\u0026quot;, \u0026quot;switchport trunk encapsulation dot1q\u0026quot;, \u0026quot;switchport mode trunk\u0026quot;, \u0026quot;switchport trunk allowed vlan 1,2\u0026quot;] print(\u0026quot;########## Connecting to Device {0} ############\u0026quot;.format(SW2)) net_connect = ConnectHandler(**SW2) net_connect.enable() print(\u0026quot;***** Sending Configuration to Device *****\u0026quot;) net_connect.send_config_set(core_sw_config) Теперь если посмотреть на код, вы увидите, что появляется сообщение «sending configuration to device», но нет подтверждения того, что это произошло. Мы могли бы добавить дополнительный код в наш скрипт, чтобы выполнить эту проверку и проверку на нашем switch или мы могли бы изменить наш сценарий, прежде чем показать нам это.
#!/usr/bin/env python from netmiko import ConnectHandler from getpass import getpass #password = getpass() SW1 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.178\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW2 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.193\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW3 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.125\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } SW4 = { \u0026quot;device_type\u0026quot;: \u0026quot;cisco_ios\u0026quot;, \u0026quot;host\u0026quot;: \u0026quot;192.168.169.197\u0026quot;, \u0026quot;username\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;access123\u0026quot;, } command = \u0026quot;show int trunk\u0026quot; for device in (SW1, SW2, SW3, SW4): net_connect = ConnectHandler(**device) print(net_connect.find_prompt()) print(net_connect.send_command(command)) net_connect.disconnect() Резервное копирование конфигураций вашего устройства Другим вариантом использования может быть захват наших сетевых конфигураций и обеспечение их резервного копирования, но опять же, мы не хотим подключаться ко всем устройствам, которые у нас есть в нашей сети, поэтому мы также можем автоматизировать это с помощью скрипта
import sys import time import paramiko import os import cmd import datetime now = datetime.datetime.now() dt_string = now.strftime(\u0026quot;%d/%m/%Y %H:%M:%S\u0026quot;) print(\u0026quot;Your backup has started at\u0026quot;, dt_string)	tic = time.perf_counter() #user = input(\u0026quot;Enter username:\u0026quot;) #password = input(\u0026quot;Enter Paswd:\u0026quot;) #enable_password = input(\u0026quot;Enter enable pswd:\u0026quot;) user = \u0026quot;admin\u0026quot; password = \u0026quot;access123\u0026quot; enable_password = \u0026quot;access123\u0026quot; port=22 f0 = open('backup.txt') for ip in f0.readlines(): ip = ip.strip() filename_prefix ='/Users/shambhu/Documents' + ip ssh = paramiko.SSHClient() ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy()) ssh.connect(ip,port, user, password, look_for_keys=False) chan = ssh.invoke_shell() time.sleep(2) chan.send('enable\\n') chan.send(enable_password +'\\n') time.sleep(1) chan.send('term len 0\\n') time.sleep(1) chan.send('sh run\\n') time.sleep(20) output = chan.recv(999999) filename = \u0026quot;%s_%.2i%.2i%i_%.2i%.2i%.2i\u0026quot; % (ip,now.year,now.month,now.day,now.hour,now.minute,now.second) f1 = open(filename, 'a') f1.write(output.decode(\u0026quot;utf-8\u0026quot;) ) f1.close() ssh.close() f0.close() toc = time.perf_counter() print(\u0026quot;Congratulations You Have Backed Up Your 90DaysOfDevOps Lab\u0026quot;) print(f\u0026quot;Your backup duration was {toc - tic:0.4f} seconds\u0026quot;) dt_string = now.strftime(\u0026quot;%d/%m/%Y %H:%M:%S\u0026quot;) print(\u0026quot;Your backup completed at\u0026quot;, dt_string) Вам также потребуется заполнить backup.txt IP-адресами, для которых вы хотите сделать резервную копию.
192.168.169.115 192.168.169.178 192.168.169.193 192.168.169.125 192.168.169.197 Запустите свой скрипт, и вы должны увидеть что-то вроде того, что показано ниже.
Это может быть я просто пишу простой скрипт печати на питоне, поэтому я также должен показать вам файлы резервных копий. Paramiko Широко используемый модуль Python для SSH. Вы можете узнать больше по официальной ссылке GitHub здесь
Мы можем установить этот модуль с помощью команды pip install paramiko.
Мы можем проверить установку, войдя в оболочку Python и импортировав модуль paramiko.
Netmiko Модуль netmiko предназначен специально для сетевых устройств, тогда как paramiko — это более широкий инструмент для обработки SSH-соединений в целом.
Netmiko, который мы использовали выше вместе с paramiko, можно установить с помощью pip install netmiko.
Netmiko поддерживает множество сетевых поставщиков и устройств, список поддерживаемых устройств можно найти на странице GitHub.
Другие модули Также стоит упомянуть несколько других модулей, на которые у нас не было возможности взглянуть, но они дают гораздо больше функциональных возможностей, когда речь идет об автоматизации сети.
netaddr используется для работы с IP-адресами и управления ими, опять же установка проста с помощью pip install netaddr
вы можете захотеть сохранить большую часть конфигурации вашего коммутатора в электронной таблице Excel, xlrd позволит вашим сценариям читать книгу Excel и преобразовывать строки и столбцы в матрицу. pip install xlrd, чтобы установить модуль.
Еще несколько случаев использования сетевой автоматизации, которые я не имел возможности изучить, можно найти здесь
Я думаю, что это завершает наш раздел «Сетевые ресурсы» #90DaysOfDevOps. Networking — это одна из областей, которую я действительно не касался какое-то время, и есть так много всего, что нужно осветить, но я надеюсь, что мои заметки и ресурсы, которыми я делюсь, будут полезны для некоторый.
Ресурсы Free Course: Introduction to EVE-NG EVE-NG - Creating your first lab 3 Necessary Skills for Network Automation Computer Networking full course Practical Networking Python Network Automation Hands-On Enterprise Automation with Python (Book) Увидимся завтра, где начнем изучать облачные вычисления и получите хорошее представление и базовые знания
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day27/"},"https://romankurnovskii.com/ru/docs/python101/chapter28_testing/":{title:"28. Введение в тестирование",tags:[],content:`Python включает пару встроенных модулей для тестирования кода. Эти два метода называются doctest и unittest. Сначала мы рассмотрим, как использовать doctest, а во втором разделе мы представим модульные тесты, используя Test Driven Development.
Тестирование с помощью doctest Модуль doctest будет искать в вашем коде фрагменты текста, напоминающие интерактивные сессии Python. Затем он выполнит эти сессии, чтобы проверить, что они работают именно так, как написано. Это означает, что если вы написали пример в docstring, который показывает вывод с пробелом или табуляцией, то фактический вывод функции также должен содержать пробел. В большинстве случаев именно в docstring вы захотите поместить свои тесты. Будут рассмотрены следующие аспекты работы с doctest:
Как запустить doctest из терминала Как использовать doctest внутри модуля Как запустить doctest из отдельного файла. Давайте начнем!
Запуск doctest через терминал Мы начнем с создания действительно простой функции, которая будет удваивать все, что ей дано. Мы включим пару тестов в docstring функции. Вот код (обязательно сохраните его под именем \u0026ldquo;dtest1.py\u0026rdquo;):
# dtest1.py def double(a): \u0026quot;\u0026quot;\u0026quot; \u0026gt;\u0026gt;\u0026gt; double(4) 8 \u0026gt;\u0026gt;\u0026gt; double(9) 18 \u0026quot;\u0026quot;\u0026quot; return a*2 Теперь нам нужно просто запустить этот код в doctest. Откройте терминал (или командную строку) и измените директории на папку, содержащую ваш скрипт. Вот скриншот того, что я сделал:
Вы заметите, что в первом примере я выполнил следующее:
python -m doctest dtest1.py Это запустило тест и ничего не вывело на экран. Когда вы не видите ничего на экране, это означает, что все тесты прошли успешно. Во втором примере показана следующая команда:
python -m doctest -v dtest1.py \u0026ldquo;-v\u0026rdquo; означает, что мы хотим получить подробный вывод, что мы и получили. Откройте код снова и добавьте пробел после \u0026ldquo;18\u0026rdquo; в строке docstring. Затем снова запустите тест. Вот вывод, который я получил:
В сообщении об ошибке говорится, что ожидалось \u0026ldquo;18\u0026rdquo;, а получилось \u0026ldquo;18\u0026rdquo;. Что здесь происходит? Мы добавили пробел после \u0026ldquo;18\u0026rdquo; в нашу строку документа, поэтому doctest на самом деле ожидал получить число \u0026ldquo;18\u0026rdquo;, за которым следует пробел. Также остерегайтесь помещать словари в качестве вывода в примерах docstring. Словари могут располагаться в любом порядке, поэтому вероятность того, что они совпадут с реальным выводом, не очень велика.
Запуск doctest внутри модуля Давайте немного изменим пример, чтобы импортировать модуль doctest и использовать его функцию testmod.
def double(a): \u0026quot;\u0026quot;\u0026quot; \u0026gt;\u0026gt;\u0026gt; double(4) 8 \u0026gt;\u0026gt;\u0026gt; double(9) 18 \u0026quot;\u0026quot;\u0026quot; return a*2 if __name__ == \u0026quot;__main__\u0026quot;: import doctest doctest.testmod(verbose=True) Здесь мы импортируем doctest и вызываем doctest.testmod. Мы передаем ему ключевой аргумент verbose=True, чтобы мы могли увидеть некоторые выходные данные. В противном случае этот скрипт будет выполняться без какого-либо вывода, что будет означать, что тесты прошли успешно.
Если вы не хотите жестко кодировать опцию verbose, вы также можете сделать это в командной строке:
python dtest2.py -v Теперь мы готовы узнать, как поместить тесты в отдельный файл.
Запуск doctest из отдельного файла Модуль doctest также поддерживает размещение тестов в отдельном файле. Это позволяет нам отделить тесты от кода. Давайте вычленим тесты из предыдущего примера и поместим их в текстовый файл с именем *tests.txt:
The following are tests for dtest2.py \u0026gt;\u0026gt;\u0026gt; from dtest2 import double \u0026gt;\u0026gt;\u0026gt; double(4) 8 \u0026gt;\u0026gt;\u0026gt; double(9) 18 Давайте запустим этот тестовый файл в командной строке. Вот как:
Вы заметите, что синтаксис для вызова doctest с текстовым файлом такой же, как и для вызова его с файлом Python. Результаты также одинаковы. В данном случае есть три теста вместо двух, потому что мы также импортируем модуль. Вы также можете запускать тесты, находящиеся в текстовом файле, внутри интерпретатора Python. Вот один из примеров:
Здесь мы просто импортируем doctest и вызываем его метод testfile. Обратите внимание, что вам также нужно передать имя файла или путь к нему функции testfile. Она вернет объект TestResults, содержащий информацию о том, сколько тестов было выполнено и сколько из них не удалось.
Разработка, управляемая тестами, с помощью unittest В этом разделе вы узнаете о разработке под управлением тестов (TDD) в Python с помощью встроенного в Python модуля unittest. Я хочу поблагодарить Мэтта и Аарона за их помощь в демонстрации того, как TDD работает в реальном мире. Чтобы продемонстрировать концепции TDD, мы рассмотрим, как забивать кегли в Python. Если вы еще не знаете правил игры в боулинг, воспользуйтесь Google. Как только вы узнали правила, пришло время написать несколько тестов. Если вы не знали, идея Test Driven Development заключается в том, что вы пишете тесты ДО того, как пишете фактический код. В этой главе мы напишем тест, а затем код для прохождения теста. Мы будем итерационно переходить от написания тестов к написанию кода, пока не закончим. В этой главе мы напишем всего три теста. Давайте начнем!
Первый тест Первым тестом мы протестируем наш игровой объект и посмотрим, сможет ли он вычислить правильное общее количество очков, если мы бросим одиннадцать раз и каждый раз будем сбивать только одну кеглю. Это должно дать нам общее число одиннадцать.
import unittest class TestBowling(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def test_all_ones(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; game = Game() game.roll(11, 1) self.assertEqual(game.score, 11) Это довольно простой тест. Мы создаем игровой объект, а затем вызываем его метод roll одиннадцать раз, причем каждый раз счет равен единице. Затем мы используем метод assertEqual из модуля unittest для проверки правильности счета игрового объекта (то есть одиннадцати). Следующий шаг - написать самый простой код, который вы можете придумать, чтобы тест прошел. Вот один из примеров:
class Game: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.score = 0 def roll(self, numOfRolls, pins): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; for roll in numOfRolls: self.score += pins Для простоты вы можете просто скопировать и вставить это в тот же файл с вашим тестом. Мы разобьем их на два файла для нашего следующего теста. В любом случае, как вы можете видеть, наш класс Game очень прост. Все, что было необходимо для прохождения теста, это свойство score и метод roll, который может его обновлять.
Давайте запустим тест и посмотрим, пройдет ли он! Самый простой способ запустить тесты - добавить следующие две строки кода в конец файла:
if __name__ == '__main__': unittest.main() Затем просто запустите файл Python через командную строку. Если вы это сделаете, вы должны получить что-то вроде следующего:
E ====================================================================== ERROR: test_all_ones (__main__.TestBowling) Constructor ---------------------------------------------------------------------- Traceback (most recent call last): File \u0026quot;C:\\Users\\Mike\\Documents\\Scripts\\Testing\\bowling\\test_one.py\u0026quot;, line 27, in test_all_ones game.roll(11, 1) File \u0026quot;C:\\Users\\Mike\\Documents\\Scripts\\Testing\\bowling\\test_one.py\u0026quot;, line 15, in roll for roll in numOfRolls: TypeError: 'int' object is not iterable ---------------------------------------------------------------------- Ran 1 test in 0.001s FAILED (errors=1) Упс! Где-то здесь у нас ошибка. Похоже, что мы передаем целое число, а затем пытаемся выполнить итерацию по нему. Это не работает! Чтобы все заработало, нам нужно изменить метод roll нашего объекта Game на следующий:
def roll(self, numOfRolls, pins): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; for roll in range(numOfRolls): self.score += pins Если вы запустите тест сейчас, вы должны получить следующее:
. ---------------------------------------------------------------------- Ran 1 test in 0.000s OK Обратите внимание на \u0026ldquo;.\u0026rdquo;, потому что это важно. Эта маленькая точка означает, что был выполнен один тест и что он прошел. Надпись \u0026ldquo;OK\u0026rdquo; в конце также указывает на этот факт. Если вы изучите исходный вывод, то заметите, что он начинается с буквы \u0026ldquo;E\u0026rdquo;, означающей ошибку, а точки там нет! Давайте перейдем к тесту №2.
Второй тест Во втором тесте мы проверим, что происходит, когда мы получаем страйк. Нам придется изменить первый тест, чтобы использовать список для количества сбитых кеглей в каждом кадре, поэтому мы рассмотрим оба теста. Вы, вероятно, обнаружите, что это довольно распространенный процесс, когда вам может понадобиться отредактировать пару тестов из-за фундаментальных изменений в том, на что вы тестируете. Обычно это происходит только в начале вашего кодирования, и в дальнейшем вы станете лучше, так что вам не придется этого делать. Поскольку я делаю это в первый раз, я не думал достаточно далеко вперед. В любом случае, давайте посмотрим на код:
from game import Game import unittest class TestBowling(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def test_all_ones(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; game = Game() pins = [1 for i in range(11)] game.roll(11, pins) self.assertEqual(game.score, 11) def test_strike(self): \u0026quot;\u0026quot;\u0026quot; A strike is 10 + the value of the next two rolls. So in this case the first frame will be 10+5+4 or 19 and the second will be 5+4. The total score would be 19+9 or 28. \u0026quot;\u0026quot;\u0026quot; game = Game() game.roll(11, [10, 5, 4]) self.assertEqual(game.score, 28) if __name__ == '__main__': unittest.main() Давайте посмотрим на наш первый тест и на то, как он изменился. Да, здесь мы немного нарушаем правила, когда речь идет о TDD. Не стесняйтесь НЕ изменять первый тест и посмотрите, что сломается. В методе test_all_ones мы установили переменную pins равной list comprehension, что создало список из одиннадцати единиц. Затем мы передали его в метод roll нашего game объекта вместе с количеством бросков.
Во втором тесте мы бросаем страйк в первом броске, пятерку во втором и четверку в третьем. Заметьте, что мы пошли по головам и сказали, что передаем одиннадцать бросков, но передаем только три. Это означает, что нам нужно установить остальные восемь бросков на нули. Далее мы используем наш надежный метод assertEqual, чтобы проверить, получили ли мы правильное общее число. Наконец, обратите внимание, что теперь мы импортируем класс Game, а не сохраняем его вместе с тестами. Теперь нам нужно реализовать код, необходимый для прохождения этих двух тестов. Давайте рассмотрим одно из возможных решений:
class Game: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.score = 0 self.pins = [0 for i in range(11)] def roll(self, numOfRolls, pins): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; x = 0 for pin in pins: self.pins[x] = pin x += 1 x = 0 for roll in range(numOfRolls): if self.pins[x] == 10: self.score = self.pins[x] + self.pins[x+1] + self.pins[x+2] else: self.score += self.pins[x] x += 1 print(self.score) Сразу после этого вы заметите, что у нас есть новый атрибут класса под названием self.pins, который содержит список кеглей по умолчанию, состоящий из одиннадцати нулей. Затем в нашем методе roll в первом цикле мы добавляем нужные очки в нужную позицию в списке self.pins. Затем во втором цикле мы проверяем, равна ли сумма сбитых кеглей десяти. Если да, то мы добавляем его и следующие два очка к счету. В противном случае мы делаем то же самое, что и раньше. В конце метода мы выводим счет, чтобы проверить, соответствует ли он нашим ожиданиям. На этом этапе мы готовы к написанию нашего последнего теста.
Третий (и последний) тест В нашем последнем тесте мы проверим правильность результата, который будет получен, если кто-то бросит запасной вариант. Тест прост, решение немного сложнее. Пока мы здесь, мы немного рефакторим код теста. Как обычно, сначала мы рассмотрим тест.
from game_v2 import Game import unittest class TestBowling(unittest.TestCase): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def setUp(self): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; self.game = Game() def test_all_ones(self): \u0026quot;\u0026quot;\u0026quot; If you don't get a strike or a spare, then you just add up the face value of the frame. In this case, each frame is worth one point, so the total is eleven. \u0026quot;\u0026quot;\u0026quot; pins = [1 for i in range(11)] self.game.roll(11, pins) self.assertEqual(self.game.score, 11) def test_spare(self): \u0026quot;\u0026quot;\u0026quot; A spare is worth 10, plus the value of your next roll. So in this case, the first frame will be 5+5+5 or 15 and the second will be 5+4 or 9. The total is 15+9, which equals 24, \u0026quot;\u0026quot;\u0026quot; self.game.roll(11, [5, 5, 5, 4]) self.assertEqual(self.game.score, 24) def test_strike(self): \u0026quot;\u0026quot;\u0026quot; A strike is 10 + the value of the next two rolls. So in this case the first frame will be 10+5+4 or 19 and the second will be 5+4. The total score would be 19+9 or 28. \u0026quot;\u0026quot;\u0026quot; self.game.roll(11, [10, 5, 4]) self.assertEqual(self.game.score, 28) if __name__ == '__main__': unittest.main() Во-первых, мы добавили метод setUp, который будет создавать для нас объект self.game для каждого теста. Если бы мы обращались к базе данных или чему-то подобному, у нас, вероятно, был бы метод tear down для закрытия соединений, файлов и тому подобных вещей. Они выполняются в начале и в конце каждого теста, соответственно, если они существуют. Тесты test_all_ones и test_strike в основном одинаковы, за исключением того, что теперь они используют \u0026ldquo;self.game\u0026rdquo;. Единственный новый тест - test_spare. В документации объясняется, как работают запасные части, а код состоит всего из двух строк. Да, вы можете разобраться в этом. Давайте посмотрим на код, который нам понадобится для прохождения этих тестов:
# game_v2.py class Game: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.score = 0 self.pins = [0 for i in range(11)] def roll(self, numOfRolls, pins): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; x = 0 for pin in pins: self.pins[x] = pin x += 1 x = 0 spare_begin = 0 spare_end = 2 for roll in range(numOfRolls): spare = sum(self.pins[spare_begin:spare_end]) if self.pins[x] == 10: self.score = self.pins[x] + self.pins[x+1] + self.pins[x+2] elif spare == 10: self.score = spare + self.pins[x+2] x += 1 else: self.score += self.pins[x] x += 1 if x == 11: break spare_begin += 2 spare_end += 2 print(self.score) Для этой части головоломки мы добавляем условный оператор в наш цикл. Чтобы вычислить значение запасного, мы используем позиции списка spare_begin и spare_end, чтобы получить нужные значения из нашего списка, а затем суммируем их. Вот для чего нужна переменная spare. Возможно, ее лучше поместить в elif, но я оставлю это на усмотрение читателя. Технически, это только первая половина запасного результата. Вторая половина - это следующие два броска, которые вы найдете в вычислениях в части elif текущего кода. Остальная часть кода не изменилась.
Другие примечания Как вы уже догадались, модуль unittest имеет гораздо больше возможностей, чем то, что было рассмотрено здесь. Существует множество других утверждений, которые можно использовать для проверки результатов. Вы можете пропускать тесты, запускать тесты из командной строки, использовать TestLoader для создания набора тестов и многое, многое другое. Обязательно прочитайте полную документацию, когда у вас будет такая возможность, так как это руководство лишь поверхностно изучает эту библиотеку.
Подведение итогов На данном этапе вы должны понимать, как эффективно использовать модули doctest и unittest в своем собственном коде. Вам следует прочитать документацию Python по этим двум модулям, поскольку там есть дополнительная информация о других опциях и функциональных возможностях, которые вы можете найти полезными. Вы также знаете немного о том, как использовать концепции Test Driven Development при написании собственных сценариев.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter28_testing/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day28/":{title:"28. DevOps в облаке",tags:["devops"],content:`Общая картина: DevOps и облака Когда дело доходит до облачных вычислений и того, что они предлагают, это очень хорошо сочетается с духом и процессами DevOps. Мы можем думать об облачных вычислениях, предоставляющих технологии и услуги, в то время как DevOps, как мы уже много раз упоминали ранее, касается процессов и их улучшения.
Но начать с этого путешествия по обучению в облаке сложно, и убедиться, что вы знаете и понимаете все элементы или лучший сервис для выбора по правильной цене, сбивает с толку. Накладывается ли на облака парадигма DevOps? Мой ответ здесь — нет, но чтобы по-настоящему воспользоваться преимуществами облачных вычислений и, возможно, избежать больших счетов за облачные вычисления, от которых пострадало так много людей, важно думать об облачных вычислениях и DevOps вместе.
Если мы посмотрим на то, что мы подразумеваем под Public Cloud в общем смысле, речь идет о снятии некоторой ответственности с управляемой службы, чтобы вы и ваша команда могли сосредоточиться на более важных аспектах, имя которых должно быть приложением и конечными пользователями. . В конце концов, Public Cloud — это просто чей-то компьютер. В этом первом разделе я хочу немного подробнее рассказать о том, что такое Public Cloud, и о некоторых блоках, которые в целом называются Public Cloud .
SaaS Первая область, которую следует рассмотреть, — это программное обеспечение как услуга (SaaS - Software as a service,). Эта услуга устраняет почти все накладные расходы на управление службой, которую вы, возможно, когда-то запускали локально. Давайте подумаем о Microsoft Exchange для нашей электронной почты. Раньше это была физическая коробка, которая находилась в вашем центре обработки данных или, может быть, в шкафу под лестницей. Вам нужно будет кормить и поить этот сервер. Под этим я подразумеваю, что вам нужно будет обновлять его, и вы будете нести ответственность за покупку серверного оборудования, скорее всего, за установку операционной системы, установку необходимых приложений, а затем за исправление, если что-то пойдет не так, вам придется устранить неполадки и получить вещи встали на свои места.
О, и вам также нужно будет убедиться, что вы делаете резервную копию своих данных, хотя по большей части это не меняется и с SaaS.
Что делает SaaS и, в частности, Microsoft 365, потому что я упомянул, что Exchange устраняет эти накладные расходы на администрирование, и они предоставляют услугу, которая обеспечивает ваши функции обмена по почте, а также многие другие параметры производительности (Office 365) и варианты хранения (OneDrive), которые в целом дают большой опыт для конечного пользователя.
Широко распространены и другие приложения SaaS, такие как Salesforce, SAP, Oracle, Google, Apple. Все это избавляет от необходимости управлять большим количеством стека.
Я уверен, что есть история с приложениями на основе DevOps и SaaS, но я изо всех сил пытаюсь выяснить, что они могут собой представлять. Я знаю, что у Azure DevOps есть отличная интеграция с Microsoft 365, которую я мог бы изучить и сообщить.
Public Cloud Далее у нас есть public cloud. Большинство людей думают об этом по-разному, некоторые считают, что это только гипермасштаберы, такие как Microsoft Azure, Google Cloud Platform и AWS. Некоторые также видят в общедоступном облаке гораздо более широкое предложение, включающее не только гиперскейлеры, но и тысячи MSP (managed service provider) по всему миру. В этом посте мы собираемся рассмотреть общедоступное облако, включая гиперскейлеры и MSP, хотя позже мы специально углубимся в один или несколько гиперскейлеров, чтобы получить базовые знания.
тысячи других компаний могли бы присоединиться к этому, я просто выбираю из местных, региональных, телекоммуникационных и глобальных брендов, с которыми я работал и о которых знаю.
В разделе SaaS мы упомянули, что облако сняло ответственность или бремя администрирования частей системы. Если SaaS, мы видим, что многие уровни абстракции удалены, то есть физические системы, сеть, хранилище, операционная система и даже приложения в некоторой степени. Когда дело доходит до облака, существуют различные уровни абстракции, которые мы можем удалить или оставить в зависимости от ваших требований.
Мы уже упоминали SaaS, но есть еще по крайней мере два, которые следует упомянуть в отношении общедоступного облака.
Инфраструктура как услуга. Вы можете думать об этом уровне как о виртуальной машине, но в то время как локально вам придется заботиться о физическом уровне в облаке, это не так, физический уровень является обязанностью облачных провайдеров, и вы будете управлять и управлять операционной системой, данными и приложениями, которые вы хотите запустить.
Платформа как услуга. Это по-прежнему снимает ответственность уровней, и на самом деле это означает, что вы берете под свой контроль данные и приложение, но вам не нужно беспокоиться об аппаратном обеспечении или операционной системе.
Есть много других предложений AaS, но это два основных принципа. Вы можете увидеть предложения вокруг StaaS (Storage as a service), которые предоставляют вам уровень хранения, но не нужно беспокоиться об оборудовании под ним. Или вы, возможно, слышали о CaaS для контейнеров как об услуге, к которой мы вернемся позже. Еще одна услуга как услуга, которую мы рассмотрим в течение следующих 7 дней, — это FaaS (Functions as a Service), где, возможно, вам не нужна работающая система. все время, и вы просто хотите, чтобы функция выполнялась как и когда.
Есть много способов, которыми общедоступное облако может предоставить уровни абстракции управления, от которых вы хотите отказаться и заплатить за них.
Private Cloud Наличие собственного центра обработки данных не осталось в прошлом. Я думаю, что это стало возрождением среди многих компаний, которым было трудно управлять моделью OPEX, а также набором навыков только в использовании общедоступного облака.
Здесь важно отметить, что общедоступное облако, скорее всего, теперь будет вашей ответственностью и будет находиться на вашей территории.
У нас есть некоторые интересные вещи, происходящие в этой сфере не только с VMware, которая доминировала в эпоху виртуализации, и с локальными инфраструктурными средами. У нас также есть гиперскейлеры, предлагающие локальную версию своих публичных облаков. Hybrid Cloud В продолжение упоминаний о публичном и частном облаке мы также можем охватить обе эти среды, чтобы обеспечить гибкость между ними, возможно, воспользоваться услугами, доступными в общедоступном облаке, а затем также воспользоваться преимуществами функций и возможностей локальной среды. или это может быть правило, которое предписывает вам хранить данные локально. Собрав все это вместе, у нас есть много вариантов, где мы будем хранить и запускать наши рабочие нагрузки. Прежде чем мы перейдем к конкретному гипермасштабу, я спросил силу Твиттера, куда нам следует двигаться? Link to Twitter Poll
Какой бы процент ни получил самый высокий процент, мы углубимся в предложения, я думаю, что важно упомянуть, что услуги во всех них очень похожи, поэтому я говорю начать с одного, потому что я обнаружил, что, зная основа одного из них и как создавать виртуальные машины, настраивать сеть и т. д. Я смог перейти к другим и быстро набраться опыта в этих областях.
В любом случае, я поделюсь отличными БЕСПЛАТНЫМИ ресурсами, которые охватывают все три гиперскейлера.
Я также собираюсь разработать сценарий, как я делал это в других разделах, где мы можем что-то построить по мере продвижения по дням.
Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course AWS Basics for Beginners - Full Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day28/"},"https://romankurnovskii.com/ru/docs/python101/chapter29_pip/":{title:"29. Установка пакетов",tags:[],content:`Когда вы только начинаете программировать на Python, вы не задумываетесь о том, что вам может понадобиться установить внешний пакет или модуль. Но когда такая необходимость возникнет, вы захотите узнать, как это сделать в кратчайшие сроки! Пакеты Python можно найти по всему интернету. Большинство популярных из них можно найти в Python Package Index (PyPI). Вы также найдете множество пакетов Python на github, bitbucket и Google code. В этой статье мы рассмотрим следующие методы установки пакетов Python:
Установка из исходного кода easy_install pip Другие способы установки пакетов Установка из исходного кода Установка из исходного кода - отличный навык. Есть и более простые способы, о которых мы расскажем далее в статье. Однако есть некоторые пакеты, которые необходимо устанавливать из исходного кода. Например, чтобы использовать easy_install, вам нужно сначала установить setuptools. Для этого вам нужно скачать tar или zip файл из индекса пакетов Python и распаковать его где-нибудь в вашей системе. Затем найдите файл setup.py. Откройте сеанс терминала и перейдите в папку, содержащую файл setup. Затем выполните следующую команду:
python setup.py install Если Python отсутствует в системном пути, вы получите сообщение об ошибке, в котором будет сказано, что команда python не найдена или является неизвестным приложением. Вы можете вызвать эту команду, используя полный путь к Python. Вот как это можно сделать, если вы работаете в Windows:
c:\\python34\\python.exe setup.py install Этот метод особенно удобен, если у вас установлено несколько версий Python и вам нужно установить пакет на разные версии. Все, что вам нужно сделать, это ввести полный путь к нужной версии Python и установить пакет на нее.
Некоторые пакеты содержат C-код, например, заголовочные файлы C, которые необходимо скомпилировать, чтобы пакет установился правильно. В Linux обычно уже установлен компилятор C/C++, и вы можете установить пакет с минимальной головной болью. В Windows для корректной компиляции пакета вам потребуется установить правильную версию Visual Studio. Некоторые говорят, что можно использовать и MingW, но я пока не нашел способа заставить его работать. Если в пакете уже есть готовый установщик для Windows, используйте его. Тогда вам вообще не придется возиться с компиляцией.
Использование easy_install После установки setuptools вы можете использовать easy_install. Вы можете найти его установленным в папке Scripts вашей установки Python. Не забудьте добавить папку Scripts в системный путь, чтобы можно было вызывать easy_install из командной строки без указания полного пути. Попробуйте выполнить следующую команду, чтобы узнать обо всех опциях easy_install:
easy_install -h Когда вы хотите установить пакет с помощью easy_install, все, что вам нужно сделать, это следующее:
easy_install package_name easy_install попытается загрузить пакет из PyPI, скомпилировать его (если необходимо) и установить. Если вы зайдете в каталог site-packages вашего Python, вы найдете файл easy-install.pth, который будет содержать запись для всех пакетов, установленных с помощью easy_install. Этот файл используется Python для помощи в импорте модуля или пакета.
Вы также можете указать easy_install установить пакет с URL или с пути на вашем компьютере. Он также может устанавливать пакеты непосредственно из tar-файла. Вы можете использовать easy_install для обновления пакета с помощью команды -upgrade (или -U). Наконец, вы можете использовать easy_install для установки eggs Python. Файлы eeg можно найти на PyPI и в других местах. Egg - это, по сути, специальный zip-файл. На самом деле, если вы измените расширение на .zip, вы сможете разархивировать файл egg.
Вот несколько примеров:
easy_install -U SQLAlchemy easy_install http://example.com/path/to/MyPackage-1.2.3.tgz easy_install /path/to/downloaded/package Существуют некоторые проблемы с easy_install. Он будет пытаться установить пакет до завершения его загрузки. Не существует способа удалить пакет с помощью easy_install. Вам придется удалить пакет самостоятельно и обновить файл easy-install.pth, удалив запись о пакете. По этим и другим причинам в сообществе Python возникло желание создать что-то другое, что привело к появлению pip.
Использование pip Программа pip фактически поставляется с Python 3.4. Если у вас более старая версия Python, то вам придется установить pip вручную. Установка pip немного отличается от того, что мы обсуждали ранее. Вы по-прежнему заходите на PyPI, но вместо загрузки пакета и запуска его сценария setup.py, вам будет предложено загрузить один сценарий get-pip.py. Затем вам нужно будет выполнить его, сделав следующее:
python get-pip.py Это позволит установить setuptools или альтернативу setuptools под названием distribute, если одна из них еще не установлена. Она также установит pip. pip работает с CPython версий 2.6, 2.7, 3.1, 3.2, 3.3, 3.4, а также с pypy. Вы можете использовать pip для установки всего, что может установить easy_install, но вызов немного отличается. Чтобы установить пакет, сделайте следующее:
pip install package_name Чтобы обновить пакет, сделайте следующее:
pip install -U PackageName Вы можете вызвать pip -h, чтобы получить полный список всего, что может сделать pip. Одна вещь, которую pip может установить, но не может установить easy_install, - это формат Python wheel. Wheel - это архив в формате ZIP со специально отформатированным именем файла и расширением .whl. Вы также можете установить wheel с помощью собственной утилиты командной строки. С другой стороны, pip не может установить яйцо. Если вам нужно установить egg, воспользуйтесь easy_install.
Замечание о зависимостях Одним из преимуществ использования easy_install и pip является то, что если пакет имеет зависимости, указанные в скрипте setup.py, то easy_install и pip попытаются загрузить и установить и их. Это может облегчить многие разочарования, когда вы пробуете новые пакеты и не знаете, что пакет A зависит от пакетов B, C и D. С easy_install или pip вам больше не нужно беспокоиться об этом.
Подведение итогов На данный момент вы можете установить практически любой пакет, который вам нужен, при условии, что пакет поддерживает вашу версию Python. Программисту Python доступно множество инструментов. Хотя упаковка в Python сейчас немного запутана, как только вы узнаете, как использовать соответствующие инструменты, вы обычно сможете установить или упаковать то, что вам нужно. Мы подробнее рассмотрим создание собственных пакетов, eggs и wheels в части V.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter29_pip/"},"https://romankurnovskii.com/ru/docs/python101/04-part_iv/":{title:"Часть IV - Советы, приемы и учебные пособия",tags:[],content:`В части IV вы узнаете, как устанавливать пакеты сторонних разработчиков из Python Package Index (PyPI). Вы узнаете немного о easy_install, pip и setup.py* и о том, как использовать эти инструменты для установки пакетов. Однако это только первая глава. Вот список пакетов, о которых вы узнаете:
configobj - работа с файлами Config более \u0026ldquo;питоническим\u0026rdquo; способом. lxml - пакет для работы с XML pylint / pyflakes - анализаторы кода Python requests - версия urllib для работы с Python SQLAlchemy - объектно-реляционный маппер для Python virtualenv - узнайте о виртуальных средах в Python Мы будем рассматривать configobj, потому что я считаю, что он работает лучше, чем ConfigParser, модуль, поставляемый вместе с Python. Пакет configobj имеет более интуитивный и мощный интерфейс, чем ConfigParser. В следующей главе мы рассмотрим модуль lxml и узнаем несколько новых способов чтения, разбора и создания XML. В четвертой главе мы рассмотрим pylint и pyflakes, которые отлично подходят для анализа кода. Они могут посмотреть на ваш модуль и проверить его на наличие ошибок. pylint также можно использовать для приведения вашего кода в соответствие с PEP8, руководством по стилю Python.
Пакет requests - отличная замена модулю urllib. Его интерфейс проще, а документация довольно хорошая. SQLAlchemy - это лучший объектно-реляционный маппер для Python. Он позволяет писать SQL-запросы, таблицы и т.д. в коде Python. Одна из его лучших особенностей заключается в том, что если вам понадобится сменить бэкенд базы данных, вам не придется сильно менять свой код, чтобы продолжать работать с этой базой данных. В последней главе мы рассмотрим virtualenv - удобный модуль, позволяющий создавать мини-виртуальные среды, в которых вы можете писать свой код. Эти виртуальные среды особенно удобны для тестирования новых модулей или просто новых выпусков модулей, прежде чем вы примените их к своей основной установке Python.
`,url:"https://romankurnovskii.com/ru/docs/python101/04-part_iv/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day29/":{title:"29. Знакомство с Microsoft Azure",tags:["devops"],content:`Знакомство с Microsoft Azure Прежде чем мы начнем, победителем опроса в Твиттере стала Microsoft Azure, отсюда и название страницы. Это было довольно интересно увидеть результаты, полученные в течение 24 часов.
Я бы сказал, что с точки зрения освещения этой темы я лучше понимаю и пользуюсь услугами, доступных в Microsoft Azure. Сегодня я склоняюсь к Amazon AWS. Однако я выделил разделы для всех трех основных облачных провайдеров.
Я ценю, что их больше, и опрос включал только эти 3, и, в частности, были некоторые комментарии об Oracle Cloud. Я хотел бы услышать больше о других облачных провайдерах, которые используются в дикой природе.
Основы Предоставляет общедоступные облачные сервисы
Географически распределены (более 60 регионов по всему миру)
Доступ через Интернет и/или частные соединения
Мультитенантная модель
Выставление счетов на основе потребления - (Плати по мере использования | Плати по мере роста)
Большое количество типов услуг и предложений для различных требований.
Microsoft Azure Global Infrastructure Сколько бы мы ни говорили о SaaS и Hybrid Cloud, мы не планируем затрагивать эти темы здесь.
Лучший способ начать и продолжить работу — щелкнуть ссылку, которая позволит вам зарегистрировать Бесплатную учетную запись Microsoft Azure
Регионы Я связал интерактивную карту выше, но мы можем видеть изображение под широтой регионов, предлагаемых на платформе Microsoft Azure по всему миру. image taken from Microsoft Docs - 01/05/2021
Вы также увидите несколько sovereign облаков, что означает, что они не связаны или не могут взаимодействовать с другими регионами, например, они будут связаны с правительствами, такими как «AzureUSGovernment», а также «AzureChinaCloud» и другими.
Когда мы развертываем наши службы в Microsoft Azure, мы выбираем регион почти для всего. Однако важно отметить, что не все услуги доступны в каждом регионе. Вы можете увидеть Продукты, доступные по регионам на момент написания моего письма, что в западно-центральной части США мы не можем использовать Azure Databricks.
Я также упомянул «почти все» выше, есть определенные службы, связанные с регионом, такие как Azure Bot Services, Bing Speech, Azure Virtual Desktop, статические веб-приложения и некоторые другие.
За кулисами регион может состоять из более чем одного центра обработки данных. Они будут называться зонами доступности.
На изображении ниже вы увидите, что это снова взято из официальной документации Microsoft, в которой описывается, что такое регион и как он состоит из зон доступности. Однако не во всех регионах есть несколько зон доступности.
В Microsoft хорошая документация, и вы можете прочитать больше о Регионах и зонах доступности здесь.
Подписки Помните, что мы упоминали, что Microsoft Azure — это облако модели потребления, и вы обнаружите, что все основные поставщики облачных услуг следуют этой модели.
Если вы являетесь Предприятием, вы можете захотеть или заключить соглашение Enterprise с Microsoft, чтобы ваша компания могла использовать эти службы Azure.
Если вы похожи на меня и используете Microsoft Azure для обучения, у нас есть несколько других вариантов.
У нас есть Бесплатная учетная запись Microsoft Azure, которая обычно дает вам несколько бесплатных облачных кредитов, которые вы можете потратить в Azure в течение некоторого времени.
Существует также возможность использовать подписку Visual Studio, которая дает вам, возможно, несколько бесплатных кредитов каждый месяц вместе с вашей годовой подпиской на Visual Studio, которая много лет назад была широко известна как MSDN. Visual Studio
Затем, наконец, вручите кредитную карту и заплатите, как вы идете, модель. Оплата по мере использования
Подписку можно рассматривать как границу между разными подписками, потенциально являющимися центрами затрат, но совершенно разными средами. Подписка — это место, где создаются ресурсы.
Management Groups Группы управления дают нам возможность разделять управление в нашей Azure AD или в нашей клиентской среде. Группы управления позволяют нам контролировать политики, RBAC (Role-based access control) и бюджеты.
Подписки принадлежат этим группам управления, поэтому у вас может быть много подписок в вашем клиенте Azure AD. Эти подписки также могут управлять политиками, RBAC и бюджетами.
Resource Manager and Resource Groups Azure Resource Manager
API на основе JSON, основанный на поставщиках ресурсов. Ресурсы принадлежат группе ресурсов и имеют общий жизненный цикл. Параллелизм Развертывания на основе JSON являются декларативными, идемпотентными и понимают зависимости между ресурсами для управления созданием и порядком. Resource Groups
Каждый ресурс Azure Resource Manager существует в одной и только одной группе ресурсов! Группы ресурсов создаются в регионе, который может содержать ресурсы из-за пределов региона. Ресурсы можно перемещать между группами ресурсов Группы ресурсов не отгорожены от других групп ресурсов, между группами ресурсов может быть связь. Группы ресурсов также могут управлять политиками, RBAC и бюджетами. Практика Давайте подключимся и убедимся, что у нас есть Подписка. Мы можем проверить нашу простую готовую Группу управления. Затем мы можем пойти и создать новую выделенную Группу ресурсов в предпочитаемом нами Регионе.
При первом входе на наш портал Azure вверху вы увидите возможность поиска ресурсов, служб и документов.
Сначала мы рассмотрим нашу подписку. Здесь вы увидите, что я использую подписку Visual Studio Professional, которая дает мне бесплатный \u0026ldquo;кредит\u0026rdquo; каждый месяц.
Если мы углубимся в это, вы получите более широкое представление и посмотрите, что происходит или что можно сделать с подпиской, мы можем увидеть информацию о выставлении счетов с функциями управления слева, где вы можете определить контроль доступа к IAM, а ниже доступно больше ресурсов.
Может возникнуть ситуация, когда у вас есть несколько подписок, и вы хотите управлять ими всеми в рамках одной, и именно здесь можно использовать группы управления для разделения групп ответственности. В моем ниже вы можете видеть, что есть только моя корневая группа арендатора с моей подпиской.
Вы также увидите на предыдущем изображении, что родительская группа управления — это тот же идентификатор, который используется в корневой группе арендатора.
Затем у нас есть группы ресурсов, здесь мы объединяем наши ресурсы и можем легко управлять ими в одном месте. У меня есть несколько созданных для различных других проектов.
Что мы собираемся делать в течение следующих нескольких дней, мы хотим создать нашу группу ресурсов. Это легко сделать в этой консоли, выбрав опцию создания на предыдущем изображении.
Происходит этап проверки, после чего у вас есть возможность просмотреть свое творение, а затем создать его. Вы также увидите внизу «Загрузить шаблон для автоматизации», это позволяет нам получить формат JSON, чтобы мы могли выполнить это просто автоматически позже, если мы захотим, мы также рассмотрим это позже. Нажмите «Create», затем в нашем списке групп ресурсов у нас теперь есть группа «90DaysOfDevOps», готовая к тому, что мы будем делать в следующем сеансе. Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course AWS Basics for Beginners - Full Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day29/"},"https://romankurnovskii.com/ru/docs/python101/chapter30_configobj/":{title:"30. ConfigObj",tags:[],content:`Python поставляется с удобным модулем под названием ConfigParser. Он хорошо подходит для создания и чтения конфигурационных файлов (они же INI-файлы). Однако Майкл Форд (автор IronPython in Action) и Никола Лароса решили написать свой собственный модуль конфигурации под названием ConfigObj. Во многих отношениях он является улучшением модуля стандартной библиотеки. Например, при чтении файла конфигурации он возвращает объект, похожий на словарь. ConfigObj также может понимать некоторые типы Python. Еще одна интересная особенность заключается в том, что вы можете создать спецификацию конфигурации, которую ConfigObj будет использовать для проверки файла конфигурации.
Начало работы Прежде всего, вам нужно получить ConfigObj. Сейчас самое время использовать знания из последней главы об установке пакетов. Вот как можно получить ConfigObj с помощью pip:
pip install configobj После того как вы установили его, мы можем двигаться дальше. Для начала откройте текстовый редактор и создайте файл с таким содержимым:
product = Sony PS3 accessories = controller, eye, memory stick # This is a comment that will be ignored retail_price = $400 Сохраните его в любом удобном для вас месте. Я назову свой файл config.ini. Теперь давайте посмотрим, как можно использовать ConfigObj для извлечения этой информации:
\u0026gt;\u0026gt;\u0026gt; from configobj import ConfigObj \u0026gt;\u0026gt;\u0026gt; config = ConfigObj(r\u0026quot;path to config.ini\u0026quot;) \u0026gt;\u0026gt;\u0026gt; config[\u0026quot;product\u0026quot;] 'Sony PS3' \u0026gt;\u0026gt;\u0026gt; config[\u0026quot;accessories\u0026quot;] ['controller', 'eye', 'memory stick'] \u0026gt;\u0026gt;\u0026gt; type(config[\u0026quot;accessories\u0026quot;]) \u0026lt;type 'list'\u0026gt; Как вы можете видеть, ConfigObj использует API dict Python для доступа к извлеченной информации. Чтобы заставить ConfigObj разобрать файл, достаточно передать ConfigObj путь к файлу. Если бы информация находилась в разделе (например, [Sony]), то вам пришлось бы предварительно поместить все в [\u0026ldquo;Sony\u0026rdquo;], например, так: config[\u0026ldquo;Sony\u0026rdquo;][\u0026ldquo;product\u0026rdquo;]. Также обратите внимание, что раздел accessories был возвращен в виде списка строк. ConfigObj возьмет любую допустимую строку со списком, разделенным запятыми, и вернет ее в виде списка Python. Вы также можете создавать многострочные строки в конфигурационном файле при условии, что вы заключите их в тройные одинарные или двойные кавычки.
Если вам нужно создать подраздел в файле, то используйте дополнительные квадратные скобки. Например, [Sony] - это верхний раздел, [[Playstation]] - подраздел, а [[[PS3]] - подраздел подраздела. Вы можете создавать подразделы любой глубины. Для получения дополнительной информации о форматировании файла я рекомендую прочитать документацию ConfigObj.
Теперь мы сделаем обратное и создадим конфигурационный файл программно.
import configobj def createConfig(path): config = configobj.ConfigObj() config.filename = path config[\u0026quot;Sony\u0026quot;] = {} config[\u0026quot;Sony\u0026quot;][\u0026quot;product\u0026quot;] = \u0026quot;Sony PS3\u0026quot; config[\u0026quot;Sony\u0026quot;][\u0026quot;accessories\u0026quot;] = ['controller', 'eye', 'memory stick'] config[\u0026quot;Sony\u0026quot;][\u0026quot;retail price\u0026quot;] = \u0026quot;$400\u0026quot; config.write() if __name__ == \u0026quot;__main__\u0026quot;: createConfig(\u0026quot;config.ini\u0026quot;) Как вы можете видеть, все, что для этого потребовалось, - это 13 строк кода. В приведенном выше коде мы создаем функцию и передаем ей путь к нашему файлу конфигурации. Затем мы создаем объект ConfigObj и устанавливаем его свойство filename. Для создания секции мы создаем пустой dict с именем \u0026ldquo;Sony\u0026rdquo;. Затем таким же образом добавляем каждую строку содержимого секции. Наконец, мы вызываем метод write нашего объекта config, чтобы записать данные в файл.
Использование configspec ConfigObj также предоставляет способ проверки ваших конфигурационных файлов с помощью configspec. Когда я упомянул, что собираюсь написать на эту тему, Стивен Спраут (Steven Sproat, создатель Whyteboard) предложил свой код configspec в качестве примера. Я взял его спецификацию и использовал ее для создания конфигурационного файла по умолчанию. В этом примере для проверки мы используем модуль validate от Foord. Я не думаю, что он включен в вашу загрузку ConfigObj, поэтому вам может понадобиться загрузить и его. Теперь давайте посмотрим на код:
import configobj, validate cfg = \u0026quot;\u0026quot;\u0026quot; bmp_select_transparent = boolean(default=False) canvas_border = integer(min=10, max=35, default=15) colour1 = list(min=3, max=3, default=list('280', '0', '0')) colour2 = list(min=3, max=3, default=list('255', '255', '0')) colour3 = list(min=3, max=3, default=list('0', '255', '0')) colour4 = list(min=3, max=3, default=list('255', '0', '0')) colour5 = list(min=3, max=3, default=list('0', '0', '255')) colour6 = list(min=3, max=3, default=list('160', '32', '240')) colour7 = list(min=3, max=3, default=list('0', '255', '255')) colour8 = list(min=3, max=3, default=list('255', '165', '0')) colour9 = list(min=3, max=3, default=list('211', '211', '211')) convert_quality = option('highest', 'high', 'normal', default='normal') default_font = string default_width = integer(min=1, max=12000, default=640) default_height = integer(min=1, max=12000, default=480) imagemagick_path = string handle_size = integer(min=3, max=15, default=6) language = option('English', 'English (United Kingdom)', 'Russian', 'Hindi', default='English') print_title = boolean(default=True) statusbar = boolean(default=True) toolbar = boolean(default=True) toolbox = option('icon', 'text', default='icon') undo_sheets = integer(min=5, max=50, default=10) \u0026quot;\u0026quot;\u0026quot; def createConfig(path): \u0026quot;\u0026quot;\u0026quot; Create a config file using a configspec and validate it against a Validator object \u0026quot;\u0026quot;\u0026quot; spec = cfg.split(\u0026quot;\\n\u0026quot;) config = configobj.ConfigObj(path, configspec=spec) validator = validate.Validator() config.validate(validator, copy=True) config.filename = path config.write() if __name__ == \u0026quot;__main__\u0026quot;: createConfig(\u0026quot;config.ini\u0026quot;) configspec дает программисту возможность указать, какие типы возвращаются для каждой строки в конфигурационном файле. Он также может быть использован для установки значения по умолчанию, min и max значений (среди прочего). Если вы выполните приведенный выше код, вы увидите, что в текущем рабочем каталоге будет создан файл config.ini, содержащий только значения по умолчанию. Если программист не указал значение по умолчанию, то эта строка даже не будет добавлена в конфигурацию.
Давайте рассмотрим происходящее подробнее, чтобы убедиться, что вы все поняли. В функции createConfig мы создаем экземпляр ConfigObj, передавая путь к файлу и задавая configspec. Обратите внимание, что configspec может быть обычным текстовым файлом или файлом python, а не строкой, как в этом примере. Далее мы создаем объект Validator. Обычное использование - просто вызвать config.validate(validator), но в этом коде я установил аргумент copy в True, чтобы можно было создать файл. В противном случае, все, что бы он сделал, это проверил, что файл, который я передал, соответствует правилам configspec. Наконец, я задаю имя файла конфига и записываю данные.
Подведение итогов Теперь вы знаете достаточно, чтобы начать работу с ConfigObj. Надеюсь, вы найдете его полезным, как и я. Не забудьте обратиться к документации модуля и прочитать больше о том, что он и validate могут делать.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter30_configobj/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day30/":{title:"30. Модули безопасности Microsoft Azure",tags:["devops"],content:`Microsoft Azure Security Models Следуя обзору Microsoft Azure, мы начнем с безопасности Azure и посмотрим, как это может помочь в наши дни. По большей части я обнаружил, что встроенных ролей было достаточно, и зная это, мы можем создавать и работать со многими различными областями аутентификации и конфигураций. Я обнаружил, что Microsoft Azure довольно продвинута с ее инструментом Active Directory по сравнению с другими общедоступными облаками.
Это одна из областей, в которой Microsoft Azure, по-видимому, работает иначе, чем другие поставщики общедоступных облаков, в Azure ВСЕГДА есть Azure AD.
Службы каталогов (Directory Services ) Azure Active Directory содержит принципы безопасности, используемые Microsoft Azure и другими облачными службами Microsoft. Аутентификация осуществляется с помощью таких протоколов, как SAML, WS-Federation, OpenID Connect и OAuth2. Запросы выполняются через REST API, который называется Microsoft Graph API. У арендаторов по умолчанию есть имя tenant.onmicrosoft.com, но они также могут иметь собственные доменные имена. Подписки связаны с арендатором Azure Active Directory. Если мы сравним с AWS, эквивалентным предложением будет AWS IAM (управление идентификацией и доступом), хотя все еще очень разные
Azure AD Connect предоставляет возможность репликации учетных записей из AD в Azure AD. Сюда также могут входить группы и иногда объекты. Это может быть гранулировано и отфильтровано. Поддерживает несколько лесов и доменов.
В Microsoft Azure Active Directory (AD) можно создавать облачные учетные записи, но большинство организаций уже учли своих пользователей в собственной локальной Active Directory.
Azure AD Connect также позволяет вам видеть не только серверы Windows AD, но и другие Azure AD, Google и другие. Это также дает возможность сотрудничать с внешними людьми и организациями, что называется Azure B2B.
Варианты аутентификации между доменными службами Active Directory и Microsoft Azure Active Directory возможны с синхронизацией удостоверений с хэшем пароля.
Передача хэша пароля необязательна, если он не используется, требуется сквозная аутентификация.
Ниже приведено видео, в котором подробно рассказывается о сквозной аутентификации.
User sign-in with Azure Active Directory Pass-through Authentication
Федерации (Federation) Справедливости ради стоит сказать, что если вы используете Microsoft 365, Microsoft Dynamics и локальную Active Directory, их довольно легко понять и интегрировать в Azure AD для федерации. Однако вы можете использовать другие службы за пределами экосистемы Microsoft.
Azure AD может выступать в качестве посредника федерации для этих других приложений сторонних производителей и других служб каталогов.
Это будет отображаться на портале Azure как корпоративные приложения, для которых существует большое количество вариантов.
Если вы прокрутите вниз страницу корпоративного приложения, вы увидите длинный список рекомендуемых приложений.
Эта опция также позволяет «принести свою» интеграцию, приложение, которое вы разрабатываете, или приложение, не являющееся галереей.
Я не изучал это раньше, но вижу, что это вполне подходящий набор функций по сравнению с другими облачными провайдерами и возможностями.
Управление доступом на основе ролей Мы уже рассмотрели в День 29 области, которые мы собираемся охватить здесь, мы можем настроить управление доступом на основе ролей в соответствии с одной из этих областей.
Subscriptions Management Group Resource Group Resources Роли можно разделить на три, в Microsoft Azure много встроенных ролей. Эти три:
Owner Contributor Reader Владелец и участник очень похожи по своим границам, однако владелец может изменять разрешения.
Другие роли относятся к определенным типам ресурсов Azure, а также к пользовательским ролям.
Мы должны сосредоточиться на назначении разрешений группам и пользователям.
Разрешения наследуются.
Если мы вернемся назад и посмотрим на группу ресурсов «90DaysOfDevOps», которую мы создали, и проверим контроль доступа (IAM) внутри, вы увидите, что у нас есть список участников и администратор доступа пользователей клиента, и у нас есть список владельцев (но Я не могу это показать)
Мы также можем проверить роли, которые мы назначили здесь, являются ли они встроенными ролями и к какой категории они относятся.
Мы также можем использовать вкладку проверки доступа, если мы хотим проверить учетную запись по этой группе ресурсов и убедиться, что учетная запись, к которой мы хотим иметь этот доступ, имеет правильные разрешения, или, может быть, мы хотим проверить, не имеет ли пользователь слишком много доступа.
Microsoft Defender for Cloud Microsoft Defender for Cloud (ранее известный как Azure Security Center) предоставляет информацию о безопасности всей среды Azure.
Единая панель мониторинга для просмотра общего состояния безопасности всех ресурсов Azure и других ресурсов (через Azure Arc) и рекомендации по усилению безопасности.
Уровень бесплатного пользования включает постоянную оценку и рекомендации по безопасности.
Платные планы для защищенных типов ресурсов (например, серверы, AppService, SQL, хранилище, контейнеры, KeyVault).
Я перешел на другую подписку для просмотра Центра безопасности Azure, и вы можете увидеть здесь, основываясь на очень небольшом количестве ресурсов, что у меня есть некоторые рекомендации в одном месте.
Azure Policy Azure Policy — это собственная служба Azure, которая помогает применять организационные стандарты и оценивать соответствие в масштабе.
Интегрирован в Microsoft Defender для облака. Azure Policy проверяет несоответствующие ресурсы и применяет исправления.
Обычно используется для управления согласованностью ресурсов, соблюдением нормативных требований, безопасностью, стоимостью и стандартами управления.
Использует формат JSON для хранения логики оценки и определения того, соответствует ли ресурс требованиям или нет, а также любых действий, которые необходимо предпринять в случае несоответствия (например, аудит, аудит, если не существует, запретить, изменить, развернуть, если не существует).
Бесплатно для использования. Исключение составляют подключенные ресурсы Azure Arc, взимаемые за сервер в месяц за использование гостевой конфигурации политики Azure.
Практика Я купил домен и хотел бы добавить этот на свой портал Azure Active Directory, Add your custom domain name using the Azure Active Directory Portal
Теперь мы можем создать нового пользователя в нашем новом домене Active Directory.
Теперь мы хотим создать группу для всех наших новых пользователей 90DaysOfDevOps в одной группе. Мы можем создать группу, как показано ниже, обратите внимание, что я использую «Динамический пользователь», это означает, что Azure AD будет запрашивать учетные записи пользователей и добавлять их динамически по сравнению с назначенными, когда вы вручную добавляете пользователя в свою группу.
Существует множество вариантов создания вашего запроса, мой план состоит в том, чтобы просто найти основное имя и убедиться, что оно содержит мой запрос.
Теперь, поскольку мы уже создали нашу учетную запись пользователя, мы можем проверить, работают ли правила. Для сравнения я также добавил здесь еще одну учетную запись, связанную с другим доменом, и вы можете видеть, что из-за этого правила наш пользователь не попадет в эту группу.
С тех пор я добавил нового пользователя, и если мы пойдем и проверим группу, мы увидим наших участников.
Если у нас есть это требование x100, то мы не собираемся делать все это в консоли, мы собираемся воспользоваться либо массовыми параметрами для создания, приглашения, удаления пользователей, либо вы захотите изучить PowerShell для достичь этого автоматизированного подхода к масштабированию.
Теперь мы можем перейти к нашей группе ресурсов и указать, что в группе ресурсов 90DaysOfDevOps мы хотим, чтобы владельцем была группа, которую мы только что создали.
Мы также можем войти сюда и запретить доступ назначений к нашей группе ресурсов.
Теперь, если мы войдем на портал Azure с нашей новой учетной записью пользователя, вы увидите, что у нас есть доступ только к нашей группе ресурсов 90DaysOfDevOps, а не к другим, показанным на предыдущих рисунках, потому что у нас нет доступа.
Вышеприведенное замечательно, если это пользователь, имеющий доступ к ресурсам внутри вашего портала Azure, но не каждый пользователь должен знать о портале, но для проверки доступа мы можем использовать Портал приложений Это портал единого входа, который мы тестируем.
Вы можете настроить этот портал под своим собственным брендом, и мы, возможно, вернемся к этому позже.
Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course AWS Basics for Beginners - Full Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day30/"},"https://romankurnovskii.com/ru/docs/python101/chapter31_lxml/":{title:"31. Парсинг XML с помощью lxml",tags:[],content:`В первой части мы рассмотрели некоторые встроенные в Python парсеры XML. В этой главе мы рассмотрим интересный пакет сторонних разработчиков, lxml от codespeak. Он использует, помимо прочего, ElementTree API. Пакет lxml имеет поддержку XPath и XSLT, включает API для SAX и API уровня C для совместимости с модулями C/Pyrex. Вот что мы рассмотрим:
Как разобрать XML с помощью lxml Пример рефакторинга Как разобрать XML с помощью lxml.objectify Как создать XML с помощью lxml.objectify В этой статье, мы используем примеры, основанные на примерах парсинга minidom, и посмотрим, как выполнять парсинг при помощи lxml Python. Вот пример XML из программы, которая была написана для отслеживания назначений:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1234360800\u0026lt;/begin\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Check MS Office website for updates\u0026lt;/subject\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;uid\u0026gt;604f4792-eb89-478b-a14f-dd34d3cc6c21-1234360800\u0026lt;/uid\u0026gt; \u0026lt;state\u0026gt;dismissed\u0026lt;/state\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Давайте узнаем, как разобрать его с помощью lxml!
Разбор XML с помощью lxml Данный пример XML показывает два назначения. Время начинается спустя секунды после эпохи. Наш uid сгенерирован на основе хеша начала времени и ключа. Время сигнала – несколько секунд после эпохи, но не раньше начала времени. Состояние – если назначение было отменено или перенесено, или нет, так или иначе. Остальная часть XML, как мы видим, в пояснении не нуждается. Давайте взглянем на то, как делается парсинг:
from lxml import etree def parseXML(xmlFile): \u0026quot;\u0026quot;\u0026quot; Parse the xml \u0026quot;\u0026quot;\u0026quot; with open(xmlFile) as fobj: xml = fobj.read() root = etree.fromstring(xml) for appt in root.getchildren(): for elem in appt.getchildren(): if not elem.text: text = \u0026quot;None\u0026quot; else: text = elem.text print(elem.tag + \u0026quot; =\u0026gt; \u0026quot; + text) if __name__ == \u0026quot;__main__\u0026quot;: parseXML(\u0026quot;example.xml\u0026quot;) Прежде всего, мы импортируем необходимые модули, а именно модуль etree из пакета lxml и функцию StringIO из встроенного модуля StringIO. Наша функция parseXML принимает один аргумент: путь к рассматриваемому XML-файлу. Мы открываем файл, читаем его и закрываем. Теперь наступает самая интересная часть! Мы используем функцию etree\u0026rsquo;s parse для разбора XML-кода, возвращаемого модулем StringIO. По не совсем понятным мне причинам, функция parse требует объект, похожий на файл.
В любом случае, далее мы выполняем итерацию по контексту (т.е. объекту lxml.etree.iterparse) и извлекаем элементы тегов. Мы добавляем условный оператор if, чтобы заменить пустые поля на слово \u0026ldquo;None\u0026rdquo;, чтобы сделать вывод немного понятнее. Вот и все.
Разбор примера с книгой Что ж, результат этого примера был довольно скучным. В большинстве случаев вы хотите сохранить извлеченные данные и что-то с ними сделать, а не просто вывести их на stdout. Так что в следующем нашем примере мы создадим структуру данных для сбора результатов. В данном примере структура наших данных будет представлять собой список словарей. Мы используем пример книги MSDN. Сохраните следующий код XML под названием example.xml.
\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;catalog\u0026gt; \u0026lt;book id=\u0026quot;bk101\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Gambardella, Matthew\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;XML Developer's Guide\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Computer\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;44.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-10-01\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;An in-depth look at creating applications with XML.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026quot;bk102\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Ralls, Kim\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Midnight Rain\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Fantasy\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-12-16\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;book id=\u0026quot;bk103\u0026quot;\u0026gt; \u0026lt;author\u0026gt;Corets, Eva\u0026lt;/author\u0026gt; \u0026lt;title\u0026gt;Maeve Ascendant\u0026lt;/title\u0026gt; \u0026lt;genre\u0026gt;Fantasy\u0026lt;/genre\u0026gt; \u0026lt;price\u0026gt;5.95\u0026lt;/price\u0026gt; \u0026lt;publish_date\u0026gt;2000-11-17\u0026lt;/publish_date\u0026gt; \u0026lt;description\u0026gt;After the collapse of a nanotechnology society in England, the young survivors lay the foundation for a new society.\u0026lt;/description\u0026gt; \u0026lt;/book\u0026gt; \u0026lt;/catalog\u0026gt; Теперь давайте разберем этот XML и поместим его в нашу структуру данных!
from lxml import etree def parseBookXML(xmlFile): with open(xmlFile) as fobj: xml = fobj.read() root = etree.fromstring(xml) book_dict = {} books = [] for book in root.getchildren(): for elem in book.getchildren(): if not elem.text: text = \u0026quot;None\u0026quot; else: text = elem.text print(elem.tag + \u0026quot; =\u0026gt; \u0026quot; + text) book_dict[elem.tag] = text if book.tag == \u0026quot;book\u0026quot;: books.append(book_dict) book_dict = {} return books if __name__ == \u0026quot;__main__\u0026quot;: parseBookXML(\u0026quot;books.xml\u0026quot;) Данный пример весьма похож на предыдущий, так что мы сосредоточимся только на различиях между ними. Перед началом итерации над контекстом, мы создадим объект пустого словаря и пустой список Python. Далее, в цикле, мы создадим наш словарь вот так:
book_dict[elem.tag] = text Текст - это либо elem.text, либо None. Наконец, если тегом является book, то мы находимся в конце раздела книги и должны добавить dict в наш список, а также сбросить dict для следующей книги. Как вы можете видеть, именно это мы и сделали. Более реалистичным примером было бы поместить извлеченные данные в класс Book. Я уже делал это с json-лентами.
Теперь мы готовы узнать, как разобрать XML с помощью lxml.objectify!
Разбор XML с помощью lxml.objectify Модуль lxml имеет модуль objectify, который может превращать XML-документы в объекты Python. Я считаю, что с \u0026ldquo;объективированными\u0026rdquo; XML-документами очень легко работать, и надеюсь, что вы тоже. Для его установки вам, возможно, придется проделать несколько шагов, поскольку pip не работает с lxml в Windows. Обязательно зайдите в индекс пакетов Python и поищите версию, созданную для вашей версии Python. Также обратите внимание, что последняя предварительная программа установки lxml поддерживает только Python 3.2 (на момент написания статьи), так что если у вас более новая версия Python, у вас могут возникнуть некоторые трудности с установкой lxml для вашей версии.
В любом случае, как только вы его установите, мы сможем снова приступить к изучению этого замечательного куска XML:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1234360800\u0026lt;/begin\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Check MS Office website for updates\u0026lt;/subject\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;uid\u0026gt;604f4792-eb89-478b-a14f-dd34d3cc6c21-1234360800\u0026lt;/uid\u0026gt; \u0026lt;state\u0026gt;dismissed\u0026lt;/state\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Теперь нам нужно написать код, который будет разбирать и изменять XML. Давайте посмотрим на эту небольшую демонстрацию, которая показывает ряд возможностей, которые предоставляет objectify.
from lxml import etree, objectify def parseXML(xmlFile): \u0026quot;\u0026quot;\u0026quot;Parse the XML file\u0026quot;\u0026quot;\u0026quot; with open(xmlFile) as f: xml = f.read() root = objectify.fromstring(xml) # returns attributes in element node as dict attrib = root.attrib # how to extract element data begin = root.appointment.begin uid = root.appointment.uid # loop over elements and print their tags and text for appt in root.getchildren(): for e in appt.getchildren(): print(\u0026quot;%s =\u0026gt; %s\u0026quot; % (e.tag, e.text)) print() # how to change an element's text root.appointment.begin = \u0026quot;something else\u0026quot; print(root.appointment.begin) # how to add a new element root.appointment.new_element = \u0026quot;new data\u0026quot; # remove the py:pytype stuff objectify.deannotate(root) etree.cleanup_namespaces(root) obj_xml = etree.tostring(root, pretty_print=True) print(obj_xml) # save your xml with open(\u0026quot;new.xml\u0026quot;, \u0026quot;w\u0026quot;) as f: f.write(obj_xml) if __name__ == \u0026quot;__main__\u0026quot;: f = r'path\\to\\sample.xml' parseXML(f) Код довольно хорошо прокомментирован, но мы все равно потратим немного времени на его изучение. В начале, мы передали наш пример файла XML и использовали objectify. Если вы хотите получить доступ к атрибутам тега, используйте свойство attrib*. Оно вернет словарь атрибутов тега. Чтобы получить доступ к элементам подтега, достаточно использовать точечную нотацию. Как видите, чтобы получить значение тега begin, можно поступить следующим образом:
begin = root.appointment.begin Следует помнить, что если в значении есть ведущие нули, то возвращаемое значение может быть усеченным. Если для вас это важно, то вместо этого используйте следующий синтаксис:
begin = root.appointment.begin.text Если вам нужно перебрать дочерние элементы, вы можете использовать метод iterchildren. Возможно, вам придется использовать вложенную структуру цикла for, чтобы получить все элементы. Изменить значение элемента так же просто, как присвоить ему новое значение.
root.appointment.new_element = \u0026quot;new data\u0026quot; Теперь мы готовы узнать, как создать XML с помощью lxml.objectify.
Создание XML с помощью lxml.objectify Подпакет lxml.objectify чрезвычайно удобен для разбора и создания XML. В этом разделе мы покажем, как создавать XML с помощью модуля lxml.objectify. Мы начнем с простого XML, а затем попытаемся воспроизвести его. Давайте начнем!
Для примера мы будем использовать следующий XML:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state\u0026gt;\u0026lt;/state\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;appointment\u0026gt; \u0026lt;begin\u0026gt;1234360800\u0026lt;/begin\u0026gt; \u0026lt;duration\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject\u0026gt;Check MS Office website for updates\u0026lt;/subject\u0026gt; \u0026lt;location\u0026gt;\u0026lt;/location\u0026gt; \u0026lt;uid\u0026gt;604f4792-eb89-478b-a14f-dd34d3cc6c21-1234360800\u0026lt;/uid\u0026gt; \u0026lt;state\u0026gt;dismissed\u0026lt;/state\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Давайте посмотрим, как мы можем использовать lxml.objectify для воссоздания этого XML:
from lxml import etree, objectify def create_appt(data): \u0026quot;\u0026quot;\u0026quot; Create an appointment XML element \u0026quot;\u0026quot;\u0026quot; appt = objectify.Element(\u0026quot;appointment\u0026quot;) appt.begin = data[\u0026quot;begin\u0026quot;] appt.uid = data[\u0026quot;uid\u0026quot;] appt.alarmTime = data[\u0026quot;alarmTime\u0026quot;] appt.state = data[\u0026quot;state\u0026quot;] appt.location = data[\u0026quot;location\u0026quot;] appt.duration = data[\u0026quot;duration\u0026quot;] appt.subject = data[\u0026quot;subject\u0026quot;] return appt def create_xml(): \u0026quot;\u0026quot;\u0026quot; Create an XML file \u0026quot;\u0026quot;\u0026quot; xml = '''\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;zAppointments\u0026gt; \u0026lt;/zAppointments\u0026gt; ''' root = objectify.fromstring(xml) root.set(\u0026quot;reminder\u0026quot;, \u0026quot;15\u0026quot;) appt = create_appt({\u0026quot;begin\u0026quot;:1181251680, \u0026quot;uid\u0026quot;:\u0026quot;040000008200E000\u0026quot;, \u0026quot;alarmTime\u0026quot;:1181572063, \u0026quot;state\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;location\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;duration\u0026quot;:1800, \u0026quot;subject\u0026quot;:\u0026quot;Bring pizza home\u0026quot;} ) root.append(appt) uid = \u0026quot;604f4792-eb89-478b-a14f-dd34d3cc6c21-1234360800\u0026quot; appt = create_appt({\u0026quot;begin\u0026quot;:1234360800, \u0026quot;uid\u0026quot;:uid, \u0026quot;alarmTime\u0026quot;:1181572063, \u0026quot;state\u0026quot;:\u0026quot;dismissed\u0026quot;, \u0026quot;location\u0026quot;:\u0026quot;\u0026quot;, \u0026quot;duration\u0026quot;:1800, \u0026quot;subject\u0026quot;:\u0026quot;Check MS Office website for updates\u0026quot;} ) root.append(appt) # remove lxml annotation objectify.deannotate(root) etree.cleanup_namespaces(root) # create the xml string obj_xml = etree.tostring(root, pretty_print=True, xml_declaration=True) try: with open(\u0026quot;example.xml\u0026quot;, \u0026quot;wb\u0026quot;) as xml_writer: xml_writer.write(obj_xml) except IOError: pass if __name__ == \u0026quot;__main__\u0026quot;: create_xml() Давайте немного разберемся в этом. Начнем с функции create_xml. В ней мы создаем корневой объект XML с помощью функции fromstring модуля objectify. Корневой объект будет содержать zAppointment в качестве своего тега. Мы устанавливаем атрибут reminder корня, а затем вызываем функцию create_appt, используя в качестве аргумента словарь. В функции create_appt мы создаем экземпляр элемента (технически, это ObjectifiedElement), который мы присваиваем нашей переменной appt. Здесь мы используем dot-notation для создания тегов для этого элемента. Наконец, мы возвращаем элемент appt обратно и добавляем его к нашему root объекту. Мы повторяем процесс для второго экземпляра назначения.
В следующем разделе функции create_xml мы удалим аннотацию lxml. Если вы этого не сделаете, ваш XML будет выглядеть следующим образом:
\u0026lt;?xml version=\u0026quot;1.0\u0026quot; ?\u0026gt; \u0026lt;zAppointments py:pytype=\u0026quot;TREE\u0026quot; reminder=\u0026quot;15\u0026quot;\u0026gt; \u0026lt;appointment py:pytype=\u0026quot;TREE\u0026quot;\u0026gt; \u0026lt;begin py:pytype=\u0026quot;int\u0026quot;\u0026gt;1181251680\u0026lt;/begin\u0026gt; \u0026lt;uid py:pytype=\u0026quot;str\u0026quot;\u0026gt;040000008200E000\u0026lt;/uid\u0026gt; \u0026lt;alarmTime py:pytype=\u0026quot;int\u0026quot;\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state py:pytype=\u0026quot;str\u0026quot;/\u0026gt; \u0026lt;location py:pytype=\u0026quot;str\u0026quot;/\u0026gt; \u0026lt;duration py:pytype=\u0026quot;int\u0026quot;\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject py:pytype=\u0026quot;str\u0026quot;\u0026gt;Bring pizza home\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt;\u0026lt;appointment py:pytype=\u0026quot;TREE\u0026quot;\u0026gt; \u0026lt;begin py:pytype=\u0026quot;int\u0026quot;\u0026gt;1234360800\u0026lt;/begin\u0026gt; \u0026lt;uid py:pytype=\u0026quot;str\u0026quot;\u0026gt;604f4792-eb89-478b-a14f-dd34d3cc6c21-1234360800\u0026lt;/uid\u0026gt; \u0026lt;alarmTime py:pytype=\u0026quot;int\u0026quot;\u0026gt;1181572063\u0026lt;/alarmTime\u0026gt; \u0026lt;state py:pytype=\u0026quot;str\u0026quot;\u0026gt;dismissed\u0026lt;/state\u0026gt; \u0026lt;location py:pytype=\u0026quot;str\u0026quot;/\u0026gt; \u0026lt;duration py:pytype=\u0026quot;int\u0026quot;\u0026gt;1800\u0026lt;/duration\u0026gt; \u0026lt;subject py:pytype=\u0026quot;str\u0026quot;\u0026gt;Check MS Office website for updates\u0026lt;/subject\u0026gt; \u0026lt;/appointment\u0026gt; \u0026lt;/zAppointments\u0026gt; Чтобы удалить всю эту ненужную аннотацию, мы вызываем следующие две функции:
objectify.deannotate(root) etree.cleanup_namespaces(root) Последняя часть головоломки - заставить lxml генерировать сам XML. Здесь мы используем модуль lxml\u0026rsquo;s etree для выполнения тяжелой работы:
obj_xml = etree.tostring(root, pretty_print=True, xml_declaration=True) Функция tostring вернет красивую строку XML, а если вы установите pretty_print в True, она обычно возвращает XML в красивом формате. Аргумент xml_declaration указывает модулю etree, включать или нет первую строку декларации (т.е. \u003c?xml version="1.0" ?\u003e).
Подведение итогов Теперь вы знаете, как использовать модули lxml etree и objectify для разбора XML. Вы также знаете, как использовать objectify для создания XML. Знание того, как использовать несколько модулей для выполнения одной и той же задачи, может быть полезно для того, чтобы увидеть, как подойти к одной и той же проблеме с разных сторон. Это также поможет вам выбрать тот инструмент, с которым вам удобнее всего работать.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter31_lxml/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day31/":{title:"31. Microsoft Azure Среда выполнения приложений",tags:["devops"],content:`Среда выполнения приложений Вслед за вчерашним обзором основ моделей безопасности в Microsoft Azure, сегодня мы собираемся изучить различные службы вычислений, доступные нам в Azure.
Параметры службы доступности Этот раздел мне близок, учитывая мою роль в управлении данными. Как и в случае с локальной средой, очень важно обеспечить доступность ваших служб.
Высокая доступность (Защита в пределах региона) Аварийное восстановление (Защита между регионами) Резервное копирование (Восстановление с момента времени) Microsoft развертывает несколько регионов в пределах геополитических границ.
Две концепции Azure для доступности услуг.
Наборы доступности (виртуальных машин) — обеспечивают отказоустойчивость в центре обработки данных.
Зоны доступности — обеспечивают отказоустойчивость между центрами обработки данных в пределах региона.
Виртуальные машины Предоставляет виртуальные машины различных серий и размеров с различными возможностями (иногда огромными) Размеры виртуальных машин в Azure Существует множество различных вариантов и фокусов для виртуальных машин, от высокопроизводительных, с малой задержкой до виртуальных машин с большим объемом памяти. У нас также есть расширяемый тип ВМ, который можно найти в серии B. Это отлично подходит для рабочих нагрузок, где у вас могут быть низкие требования к ЦП по большей части, но требуется, чтобы, возможно, один раз в месяц требовалась всплеск производительности. Виртуальные машины размещаются в виртуальной сети, которая может обеспечить подключение к любой сети. Поддержка гостевых ОС Windows и Linux. Существуют также ядра, настроенные для Azure, если речь идет о конкретных дистрибутивах Linux. Ядра, настроенные Azure Шаблоны В Microsoft Azure шаблоны исполнений можно конфигурировать с помощью JSON.
Существует несколько различных порталов и консолей управления, которые мы можем использовать для создания наших ресурсов. Предпочтительнее будет через шаблоны JSON.
Идемпотентные развертывания в инкрементном или полном режиме — т.е. повторяемое желаемое состояние.
Существует большой выбор шаблонов, которые могут экспортировать развернутые определения ресурсов. Мне нравится думать об этой функции шаблонов как о чем-то вроде AWS CloudFormation или, возможно, о Terraform для мультиоблачного варианта. Подробнее о Terraform мы расскажем в разделе «Инфраструктура как код».
Масштабирование Автоматическое масштабирование — это крупная функция общедоступного облака, позволяющая сократить ресурсы, которые вы не используете, или активировать, когда они вам нужны.
В Azure у нас есть так называемые масштабируемые наборы виртуальных машин (VMSS) для IaaS. Это позволяет автоматически создавать и масштабировать изображение золотого стандарта на основе расписаний и показателей.
Это идеально подходит для обновления окон, чтобы вы могли обновлять свои образы и развертывать их с наименьшими последствиями.
В другие службы, такие как службы приложений Azure, встроено автоматическое масштабирование.
Контейнеры Мы не рассмотрели контейнеры как пример использования и то, что и как они могут и должны быть необходимы в нашем учебном путешествии по DevOps, но мы должны упомянуть, что у Azure есть некоторые конкретные службы, ориентированные на контейнеры, которые следует упомянуть.
Служба Azure Kubernetes (AKS) (Azure Kubernetes Service) — предоставляет управляемое решение Kubernetes.
Экземпляры контейнеров Azure — контейнеры как услуга с посекундной оплатой. Запустите образ и интегрируйте его с вашей виртуальной сетью, не нуждаясь в оркестровке контейнеров.
Service Fabric — имеет множество возможностей, но включает оркестрацию для экземпляров контейнеров.
Azure также имеет реестр контейнеров, который предоставляет частный реестр для образов Docker, диаграмм Helm, артефактов Open Container Initiative (OCI) и образов. Подробнее об этом снова, когда мы дойдем до раздела контейнеров.
Многие службы контейнеров действительно могут использовать контейнеры \u0026ldquo;под капотом\u0026rdquo;, но это абстрагируется от наших требований к управлению.
Службы приложений Службы приложений Azure предоставляют решение для размещения приложений, которое обеспечивает простой способ установки служб. Автоматическое развертывание и масштабирование. Поддерживает решения на базе Windows и Linux. Службы выполняются в плане службы приложений, который имеет тип и размер. Количество различных сервисов, включая веб-приложения, приложения API и мобильные приложения. Поддержка слотов развертывания для надежного тестирования и продвижения. Бессерверные вычисления Цель бессерверных вычислений заключается в том, что мы платим только за время выполнения функции, и нам не нужно постоянно запускать виртуальные машины или приложения PaaS. Мы просто запускаем нашу функцию, когда она нам нужна, а затем она исчезает.
Функции Azure — предоставляет бессерверный код. Если мы вернемся к нашему первому взгляду на общедоступное облако, вы вспомните уровень абстракции управления, с бессерверными функциями вы будете управлять только кодом.
У меня есть план, ориентированный на события в больших масштабах, когда я получу здесь немного практики, надеюсь, позже.
Обеспечивает входную и выходную привязку ко многим Azure и сторонним службам.
Поддерживает множество различных языков программирования. (C#, NodeJS, Python, PHP, bash, Golang, Rust или любой исполняемый файл)
Сетка событий Azure позволяет запускать логику из служб и событий.
Приложение Azure Logic обеспечивает графический рабочий процесс и интеграцию.
Мы также можем рассмотреть пакетную службу Azure, которая может выполнять крупномасштабные задания на узлах Windows и Linux с согласованным управлением и планированием.
Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course AWS Basics for Beginners - Full Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day31/"},"https://romankurnovskii.com/ru/docs/python101/chapter32_pylint/":{title:"32. Анализ кода Python",tags:[],content:`Анализ кода Python может быть тяжелой темой, но он может быть очень полезен для улучшения ваших программ. Существует несколько анализаторов кода Python, которые вы можете использовать для проверки вашего кода на соответствие стандартам. pylint, вероятно, является самым популярным. Он очень конфигурируемый, настраиваемый и подключаемый. Он также проверяет ваш код на соответствие PEP8, официальному руководству по стилю Python Core, и ищет ошибки программирования.
Обратите внимание, что pylint проверяет ваш код на соответствие большинству, но не всем стандартам PEP8. Мы уделим немного времени изучению другого пакета анализа кода, который называется pyflakes.
Начало работы с pylint Пакет pylint не входит в состав Python, поэтому для его загрузки необходимо обратиться к индексу пакетов Python (PyPI) или на веб-сайт пакета. Вы можете использовать следующую команду, чтобы сделать всю работу за вас:
pip install pylint Если все идет по плану, у вас должен быть установлен pylint, и мы готовы продолжить.
Анализ вашего кода После установки pylint вы можете запустить его в командной строке, без каких либо аргументов, что бы увидеть, какие опции он принимает. Если это не сработало, можете прописать полный путь, вот так:
c:\\Python34\\Scripts\\pylint Теперь нам нужен код для анализа. Вот фрагмент кода, в котором есть четыре ошибки. Сохраните его в файл с именем crummy_code.py:
import sys class CarClass: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, color, make, model, year): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.color = color self.make = make self.model = model self.year = year if \u0026quot;Windows\u0026quot; in platform.platform(): print(\u0026quot;You're using Windows!\u0026quot;) self.weight = self.getWeight(1, 2, 3) def getWeight(this): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; return \u0026quot;2000 lbs\u0026quot; Можете ли вы обнаружить ошибки без выполнения кода? Давайте посмотрим, сможет ли pylint найти проблемы!
pylint crummy_code.py При выполнении этой команды на экран будет выведено много данных. Вот неполный пример:
c:\\py101\u0026gt;c:\\Python34\\Scripts\\pylint crummy_code.py No config file found, using default configuration ************* Module crummy_code C: 2, 0: Trailing whitespace (trailing-whitespace) C: 5, 0: Trailing whitespace (trailing-whitespace) C: 12, 0: Trailing whitespace (trailing-whitespace) C: 15, 0: Trailing whitespace (trailing-whitespace) C: 17, 0: Trailing whitespace (trailing-whitespace) C: 1, 0: Missing module docstring (missing-docstring) C: 3, 0: Empty class docstring (empty-docstring) C: 3, 0: Old-style class defined. (old-style-class) E: 13,24: Undefined variable 'platform' (undefined-variable) E: 16,36: Too many positional arguments for function call (too-many-function-args) C: 18, 4: Invalid method name \u0026quot;getWeight\u0026quot; (invalid-name) C: 18, 4: Empty method docstring (empty-docstring) E: 18, 4: Method should have \u0026quot;self\u0026quot; as first argument (no-self-argument) R: 18, 4: Method could be a function (no-self-use) R: 3, 0: Too few public methods (1/2) (too-few-public-methods) W: 1, 0: Unused import sys (unused-import) Давайте немного притормозим и разберемся. Сначала нам нужно понять, что означают буквы: С – конвенция (convention), R – рефакторинг (refactor), W – предупреждение (warning), E – ошибка (error). Наш pylint нашел 3 ошибки, 4 проблемы с конвенцией, 2 строки, которые нуждаются в рефакторинге и одно предупреждение. Предупреждение и 3 ошибки – это как раз то, что я искал. Мы попытаемся исправить этот код и устранить ряд проблем. Для начала мы наведем порядок в импортах, и изменить функцию getWeight на get_weight, в связи с тем, что camelCase не используется в названиях методов. Нам также нужно исправить вызов get_weight, чтобы он передавал правильное количество аргументов и исправить его, чтобы “self” выступал в качестве первого аргумента. Взглянем на новый код:
# crummy_code_fixed.py import platform class CarClass: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, color, make, model, year): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.color = color self.make = make self.model = model self.year = year if \u0026quot;Windows\u0026quot; in platform.platform(): print(\u0026quot;You're using Windows!\u0026quot;) self.weight = self.get_weight(3) def get_weight(self, this): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; return \u0026quot;2000 lbs\u0026quot; Давайте запустим этот новый код в pylint и посмотрим, насколько мы улучшили результаты. Для краткости мы снова покажем только первую секцию:
c:\\py101\u0026gt;c:\\Python34\\Scripts\\pylint crummy_code_fixed.py No config file found, using default configuration ************* Module crummy_code_fixed C: 1,0: Missing docstring C: 4,0:CarClass: Empty docstring C: 21,4:CarClass.get_weight: Empty docstring W: 21,25:CarClass.get_weight: Unused argument 'this' R: 21,4:CarClass.get_weight: Method could be a function R: 4,0:CarClass: Too few public methods (1/2) Это очень помогло! Если бы мы добавили докстринги, мы могли бы вдвое сократить количество проблем. Теперь мы готовы взглянуть на pyflakes!
Начало работы с pyflakes Проект pyflakes является частью проекта Divmod. Pyflakes не выполняет код, который он проверяет, так же как pylint не выполняет код, который он анализирует. Вы можете установить pyflakes с помощью pip, easy_install или из исходников.
Мы начнем с запуска pyflakes в изначальной версии той же части кода, которую мы использовали для проверки pylint. Вот и он:
import sys class CarClass: \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, color, make, model, year): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; self.color = color self.make = make self.model = model self.year = year if \u0026quot;Windows\u0026quot; in platform.platform(): print(\u0026quot;You're using Windows!\u0026quot;) self.weight = self.getWeight(1, 2, 3) def getWeight(this): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; return \u0026quot;2000 lbs\u0026quot; Как мы отмечали в предыдущем разделе, в этом поломанном коде четыре ошибки, три из которых препятствуют работе программы. Давайте посмотрим, что же pyflakes может найти. Попытайтесь запустить данную команду и на выходе вы должны получить следующее:
c:\\py101\u0026gt;c:\\Python34\\Scripts\\pyflakes.exe crummy_code.py crummy_code.py:1: 'sys' imported but unused crummy_code.py:13: undefined name 'platform' Хотя pyflakes очень быстро вернул этот результат, он не нашел всех ошибок. Вызов метода getWeight передает слишком много аргументов, а сам метод getWeight определен неверно, поскольку у него нет аргумента self. На самом деле, вы можете называть первый аргумент как угодно, но по традиции его обычно называют self. Если бы вы исправили свой код в соответствии с тем, что сказал вам pyflakes, ваш код все равно не работал бы.
Подведение итогов Следующим шагом будет попытка запустить pylint и pyflakes в вашем собственном коде, либо же в пакете Python, вроде SQLAlchemy, и посмотреть, что вы получите на выходе. С помощью этих инструментов можно многое узнать о собственном коде. pylint интегрирован во многие популярные IDE для Python, такие как Wingware, Editra и PyDev. Некоторые предупреждения от pylint могут показаться вам раздражающими или вообще неприменимыми. Есть способы подавить такие вещи, как предупреждения об устаревании, с помощью опций командной строки. Или вы можете использовать команду -generate-rcfile для создания примера конфигурационного файла, который поможет вам контролировать pylint. Обратите внимание, что pylint и pyflakes не импортируют ваш код, поэтому вам не нужно беспокоиться о нежелательных побочных эффектах.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter32_pylint/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day32/":{title:"32. Модели хранилища Microsoft Azure",tags:["devops"],content:`Модели хранилища Службы хранилища Службы хранилища Azure предоставляются учетными записями хранения. Доступ к учетным записям хранения в основном осуществляется через REST API. Учетная запись хранения должна иметь уникальное имя, являющееся частью DNS-имени \u0026lt;Storage Account name\u0026gt;.core.windows.net. Различные варианты репликации и шифрования. Находится в группе ресурсов Мы можем создать нашу группу хранения, просто выполнив поиск группы хранения в строке поиска в верхней части портала Azure.
Затем мы можем выполнить шаги по созданию нашей учетной записи хранения, помня, что это имя должно быть уникальным, а также оно должно быть написано строчными буквами, без пробелов, но может включать цифры.
Мы также можем выбрать уровень избыточности, который мы хотели бы использовать для нашей учетной записи хранения и всего, что мы здесь храним. Чем дальше по списку, тем дороже вариант, но также и распространение ваших данных.
Даже опция избыточности по умолчанию дает нам 3 копии наших данных.
Azure Storage Redundancy
Концепции из ссылки выше:
Локально-избыточное хранилище — трижды реплицирует ваши данные в пределах одного центра обработки данных в основном регионе.
Геоизбыточное хранилище — трижды синхронно копирует ваши данные в одном физическом расположении в основном регионе с помощью LRS.
Хранилище с избыточностью в пределах зоны — синхронно реплицирует данные службы хранилища Azure в трех зонах доступности Azure в основном регионе.
Хранилище с избыточностью в геозонах — сочетает в себе высокую доступность, обеспечиваемую избыточностью в зонах доступности, с защитой от региональных сбоев, обеспечиваемой георепликацией. Данные в учетной записи хранения GZRS копируются в три зоны доступности Azure в основном регионе, а также реплицируются во второй географический регион для защиты от региональных аварий.
Просто возвращаюсь к параметрам производительности. У нас есть Стандарт и Премиум на выбор. В нашем пошаговом руководстве мы выбрали «Стандартный», но «Премиум» дает вам некоторые специфические опции. Затем в раскрывающемся списке вы можете увидеть, что у нас есть эти три варианта на выбор. Для учетной записи хранения доступно множество дополнительных параметров, но пока нам не нужно вдаваться в это. Эти параметры связаны с шифрованием и защитой данных.
Управляемые диски Доступ к хранилищу можно получить несколькими способами.
Аутентифицированный доступ через:
Общий ключ для полного контроля. Shared Access Signature для делегированного, детализированного доступа. Azure Active Directory (где доступно) Публичный доступ:
Общий доступ также может быть предоставлен для включения анонимного доступа, в том числе через HTTP. — Примером этого может быть размещение базового контента и файлов в блочном BLOB-объекте, чтобы браузер мог просматривать и скачивать эти данные. Если вы получаете доступ к своему хранилищу из другой службы Azure, трафик остается в Azure.
Когда дело доходит до производительности хранилища, у нас есть два разных типа:
Standard - Максимальное количество операций ввода-вывода в секунду Premium - Гарантированное количество операций ввода-вывода в секунду Существует также разница между неуправляемыми и управляемыми дисками, которую следует учитывать при выборе правильного хранилища для поставленной задачи.
Хранилище виртуальной машины Диски ОС виртуальной машины обычно хранятся в постоянном хранилище. Некоторым рабочим нагрузкам без сохранения состояния не требуется постоянное хранилище, и уменьшение задержки является большим преимуществом. Существуют виртуальные машины, поддерживающие эфемерные управляемые диски ОС, созданные в локальном хранилище узла. Их также можно использовать с масштабируемыми наборами виртуальных машин. Управляемые диски — это надежное блочное хранилище, которое можно использовать с виртуальными машинами Azure. Вы можете иметь Ultra Disk Storage, Premium SSD, Standard SSD, Standard HDD. Они также несут некоторые характеристики.
Поддержка снимков и изображений Простое перемещение между SKU Лучшая доступность в сочетании с наборами доступности Плата взимается в зависимости от размера диска, а не от использованного хранилища. Хранилище архивов Cool Tier — доступен классный уровень хранилища для блокировки и добавления больших двоичных объектов. Более низкая стоимость хранения Более высокая стоимость сделки. Archive Tier* — Архивное хранилище доступно для блочных больших двоичных объектов. Это настраивается для каждого BLOB-объекта. Более низкая стоимость, более длительная задержка поиска данных. Такая же надежность данных, как и в обычном хранилище Azure. Пользовательские уровни данных могут быть включены по мере необходимости. Общий доступ к файлам Из вышеописанного создания нашей учетной записи хранения теперь мы можем создавать общие файловые ресурсы.
Это обеспечит файловые ресурсы SMB2.1 и 3.0 в Azure.
Можно использовать в Azure и извне через SMB3 и порт 445, открытый для Интернета.
Предоставляет общее хранилище файлов в Azure.
Можно сопоставить с помощью стандартных клиентов SMB в дополнение к REST API.
Вы также можете почитать Azure NetApp Files (SMB и NFS)
Службы кэширования и мультимедиа Сеть доставки содержимого Azure предоставляет кэш статического веб-содержимого с местоположениями по всему миру.
Службы мультимедиа Azure предоставляют технологии транскодирования мультимедиа в дополнение к службам воспроизведения.
Модели баз данных Microsoft Azure Еще в День 28 мы рассмотрели различные варианты обслуживания. Одним из них была PaaS (Platform as a Service) (платформа как услуга), где вы абстрагируете большую часть инфраструктуры и операционной системы, и вам остается контролировать приложение или, в данном случае, модели базы данных.
Реляционные базы данных База данных SQL Azure предоставляет реляционную базу данных как службу на основе Microsoft SQL Server.
Это SQL, работающий с последней веткой SQL с доступным уровнем совместимости базы данных, где требуется конкретная версия функциональности.
Есть несколько вариантов того, как это можно настроить: мы можем предоставить единую базу данных, которая предоставляет одну базу данных в экземпляре, в то время как эластичный пул позволяет использовать несколько баз данных, которые совместно используют пул емкости и совместно масштабируются.
Доступ к этим экземплярам базы данных можно получить как к обычным экземплярам SQL.
Дополнительные управляемые предложения для MySQL, PostgreSQL и MariaDB.
Решения NoSQL Azure Cosmos DB — это реализация NoSQL, не зависящая от схемы.
99,99% SLA
Глобально распределенная база данных с однозначными задержками на 99-м процентиле в любой точке мира с автоматическим возвратом в исходное положение.
Ключ раздела, используемый для разделения/разбиения/распределения данных.
Поддерживает различные модели данных (документы, ключ-значение, график, удобный для столбцов)
Поддерживает различные API (DocumentDB SQL, MongoDB, Azure Table Storage и Gremlin).
Доступны различные модели согласованности, основанные на теореме CAP.
Кэширование Не вдаваясь в подробности о системах кэширования, таких как Redis, я хотел добавить, что у Microsoft Azure есть служба под названием Azure Cache for Redis.
Кэш Azure для Redis предоставляет хранилище данных в памяти на основе программного обеспечения Redis.
Это реализация Redis Cache с открытым исходным кодом. Размещенный безопасный экземпляр кэша Redis. Доступны разные уровни Приложение должно быть обновлено, чтобы использовать кеш. Предназначен для приложения, которое имеет высокие требования к чтению по сравнению с записью. На основе хранилища ключей-значений. Я ценю, что за последние несколько дней было много заметок и теории о Microsoft Azure, но я хотел охватить строительные блоки, прежде чем мы перейдем к практическим аспектам того, как эти компоненты объединяются и работают.
У нас есть еще немного теории, связанной с сетью, прежде чем мы сможем запустить и запустить некоторые основанные на сценариях развертывания сервисов. Мы также хотим взглянуть на некоторые различные способы взаимодействия с Microsoft Azure по сравнению с порталом, который мы использовали до сих пор.
Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day32/"},"https://romankurnovskii.com/ru/docs/python101/chapter33_requests/":{title:"33. Пакет requests",tags:[],content:`Пакет requests - это более питоническая замена для собственного urllib в Python. API пакета requests во многом проще в работе.
Использование requests Давайте рассмотрим несколько примеров использования пакета requests. Мы будем использовать серию небольших фрагментов кода, чтобы помочь объяснить, как использовать эту библиотеку.
\u0026gt;\u0026gt;\u0026gt; r = requests.get(\u0026quot;http://www.google.com\u0026quot;) Этот пример возвращает объект Response. Вы можете использовать методы объекта Response, чтобы узнать много нового о том, как можно использовать запросы. Давайте воспользуемся функцией dir в Python, чтобы посмотреть, какие методы нам доступны:
\u0026gt;\u0026gt;\u0026gt; dir(r) ['__attrs__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_content', '_content_consumed', 'apparent_encoding', 'close', 'connection', 'content', 'cookies', 'elapsed', 'encoding', 'headers', 'history', 'iter_content', 'iter_lines', 'json', 'links', 'ok', 'raise_for_status', 'raw', 'reason', 'request', 'status_code', 'text', 'url'] Если вы запустите следующий метод, вы сможете увидеть исходный код веб-страницы:
\u0026gt;\u0026gt;\u0026gt; r.text Вывод этой команды слишком длинный, чтобы включать его в книгу, поэтому обязательно попробуйте сами. Если вы хотите взглянуть на заголовки веб-страниц, вы можете выполнить следующее:
\u0026gt;\u0026gt;\u0026gt; r.headers Обратите внимание, что атрибут headers возвращает диктоподобный объект и не является вызовом функции. Мы не показываем вывод, так как заголовки веб-страниц имеют тенденцию быть слишком широкими, чтобы правильно отображаться в книге. В объекте Response есть множество других замечательных функций и атрибутов. Например, вы можете получить cookies, ссылки на странице и status_code, который вернула страница.
Пакет requests поддерживает следующие типы HTTP-запросов: POST, GET, PUT, DELETE, HEAD и OPTIONS. Если страница возвращает json, вы можете получить к нему доступ, вызвав метод json объекта Response. Давайте рассмотрим практический пример.
Как отправить веб-форму В этом разделе мы сравним отправку веб-формы с помощью requests и urllib. Давайте начнем с изучения того, как отправить веб-форму. Мы будем выполнять веб-поиск на сайте duckduckgo.com по термину python и сохранять результат в виде HTML-файла. Мы начнем с примера, в котором используется urllib:
import urllib.request import urllib.parse import webbrowser data = urllib.parse.urlencode({'q': 'Python'}) url = 'http://duckduckgo.com/html/' full_url = url + '?' + data response = urllib.request.urlopen(full_url) with open(\u0026quot;results.html\u0026quot;, \u0026quot;wb\u0026quot;) as f: f.write(response.read()) webbrowser.open(\u0026quot;results.html\u0026quot;) Первое, что вам нужно сделать, когда вы хотите отправить веб-форму, - это выяснить, как называется форма и каков url, на который вы будете отправлять сообщение. Если вы перейдете на сайт duckduckgo и просмотрите источник, вы заметите, что его действие указывает на относительную ссылку \u0026ldquo;/html\u0026rdquo;. Таким образом, наш url будет \u0026ldquo;http://duckduckgo.com/html\u0026quot;. Поле ввода имеет имя \u0026ldquo;q\u0026rdquo;, поэтому, чтобы передать duckduckgo поисковый запрос, мы должны конкатенировать url с полем \u0026ldquo;q\u0026rdquo;. Результаты считываются и записываются на диск. Теперь давайте выясним, чем отличается этот процесс при использовании пакета requests.
Пакет requests выполняет отправку форм немного более элегантно. Давайте посмотрим:
import requests url = 'https://duckduckgo.com/html/' payload = {'q':'python'} r = requests.get(url, params=payload) with open(\u0026quot;requests_results.html\u0026quot;, \u0026quot;wb\u0026quot;) as f: f.write(r.content) При использовании запросов вам просто нужно создать словарь с именем поля в качестве ключа и поисковым термином в качестве значения. Затем вы используете requests.get для выполнения поиска. Наконец, вы используете полученный объект requests, \u0026ldquo;r\u0026rdquo;, и обращаетесь к его свойству content, которое сохраняете на диске.
Ресурсы https://www.w3schools.com/python/module_requests.asp `,url:"https://romankurnovskii.com/ru/docs/python101/chapter33_requests/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day33/":{title:"33. Сетевые модели Microsoft Azure \u002b Управление Azure",tags:["devops"],content:`Мы рассмотрим сетевые модели в Microsoft Azure и некоторые варианты управления для Azure. До сих пор мы использовали только платформу Azure, но упомянули и другие области, которые можно использовать для управления и создания наших ресурсов на платформе.
Сетевые модели Azure Виртуальные сети Виртуальная сеть — это конструкция, созданная в Azure. Виртуальной сети назначен один или несколько диапазонов IP-адресов. Виртуальные сети живут в рамках подписки внутри региона. В виртуальной сети создаются виртуальные подсети для разбиения сетевого диапазона. Виртуальные машины размещаются в виртуальных подсетях. Все виртуальные машины в виртуальной сети могут обмениваться данными. 65 536 частных IP-адресов на виртуальную сеть. Платите только за исходящий трафик из региона. (данные покидают регион) Поддерживаются IPv4 и IPv6. IPv6 для общедоступных и внутри виртуальных сетей. Мы можем сравнить виртуальные сети Azure с AWS VPC. Однако следует отметить некоторые отличия:
В AWS создается виртуальная сеть по умолчанию, чего нет в Microsoft Azure, вам необходимо создать свою первую виртуальную сеть в соответствии с вашими требованиями. Все виртуальные машины в Azure по умолчанию имеют доступ к Интернету через NAT. Нет шлюзов NAT в соответствии с AWS. В Microsoft Azure нет понятия частных или общедоступных подсетей. Общедоступные IP-адреса — это ресурс, который может быть назначен виртуальным сетевым адаптерам или балансировщикам нагрузки. Виртуальная сеть и подсети имеют свои собственные списки управления доступом, позволяющие делегировать уровень подсети. Подсети в зонах доступности, тогда как в AWS у вас есть подсети для каждой зоны доступности. У нас также есть виртуальный сетевой пиринг. Пиринг между виртуальными сетями позволяет эффективно соединить две Виртуальные сети Azure. После создания пиринговой связи две виртуальные сети выглядят как одна сеть в плане подключения. Точно так же трафик между виртуальными машинами в одноранговых виртуальных сетях использует магистральную инфраструктуру Майкрософт. Как и трафик между виртуальными машинами в одной сети, трафик направляется только через частную сеть корпорации Майкрософт.
Контроль доступа Azure использует группы безопасности сети, они сохраняют состояние. Разрешить создавать правила, а затем назначать их группе безопасности сети. Группы безопасности сети применяются к подсетям или виртуальным машинам. При применении к подсети он по-прежнему применяется к сетевой карте виртуальной машины и не является \u0026ldquo;Edge\u0026rdquo; устройством. Правила объединены в группу безопасности сети. В зависимости от приоритета возможны гибкие конфигурации. Более низкий номер приоритета означает высокий приоритет. Большая часть логики построена на IP-адресах, но также могут использоваться некоторые теги и метки. Description Priority Source Address Source Port Destination Address Destination Port Action Inbound 443 1005 * * * 443 Allow ILB 1010 Azure LoadBalancer * * 10000 Allow Deny All Inbound 4000 * * * * DENY У нас также есть группы безопасности приложений (Application Security Groups) (ASG) .
Где журналы потоков групп безопасности) (NSG) (Network Security Groups) сети сосредоточены на диапазонах IP-адресов, которые может быть сложно поддерживать для растущих сред. ASG позволяют определять настоящие имена (моникеры) для различных ролей приложений (веб-серверы, серверы БД, WebApp1 и т. д.). Сетевая карта виртуальной машины становится членом одной или нескольких групп ASG. Затем группы ASG можно использовать в правилах, которые являются частью групп безопасности сети, для управления потоком связи и по-прежнему могут использовать функции NSG, такие как теги обслуживания.
Action Name Source Destination Port Allow AllowInternettoWeb Internet WebServers 443(HTTPS) Allow AllowWebToApp WebServers AppServers 443(HTTPS) Allow AllowAppToDB AppServers DbServers 1443 (MSSQL) Deny DenyAllinbound Any Any Any Балансировщики нагрузки Load Balancing. В Microsoft Azure есть два отдельных решения для балансировки нагрузки. (От Microsoft Azure и сторонние на маркетплейсе) Оба могут работать с внешними или внутренними конечными ендпоинтами.
Балансировщик нагрузки (Layer 4), поддерживающий распределение на основе хэшей и переадресацию портов. Шлюз приложений (Layer 7) поддерживает такие функции, как разгрузка SSL, сопоставление сеансов на основе файлов cookie и маршрутизация контента на основе URL-адресов. Кроме того, с помощью шлюза приложений вы можете дополнительно использовать компонент брандмауэра веб-приложения.
Средства управления Azure Мы потратили большую часть нашего теоретического времени на изучение портала Azure, я бы предположил, что когда дело доходит до следования культуре DevOps и обработки многих этих задач, особенно связанных с подготовкой, будет выполняться через API или инструмент командной строки. Я хотел коснуться некоторых из тех других инструментов управления, которые у нас есть, поскольку нам нужно знать это, когда мы автоматизируем подготовку наших сред Azure.
Портал Azure Портал Microsoft Azure — это веб-консоль, которая представляет собой альтернативу инструментам командной строки. Вы можете управлять своими подписками на портале Azure. Создавайте, управляйте и контролируйте все, от простого веб-приложения до сложных облачных развертываний. Еще одна вещь, которую вы найдете на портале, — это хлебные крошки. JSON, как упоминалось ранее, является основой всех ресурсов Azure. Возможно, вы начнете с портала, чтобы понять функции, службы и функциональные возможности, а затем позже поймете JSON внизу, чтобы включить в ваши автоматизированные рабочие процессы.
Существует также портал Azure Preview, который можно использовать для просмотра и тестирования новых и предстоящих услуг и улучшений.
PowerShell Прежде чем мы перейдем к Azure PowerShell, стоит сначала познакомиться с PowerShell. PowerShell — это среда автоматизации задач и управления конфигурацией, оболочка командной строки и язык сценариев. Мы могли бы и осмелились сказать это, сравнив это с тем, что мы рассмотрели в разделе Linux, посвященном сценариям оболочки. PowerShell впервые появился в ОС Windows, но теперь он кроссплатформенный.
Azure PowerShell — это набор командлетов для управления ресурсами Azure непосредственно из командной строки PowerShell.
При желании мы можеем подключиться к подписке с помощью команды PowerShell «Connect-AzAccount».
Затем, если мы хотим найти некоторые конкретные команды, связанные с виртуальными машинами Azure, мы можем запустить следующую команду. Вы можете потратить часы на изучение и понимание этого языка программирования PowerShell. Microsoft предлагает отличные краткие руководства по началу работы и подготовке служб из PowerShell здесь
Visual Studio Code Visual Studio Code — это бесплатный редактор исходного кода, созданный Microsoft для Windows, Linux и macOS.
В Visual Studio Code встроено множество интеграций и инструментов, которые вы можете использовать для взаимодействия с Microsoft Azure и службами внутри. Cloud Shell Azure Cloud Shell — это интерактивная, аутентифицированная, доступная через браузер оболочка для управления ресурсами Azure. Это обеспечивает гибкость выбора оболочки, которая лучше всего подходит для вашей работы.
Как видно из рисунка ниже, когда мы впервые запускаем Cloud Shell на портале, мы можем выбирать между Bash и PowerShell.
Чтобы использовать облачную оболочку, вам нужно будет предоставить немного места в своей подписке.
Когда вы выбираете использование облачной оболочки, она запускает компьютер, эти компьютеры являются временными, но ваши файлы сохраняются двумя способами; через образ диска и подключенный файловый обменник.
Cloud Shell работает на временном хосте, предоставляемом для каждого сеанса и каждого пользователя. Время ожидания Cloud Shell истекает через 20 минут без интерактивной активности. Cloud Shell требует подключения общего файлового ресурса Azure. — Cloud Shell использует один и тот же файловый ресурс Azure как для Bash, так и для PowerShell. Cloud Shell назначается по одному компьютеру для каждой учетной записи пользователя. Cloud Shell сохраняет $HOME, используя образ размером 5 ГБ, хранящийся в вашей общей папке. Разрешения установлены как у обычного пользователя Linux в Bash Подробнее о Cloud Shell
Azure CLI Azure CLI можно установить в Windows, Linux и macOS. После установки вы можете ввести «az», а затем другие команды для создания, обновления, удаления и просмотра ресурсов Azure.
Когда я впервые приступил к изучению Azure, меня немного смутило наличие Azure PowerShell и Azure CLI.
Я также хотел бы получить отзывы от сообщества по этому поводу. Но я вижу, что Azure PowerShell — это модуль, добавленный в Windows PowerShell или PowerShell Core (также доступен в других ОС, но не во всех), тогда как Azure CLI — это кроссплатформенная программа командной строки, которая подключается к Azure и выполняет эти команды. .
Обе эти опции имеют разный синтаксис, хотя, насколько я вижу и что я сделал, они могут выполнять очень похожие задачи.
Например, для создания виртуальной машины из PowerShell будет использоваться командлет New-AzVM, а в Azure CLI — az VM create.
Ранее вы видели, что в моей системе установлен модуль Azure PowerShell, но затем у меня также установлен Azure CLI, который можно вызывать через PowerShell на моем компьютере с Windows.
Вывод здесь, как мы уже упоминали, заключается в выборе правильного инструмента. Azure работает на основе автоматизации. Каждое действие, которое вы совершаете внутри портала, где-то преобразуется в код, выполняемый для чтения, создания, изменения или удаления ресурсов.
Сравнение Azure CLI Кроссплатформенный интерфейс командной строки, устанавливаемый на Windows, macOS, Linux Работает в Windows PowerShell, Cmd или Bash и других оболочках Unix. Azure PowerShell Кроссплатформенный модуль PowerShell, работает на Windows, macOS, Linux Требуется Windows PowerShell или PowerShell Если по какой-то причине вы не можете использовать PowerShell в своей среде, но можете использовать .mdor bash, тогда Azure CLI будет вашим выбором.
Завтра попробуем создать несколько сценариев и приступим к работе в Azure.
Ресурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course AWS Basics for Beginners - Full Course `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day33/"},"https://romankurnovskii.com/ru/docs/python101/chapter34_sqlalchemy/":{title:"34. Пакет SQLAlchemy",tags:[],content:`SQLAlchemy обычно называют объектно-реляционным маппером (Object Relational Mapper - ORM), хотя он гораздо более полнофункциональный, чем любой другой ORM для Python, который я использовал, например, SqlObject или тот, который встроен в Django. SQLAlchemy был создан парнем по имени Майкл Байер. Поскольку я помешан на музыке, мы создадим простую базу данных для хранения информации об альбомах. База данных не является базой данных без некоторых отношений, поэтому мы создадим две таблицы и соединим их. Вот несколько других вещей, которые мы будем изучать:
Добавление данных в каждую таблицу
Изменение данных
Удаление данных
Базовые запросы
Но сначала нам нужно создать базу данных, поэтому именно с этого мы и начнем наше путешествие. Чтобы следовать этому руководству, вам потребуется установить SQLAlchemy. Для этого мы будем использовать pip:
pip install sqlalchemy Теперь мы готовы приступить к работе!
Как создать базу данных Создать базу данных с помощью SQLAlchemy очень просто. SQLAlchemy использует метод Declarative для создания баз данных. Мы напишем некоторый код для создания базы данных, а затем объясним, как он работает. Если вам нужен способ просмотра базы данных SQLite, я бы рекомендовал плагин SQLite Manager для Firefox. Вот некоторый код для создания таблиц нашей базы данных:
# table_def.py from sqlalchemy import create_engine, ForeignKey from sqlalchemy import Column, Date, Integer, String from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import relationship, backref engine = create_engine('sqlite:///mymusic.db', echo=True) Base = declarative_base() class Artist(Base): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; __tablename__ = \u0026quot;artists\u0026quot; id = Column(Integer, primary_key=True) name = Column(String) class Album(Base): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; __tablename__ = \u0026quot;albums\u0026quot; id = Column(Integer, primary_key=True) title = Column(String) release_date = Column(Date) publisher = Column(String) media_type = Column(String) artist_id = Column(Integer, ForeignKey(\u0026quot;artists.id\u0026quot;)) artist = relationship(\u0026quot;Artist\u0026quot;, backref=backref(\u0026quot;albums\u0026quot;, order_by=id)) # create tables Base.metadata.create_all(engine) Если вы запустите этот код, вы должны увидеть что-то похожее на следующий результат:
2014-04-03 09:43:57,541 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine PRAGMA table_info(\u0026quot;artists\u0026quot;) 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine PRAGMA table_info(\u0026quot;albums\u0026quot;) 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine CREATE TABLE artists ( id INTEGER NOT NULL, name VARCHAR, PRIMARY KEY (id) ) 2014-04-03 09:43:57,551 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,661 INFO sqlalchemy.engine.base.Engine COMMIT 2014-04-03 09:43:57,661 INFO sqlalchemy.engine.base.Engine CREATE TABLE albums ( id INTEGER NOT NULL, title VARCHAR, release_date DATE, publisher VARCHAR, media_type VARCHAR, artist_id INTEGER, PRIMARY KEY (id), FOREIGN KEY(artist_id) REFERENCES artists (id) ) 2014-04-03 09:43:57,661 INFO sqlalchemy.engine.base.Engine () 2014-04-03 09:43:57,741 INFO sqlalchemy.engine.base.Engine COMMIT Почему это произошло? Потому что когда мы создали объект engine, мы установили его параметр echo в True. Engine - это место, где находится информация о подключении к базе данных, и в нем есть все материалы DBAPI, которые делают возможным взаимодействие с вашей базой данных. Вы заметите, что мы создаем базу данных SQLite. Начиная с Python 2.5, SQLite поддерживается языком. Если вы хотите подключиться к какой-либо другой базе данных, то вам нужно будет отредактировать строку подключения. На всякий случай, если вы не понимаете, о чем идет речь, вот код:
engine = create_engine('sqlite:///mymusic.db', echo=True) Строка sqlite:///mymusic.db - это наша строка подключения. Далее мы создаем экземпляр декларативной базы, на которой мы будем основывать наши классы таблиц. Далее у нас есть два класса, Artist и Album, которые определяют, как будут выглядеть наши таблицы базы данных. Вы заметите, что у нас есть Columns, но нет имен столбцов. SQLAlchemy фактически использует имена переменных в качестве имен столбцов, если вы специально не укажете их в определении Column. Обратите внимание, что мы используем поле id Integer в качестве первичного ключа в обоих классах. Это поле будет автоинкрементным. Остальные столбцы не требуют пояснений, пока вы не дойдете до ForeignKey. Здесь вы увидите, что мы связываем artist_id с id в таблице Artist. Директива relationship указывает SQLAlchemy связать класс/таблицу Album с таблицей Artist. Благодаря тому, как мы установили ForeignKey, директива отношения сообщает SQLAlchemy, что это отношение \u0026ldquo;многие к одному\u0026rdquo;, что нам и нужно. Много альбомов к одному исполнителю. Вы можете прочитать больше об отношениях между таблицами здесь.
Последняя строка сценария создаст таблицы в базе данных. Если вы запустите этот сценарий несколько раз, он не сделает ничего нового после первого раза, поскольку таблицы уже созданы. Однако вы можете добавить еще одну таблицу, и тогда будет создана новая.
Как вставлять/добавлять данные в таблицы База данных не очень полезна, если в ней нет данных. В этом разделе мы покажем вам, как подключиться к базе данных и добавить некоторые данные в две таблицы. Гораздо проще посмотреть на код и затем объяснить его, так что давайте сделаем это!
# add_data.py import datetime from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from table_def import Album, Artist engine = create_engine('sqlite:///mymusic.db', echo=True) # create a Session Session = sessionmaker(bind=engine) session = Session() # Create an artist new_artist = Artist(name=\u0026quot;Newsboys\u0026quot;) new_artist.albums = [Album(title=\u0026quot;Read All About It\u0026quot;, release_date=datetime.date(1988,12,1), publisher=\u0026quot;Refuge\u0026quot;, media_type=\u0026quot;CD\u0026quot;)] # add more albums more_albums = [Album(title=\u0026quot;Hell Is for Wimps\u0026quot;, release_date=datetime.date(1990,7,31), publisher=\u0026quot;Star Song\u0026quot;, media_type=\u0026quot;CD\u0026quot;), Album(title=\u0026quot;Love Liberty Disco\u0026quot;, release_date=datetime.date(1999,11,16), publisher=\u0026quot;Sparrow\u0026quot;, media_type=\u0026quot;CD\u0026quot;), Album(title=\u0026quot;Thrive\u0026quot;, release_date=datetime.date(2002,3,26), publisher=\u0026quot;Sparrow\u0026quot;, media_type=\u0026quot;CD\u0026quot;)] new_artist.albums.extend(more_albums) # Add the record to the session object session.add(new_artist) # commit the record the database session.commit() # Add several artists session.add_all([ Artist(name=\u0026quot;MXPX\u0026quot;), Artist(name=\u0026quot;Kutless\u0026quot;), Artist(name=\u0026quot;Thousand Foot Krutch\u0026quot;) ]) session.commit() Сначала нам нужно импортировать определения наших таблиц из предыдущего сценария. Затем мы подключаемся к базе данных с помощью нашего движка и создаем нечто новое - объект Session. Сессия - это наш манипулятор к базе данных, позволяющий нам взаимодействовать с ней. Мы используем его для создания, изменения и удаления записей, а также используем сессии для запросов к базе данных. Далее мы создаем объект Artist и добавляем альбом. Вы заметите, что для добавления альбома достаточно создать список объектов Album и установить свойство \u0026ldquo;albums\u0026rdquo; объекта artist в этот список или расширить его, как показано во второй части примера. В конце сценария мы добавляем еще трех исполнителей с помощью add_all. Как вы уже, наверное, заметили, для записи данных в базу данных необходимо использовать метод commit объекта session. Теперь пришло время перейти к изменению данных.
Как изменять записи с помощью SQLAlchemy Что произойдет, если вы сохранили плохие данные. Например, вы неправильно ввели название своего любимого альбома или ошиблись с датой выхода фанатского издания, которым вы владеете? Тогда вам нужно узнать, как изменить эту запись! Это будет нашей отправной точкой в изучении запросов SQLAlchemy, поскольку вам нужно найти запись, которую нужно изменить, это значит, что вам нужно написать запрос к ней. Вот код, который покажет нам этот путь:
# modify_data.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from table_def import Album, Artist engine = create_engine('sqlite:///mymusic.db', echo=True) # create a Session Session = sessionmaker(bind=engine) session = Session() # querying for a record in the Artist table res = session.query(Artist).filter(Artist.name==\u0026quot;Kutless\u0026quot;).first() print(res.name) # changing the name res.name = \u0026quot;Beach Boys\u0026quot; session.commit() # editing Album data artist, album = session.query(Artist, Album).filter( Artist.id==Album.artist_id).filter(Album.title==\u0026quot;Thrive\u0026quot;).first() album.title = \u0026quot;Step Up to the Microphone\u0026quot; session.commit() Наш первый запрос ищет артиста по имени, используя метод filter. Функция .first() сообщает SQLAlchemy, что нам нужен только первый результат. Мы могли бы использовать .all(), если бы думали, что результатов будет несколько и нам нужны все из них. В любом случае, этот запрос возвращает объект Artist, которым мы можем манипулировать. Как вы видите, мы изменили name с Kutless на Beach Boys, а затем зафиксировали изменения.
Запрос к объединенной таблице немного сложнее. На этот раз мы написали запрос, который запрашивает обе наши таблицы. Он фильтрует по идентификатору исполнителя и названию альбома. Он возвращает два объекта: исполнителя и альбом. Получив их, мы можем легко изменить название альбома. Разве это не просто? На этом этапе, вероятно, стоит отметить, что если мы ошибочно добавили что-то в сессию, мы можем откатить наши изменения/добавления/удаления с помощью session.rollback(). Кстати, об удалении, давайте разберемся с этим вопросом!
Как удалять записи в SQLAlchemy Иногда вам просто необходимо удалить запись. Неважно, потому что вы вовлечены в тайну или потому что вы не хотите, чтобы люди знали о вашей любви к музыке Бритни Спирс, вам просто нужно избавиться от улик. В этом разделе мы покажем вам, как это сделать! К счастью для нас, SQLAlchemy позволяет удалять записи очень просто. Просто взгляните на следующий код!
# deleting_data.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from table_def import Album, Artist engine = create_engine('sqlite:///mymusic.db', echo=True) # create a Session Session = sessionmaker(bind=engine) session = Session() res = session.query(Artist).filter(Artist.name==\u0026quot;MXPX\u0026quot;).first() session.delete(res) session.commit() Как видите, все, что вам нужно было сделать, это создать еще один SQL-запрос, чтобы найти запись, которую вы хотите удалить, а затем вызвать session.delete(res). В данном случае мы удалили нашу запись MXPX. Мы уже видели запросы в действии, но давайте рассмотрим их подробнее и посмотрим, сможем ли мы узнать что-то новое.
Основные SQL-запросы SQLAlchemy SQLAlchemy предоставляет все запросы, которые могут вам понадобиться. Мы потратим немного времени на рассмотрение нескольких основных из них, таких как пара простых SELECT, JOINed SELECT и использование запроса LIKE. Вы также узнаете, где можно найти информацию о других типах запросов. А пока давайте посмотрим на код:
# queries.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from table_def import Album, Artist engine = create_engine('sqlite:///mymusic.db', echo=True) # create a Session Session = sessionmaker(bind=engine) session = Session() # how to do a SELECT * (i.e. all) res = session.query(Artist).all() for artist in res: print(artist.name) # how to SELECT the first result res = session.query(Artist).filter(Artist.name==\u0026quot;Newsboys\u0026quot;).first() # how to sort the results (ORDER_BY) res = session.query(Album).order_by(Album.title).all() for album in res: print(album.title) # how to do a JOINed query qry = session.query(Artist, Album) qry = qry.filter(Artist.id==Album.artist_id) artist, album = qry.filter(Album.title==\u0026quot;Step Up to the Microphone\u0026quot;).first() # how to use LIKE in a query res = session.query(Album).filter(Album.publisher.like(\u0026quot;S%a%\u0026quot;)).all() for item in res: print(item.publisher) Первый запрос, который мы выполним, возьмет всех исполнителей в базе данных (SELECT *) и выведет каждое из полей с их именами. Далее вы увидите, как просто выполнить запрос для конкретного исполнителя и вернуть только первый результат. Третий запрос показывает, как выполнить SELECT * для таблицы Album и упорядочить результаты по названию альбома. Четвертый запрос - это тот же запрос (запрос на JOIN), который мы использовали в разделе редактирования, только мы разбили его на части, чтобы он лучше соответствовал стандартам PEP8 в отношении длины строки. Еще одна причина разбивать длинные запросы - они становятся более читабельными и их легче исправить, если вы что-то напутали. В последнем запросе используется LIKE, который позволяет нам искать по образцу или искать то, что \u0026ldquo;похоже\u0026rdquo; на заданную строку. В данном случае мы хотели найти все записи, в которых издательство начиналось с заглавной буквы \u0026ldquo;S\u0026rdquo;, какого-либо символа, буквы \u0026ldquo;a\u0026rdquo;, а затем чего-либо еще. Таким образом, это будет соответствовать, например, издателям Sparrow и Star.
SQLAlchemy также поддерживает IN, IS NULL, NOT, AND, OR и все другие ключевые слова фильтрации, которые используют большинство DBA. SQLAlchemy также поддерживает литеральный SQL, скаляры и т.д. и т.п.
Подведение итогов На данном этапе вы должны знать SQLAlchemy достаточно хорошо, чтобы уверенно приступить к ее использованию. Проект также имеет отличную документацию, с помощью которой вы сможете найти ответы практически на все вопросы. Если вы застряли, группа пользователей SQLAlchemy / список рассылки очень отзывчива к новым пользователям, и даже главные разработчики готовы помочь вам разобраться во всем.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter34_sqlalchemy/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day34/":{title:"34. Практические скрипты Microsoft Azure",tags:["devops"],content:'Практические скрипты Microsoft Azure Последние 6 дней были сосредоточены на Microsoft Azure и общедоступном облаке в целом, большая часть этой основы должна была содержать много теории, чтобы понять строительные блоки Azure, но также это будет хорошо перенесено на других крупных облачных провайдеров. .\nВ самом начале я упомянул о базовых знаний об общедоступном облаке и выборе одного провайдера, по крайней мере, для начала. Если вы танцуете между разными облаками, я считаю, что вы можете довольно легко заблудиться, тогда как выбрав одно, вы поймете основы. и когда они у вас есть, довольно легко прыгнуть в другие облака и ускорить свое обучение.\nНа этом заключительном занятии я буду выбирать свои практические скрипты с этой страницы, которая является справочной информацией, созданной Microsoft и используемой для подготовки к AZ-104 Администратор Microsoft Azure\nЗдесь есть некоторые из них, такие как контейнеры и Kubernetes, которые мы еще не рассмотрели подробно, поэтому я не хочу пока вдаваться в них.\nВ предыдущих постах мы создали большинство модулей 1,2 и 3.\nВиртуальная сеть Мы пройдем пройти модуль 04:\nЯ прошел по инструкции и изменил несколько названий на #90DaysOfDevOps. Я также вместо использования Cloud Shell вошел в систему с моим новым пользователем, созданным в предыдущие дни с помощью Azure CLI на моем компьютере с Windows.\nВы можете сделать это, используя az login, который откроет браузер и позволит вам аутентифицировать свою учетную запись.\nЗатем я создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для выполнения некоторых из приведенных ниже задач. Вы можете найти связанные файлы в этой папке. (Облако\\01Виртуальная сеть)\nMod04_90DaysOfDevOps-vms-loop-parameters.json { \u0026quot;$schema\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\u0026quot;, \u0026quot;contentVersion\u0026quot;: \u0026quot;1.0.0.0\u0026quot;, \u0026quot;parameters\u0026quot;: { \u0026quot;vmSize\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;Standard_D2s_v3\u0026quot; }, \u0026quot;adminUsername\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;Student\u0026quot; }, \u0026quot;adminPassword\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;Pa55w.rd1234\u0026quot; } } } Mod04_90DaysOfDevOps-vms-loop-template.json { \u0026quot;$schema\u0026quot;: \u0026quot;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026quot;, \u0026quot;contentVersion\u0026quot;: \u0026quot;1.0.0.0\u0026quot;, \u0026quot;parameters\u0026quot;: { \u0026quot;vmSize\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;Standard_D2s_v3\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;VM size\u0026quot; } }, \u0026quot;vmName\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;90day-vm\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;VM name Prefix\u0026quot; } }, \u0026quot;vmCount\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;defaultValue\u0026quot;: 2, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;Number of VMs\u0026quot; } }, \u0026quot;adminUsername\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;Admin username\u0026quot; } }, \u0026quot;adminPassword\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;securestring\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;Admin password\u0026quot; } }, \u0026quot;virtualNetworkName\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;defaultValue\u0026quot;: \u0026quot;90daysofdevops\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;description\u0026quot;: \u0026quot;Virtual network name\u0026quot; } } }, \u0026quot;variables\u0026quot;: { \u0026quot;nic\u0026quot;: \u0026quot;90daysofdevops\u0026quot;, \u0026quot;virtualNetworkName\u0026quot;: \u0026quot;[parameters(\'virtualNetworkName\')]\u0026quot;, \u0026quot;subnetName\u0026quot;: \u0026quot;subnet\u0026quot;, \u0026quot;subnet0Name\u0026quot;: \u0026quot;subnet0\u0026quot;, \u0026quot;subnet1Name\u0026quot;: \u0026quot;subnet1\u0026quot;, \u0026quot;computeApiVersion\u0026quot;: \u0026quot;2018-06-01\u0026quot;, \u0026quot;networkApiVersion\u0026quot;: \u0026quot;2018-08-01\u0026quot; }, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;[concat(parameters(\'vmName\'),copyIndex())]\u0026quot;, \u0026quot;copy\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;VMcopy\u0026quot;, \u0026quot;count\u0026quot;: \u0026quot;[parameters(\'vmCount\')]\u0026quot; }, \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Compute/virtualMachines\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;[variables(\'computeApiVersion\')]\u0026quot;, \u0026quot;location\u0026quot;: \u0026quot;[resourceGroup().location]\u0026quot;, \u0026quot;comments\u0026quot;: \u0026quot;Creating VMs\u0026quot;, \u0026quot;dependsOn\u0026quot;: [ \u0026quot;[concat(variables(\'nic\'),copyIndex())]\u0026quot; ], \u0026quot;properties\u0026quot;: { \u0026quot;osProfile\u0026quot;: { \u0026quot;computerName\u0026quot;: \u0026quot;[concat(parameters(\'vmName\'),copyIndex())]\u0026quot;, \u0026quot;adminUsername\u0026quot;: \u0026quot;[parameters(\'adminUsername\')]\u0026quot;, \u0026quot;adminPassword\u0026quot;: \u0026quot;[parameters(\'adminPassword\')]\u0026quot;, \u0026quot;windowsConfiguration\u0026quot;: { \u0026quot;provisionVmAgent\u0026quot;: \u0026quot;true\u0026quot; } }, \u0026quot;hardwareProfile\u0026quot;: { \u0026quot;vmSize\u0026quot;: \u0026quot;[parameters(\'vmSize\')]\u0026quot; }, \u0026quot;storageProfile\u0026quot;: { \u0026quot;imageReference\u0026quot;: { \u0026quot;publisher\u0026quot;: \u0026quot;MicrosoftWindowsServer\u0026quot;, \u0026quot;offer\u0026quot;: \u0026quot;WindowsServer\u0026quot;, \u0026quot;sku\u0026quot;: \u0026quot;2019-Datacenter\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;latest\u0026quot; }, \u0026quot;osDisk\u0026quot;: { \u0026quot;createOption\u0026quot;: \u0026quot;fromImage\u0026quot; }, \u0026quot;dataDisks\u0026quot;: [] }, \u0026quot;networkProfile\u0026quot;: { \u0026quot;networkInterfaces\u0026quot;: [ { \u0026quot;properties\u0026quot;: { \u0026quot;primary\u0026quot;: true }, \u0026quot;id\u0026quot;: \u0026quot;[resourceId(\'Microsoft.Network/networkInterfaces\', concat(variables(\'nic\'),copyIndex()))]\u0026quot; } ] } } }, { \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Network/virtualNetworks\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;[variables(\'virtualNetworkName\')]\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;[variables(\'networkApiVersion\')]\u0026quot;, \u0026quot;location\u0026quot;: \u0026quot;[resourceGroup().location]\u0026quot;, \u0026quot;comments\u0026quot;: \u0026quot;Virtual Network\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;addressSpace\u0026quot;: { \u0026quot;addressPrefixes\u0026quot;: [ \u0026quot;10.40.0.0/22\u0026quot; ] }, \u0026quot;subnets\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;[variables(\'subnet0Name\')]\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;addressPrefix\u0026quot;: \u0026quot;10.40.0.0/24\u0026quot; } }, { \u0026quot;name\u0026quot;: \u0026quot;[variables(\'subnet1Name\')]\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;addressPrefix\u0026quot;: \u0026quot;10.40.1.0/24\u0026quot; } } ] } }, { \u0026quot;name\u0026quot;: \u0026quot;[concat(variables(\'nic\'),copyIndex())]\u0026quot;, \u0026quot;copy\u0026quot;:{ \u0026quot;name\u0026quot;: \u0026quot;nicCopy\u0026quot;, \u0026quot;count\u0026quot;: \u0026quot;[parameters(\'vmCount\')]\u0026quot; }, \u0026quot;type\u0026quot;: \u0026quot;Microsoft.Network/networkInterfaces\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;[variables(\'networkApiVersion\')]\u0026quot;, \u0026quot;location\u0026quot;: \u0026quot;[resourceGroup().location]\u0026quot;, \u0026quot;comments\u0026quot;: \u0026quot;Primary NIC\u0026quot;, \u0026quot;dependsOn\u0026quot;: [ \u0026quot;[concat(\'Microsoft.Network/virtualNetworks/\', variables(\'virtualNetworkName\'))]\u0026quot; ], \u0026quot;properties\u0026quot;: { \u0026quot;ipConfigurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;ipconfig1\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;subnet\u0026quot;: { \u0026quot;id\u0026quot;: \u0026quot;[resourceId(\'Microsoft.Network/virtualNetworks/subnets\', variables(\'virtualNetworkName\'), concat(variables(\'subnetName\'),copyIndex()))]\u0026quot; }, \u0026quot;privateIPAllocationMethod\u0026quot;: \u0026quot;Dynamic\u0026quot; } } ] } } ], \u0026quot;outputs\u0026quot;: {} } Module4_90DaysOfDevOps.ps1 $rgName = \'90DaysOfDevOps\' New-AzResourceGroupDeployment ` -ResourceGroupName $rgName ` -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-template.json ` -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-parameters.json Убедитесь, что вы изменили расположение файла в скрипте в соответствии с вашей средой.\nНа этом первом этапе у нас нет виртуальной сети или виртуальных машин, созданных в нашей среде, у меня есть только место хранения облачной оболочки, настроенное в моей группе ресурсов.\nСначала я запускаю свой скрипт в PowerShell\n$rgName = \'90DaysOfDevOps\' New-AzResourceGroupDeployment ` -ResourceGroupName $rgName ` -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-template.json ` -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-parameters.json Задача 1: Создать и настроить виртуальную сеть\nЗадача 2. Развернуть виртуальные машины в виртуальной сети.\nЗадача 3. Настройка частных и общедоступных IP-адресов виртуальных машин Azure.\nЗадача 4: Настройка групп безопасности сети\nЗадача 5. Настройка Azure DNS для внутреннего разрешения имен. Управление сетевым трафиком Переходим к модулю 06:\nДля этого практического занятия я создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для создания некоторых из приведенных ниже задач.\nЗадача 1: Обеспечение лабораторной среды Запустим PowerShell скрипт\n$rgName = \'90DaysOfDevOps\' New-AzResourceGroupDeployment ` -ResourceGroupName $rgName ` -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\02TrafficManagement\\Mod06_90DaysOfDevOps-vms-loop-template.json ` -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\02TrafficManagement\\Mod06_90DaysOfDevOps-vms-loop-parameters.json $location = (Get-AzResourceGroup -ResourceGroupName $rgName).location $vmNames = (Get-AzVM -ResourceGroupName $rgName).Name foreach ($vmName in $vmNames) { Set-AzVMExtension ` -ResourceGroupName $rgName ` -Location $location ` -VMName $vmName ` -Name \'networkWatcherAgent\' ` -Publisher \'Microsoft.Azure.NetworkWatcher\' ` -Type \'NetworkWatcherAgentWindows\' ` -TypeHandlerVersion \'1.4\' } Задача 2. Настройка топологии узловой сети Задача 3. Проверка транзитивности пиринга виртуальной сети.\nДля этого моя группа 90DaysOfDevOps не имела доступа к Network Watcher из-за разрешений, я ожидаю, что это связано с тем, что Network Watcher — это один из тех ресурсов, которые не привязаны к группе ресурсов, где наш RBAC был покрыт для этого пользователя. Я добавил в группу 90DaysOfDevOps роль участника Network Watcher из восточной части США.\nЭто ожидаемо, поскольку виртуальные сети с двумя лучами не связаны друг с другом (пиринг виртуальных сетей не является транзитивным).\nЗадача 4. Настройка маршрутизации в топологии «концентратор-луч». У меня была еще одна проблема: моя учетная запись не могла запустить скрипт от имени моего пользователя в группе 90DaysOfDevOps, в чем я не уверен, поэтому я вернулся в свою основную учетную запись администратора. Группа 90DaysOfDevOps является владельцем всего в группе ресурсов 90DaysOfDevOps, поэтому хотелось бы понять, почему я не могу запустить команду внутри виртуальной машины?\nTask 5: Подключаем Azure Load Balancer Task 6: Подключаем Azure Application Gateway Хранищиле Azure Переходим к модулю 07:\nДля этого практического занятия я также создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для создания некоторых из приведенных ниже задач.\nЗадача 1: Обеспечение лабораторной среды Сначала запускаем PowerShell script\n$rgName = \'90DaysOfDevOps\' New-AzResourceGroupDeployment ` -ResourceGroupName $rgName ` -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\03Storage\\Mod07_90DaysOfDevOps-vm-template.json ` -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\03Storage\\Mod07_90DaysOfDevOps-vm-parameters.json ` -AsJob Файл `Mod07_90DaysOfDevOps-vm-template.json` ``` { "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#", "contentVersion": "1.0.0.0", "parameters": { "vmSize": { "type": "string", "defaultValue": "Standard_D2s_v3", "metadata": { "description": "Virtual machine size" } }, "adminUsername": { "type": "string", "metadata": { "description": "Admin username" } }, "adminPassword": { "type": "securestring", "metadata": { "description": "Admin password" } } }, "variables": { "vmName": "90Days-vm0", "nicName": "90Days-nic0", "virtualNetworkName": "90Days-vnet0", "publicIPAddressName": "90Days-pip0", "nsgName": "90Days-nsg0", "vnetIpPrefix": "10.70.0.0/22", "subnetIpPrefix": "10.70.0.0/24", "subnetName": "subnet0", "subnetRef": "[resourceId(\'Microsoft.Network/virtualNetworks/subnets\', variables(\'virtualNetworkName\'), variables(\'subnetName\'))]", "computeApiVersion": "2018-06-01", "networkApiVersion": "2018-08-01" }, "resources": [ { "name": "[variables(\'vmName\')]", "type": "Microsoft.Compute/virtualMachines", "apiVersion": "[variables(\'computeApiVersion\')]", "location": "[resourceGroup().location]", "dependsOn": [ "[variables(\'nicName\')]" ], "properties": { "osProfile": { "computerName": "[variables(\'vmName\')]", "adminUsername": "[parameters(\'adminUsername\')]", "adminPassword": "[parameters(\'adminPassword\')]", "windowsConfiguration": { "provisionVmAgent": "true" } }, "hardwareProfile": { "vmSize": "[parameters(\'vmSize\')]" }, "storageProfile": { "imageReference": { "publisher": "MicrosoftWindowsServer", "offer": "WindowsServer", "sku": "2019-Datacenter", "version": "latest" }, "osDisk": { "createOption": "fromImage" }, "dataDisks": [] }, "networkProfile": { "networkInterfaces": [ { "properties": { "primary": true }, "id": "[resourceId(\'Microsoft.Network/networkInterfaces\', variables(\'nicName\'))]" } ] } } }, { "type": "Microsoft.Network/virtualNetworks", "name": "[variables(\'virtualNetworkName\')]", "apiVersion": "[variables(\'networkApiVersion\')]", "location": "[resourceGroup().location]", "comments": "Virtual Network", "properties": { "addressSpace": { "addressPrefixes": [ "[variables(\'vnetIpPrefix\')]" ] }, "subnets": [ { "name": "[variables(\'subnetName\')]", "properties": { "addressPrefix": "[variables(\'subnetIpPrefix\')]" } } ] } }, { "name": "[variables(\'nicName\')]", "type": "Microsoft.Network/networkInterfaces", "apiVersion": "[variables(\'networkApiVersion\')]", "location": "[resourceGroup().location]", "comments": "Primary NIC", "dependsOn": [ "[variables(\'publicIpAddressName\')]", "[variables(\'nsgName\')]", "[variables(\'virtualNetworkName\')]" ], "properties": { "ipConfigurations": [ { "name": "ipconfig1", "properties": { "subnet": { "id": "[variables(\'subnetRef\')]" }, "privateIPAllocationMethod": "Dynamic", "publicIpAddress": { "id": "[resourceId(\'Microsoft.Network/publicIpAddresses\', variables(\'publicIpAddressName\'))]" } } } ], "networkSecurityGroup": { "id": "[resourceId(\'Microsoft.Network/networkSecurityGroups\', variables(\'nsgName\'))]" } } }, { "name": "[variables(\'publicIpAddressName\')]", "type": "Microsoft.Network/publicIpAddresses", "apiVersion": "[variables(\'networkApiVersion\')]", "location": "[resourceGroup().location]", "comments": "Public IP for Primary NIC", "properties": { "publicIpAllocationMethod": "Dynamic" } }, { "name": "[variables(\'nsgName\')]", "type": "Microsoft.Network/networkSecurityGroups", "apiVersion": "[variables(\'networkApiVersion\')]", "location": "[resourceGroup().location]", "comments": "Network Security Group (NSG) for Primary NIC", "properties": { "securityRules": [ { "name": "default-allow-rdp", "properties": { "priority": 1000, "sourceAddressPrefix": "*", "protocol": "Tcp", "destinationPortRange": "3389", "access": "Allow", "direction": "Inbound", "sourcePortRange": "*", "destinationAddressPrefix": "*" } } ] } } ], "outputs": {} } ``` Файл `Mod07_90DaysOfDevOps-vm-parameters.json` ``` { "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#", "contentVersion": "1.0.0.0", "parameters": { "vmSize": { "value": "Standard_D2s_v3" }, "adminUsername": { "value": "Student" }, "adminPassword": { "value": "Pa55w.rd1234" } } } ``` Задача 2. Создание и настройка учетных записей хранения Azure. Задача 3. Управление хранилищем BLOB-объектов Задача 4. Управление проверкой подлинности и авторизацией для службы хранилища Azure. Я был немного нетерпелив, ожидая, что это все сработает, но в конце концов это сработало.\nЗадача 5. Создание и настройка общих папок Azure Files. В команде запуска это не сработает с michael.cade@90DaysOfDevOps.com, поэтому я использовал свою учетную запись с повышенными правами.\nЗадача 6. Управление сетевым доступом для службы хранилища Azure. Serverless (внедрение веб-приложений) Переходим к модулю 09a:\nЗадача 1. Создание веб-приложения Azure. Задача 2. Создание промежуточного слота развертывания. Задача 3. Настройка параметров развертывания веб-приложений. Задача 4. Развертывание кода в промежуточном слоте развертывания. Задача 5: Поменять промежуточные слоты местами Задача 6. Настройка и тестирование автоматического масштабирования веб-приложения Azure. $rgName = \'90DaysOfDevOps\' $webapp = Get-AzWebApp -ResourceGroupName $rgName #The following following will start an infinite loop that sends the HTTP requests to the web app while ($true) { Invoke-WebRequest -Uri $webapp.DefaultHostName } На этом мы завершаем раздел о Microsoft Azure и public cloud в целом.\nРесурсы Hybrid Cloud and MultiCloud Microsoft Azure Fundamentals Google Cloud Digital Leader Certification Course Далее мы углубимся в системы контроля версий, особенно в git, а затем также рассмотрим обзоры репозиториев кода, и мы выберем GitHub, так как это мой предпочтительный вариант.\n',url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day34/"},"https://romankurnovskii.com/ru/docs/python101/chapter35_virtualenv/":{title:"35. virtualenv",tags:[],content:`Виртуальные среды могут быть очень удобны для тестирования программного обеспечения. Это верно и в кругах программистов. Ян Бикинг создал проект virtualenv, который является инструментом для создания изолированных сред Python. Вы можете использовать эти среды для тестирования новых версий вашего программного обеспечения, новых версий пакетов, от которых вы зависите, или просто в качестве \u0026ldquo;песочницы\u0026rdquo; для опробования нового пакета в целом. Вы также можете использовать virtualenv в качестве рабочей среды, когда вы не можете скопировать файлы в пакеты сайта, потому что он находится на общем хосте. Когда вы создаете виртуальную среду с помощью virtualenv, он создает папку и копирует в нее Python вместе с папкой site-packages и несколькими другими. Он также устанавливает pip. Как только ваша виртуальная среда станет активной, вы будете использовать обычный Python. А когда вы закончите, вы можете просто удалить папку для очистки. Никакой суеты, никакой суеты. Кроме того, вы можете продолжать использовать его для разработки.
В этой главе мы потратим некоторое время на знакомство с virtualenv и на то, как использовать его для создания собственной магии.
Установка Прежде всего, вам, вероятно, потребуется установить virtualenv. Вы можете использовать pip или easy_install для его установки или скачать файл virtualenv.py с их сайта и установить его из исходников с помощью сценария setup.py.
Если у вас Python 3.4, вы обнаружите, что у вас есть модуль venv, который использует API, очень похожий на пакет virtualenv. Однако в этой главе речь пойдет только о пакете virtualenv.
Создание виртуальной среды Создать виртуальную песочницу с помощью пакета virtualenv довольно просто. Все, что вам нужно сделать, это следующее:
python virtualenv.py FOLDER_NAME Где FOLDER_NAME* - это имя папки, в которую вы хотите поместить вашу песочницу. На моей машине Windows 7 путь к папке C:\\Python34\\Scripts добавлен, поэтому я могу просто вызвать virtualenv.py FOLDER_NAME без части python. Если вы не передадите ему ничего, то получите список опций, выведенный на экран. Допустим, мы создадим проект под названием sandbox. Как мы будем его использовать? Ну, нам нужно активировать его. Вот как:
В Posix вы сделаете source bin/activate, а в Windows вы сделаете .\\path\\to\\env\\Scripts\\activate. в командной строке. Давайте проделаем эти шаги. Мы создадим папку \u0026ldquo;песочница\u0026rdquo; на рабочем столе, чтобы вы могли увидеть пример. Вот как это выглядит на моей машине:
C:\\Users\\mdriscoll\\Desktop\u0026gt;virtualenv sandbox New python executable in sandbox\\Scripts\\python.exe Installing setuptools................done. Installing pip...................done. C:\\Users\\mdriscoll\\Desktop\u0026gt;sandbox\\Scripts\\activate (sandbox) C:\\Users\\mdriscoll\\Desktop\u0026gt; Как только ваша виртуальная среда будет активирована, вы увидите, что ваше приглашение изменится и будет включать префикс имени папки, которую вы создали, в данном случае это sandbox. Это дает вам знать, что вы используете свою песочницу. Теперь вы можете использовать pip для установки других пакетов в вашу виртуальную среду. Когда вы закончите, просто вызовите сценарий deactivate, чтобы выйти из среды.
Есть несколько флагов, которые вы можете передать virtualenv при создании виртуальной среды, о которых вам следует знать. Например, вы можете использовать -system-site-packages, чтобы унаследовать пакеты от пакетов сайта Python по умолчанию. Если вы хотите использовать distribute, а не setuptools, вы можете передать virtualenv флаг -distribute.
virtualenv также предоставляет возможность просто установить библиотеки, но использовать для их запуска сам системный Python. Согласно документации, для этого достаточно создать специальный скрипт.
Есть также изящный (и экспериментальный) флаг -relocatable, который можно использовать, чтобы сделать папку перемещаемой. Однако на момент написания этой статьи он НЕ работает в Windows, поэтому я не смог его проверить.
Наконец, есть флаг -extra-search-dir, который можно использовать для сохранения виртуальной среды в автономном режиме. По сути, он позволяет добавить каталог в путь поиска дистрибутивов, из которых pip или easy_install могут устанавливать. Таким образом, вам не нужно иметь доступ к интернету для установки пакетов.
Подведение итогов На данном этапе вы должны уметь использовать virtualenv самостоятельно. На этом этапе стоит упомянуть еще пару проектов. Это библиотека virtualenvwrapper Дуга Хеллмана, которая еще больше упрощает создание, удаление и управление виртуальными окружениями, а также zc.buildout, который, вероятно, является ближайшим конкурентом virtualenv. Я рекомендую ознакомиться с ними обеими, поскольку они могут помочь вам в ваших приключениях в программировании.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter35_virtualenv/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day35/":{title:"35. Git — контроль версий",tags:["devops","git"],content:`Общая картина: Git — контроль версий Прежде чем мы перейдем к git, нам нужно понять, что такое контроль версий? В этой статье мы рассмотрим, что такое контроль версий и основы git.
Что такое контроль версий? Git — не единственная система контроля версий, поэтому рассмотрим, какие варианты и какие методологии доступны для контроля версий.
Наиболее очевидным и большим преимуществом контроля версий является возможность отслеживать историю проекта. Мы можем посмотреть на этот репозиторий с помощью git log и увидеть, что у нас есть много коммитов и много комментариев, а также то, что произошло на данный момент в проекте. Не волнуйтесь, мы перейдем к командам позже. А теперь подумайте, если бы это был настоящий программный проект, полный исходного кода, и несколько человек в разное время принимают участие в нашем программном обеспечении, разные авторы, а затем и рецензенты, все регистрируются здесь, чтобы мы знали, что произошло, когда, кем и кто рецензировал.
Управление версиями, прежде чем это стало крутым, было чем-то вроде ручного создания копии вашей версии, прежде чем вы вносили изменения. Возможно, вы также закомментируете старый бесполезный код на всякий случай.
Тем не менее, Управление версиями не является резервной копией!
Еще одним преимуществом контроля версий является возможность управления несколькими версиями проекта. Давайте создадим пример, у нас есть бесплатное приложение, доступное во всех операционных системах, а затем у нас есть платное приложение, также доступное во всех операционных системах. БОльшая часть кода используется обоими приложениями. Мы могли бы копировать и вставлять наш код при каждом коммите в каждое приложение, но это будет очень грязно, особенно если вы масштабируете свою разработку более чем на одного человека, а также будут допущены ошибки.
В премиум-приложении у нас будут дополнительные функции, назовем их премиальными коммитами, бесплатная версия будет содержать только обычные коммиты.
Способ, которым это достигается в системе управления версиями, — это ветвление (branching).
Ветвление позволяет использовать два потока кода для одного и того же приложения, как мы указали выше. Но мы по-прежнему хотим, чтобы новые функции, которые появляются в нашей бесплатной версии исходного кода, были в нашей премиум-версии, и для этого у нас есть то, что называется слиянием.
Теперь это такое же простое, но слияние может быть сложным, потому что у вас может быть команда, работающая над бесплатной версией, и другая команда, работающая над платной премиальной версией, и что, если обе они изменят код, который влияет на аспекты общего кода. Может быть, переменная обновляется и что-то ломает. Тогда у вас есть конфликт, который нарушает одну из функций. Контроль версий не может устранить конфликты, которые зависят от вас. Но контроль версий позволяет легко управлять этим.
Основная причина, по которой вы до сих пор не взялись за управление версиями, — это возможность совместной работы. Возможность делиться кодом между разработчиками, и когда я говорю код, как я уже говорил раньше, все чаще и чаще мы видим гораздо больше вариантов использования по другим причинам для использования системы управления версиями, может быть, это совместная презентация, над которой вы работаете с коллегой, или вызов 90DaysOfDevOps. где у вас есть сообщество, предлагающее свои исправления и обновления на протяжении всего проекта.
Без контроля версий, как команды разработчиков программного обеспечения вообще справлялись с этим? Когда я работаю над своими проектами, мне достаточно трудно следить за вещами. Я ожидаю, что они разделят код на каждый функциональный модуль. Возможно, небольшая часть головоломки заключалась в том, чтобы собрать воедино кусочки, а затем решить проблемы и проблемы, прежде чем что-либо было выпущено.
С контролем версий у нас есть единственный источник правды. Мы все еще можем работать над разными модулями, но это позволяет нам лучше взаимодействовать.
Еще одна вещь, которую следует упомянуть здесь, это то, что не только разработчики могут извлечь выгоду из контроля версий. Все члены команды должны иметь представление, но также и инструменты управления проектом и т.д. У нас также может быть build машина, например Jenkins, о которой мы поговорим в другом модуле. Зада подобных инструментов - создать и упаковывать систему, автоматизируя тесты и предоставляя метрики.
Что такое Git? Git — это инструмент, который отслеживает изменения в исходном коде или любом файле, или мы могли бы также сказать, что Git — это распределенная система контроля версий с открытым исходным кодом.
Есть много способов, которыми git можно использовать в наших системах, чаще всего или, по крайней мере, для меня я видел его в командной строке, но у нас также есть графические пользовательские интерфейсы и инструменты, такие как Visual Studio Code, которые имеют операции с поддержкой git, которые мы может воспользоваться.
Теперь мы пройдемся по общему обзору еще до того, как установим Git на нашу локальную машину.
Возьмем папку, которую мы создали ранее.
Чтобы использовать эту папку с контролем версий, нам сначала нужно инициировать этот каталог с помощью команды \`git init. А пока представьте, что эта команда помещает наш каталог в качестве репозитория в базу данных где-то на нашем компьютере.
Теперь мы можем создать несколько файлов и папок, и наш исходный код может начаться, или, может быть, он уже есть, и у нас уже есть что-то здесь. Мы можем использовать команду git add ., которая помещает все файлы и папки в нашем каталоге в снимок, но мы еще ничего не зафиксировали в этой базе данных. Мы просто говорим, что все файлы с . готовы к добавлению.
Затем мы хотим продолжить и зафиксировать наши файлы, мы делаем это с помощью команды git commit -m \u0026quot;My First Commit\u0026quot;. Мы можем указать причину нашей фиксации, и это предлагается, чтобы мы знали, что произошло для каждой фиксации.
Теперь мы можем увидеть, что произошло в истории проекта. С помощью команды git log.
Мы также можем проверить состояние нашего репозитория с помощью git status, это показывает, что нам нечего коммитить, и мы можем добавить новый файл с именем samplecode.ps1. Если мы затем запустим тот же статус \`git, вы увидите, что мы файл для фиксации.
Добавьте наш новый файл с помощью команды git add samplecode.ps1, а затем мы снова запустим git status и увидим, что наш файл готов к фиксации.
Затем выполните команду git commit -m \u0026ldquo;My Second Commit\u0026rdquo;.
Другой git status теперь показывает, что все снова чисто.
Затем мы можем использовать команду git log, которая показывает последние изменения и первую фиксацию.
Если мы хотим увидеть изменения между нашими коммитами, то есть какие файлы были добавлены или изменены, мы можем использовать git diff b8f8 709a
Затем отображается то, что изменилось, в нашем случае мы добавили новый файл.
Мы также можем, и мы углубимся в это позже, но мы можем прыгать вокруг наших коммитов, то есть мы можем путешествовать во времени! Используя наш номер фиксации, мы можем использовать команду git checkout 709a, чтобы вернуться назад во времени, не теряя наш новый файл.
Но в равной степени мы также захотим двигаться вперед, и мы можем сделать это таким же образом с номером коммита, или вы можете видеть здесь, что мы используем команду git switch -, чтобы отменить нашу операцию.
TLDR;
Отслеживание истории проектов Управление несколькими версиями проекта Обмен кодом между разработчиками и более широкий круг команд и инструментов Координация работы в команде Это могло показаться прыжком, но, надеюсь, вы можете увидеть, даже не зная, что команды использовали возможности и общую картину, лежащую в основе контроля версий.
Далее мы установим и настроим git на вашем локальном компьютере и немного углубимся в некоторые другие варианты использования и команды, которые мы можем реализовать в Git.
Ресурсы What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day35/"},"https://romankurnovskii.com/ru/docs/python101/chapter36_creating_modules_and_packages/":{title:"36. Создание модулей и пакетов",tags:[],content:`Создание модулей Python - это то, что большинство программистов Python делают каждый день, даже не задумываясь об этом. Каждый раз, когда вы сохраняете новый сценарий Python, вы создаете новый модуль. Вы можете импортировать свой модуль в другие модули. Пакет - это коллекция связанных модулей. То, что вы импортируете в свои сценарии из стандартной библиотеки, является модулями или пакетами. В этой главе мы узнаем, как создавать модули и пакеты. Мы уделим больше времени пакетам, поскольку они сложнее модулей.
Как создать модуль Python Мы начнем с создания суперпростого модуля. Этот модуль будет предоставлять нам базовую арифметику и не будет обрабатывать ошибки. Вот наш первый пример:
def add(x, y): return x + y def division(x, y): return x / y def multiply(x, y): return x * y def subtract(x, y): return x - y У этого кода, конечно, есть проблемы. Отсутствует проверка ошибок при делении на ноль или смешивании строк и чисел. Но дело не в этом. Дело в том, что если вы сохраните этот код, у вас будет полностью квалифицированный модуль. Назовем его arithmetic.py. Что можно сделать с модулем? Вы можете импортировать его и использовать любую из определенных функций или классов, которые находятся внутри него. А можно сделать его исполняемым, немного подправить и отполировать. Давайте сделаем и то, и другое!
Сначала мы напишем небольшой скрипт, который импортирует наш модуль и запускает функции в нем. Сохраните следующий файл под именем math_test.py:
import arithmetic print(arithmetic.add(5, 8)) print(arithmetic.subtract(10, 5)) print(arithmetic.division(2, 7)) print(arithmetic.multiply(12, 6)) Теперь давайте изменим исходный скрипт так, чтобы его можно было запускать из командной строки. Вот один из самых простых способов сделать это:
def add(x, y): return x + y def division(x, y): return x / y def multiply(x, y): return x * y def subtract(x, y): return x - y if __name__ == \u0026quot;__main__\u0026quot;: import sys print(sys.argv) v = sys.argv[1].lower() valOne = int(sys.argv[2]) valTwo = int(sys.argv[3]) if v == \u0026quot;a\u0026quot;: print(add(valOne, valTwo)) elif v == \u0026quot;d\u0026quot;: print(division(valOne, valTwo)) elif v == \u0026quot;m\u0026quot;: print(multiply(valOne, valTwo)) elif v == \u0026quot;s\u0026quot;: print(subtract(valOne, valTwo)) else: pass Правильным способом выполнения этого сценария было бы использование модуля Python optparse (pre-2.7) или argparse (2.7+). Вам следует потратить некоторое время, чтобы разобраться с одним из этих модулей в качестве учебного упражнения. А пока мы перейдем к пакетам!
Как создать пакет Python Основное различие между модулем и пакетом заключается в том, что пакет - это набор модулей И у него есть файл init.py. В зависимости от сложности пакета, он может иметь более одного init.py. Давайте рассмотрим простую структуру папок, чтобы сделать это более очевидным, а затем создадим простой код, который будет следовать структуре, которую мы определили.
mymath/ __init__.py adv/ __init__.py sqrt.py add.py subtract.py multiply.py divide.py Теперь нам просто нужно повторить эту структуру в нашем собственном пакете. Давайте попробуем это сделать! Создайте каждый из этих файлов в дереве папок, как показано в примере выше. Для файлов сложения, вычитания, умножения и деления можно использовать функции, которые мы создали в предыдущем примере. Для модуля sqrt.py мы будем использовать следующий код.
# sqrt.py import math def squareroot(n): return math.sqrt(n) Вы можете оставить оба файла init.py пустыми, но тогда вам придется писать код типа mymath.add.add(x,y), что довольно некрасиво, поэтому мы добавим следующий код во внешний init.py, чтобы облегчить понимание и использование нашего пакета.
# outer __init__.py from . add import add from . divide import division from . multiply import multiply from . subtract import subtract from .adv.sqrt import squareroot Теперь мы должны иметь возможность использовать наш модуль, когда он находится в пути к Python. Для этого вы можете скопировать папку в папку site-packages вашего Python. В Windows она находится в следующем общем месте: C:\\Python34\\Lib\\site-packages. В качестве альтернативы вы можете отредактировать путь на лету в коде теста. Давайте посмотрим, как это делается:
import sys # modify this path to match your environment sys.path.append('C:\\Users\\mdriscoll\\Documents') import mymath print(mymath.add(4,5)) print(mymath.division(4, 2)) print(mymath.multiply(10, 5)) print(mymath.squareroot(48)) Обратите внимание, что мой путь НЕ включает папку mymath. Вы хотите добавить родительскую папку, в которой находится ваш новый модуль, а не саму папку модуля. Если вы сделаете это, то приведенный выше код будет работать.
Вы также можете создать сценарий setup.py и установить ваш пакет в режиме разработки. Вот пример скрипта setup.py:
#!/usr/bin/env python from setuptools import setup # This setup is suitable for \u0026quot;python setup.py develop\u0026quot;. setup(name='mymath', version='0.1', description='A silly math package', author='Mike Driscoll', author_email='mike@mymath.org', url='http://www.mymath.org/', packages=['mymath', 'mymath.adv'], ) Сохраните этот сценарий на один уровень выше папки mymath. Чтобы установить пакет в режиме разработки, выполните следующие действия:
python setup.py develop Это установит файл ссылки в папку site-packages, который будет указывать на то место, где находится ваш пакет. Это удобно для тестирования без фактической установки пакета.
Поздравляем! Вы только что создали пакет Python!
Подведение итогов Вы только что узнали, как создавать свои собственные модули и пакеты. Вы обнаружите, что чем больше вы пишете, тем чаще вы создаете программы, в которых есть части, которые вы хотите использовать повторно. Эти многократно используемые части кода можно объединить в модули. В конце концов, у вас будет достаточно связанных модулей, и вы захотите объединить их в пакет. Теперь у вас есть инструменты, чтобы сделать это!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter36_creating_modules_and_packages/"},"https://romankurnovskii.com/ru/docs/python101/05-part_v/":{title:"Часть V - Упаковка и распространение",tags:[],content:`В части V вы узнаете об упаковке Python и различных методах распространения вашего кода. Вы узнаете о следующем:
Как создать модуль и пакет Публикация пакетов в Python Packaging Index (PyPI) Python eggs Python wheels py2exe bb_freeze cx_Freeze PyInstaller GUI2Exe Как создать инсталлятор с помощью InnoSetup В первой главе этого раздела описано, как создать модуль или пакет. Затем, в следующей главе, мы рассмотрим публикацию нашего пакета в PyPI. Далее мы узнаем, как создать и установить Python egg и Python wheel.
В следующих четырех главах мы рассмотрим, как создавать двоичные файлы с помощью следующих пакетов сторонних разработчиков: py2exe, bb_freeze, cx_Freeze и PyInstaller. Единственный пакет в этом списке, который действительно совместим с Python 3 - это cx_Freeze.
В предпоследней главе мы покажем вам, как использовать GUI2Exe, небольшой аккуратный пользовательский интерфейс, который был создан для работы поверх py2exe, bb_freeze и др. GUI2Exe делает создание двоичных файлов еще проще!
В последней главе этого раздела будет показано, как создать программу установки с помощью InnoSetup. Давайте приступим!
`,url:"https://romankurnovskii.com/ru/docs/python101/05-part_v/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day36/":{title:"36. Установка и настройка Git",tags:["devops"],content:`Установка и настройка Git Git — это кроссплатформенный инструмент с открытым исходным кодом для контроля версий. Если я нравлюсь вам, вы используете Ubuntu или большинство сред Linux, вы можете обнаружить, что у вас уже установлен git, но мы собираемся выполнить установку и настройку.
Даже если у вас уже установлен git в вашей системе, также рекомендуется убедиться, что мы в курсе последних событий.
Установка Git Мы будем работать с Windows и Linux, но вы также можете найти macOS в списке здесь
Для Windows мы можем загрузить наши установщики с официального сайта.
Вы также можете использовать winget на своем компьютере с Windows, думайте об этом как о своем диспетчере пакетов приложений Windows.
Прежде чем мы что-либо установим, давайте посмотрим, какая версия у нас есть на нашей машине с Windows. Откройте окно PowerShell и запустите git --version
Мы также можем проверить нашу версию Git для Ubuntu.
Загружаем последнюю версию установщика. Важно отметить, что git удалит предыдущие версии перед установкой последней.
Это означает, что процесс, показанный ниже, по большей части такой же, как если бы вы устанавливали не из git.
Это очень простая установка. После загрузки дважды щелкните и начните. Прочтите лицензионное соглашение GNU. Но помните, что это бесплатное программное обеспечение с открытым исходным кодом.
Теперь мы можем выбрать дополнительные компоненты, которые мы хотели бы также установить, но также связать с git. В Windows я всегда устанавливаю Git Bash, так как это позволяет нам запускать сценарии bash в Windows.
Затем мы можем выбрать, какой исполняемый файл SSH мы хотим использовать. IN оставьте это как пакетный OpenSSH, который вы могли видеть в разделе Linux.
Затем у нас есть экспериментальные функции, которые мы можем захотеть включить, мне они не нужны, поэтому я не включаю, вы всегда можете вернуться во время установки и включить их позже.
Установка завершена, теперь мы можем открыть Git Bash или последние примечания к выпуску.
Последняя проверка — посмотреть в нашем окне PowerShell, какая у нас сейчас версия git.
Супер простые вещи, и теперь мы на последней версии. На нашей машине с Linux мы немного отстали, поэтому мы также можем пройти этот процесс обновления.
Я просто запускаю команду sudo apt-get install git.
Вы также можете запустить следующее, которое добавит репозиторий git для установки программного обеспечения.
sudo add-apt-repository ppa:git-core/ppa -y sudo apt-get update sudo apt-get install git -y git --version Настройка Git Когда мы впервые используем git, нам нужно определить некоторые настройки,
Имя Эл. адрес Редактор по умолчанию Окончание строки Это можно сделать на трех уровнях
System = Все пользователи Global = все репозитории текущего пользователя Local = текущий репозиторий Пример:
git config --global user.name \u0026quot;My Name\u0026quot; git config --global user.email email@example.com\u0026quot; В зависимости от вашей операционной системы будет определять текстовый редактор по умолчанию. На моей машине с Ubuntu без настройки следующая команда использует Тano. Приведенная ниже команда изменит это на код Visual Studio.
git config --global core.editor \u0026quot;code --wait\u0026quot;
Чтобы увидеть всю конфигурацию git, мы можем использовать команду git config --global -e
На любом компьютере этот файл будет называться .gitconfig, на моем компьютере с Windows вы найдете его в каталоге своей учетной записи пользователя. Теория Git Я упомянул во вчерашнем посте, что существуют и другие типы контроля версий, и мы можем разделить их на два разных типа. Один клиент-сервер, а другой распределенный.
Клиент-серверный контроль версий До появления git клиент-сервер был де-факто методом контроля версий. Примером этого может быть Apache Subversion, которая представляет собой систему управления версиями с открытым исходным кодом, основанную в 2000 году.
В этой модели управления версиями клиент-сервер на первом этапе разработчик загружает исходный код, фактические файлы с сервера. Это не устраняет конфликты, но устраняет сложность конфликтов и способы их разрешения.
Теперь, например, скажем, у нас есть два разработчика, работающих над одними и теми же файлами, и один из них выигрывает гонку и первым фиксирует или загружает свой файл обратно на сервер со своими новыми изменениями. Когда второй разработчик идет на обновление, у них возникает конфликт.
Итак, теперь разработчику нужно вывести первое изменение кода разработчика рядом с его проверкой, а затем зафиксировать, как только эти конфликты будут урегулированы.
Распределенный контроль версий Git — не единственная распределенная система контроля версий. Но это очень де-факто.
Некоторые из основных преимуществ Git:
Быстрый Гибкий Безопасный и надежный В отличие от модели управления версиями клиент-сервер, каждый разработчик загружает исходный репозиторий, то есть все. История коммитов, все ветки и т.д. и т.п.
Ресурсы What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day36/"},"https://romankurnovskii.com/ru/docs/python101/chapter37_pypi_packaging/":{title:"37. Как добавить пакет в PyPI",tags:[],content:`В предыдущей главе мы создали пакет под названием mymath. В этой главе мы узнаем, как разместить его на Python Packaging Index (PyPI). Для этого нам сначала нужно узнать, как создать файл setup.py Для справки, вот наша текущая иерархия папок:
mymath/ __init__.py adv/ __init__.py sqrt.py add.py subtract.py multiply.py divide.py Это означает, что у вас есть папка mymath со следующими файлами в ней: init.py, add.py, subtract.py, multiply.py и divide.py. Внутри папки mymath также есть папка adv. В папке adv у вас будет два файла: init.py и sqrt.py.
Создание файла setup.py Мы начнем с создания очень простого скрипта setup.py. Вот самый простой из них:
from distutils.core import setup setup(name='mymath', version='0.1', packages=['mymath', 'mymath.adv'], ) Это то, что можно написать для внутреннего пакета. Для загрузки в PyPI вам нужно будет указать немного больше информации:
from distutils.core import setup setup(name='mymath', version='0.1', description='A silly math package', author='Mike Driscoll', author_email='mike@mymath.org', url='http://www.mymath.org/', packages=['mymath', 'mymath.adv'], ) Теперь, когда мы закончили с этим, мы должны протестировать наш сценарий. Можно создать виртуальную среду, используя указания из главы 35, или просто установить ваш код в вашу установку Python, вызвав следующую команду:
python setup.py install В качестве альтернативы можно использовать метод, описанный в конце прошлой главы, в которой вы создали специальный setup.py, который установили в режиме разработки. Обратите внимание, что в прошлой главе мы использовали setuptools, а в этой - distutils. Причина в том, что в setuptools есть команда develop, а в distutils - нет.
Теперь нам нужно зарегистрировать наш пакет в PyPI.
Регистрация пакетов Зарегистрировать пакет очень просто. Поскольку это ваш первый пакет, вы захотите зарегистрироваться на тестовом сервере PyPI, а не на реальном. Вам может понадобиться создать файл .pypirc и ввести адрес сервера Test PyPI. Смотрите следующий раздел для получения дополнительной информации. Как только вы это сделаете, вам просто нужно будет выполнить следующую команду:
python setup.py register Вы получите список опций, в которых предлагается войти в систему, зарегистрироваться, попросить сервер выслать вам пароль или выйти из системы. Если имя пользователя и пароль сохранены на вашем компьютере, вы не увидите этого сообщения. Если вы уже зарегистрированы, можно войти в систему, и метаданные вашего пакета будут загружены.
Загрузка пакетов в PyPI Вы, вероятно, захотите начать с тестирования на тестовом сервере PyPI, который находится по адресу https://testpypi.python.org/pypi. Вам придется зарегистрироваться, так как он использует другую базу данных, в отличии от основного сайта. Как только вы это сделаете, вы, возможно, захотите создать файл .pypirc где-нибудь в пути вашей операционной системы. В Linux можно использовать $HOME, чтобы найти его, а в Windows - переменную окружения HOME. Этот путь является местом сохранения файла. Ниже приведен пример того, что может быть в файле pypirc с сайта https://wiki.python.org/moin/TestPyPI:
[distutils] index-servers= pypi test [test] repository = https://testpypi.python.org/pypi username = richard password = \u0026lt;your password goes here\u0026gt; [pypi] repository = http://pypi.python.org/pypi username = richard password = \u0026lt;your password goes here\u0026gt; Я настоятельно рекомендую вам подробно ознакомиться с документацией, чтобы понять все опции, которые можно добавить в этот конфигурационный файл.
Чтобы загрузить некоторые файлы в PyPI, вам нужно будет создать несколько дистрибутивов.
python setup.py sdist bdist_wininst upload Когда вы выполните приведенную выше команду, она создаст папку dist. Команда sdist создаст архивный файл (zip в Windows, tarball в Linux). Команда bdist_wininst создаст простой исполняемый файл установщика для Windows. Команда upload загрузит эти два файла на PyPI.
В файле setup.py можно добавить поле long_description, которое будет использоваться PyPI для создания домашней страницы вашего пакета на PyPI. Можно использовать reStructuredText для форматирования вашего описания. Или можно не добавлять описание и принять форматирование PyPI по умолчанию.
Если вам нужен полный список команд, которые можно использовать в setup.py, попробуйте выполнить следующую команду:
python setup.py --help-commands Вы также должны добавить файл README.txt, который объясняет, как установить и использовать ваш пакет.
Ссылки https://pylint.pycqa.org/en/latest `,url:"https://romankurnovskii.com/ru/docs/python101/chapter37_pypi_packaging/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day37/":{title:"37. Шпаргалка по Git",tags:["devops","git"],content:`Знакомство с Git В последних двух постах мы узнали о системах контроля версий и некоторых основных рабочих процессах git как системы контроля версий День 35. Затем мы установили git в нашу систему, обновили и настроили. Мы также немного углубились в теорию между системой контроля версий клиент-сервер и Git, которая является распределенной системой контроля версий День 36.
Теперь мы пройдемся по некоторым командам и вариантам использования, которые мы все обычно видим в git.
Где получить помощь по git? Будут времена, когда вы просто не сможете вспомнить или просто не знаете команду, которая вам нужна для работы с git. Вам понадобится помощь.
Само собой разумеется, что Google или любая другая поисковая система, вероятно, будет вашим первым портом захода при поиске помощи.
Во-вторых, следующим местом будет официальный сайт git и документация. git-scm.com/docs Здесь вы найдете не только подробные ссылки на все доступные команды, но и множество различных ресурсов.
Мы также можем получить доступ к этой же документации, которая очень полезна, если у вас нет подключения к терминалу. Например, если мы выбрали команду git add, мы можем запустить git add --help, и мы увидим ниже руководство.
Мы также можем в оболочке использовать git add -h, который даст нам краткий обзор доступных опций.
Мифы Git «У Git нет контроля доступа» — вы можете уполномочить \u0026ldquo;лидера\u0026rdquo; поддерживать исходный код.
«Git слишком тяжелый» — у Git есть возможность предоставлять неглубокие репозитории, что в основном означает меньший объем истории, если у вас большие проекты.
Недостатки Не идеально подходит для двоичных файлов. Отлично подходит для исходного кода, но не подходит, например, для исполняемых файлов или видео.
Git не удобен для пользователя, тот факт, что нам приходится тратить время на обсуждение команд и функций инструмента, вероятно, является ключевым признаком этого.
В целом, git сложно освоить, но легко использовать.
Экосистема git Я хочу кратко рассказать об экосистеме вокруг git, но не углубляться в некоторые из этих областей, но я думаю, что важно отметить их здесь на высоком уровне.
Почти все современные инструменты разработки поддерживают Git.
Инструменты разработчика. Мы уже упоминали код Visual Studio, но вы найдете плагины git и интеграции в возвышенный текст и другие текстовые редакторы и IDE.
Командные инструменты. Также упоминаются такие инструменты, как Jenkins с точки зрения CI/CD, Slack из среды обмена сообщениями и Jira для управления проектами и отслеживания проблем.
Облачные провайдеры. Все крупные облачные провайдеры поддерживают git, Microsoft Azure, Amazon AWS, Google Cloud Platform.
Сервисы на основе Git. Затем у нас есть GitHub, GitLab и BitBucket, о которых мы поговорим более подробно позже. Я слышал об этих сервисах как о социальной сети для кода!
Шпаргалка по Git Мы не рассмотрели большинство этих команд, но просмотрев некоторые шпаргалки, доступные в Интернете, я хотел задокументировать некоторые из команд git и их назначение. Нам не нужно запоминать все это, и с большей практикой и использованием вы выберете, по крайней мере, основы git.
Основы Git Command Example Description git init git init \u0026lt;directory\u0026gt; создает пустой репозиторий git в указанном каталоге. git clone git clone \u0026lt;repo\u0026gt; клонирует репозиторий, расположенный в , на локальный компьютер. git config git config user.name определяет имя автора, которое будет использоваться для всех коммитов в текущем репозитории, system, global, local флаг для установки параметров конфигурации. git add git add \u0026lt;directory\u0026gt; он подготовит все изменения в для следующего коммита. Мы также можем добавить и \u0026lt;.\u0026gt; для добавления всех изменененных файлов всего. git commit -m git commit -m \u0026quot;\u0026lt;message\u0026gt;\u0026quot; фиксирует промежуточный коммит, запишет , чтобы подробно описать, что точно сохраняем. git status git status выведит список файлов, которые помещены в архив, не помещены в архив и не отслеживаются. git log git log Отображение всей истории коммитов в формате по умолчанию. У этой команды есть дополнительные параметры. git diff git diff Показать неустановленные изменения между вашим индексом и рабочим каталогом. Git Отмена изменений Command Example Description git revert git revert \u0026lt;commit\u0026gt; создает новую фиксацию, которая отменяет все изменения, сделанные в , а затем примените ее к текущей ветке. git reset git reset \u0026lt;file\u0026gt; убрать из индекса коммита (изменения не теряются). git clean git clean -n увидеть, какие файлы являются лишними, перед их непосредственным удалением git clean git clean -f удалить неотслеживаемые файлы и папки из рабочей копии git clean git clean -fd удалить их Git переписать историю Command Example Description git commit git commit --amend Заменяет последний коммит поэтапными изменениями и последним коммитом. Используйте без статуса stage, чтобы отредактировать сообщение последнего коммита. git rebase git rebase \u0026lt;base\u0026gt; Перебазировать текущую ветку на . может быть идентификатором фиксации, именем ветки, тегом или относительной ссылкой на HEAD. git reflog git reflog Показать журнал изменений в HEAD локального репозитория. Добавьте флаг \u0026ndash;relative-date для отображения информации о дате или \u0026ndash;all для отображения всех ссылок. Git Branches Command Example Description git branch git branch Перечислите все ветки в вашем репо. Добавьте аргумент , чтобы создать новую ветку с именем . git checkout git checkout -b \u0026lt;branch\u0026gt; Создайте и извлеките новую ветку с именем . Отбросьте флаг -b, чтобы проверить существующую ветку. git merge git merge \u0026lt;branch\u0026gt; Объединить ветку с текущей веткой. Git Remote Repositories Command Example Description git remote add git remote add \u0026lt;name\u0026gt; \u0026lt;url\u0026gt; Создайте новое подключение к удаленному репозиторию. После добавления пульта вы можете использовать в качестве ярлыка для в других командах. git fetch git fetch \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; Выбирает конкретную \u0026lt;ветку\u0026gt; из репозитория. Оставьте , чтобы получить все удаленные ссылки. git pull git pull \u0026lt;remote\u0026gt; Получить указанную удаленную копию текущей ветки и немедленно объединить ее с локальной копией. git push git push \u0026lt;remote\u0026gt; \u0026lt;branch\u0026gt; Отправьте ветку на вместе с необходимыми коммитами и объектами. Создает именованную ветку в удаленном репо, если она не существует. Git Diff Command Example Description git diff HEAD git diff HEAD Показать разницу между рабочим каталогом и последним коммитом. git diff \u0026ndash;cached git diff --cached Показать разницу между поэтапными изменениями и последней фиксацией Git Config Command Example Description git config \u0026ndash;global user.name \u0026lt;имя\u0026gt; git config --global user.name \u0026lt;имя\u0026gt; Определите имя автора, которое будет использоваться для всех коммитов текущим пользователем. git config \u0026ndash;global user.email git config --global user.email \u0026lt;email\u0026gt; Определите адрес электронной почты автора, который будет использоваться для всех коммитов текущего пользователя. git config \u0026ndash;global alias \u0026lt;алиас-имя\u0026gt; git config --global alias \u0026lt;alias-name\u0026gt; \u0026lt;git-command\u0026gt; Создать ярлык для команды git. git config \u0026ndash;system core.editor \u0026lt;редактор\u0026gt; git config --system core.editor \u0026lt;редактор\u0026gt; Установите текстовый редактор, который будет использоваться командами для всех пользователей на машине. Аргумент должен быть командой, запускающей нужный редактор. git config \u0026ndash;global \u0026ndash;edit git config --global --edit Откройте файл глобальной конфигурации в текстовом редакторе для редактирования вручную. Git Rebase Command Example Description git rebase -i git rebase -i \u0026lt;base\u0026gt; Интерактивно перебазировать текущую ветку на . Запускает редактор для ввода команд того, как каждый коммит будет перенесен в новую базу. Git Pull Command Example Description git pull \u0026ndash;rebase git pull --rebase \u0026lt;remote\u0026gt; Получить удаленную копию текущей ветки и перебазировать ее в локальную копию. Использует git rebase вместо слияния для интеграции веток. Git Reset Command Example Description git reset git reset Сбросьте промежуточную область, чтобы она соответствовала самой последней фиксации, но оставьте рабочий каталог без изменений. git reset \u0026ndash;hard git reset --hard Сбросить промежуточную область и рабочий каталог, чтобы они соответствовали самой последней фиксации, и перезаписать все изменения в рабочем каталоге git reset git reset \u0026lt;commit\u0026gt; Переместите конец текущей ветки назад к , сбросьте промежуточную область, чтобы она соответствовала, но оставьте рабочий каталог в покое git reset \u0026ndash;hard git reset --hard \u0026lt;commit\u0026gt; То же, что и предыдущее, но сбрасывает и промежуточную область, и рабочий каталог, чтобы они совпадали. Удаляет незафиксированные изменения и все фиксации после . Git Push Command Example Description git push \u0026ndash;force git push \u0026lt;remote\u0026gt; --force Делает git push, даже если это приводит к слиянию без быстрой перемотки вперед. Не используйте флаг \u0026ndash;force, если вы абсолютно не уверены, что знаете, что делаете. git push \u0026ndash;all git push \u0026lt;remote\u0026gt; --all Переместите все свои локальные ветки на указанный удаленный сервер. git push \u0026ndash;tags git push \u0026lt;remote\u0026gt; --tags Теги не добавляются автоматически при отправке ветки или использовании флага \u0026ndash;all. Флаг \u0026ndash;tags отправляет все ваши локальные теги в удаленное репо. Ресурсы What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial Git cheatsheet `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day37/"},"https://romankurnovskii.com/ru/docs/python101/chapter38_eggs/":{title:"38. Python egg",tags:[],content:`Python egg - это старый формат распространения Python. Новый формат называется Python wheel, который мы рассмотрим в следующей главе. Файл egg - это, по сути, zip-файл с другим расширением. Python может импортироваться непосредственно из egg. Для работы с eggs вам понадобится пакет SetupTools. SetupTools - это оригинальный основной метод загрузки и установки пакетов Python из PyPI и других источников через командную строку, что-то вроде apt-get для Python. Существовал форк SetupTools под названием distribute, который в итоге был объединен обратно в SetupTools. Я упоминаю об этом только потому, что вы можете встретить ссылки на этот форк, если будете много читать о Python eggs вне этой книги.
Несмотря на то, что от формата eggs сейчас отказываются, вам все еще нужно знать о нем, поскольку существует множество пакетов, распространяемых с помощью этой технологии. Возможно, пройдут годы, прежде чем все перестанут использовать eggs. Давайте научимся создавать свои собственные!
Создание egg
Вы можете думать об egg как об альтернативе исходному дистрибутиву или исполняемому файлу Windows, но следует отметить, что для eggs чистого Python файл egg является полностью кроссплатформенным. Мы рассмотрим, как создать собственное egg, используя пакет, который мы создали в предыдущей главе о модулях и пакетах. Чтобы приступить к созданию egg, вам нужно создать новую папку и поместить в нее папку mymath. Затем создайте файл setup.py в родительском каталоге папки mymath со следующим содержимым:
from setuptools import setup, find_packages setup( name = \u0026quot;mymath\u0026quot;, version = \u0026quot;0.1\u0026quot;, packages = find_packages() ) В Python есть собственный пакет для создания дистрибутивов, который называется distutils. Однако вместо использования функции установки distutils в Python, мы используем функцию установки setuptools. Мы также используем функцию find_packages от setuptools, которая будет автоматически искать любые пакеты в текущем каталоге и добавлять их в egg. Чтобы создать egg, вам нужно выполнить следующее из командной строки:
c:\\Python34\\python.exe setup.py bdist_egg Это приведет к появлению большого количества выходных данных, но когда все будет готово, вы увидите, что у вас есть три новые папки: build, dist и mymath.egg-info. Нам важна только папка dist, в которой вы найдете файл egg, mymath-0.1-py3.4.egg. Обратите внимание, что на моей машине я заставил программу запускаться на Python 3.4, чтобы она создала egg на этой версии Python. Сам файл egg - это, по сути, zip-файл. Если вы измените расширение на \u0026ldquo;zip\u0026rdquo;, вы сможете заглянуть внутрь и увидеть, что в нем есть две папки: mymath и EGG-INFO. Теперь вы можете указать easy_install на egg в вашей файловой системе, и он установит ваш пакет.
Подведение итогов Теперь настала ваша очередь. Зайдите в индекс пакетов Python и найдите несколько модулей чистого Python для загрузки. Затем попробуйте создать eggs, используя приемы, которые вы изучили в этой главе. Если вы хотите установить egg, вы можете использовать easy_install. Удалить egg немного сложнее. Вам придется перейти к месту его установки и удалить папку и/или файл egg, которые оно установило, а также удалить запись о пакете из файла easy-install.pth. Все эти элементы можно найти в папке site-packages вашего Python.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter38_eggs/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day38/":{title:"38. Staging и Изменения",tags:["devops"],content:`Working directory Git - это система трёх основных стадий: working directory, staging area и repository. Пройдем поэтапно каждую стадию.
Создадим пустую папку.
mkdir my_fodler cd my_folder Сделаем инициализацию git проекта.
git init После инициализации git репозитория создается скрытая папка .git Здесь хранятся сведения о репозитории git, а также информация о наших ветках и коммитах.
Staging/Stage Сейчас у нас пустая папка. Создадим пустой файл README.md и выполним команду
git status Git знает о новом файле, но этот файл еще не зафиксирован в staging. Текущее расположение файла - Working directory, директория, где проиниализирован .git проект.
staging - это хранилище для файлов с изменениями, информация о которых попадет в единый коммит
Чтобы файл перешел в staging, необходимо его добавить. Для этого выполним команду
git add README.md После добавления файла в staging area, цвет поменялся на зеленый
Можно добавить все измененные файлы с помощью команды
git add . Знак . означает, что мы хотим добавить все обновленные файлы и папки.
Далее необходимо зафиксировать изменения в репозитории. Для этого выполним команду
git commit -m \u0026quot;Add README.md (или другой значимый комментарий)\u0026quot; Коммит изменений В процессе работы мы добавляем много различных файлов. Если мы захотим добавить более длинный и осмысленный коммит, то можно запусть команду без комментария
git commit Откроется стандартный редактор текста. Записываем комментарий и сохраняем. Проверим результат
git status Требования к именам коммитов У каждой компании/проекта есть свои требования к именам коммитов. В компании может быть несколько проектов, каждый из которых должен иметь свои требования к именам коммитов. В проекте может быть несколько веток, каждая из которых должна иметь свои требования к именам коммитов.
Существует гайдлайн, на который можно ориентироваться. Такой подход точно будет понятен для всех новых проектов. Некоторые проекты, соблюдабщие данную конвенцию: angular, electron
Коммит:
Должен использоваться present tense (\u0026ldquo;add feature\u0026rdquo; not \u0026ldquo;added feature\u0026rdquo;) Должен использоваться imperative mood (\u0026ldquo;move cursor to\u0026hellip;\u0026rdquo; not \u0026ldquo;moves cursor to\u0026hellip;\u0026rdquo;) Примеры имен коммитов init: - используется для начала проекта/таска. Примеры:
init: start youtube-task init: start mentor-dashboard task feat: - это реализованная новая функциональность из технического задания (добавил поддержку зумирования, добавил footer, добавил карточку продукта). Примеры:
feat: add basic page layout feat: implement search box feat: implement request to youtube API feat: implement swipe for horizontal list feat: add additional navigation button feat: add banner feat: add social links feat: add physical security section feat: add real social icons fix: - исправил ошибку в ранее реализованной функциональности. Примеры:
fix: implement correct loading data from youtube fix: change layout for video items to fix bugs fix: relayout header for firefox fix: adjust social links for mobile refactor: - новой функциональности не добавлял / поведения не менял. Файлы в другие места положил, удалил, добавил. Изменил форматирование кода (white-space, formatting, missing semi-colons, etc). Улучшил алгоритм, без изменения функциональности. Примеры:
refactor: изменение структуры проекта refactor: переименование переменных для лучшей читабельности refactor: применить eslint refactor: применить prettier docs: - используется при работе с документацией/readme проекта. Примеры:
docs: обновить readme с дополнительной информацией docs: обновить описание метода run() Пропуск Staging Area Можно сразу добавить коммит, добавим параметр -a в git commit:
Удаление файлов Фиксация удаления как и добавления файлов происхоит через комит
Создадим файл -\u0026gt; Добавим в stage -\u0026gt; Удалим файл
touch old_file.txt git add old_file.txt git commit -m \u0026quot;add old_file to be removed\u0026quot; Удаляем файл
git rm old_file.txt git status Переименование/Перемещение файлов Мы можем переименовывать или перемещать файлы в проекте средствами операционной системы. Таке это можно делать командами git.
Пример:
git mv old_file.txt new_file.txt Пропуск/игнорирование файлов В Git это можно сделать рзличными способами:
Игнорировать изменения в неотслеченных файлах с помощью .gitignore файла Игнорировать изменения в неотслеченных файлах с помощью exclude файла Остановка отслеживания файла и пропуск изменений с помощью git update-index Остановка отслеживания файла и пропуск изменений с помощью git rm .gitignore Достаточно в файл .gitignore добавить путь до файлов или папок, которые необходимо игнорировать
После обновления файл переходит в категорию Untracked files
Если файлы уже добавлены в stage, но нужно убрать файл, то можно использовать команду git rm --cached
Status сокращенно git status -s Ресурсы What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day38/"},"https://romankurnovskii.com/ru/docs/python101/chapter39_wheels/":{title:"39. Python wheels",tags:[],content:`Первым распространенным форматом упаковки Python был файл .egg. Теперь в городе появился новый формат под названием wheel (.whl). Согласно описанию в Python Packaging Index, wheel предназначен для содержания всех файлов для установки, совместимой с PEP 376, в формате, очень близком к формату на диске. В этой главе мы узнаем, как создать wheel и затем установить его в virtualenv.
Начало работы Рекомендуемый способ работы с wheels - использование pip. Убедитесь, что вы установили последнюю версию pip, так как более ранние версии не поддерживают формат wheel. Если вы не уверены, что у вас установлена последняя версия pip, вы можете выполнить следующую команду:
pip install --upgrade pip Если у вас нет последней версии, то эта команда обновит pip. Теперь мы готовы к созданию wheel!
Создание wheel Прежде всего, вам нужно установить пакет wheel:
pip install wheel Это было легко! Далее мы будем использовать пакет unidecode для создания нашего первого wheel, так как на момент написания статьи он еще не был создан, и я сам использовал этот пакет в нескольких проектах. Пакет unidecode берет строку текста и пытается заменить любой юникод на его эквивалент ASCII. Это очень удобно, когда нужно очистить данные, предоставленные пользователем, от странных аномалий. Вот команда, которую нужно выполнить, чтобы создать wheel для этого пакета:
pip wheel --wheel-dir=my_wheels Unidecode Вот скриншот вывода, который я получил при запуске этой программы:
Теперь у вас должно быть wheel с именем Unidecode-0.04.14-py26-none-any.whl в папке my_wheels. Давайте узнаем, как установить наше новое wheel!
Установка wheel Python Давайте создадим virtualenv для тестирования. Для создания виртуальной среды тестирования мы используем следующую команду:
virtualenv test Это предполагает, что virtualenv находится в вашем системном пути. Если вы получите ошибку unrecognized command, то вам, вероятно, придется указать полный путь (например, c:\\Python34\\Scripts\\virtualenv). Выполнение этой команды создаст для нас виртуальную песочницу, в которой будет работать pip. Обязательно запустите скрипт activate из папки Scripts папки test, чтобы включить virtuanenv, прежде чем продолжить. Ваш virtualenv не включает wheel, поэтому вам придется установить wheel снова:
pip install wheel После этого мы можем установить наше wheel с помощью следующей команды:
pip install --use-wheel --no-index --find-links=path/to/my_wheels Unidecode Чтобы проверить, что это сработало, запустите Python из папки Scripts в вашем виртуальном окружении и попробуйте импортировать unidecode. Если он импортируется, значит, вы успешно установили wheel!
Файл *.whl похож на *.egg тем, что это, по сути, замаскированный файл *.zip. Если вы переименуете расширение с *.whl на *.zip, вы сможете открыть его с помощью выбранного вами приложения zip и изучить файлы и папки внутри по своему усмотрению.
Подведение итогов Теперь вы должны быть готовы к созданию собственных wheels. Это хороший способ создать локальный репозиторий зависимостей для вашего проекта (проектов), который можно быстро установить. Вы можете создать несколько различных репозиториев wheel, чтобы легко переключаться между различными наборами версий для тестирования. В сочетании с virtualenv вы получаете действительно простой способ увидеть, как новые версии зависимостей могут повлиять на ваш проект, без необходимости загружать их несколько раз.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter39_wheels/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day39/":{title:"39. Просмотр, удаление, отмена и восстановление",tags:["devops"],content:`GIT - Просмотр, удаление, отмена и восстановление Просмотр файлов в Stagig area и Working area Если некоторые файлы/папки уже добавлены в staging area, то можно просмотреть их рахницу по отношению в главной ветке комадой: git diff --staged
Это покажет нам все внесенные изменения и все новые файлы, которые мы добавили или удалили.
Изменения в измененных файлах обозначаются символами --- или +++ Вы можете видеть ниже, что мы только что добавили +add some text, что означает, что это новые строки.
Мы также можем запустить git diff, чтобы сравнить наш staging area с нашим рабочим каталогом. Если мы внесем некоторые изменения в наш только что добавленный файл code.txt и добавим несколько строк текста.
Инструменты для визуального отображения Вот несколько инструментов для визуального сравнения коммитов и веток:
KDiff3 P4Merge WinMerge (только для Windows) VSCode Если запустим git difftool то запустится визуальный инструмент сравнения по умолчанию. Проверить текущие настройки git config --global -e Чтобы установить инструмент в git, выполним следующую команду git config --global diff.tool vscode. Теперь при запуске git difftool откроется vscode После этого открывается редактор VScode на странице diff и сравнивает их, мы изменили только один файл, добавив строку кода с правой стороны. Можем использовать git difftool --staged для сравнения файлов в staging area с \u0026ldquo;прокомиченными\u0026rdquo; файлами. VScode, как и большинство IDE, имеют встроенную функциональность, поэтому очень редко вам понадобится запускать эти команды из терминала, хотя это полезно, если у вас по какой-то причине не установлена IDE.
Просмотр истории изменений Просмотреть историю изменений в Git можно командой git log
Каждый коммит имеет свою шестнадцатеричную строку, уникальную для репозитория. Здесь вы можете увидеть, над какой веткой мы работаем, а также автора, дату и комментарий коммита.
У нас также есть git log --oneline, и это даёт нам гораздо меньшую версию шестнадцатеричной строки, которую мы можем использовать в других командах diff.
Чтобы просмотреть коммиты с самого первого, а не послденего, как по умолчанию, запустим git log --oneline --reverse, и теперь мы видим наш первый коммит в верхней части страницы.
Просмотр коммита Можно просмотреть данные ко конкретном коммите более детально: git show или git show \u0026lt;commit ID\u0026gt;
Мы также можем использовать git show HEAD~1, где 1 - это количество шагов назад от текущей версии, к которой мы хотим вернуться.
Это отличный вариант, если вам нужна подробная информация о файлах, но если мы хотим получить список всех файлов в дереве для всего каталога снимков. Мы можем добиться этого, используя команду git ls-tree HEAD~1, снова вернувшись на один снимок назад от последнего коммита. Ниже мы видим два пятна, которые обозначают файлы, в то время как дерево обозначает каталог. В этой информации вы также можете увидеть коммиты и теги.
Проверим коммит
Unstaging Бывают случаи, когда вы, возможно, использовали git add ., но на самом деле есть файлы, которые вы пока не хотите фиксировать в этом снапшоте. В этом примере ниже я добавил newfile.txt в область staging, но я не готов зафиксировать этот файл, поэтому я собираюсь использовать git restore --staged newfile.txt, чтобы отменить шаг git add.
Мы также можем сделать то же самое с изменёнными файлами, такими как main.js, и снять фиксацию, см. выше у нас есть greem M для modified, а ниже мы снимаем фиксацию этих изменений.
Я нашел эту команду весьма полезной во время 90DaysOfDevOps, поскольку иногда я работаю заранее, когда чувствую, что хочу сделать заметки для следующего дня, но не хочу фиксировать и выкладывать в публичный репозиторий GitHub.
Отмена локальных изменений Иногда мы можем вносить изменения, но эти изменения нас не устраивают, и мы хотим их отбросить. Мы снова воспользуемся командой git restore и сможем восстановить файлы из наших снимков или предыдущих версий. Мы можем запустить команду git restore . для нашего каталога, и мы восстановим все из нашего снимка, но обратите внимание, что наш неотслеживаемый файл все еще присутствует. Нет предыдущего отслеживаемого файла под названием newfile.txt.
Теперь, чтобы удалить newfile.txt или любой другой неотслеживаемый файл. Мы можем использовать git clean, но получим только предупреждение.
Или, если мы знаем о последствиях, мы можем запустить git clean -fd, чтобы принудительно удалить все каталоги.
Восстановление файла до более ранней версии Как мы уже упоминали, большая часть того, чем может помочь Git, - это возможность восстановления копий файлов из снимков (это не резервное копирование, но это очень быстрая точка восстановления). Я советую вам также сохранять копии вашего кода в других местах, используя для этого решение для резервного копирования.
В качестве примера давайте удалим наш самый важный файл в каталоге, обратите внимание, что мы используем команды на базе unix для удаления этого файла из каталога, а не команды git.
Теперь у нас нет readme.mdin в нашей рабочей директории. Мы могли бы использовать git rm readme.md и тогда это было бы отражено в нашей базе данных git. Давайте также удалим его отсюда, чтобы имитировать его полное удаление.
Теперь зафиксируем это с сообщением и докажем, что у нас больше нет ничего в рабочем каталоге или в области постановки.
Была допущена ошибка, и теперь нам нужно вернуть этот файл!
Мы можем использовать команду git undo, которая отменит последний коммит, но что если это было давно? Мы можем использовать команду git log, чтобы найти наши коммиты, и тогда мы обнаружим, что наш файл находится в последнем коммите, но мы не хотим, чтобы все эти коммиты были отменены, поэтому мы можем использовать эту команду git restore --source=HEAD~1 README.md, чтобы найти файл и восстановить его из нашего снимка.
Вы можете видеть, что с помощью этого процесса мы вернули файл в наш рабочий каталог.
Теперь у нас есть новый неотслеживаемый файл, и мы можем использовать наши команды, упомянутые ранее, для отслеживания, этапа и фиксации наших файлов и изменений.
Rebase / Merge Это, кажется, самая большая головная боль, когда речь заходит о Git и о том, когда использовать rebase, а когда использовать merge в ваших git-репозиториях.
Прежде всего, нужно знать, что и git rebase, и git merge решают одну и ту же задачу. Оба они интегрируют изменения из одной ветки в другую. Однако они делают это по-разному.
Давайте начнем с новой функции в новой выделенной ветке. Основная ветку продолжает работу с новыми коммитами.
Простой вариант здесь - использовать git merge feature main, который объединит основную ветку с веткую feature.
Слияние простое, потому что оно неразрушающее. Существующие ветви никак не изменяются. Однако это также означает, что функциональная ветку будет иметь неактуальный коммит слияния каждый раз, когда вам нужно будет включить изменения, внесённые выше по течению. Если main очень занят или активен, это может привести к загрязнению истории функциональной ветви.
В качестве альтернативного варианта мы можем перебазировать функциональную ветку на основную ветку с помощью команды
git checkout feature git rebase main Это перемещает ветку feature (всю ветку feature), эффективно включая все новые коммиты в main. Но вместо использования коммита слияния, rebasing переписывает историю проекта, создавая совершенно новые коммиты для каждого коммита в исходной ветке.
Самым большим преимуществом ребасинга является гораздо более чистая история проекта. Это также устраняет ненужные коммиты слияния. и если сравнить последние два изображения, то можно увидеть, что история проекта намного чище.
Хотя это еще не окончательный вывод, потому что выбор более чистой истории также связан с компромиссами. Если вы не будете следовать The Golden rule of rebasing, переписывание истории проекта может стать потенциально катастрофой для вашего рабочего процесса совместной работы. И, что менее важно, при пересборке теряется контекст, предоставляемый коммитом слияния - вы не можете увидеть, когда изменения, внесенные выше по течению, были включены в функцию.
Ссылки What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial Git cheatsheet Exploring the Git command line – A getting started guide `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day39/"},"https://romankurnovskii.com/ru/docs/python101/chapter40_py2exe/":{title:"40. py2exe",tags:[],content:`Проект py2exe раньше был основным способом создания исполняемых файлов Windows из ваших приложений Python. На PyPI лежит версия, которая будет работать и с Python 2 и 3.
У вас есть несколько вариантов для приложения. Вы можете создать программу, которая будет работать только в терминале, вы можете создать графический интерфейс пользователя (GUI) для рабочего стола или создать веб-приложение. Мы создадим очень простой настольный интерфейс, который ничего не делает, кроме отображения формы, которую пользователь может заполнить. Мы будем использовать инструментарий wxPython GUI, чтобы продемонстрировать, как py2exe может подбирать пакеты без нашего указания.
Создание простого графического интерфейса Вам нужно перейти на сайт wxPython (www.wxpython.org) и загрузить копию, соответствующую вашей версии Python. Если у вас 32-битный Python, убедитесь, что вы скачали 32-битный wxPython. Вы не сможете использовать easy_install или pip для установки wxPython, если только вы не получите самую современную версию wxPython от Phoenix, поэтому вам придется взять копию, предварительно собранную для вашей системы, либо с сайта wxPython, либо из менеджера пакетов вашей системы. Я рекомендую использовать по крайней мере wxPython 2.9 или выше.
Давайте напишем немного кода!
import wx class DemoPanel(wx.Panel): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Panel.__init__(self, parent) labels = [\u0026quot;Name\u0026quot;, \u0026quot;Address\u0026quot;, \u0026quot;City\u0026quot;, \u0026quot;State\u0026quot;, \u0026quot;Zip\u0026quot;, \u0026quot;Phone\u0026quot;, \u0026quot;Email\u0026quot;, \u0026quot;Notes\u0026quot;] mainSizer = wx.BoxSizer(wx.VERTICAL) lbl = wx.StaticText(self, label=\u0026quot;Please enter your information here:\u0026quot;) lbl.SetFont(wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD)) mainSizer.Add(lbl, 0, wx.ALL, 5) for lbl in labels: sizer = self.buildControls(lbl) mainSizer.Add(sizer, 1, wx.EXPAND) self.SetSizer(mainSizer) mainSizer.Layout() def buildControls(self, label): \u0026quot;\u0026quot;\u0026quot; Put the widgets together \u0026quot;\u0026quot;\u0026quot; sizer = wx.BoxSizer(wx.HORIZONTAL) size = (80,40) font = wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD) lbl = wx.StaticText(self, label=label, size=size) lbl.SetFont(font) sizer.Add(lbl, 0, wx.ALL|wx.CENTER, 5) if label != \u0026quot;Notes\u0026quot;: txt = wx.TextCtrl(self, name=label) else: txt = wx.TextCtrl(self, style=wx.TE_MULTILINE, name=label) sizer.Add(txt, 1, wx.ALL, 5) return sizer class DemoFrame(wx.Frame): \u0026quot;\u0026quot;\u0026quot; Frame that holds all other widgets \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Frame.__init__(self, None, wx.ID_ANY, \u0026quot;Py2Exe Tutorial\u0026quot;, size=(600,400) ) panel = DemoPanel(self) self.Show() if __name__ == \u0026quot;__main__\u0026quot;: app = wx.App(False) frame = DemoFrame() app.MainLoop() Если вы выполните приведенный выше код, вы должны увидеть что-то вроде следующего:
Давайте немного разложим это по полочкам. Мы создаем два класса, DemoPanel и DemoFrame. В wxPython объект wx.Frame используется для создания реального \u0026ldquo;окна\u0026rdquo;, которое вы видите в большинстве случаев. Вы добавляете wx.Panel, чтобы придать вашему приложению соответствующий вид и ощущение, а также добавить табуляцию между полями. Родителем объекта панели является фрейм. Фрейм, будучи виджетом верхнего уровня, не имеет родителя. Панель содержит все остальные виджеты в этом примере. Для компоновки виджетов мы используем сайзеры. Сайзеры позволяют разработчику создавать виджеты, размер которых будет изменяться соответствующим образом при изменении размера самого окна. Вы также можете разместить виджеты на панели с помощью абсолютного позиционирования, что не рекомендуется. В конце мы вызываем метод MainLoop объекта wx.App, чтобы запустить цикл событий, который позволяет wxPython реагировать на события мыши и клавиатуры (такие как щелчок, ввод текста и т.д.).
Теперь мы готовы узнать, как упаковать это приложение в исполняемый файл!
Файл py2exe setup.py Ключевым элементом любого скрипта py2exe является файл setup.py. Этот файл определяет, что будет включено или исключено, как сильно мы будем сжимать и упаковывать, и многое другое! Вот простейшая установка, которую мы можем использовать с приведенным выше скриптом wx:
from distutils.core import setup import py2exe setup(windows=['sampleApp.py']) Как вы видите, мы импортируем метод setup из distutils.core, а затем импортируем py2exe. Далее мы вызываем setup с параметром ключевого слова windows и передаем ему имя главного файла внутри объекта python list. Если бы вы создавали проект без графического интерфейса, то вместо windows вы бы использовали клавишу console. Чтобы запустить этот фрагмент, сохраните его в той же папке, что и ваш скрипт wxPython, откройте командную строку и перейдите в то место, где вы сохранили эти два файла. Затем введите python setup.py py2exe, чтобы запустить его. Если все идет хорошо, вы увидите много вывода, заканчивающегося примерно так:
Если вы используете Python 2.6, вы можете получить ошибку MSVCP90.dll не найден. Если вы увидите эту ошибку, вам, вероятно, придется найти Microsoft Visual C++ 2008 Redistributable Package и установить его, чтобы DLL стала доступна в вашей системе. Иногда бывает так, что вы создаете исполняемый файл, а затем, когда вы его запускаете, он просто не загружается правильно. Обычно при этом создается файл журнала, который можно использовать для выяснения причины. Я также нашел инструмент под названием Dependency Walker, который можно запустить против вашего исполняемого файла, и он может рассказать вам о недостающих элементах, не относящихся к Python (например, DLL и т.д.).
Я хотел бы отметить, что файл setup.py не включает wxPython в явном виде. Это означает, что py2exe был достаточно умен, чтобы включить пакет wxPython автоматически. Давайте потратим немного времени, чтобы узнать немного больше о включении и исключении пакетов.
Создание расширенного файла setup.py Давайте посмотрим, какие еще возможности дает нам py2exe для создания двоичных файлов, создав более сложный файл setup.py.
from distutils.core import setup import py2exe includes = [] excludes = ['_gtkagg', '_tkagg', 'bsddb', 'curses', 'email', 'pywin.debugger', 'pywin.debugger.dbgcon', 'pywin.dialogs', 'tcl', 'Tkconstants', 'Tkinter'] packages = [] dll_excludes = ['libgdk-win32-2.0-0.dll', 'libgobject-2.0-0.dll', 'tcl84.dll', 'tk84.dll'] setup( options = {\u0026quot;py2exe\u0026quot;: {\u0026quot;compressed\u0026quot;: 2, \u0026quot;optimize\u0026quot;: 2, \u0026quot;includes\u0026quot;: includes, \u0026quot;excludes\u0026quot;: excludes, \u0026quot;packages\u0026quot;: packages, \u0026quot;dll_excludes\u0026quot;: dll_excludes, \u0026quot;bundle_files\u0026quot;: 3, \u0026quot;dist_dir\u0026quot;: \u0026quot;dist\u0026quot;, \u0026quot;xref\u0026quot;: False, \u0026quot;skip_archive\u0026quot;: False, \u0026quot;ascii\u0026quot;: False, \u0026quot;custom_boot_script\u0026quot;: '', } }, windows=['sampleApp.py'] ) Это довольно понятно, но все же давайте разберемся. Сначала мы создадим несколько списков, которые мы передадим в параметр options функции setup.
Список includes предназначен для специальных модулей, которые вам нужно включить. Иногда py2exe не может найти определенные модули, поэтому их нужно указать вручную. Список excludes - это список модулей, которые нужно исключить из вашей программы. В данном случае нам не нужен Tkinter, так как мы используем wxPython. Этот список исключений GUI2Exe будет исключать по умолчанию. Список packages - это список конкретных пакетов для включения. Опять же, иногда py2exe просто не может что-то найти. Мне уже приходилось включать сюда email, PyCrypto или lxml. Обратите внимание, что если список excludes содержит что-то, что вы пытаетесь включить в списки packages или includes, py2exe может продолжать исключать это. dll_excludes - исключает dll, которые не нужны в нашем проекте. В словаре options у нас есть еще несколько опций, на которые стоит обратить внимание. Ключ compressed указывает py2exe, сжимать или нет zip-файл, если он установлен. Ключ optimize задает уровень оптимизации. Ноль - это отсутствие оптимизации, а 2 - самый высокий уровень. Установив optimize на 2, мы можем уменьшить размер папки примерно на один мегабайт. Ключ bundle_files связывает dlls в zip-файл или exe. Допустимыми значениями для bundle_files являются:
1 = упаковывать все, включая интерпретатор Python. 2 = упаковывать все, кроме интерпретатора Python. 3 = не упаковывать (по умолчанию). Несколько лет назад, когда я только начинал изучать py2exe, я спросил в их списке рассылки, какой вариант лучше, потому что у меня были проблемы с вариантом bundle 1. Мне ответили, что вариант 3, вероятно, самый стабильный. Я перешел на него и перестал испытывать случайные проблемы, поэтому сейчас я рекомендую именно его. Если вам не нравится распространять более одного файла, заархивируйте их или создайте программу установки. Единственный вариант, который я использую в этом списке, это dist_dir. Я использую его для экспериментов с различными вариантами сборки или для создания пользовательских сборок, когда я не хочу перезаписывать свою основную хорошую сборку. Обо всех остальных опциях вы можете прочитать на сайте py2exe.
Пакет py2exe не поддерживает включение eggs Python в свои двоичные файлы, поэтому если вы установили пакет, от которого зависит ваше приложение, как egg, то при создании исполняемого файла он не будет работать. Вам придется убедиться, что ваши зависимости установлены нормально.
Существует несколько альтернатив py2exe, таких как bbfreeze, cx_freeze и PyInstaller. Создание исполняемых файлов может быть нелегким делом, но наберитесь терпения и настойчиво пройдите через это.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter40_py2exe/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day40/":{title:"40. GitHub | GitLab | BitBucket",tags:["devops"],content:`Социальная сеть для кода Изучение GitHub | GitLab | BitBucket
Сегодня я хочу рассказать о некоторых сервисах на основе git, о которых мы, вероятно, все слышали и ожидаем, что будем использовать их ежедневно.
GitHub Наиболее распространенным, по крайней мере для меня, является GitHub, GitHub — это веб-хостинг для git. Чаще всего он используется разработчиками программного обеспечения для хранения своего кода. Управление исходным кодом с функциями контроля версий git, а также множеством дополнительных функций. Это позволяет командам или открытым участникам легко общаться и обеспечивает социальный аспект кодирования. (отсюда и название социальной сети) С 2018 года GitHub является частью Microsoft.
GitHub существует уже довольно давно и был основан в 2007-2008 годах. Сегодня на платформе более 40 миллионов пользователей.
Основные возможности GitHub
Code Repository Pull Requests Project Management toolset - Issues CI / CD Pipeline - GitHub Actions С точки зрения ценообразования GitHub предлагает различные уровни ценообразования для своих пользователей. Дополнительную информацию можно найти на странице Цены.
Для этого мы рассмотрим бесплатный уровень.
Я собираюсь использовать свою уже созданную учетную запись GitHub во время этого пошагового руководства, если у вас нет учетной записи, то на открывающейся странице GitHub есть вариант регистрации и несколько простых шагов для настройки.
GitHub opening page Когда вы впервые входите в свою учетную запись GitHub, вы получаете страницу, содержащую множество виджетов, дающих вам варианты того, где и что вы хотели бы увидеть или сделать. Во-первых, у нас есть «All Activity», это даст вам представление о том, что происходит с вашими репозиториями или действиями в целом, связанными с вашей организацией или учетной записью.
Затем у нас есть наши репозитории кода, либо наши собственные, либо репозитории, с которыми мы недавно взаимодействовали. Мы также можем быстро создавать новые репозитории или репозитории поиска.
Затем у нас есть наша недавняя активность, для меня это проблемы и pull requests, которые я недавно создал или в которых участвовал.
В правой части страницы есть несколько ссылок на репозитории, которые могут нас заинтересовать, скорее всего, на основе вашей недавней активности или собственных проектов.
Честно говоря, я очень редко бываю на своей домашней странице, которую мы только что видели и описали, хотя теперь я вижу, что лента может быть действительно полезной, чтобы помочь взаимодействовать с сообществом немного лучше в определенных проектах.
Далее, если мы хотим зайти в наш профиль на GitHub, мы можем перейти в правый верхний угол, и на вашем изображении будет выпадающий список, который позволит вам перемещаться по вашему аккаунту. Отсюда для доступа к своему профилю выберите \u0026ldquo;Ваш профиль\u0026rdquo;
Далее появится страница вашего профиля, по умолчанию, если вы не измените свою конфигурацию, вы не увидите того, что есть у меня, я добавил некоторые функции, которые показывают мои последние записи в блоге на vZilla, а также мои последние видео на моем канале YouTube.
Лично вы не собираетесь тратить много времени на просмотр своего профиля, но это хорошая страница профиля, которой можно поделиться со своей сетью, чтобы они могли увидеть крутые проекты, над которыми вы работаете.
Затем мы можем перейти к основному элементу GitHub - репозиториям. Здесь вы увидите свои собственные репозитории, а если у вас есть частные репозитории, они также будут показаны в этом длинном списке.
Поскольку этот репозиторий так важен для GitHub, позвольте мне выбрать довольно загруженный в последнее время и просмотреть некоторые основные функции, которые мы можем использовать здесь, в дополнение ко всему, что я уже использую, когда дело доходит до редактирования нашего кода в git. моя локальная система.
Прежде всего, в предыдущем окне я выбрал репозиторий 90DaysOfDevOps, и мы видим это представление. Вы можете видеть из этого представления, что у нас есть много информации, у нас есть наша основная структура кода в середине, показывающая наши файлы и папки, которые хранятся в нашем репозитории. Наш файл readme.md отображается внизу. Справа от страницы у нас есть раздел о репозитории, где у репозитория есть описание и назначение. Затем у нас есть много информации под этим, показывающей, сколько людей отметили проект, разветвились и смотрят.
Если мы прокрутим вниз немного дальше, вы также увидите, что у нас есть Releases, они относятся к части задачи golang. У нас нет никаких пакетов в нашем проекте, здесь перечислены наши соавторы. Затем у нас есть используемые языки, опять же из разных разделов задачи.
В верхней части страницы вы увидите список вкладок. Они могут различаться, и их можно изменить, чтобы отображались только те, которые вам нужны. Вы увидите здесь, что я не использую все это, и я должен удалить их, чтобы убедиться, что весь мой репозиторий в порядке.
Во-первых, у нас была вкладка кода, которую мы только что обсуждали, но эти вкладки всегда доступны при навигации по репозиторию, что очень полезно, так что мы можем быстро и легко переходить между разделами. Далее у нас есть вкладка вопросов.
Проблемы позволяют отслеживать вашу работу на GitHub, где происходит разработка. В этом конкретном репозитории вы можете увидеть, что у меня есть некоторые проблемы, связанные с добавлением диаграмм или опечаток, но также у нас есть проблема, указывающая на необходимость или требование для китайской версии репозитория.
Если это был репозиторий кода, то это отличное место, чтобы сообщить о проблемах или проблемах с сопровождающими, но помните, будьте внимательны и подробны в отношении того, о чем вы сообщаете, давайте как можно больше подробностей.
Следующая вкладка — Pull Requests. Pull Requests позволяют вам сообщать другим об изменениях, которые вы отправили в ветку в репозитории. Здесь кто-то мог разветвить ваш репозиторий, внести изменения, такие как исправления ошибок или улучшения функций, или просто опечататься во многих случаях в этом репозитории.
Мы рассмотрим разветвление позже.
Я считаю, что следующая вкладка совершенно новая? Но я подумал, что для такого проекта, как #90DaysOfDevOps, это может действительно помочь направить контент, а также помочь сообществу, когда они проходят свой собственный путь обучения. Я создал несколько дискуссионных групп для каждого раздела задачи, чтобы люди могли присоединиться и обсудить.
Вкладка \u0026ldquo;Actions\u0026rdquo; позволит вам создавать, тестировать и развертывать код и многое другое прямо из GitHub. GitHub Actions будет чем-то, что мы рассмотрим в разделе задачи, посвященном CI/CD, но именно здесь мы можем установить некоторую конфигурацию, чтобы автоматизировать шаги для нас.
В моем основном профиле GitHub я использую GitHub Actions для получения последних сообщений в блогах и видео на YouTube, чтобы обновлять информацию на этом домашнем экране.
Я уже говорил о том, что GitHub - это не только хранилище исходного кода, но и инструмент управления проектами. Вкладка \u0026ldquo;Проект\u0026rdquo; позволяет нам создавать проектные таблицы типа канбан, чтобы мы могли связывать проблемы и PR для лучшего сотрудничества над проектом и иметь видимость этих задач. Я знаю, что проблемы, как мне кажется, являются хорошим местом для регистрации запросов о возможностях, и это так, но страница вики позволяет составить полную дорожную карту проекта с указанием текущего состояния и в целом лучше документировать ваш проект, будь то устранение неполадок или контент типа how-to.
Не совсем применимо к этому проекту, но вкладка Security действительно существует для того, чтобы убедиться, что участники проекта знают, как обращаться с определенными задачами, здесь мы можем определить политику, а также дополнения для сканирования кода, чтобы убедиться, что ваш код, например, не содержит секретных переменных окружения.
Для меня вкладка insights очень важна, она предоставляет так много информации о репозитории, начиная от того, сколько активности происходило и заканчивая коммитами и проблемами, а также сообщает о посещаемости репозитория. В левой части вы можете увидеть список, который позволяет вам подробно ознакомиться с метриками репозитория.
Наконец, у нас есть вкладка Settings, где мы можем подробно описать, как мы управляем нашим репозиторием, в настоящее время я единственный сопровождающий репозитория, но мы можем разделить эту ответственность. Здесь мы можем определить интеграции и другие подобные задачи.
Это был очень быстрый обзор GitHub, я думаю, что есть еще несколько областей, которые я, возможно, упомянул и которые нуждаются в более подробном объяснении. Как уже упоминалось, GitHub содержит миллионы репозиториев, в которых в основном хранится исходный код, и они могут быть общедоступными или частными.
Forking Я собираюсь больше рассказать об Open-Source на завтрашней сессии, но большая часть любого репозитория кода — это возможность сотрудничать с сообществом. Давайте подумаем о сценарии: мне нужна копия репозитория, потому что я хочу внести в него некоторые изменения, может быть, я хочу исправить ошибку или, может быть, я хочу что-то изменить, чтобы использовать его для моего варианта использования, который, возможно, не был предполагаемый вариант использования для первоначального сопровождающего кода. Это то, что мы бы назвали разветвлением репозитория. Форк — это копия репозитория. Разветвление репозитория позволяет вам свободно экспериментировать с изменениями, не затрагивая исходный проект.
Позвольте мне вернуться на начальную страницу после входа в систему и увидеть один из предложенных репозиториев.
Если мы нажмем на этот репозиторий, мы получим тот же вид, что и репозиторий 90DaysOfDevOps.
Если мы обратим внимание, ниже у нас есть 3 варианта: watch, fork и star.
Watch - обновление, когда что-то происходит с хранилищем. Fork - копия репозитория. Star - \u0026ldquo;Я думаю, что ваш проект крутой\u0026rdquo;. Учитывая наш сценарий, когда нам нужна копия репозитория для работы, мы воспользуемся опцией fork. Если вы являетесь членом нескольких организаций, то вам придётся выбрать, где будет происходить форк, я выберу свой профиль.
Теперь у нас есть собственная копия репозитория, над которой мы можем свободно работать и изменять по своему усмотрению. Это начало процесса подачи запросов на исправление, о котором мы уже вкратце упоминали, но более подробно рассмотрим завтра. Хорошо, я слышу, как вы говорите, но как мне внести изменения в этот репозиторий и код, если он находится на веб-сайте, ну, вы можете просматривать и редактировать на веб-сайте, но это не будет таким же, как использование вашей любимой IDE в вашей локальной системе. с вашей любимой цветовой темой. Чтобы получить копию этого репозитория на нашем локальном компьютере, мы выполним клонирование репозитория. Это позволит нам работать над вещами локально, а затем отправлять наши изменения обратно в нашу разветвленную копию репозитория.
У нас есть несколько вариантов получения копии этого кода, как вы можете видеть ниже.
Доступна локальная версия GitHub Desktop, которая дает вам визуальное настольное приложение для отслеживания изменений и отправки и получения изменений между локальным и github.
Для этой небольшой демонстрации я буду использовать URL-адрес HTTPS, который мы видим там.
Теперь на нашей локальной машине я перейду в каталог, в который я хочу загрузить этот репозиторий, а затем выполню команду git clone url.
Теперь мы можем обратиться к VScode, чтобы действительно внести некоторые изменения.
Теперь давайте сделаем некоторые изменения, я хочу изменить все эти ссылки и заменить их на что-то другое.
Теперь, если мы вернемся на GitHub и найдем наш readme.mdin в этом репозитории, вы сможете увидеть несколько изменений, которые я внес в файл.
На данном этапе это может быть завершено, и мы можем быть довольны нашим изменением, поскольку мы единственные люди, которые будут использовать наше новое изменение, но, возможно, это было изменение ошибки, и если это так, то мы захотим внести свой вклад через Pull Request чтобы уведомить сопровождающих исходного репозитория о наших изменениях и посмотреть, примут ли они наши изменения.
Мы можем сделать это, используя кнопку вклада, выделенную ниже. Я расскажу об этом подробнее завтра, когда мы рассмотрим рабочие процессы с открытым исходным кодом.
Я долго просматривал GitHub и слышал, как некоторые из вас плачут, но как насчет других вариантов!
Ну, есть, и я собираюсь найти некоторые ресурсы, которые охватывают основы для некоторых из них. В своих путешествиях вы столкнетесь с GitLab и BitBucket, и хотя они основаны на git, у них есть свои отличия.
Вы также столкнетесь с размещенными вариантами. Чаще всего здесь я видел GitLab как размещенную версию по сравнению с GitHub Enterprise (не верите, что есть бесплатный размещенный GitHub?)
Ресурсы Learn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners BitBucket Tutorials Playlist What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial Git cheatsheet `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day40/"},"https://romankurnovskii.com/ru/docs/python101/chapter41_bb_freeze/":{title:"41. bbfreeze",tags:[],content:`Пакет bbfreeze также позволяет нам создавать двоичные файлы, но только в Linux и Windows. Когда вы создаете двоичный файл в Linux, результат будет работать только на машинах, имеющих ту же аппаратную архитектуру и версию libc, что ограничивает его полезность в Linux. Также следует отметить, что bbfreeze работает только с Python версий 2.4 - 2.7. Вы можете использовать easy_install или pip для установки пакета bbfreeze в вашу систему. Пакет bbfreeze включает поддержку egg, поэтому он может включать зависимости eggs в ваши двоичные файлы, в отличие от py2exe. Вы также можете замораживать несколько скриптов одновременно, включать интерпретатор Python и многое другое.
Начало работы с bbfreeze Вы можете использовать easy_install для загрузки и установки bbfreeze или просто загрузить его исходники или файл egg непосредственно из Python Package Index (PyPI). В этой статье мы попробуем использовать его на простом скрипте генератора конфигурационных файлов, а также опробуем его на программе wxPython из главы py2exe.
# config_1.py import configobj def createConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Create the configuration file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() inifile = configFile config.filename = inifile config['server'] = \u0026quot;http://www.google.com\u0026quot; config['username'] = \u0026quot;mike\u0026quot; config['password'] = \u0026quot;dingbat\u0026quot; config['update interval'] = 2 config.write() def getConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Open the config file and return a configobj \u0026quot;\u0026quot;\u0026quot; return configobj.ConfigObj(configFile) def createConfig2(path): \u0026quot;\u0026quot;\u0026quot; Create a config file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() config.filename = path config[\u0026quot;Sony\u0026quot;] = {} config[\u0026quot;Sony\u0026quot;][\u0026quot;product\u0026quot;] = \u0026quot;Sony PS3\u0026quot; config[\u0026quot;Sony\u0026quot;][\u0026quot;accessories\u0026quot;] = ['controller', 'eye', 'memory stick'] config[\u0026quot;Sony\u0026quot;][\u0026quot;retail price\u0026quot;] = \u0026quot;$400\u0026quot; config.write() if __name__ == \u0026quot;__main__\u0026quot;: createConfig2(\u0026quot;sampleConfig2.ini\u0026quot;) В этом скрипте есть несколько функций, которые довольно бессмысленны, но мы оставим их для примера. Согласно документации bbfreeze, мы должны быть в состоянии создать двоичный файл со следующей строкой, введенной в командную строку:
bb-freeze config_1.py Это предполагает, что в вашем пути есть C:\\Python27\\Scripts. Если у вас его нет, вам придется ввести полный путь (например, C:\\Python27\\Scripts\\bb-freeze config_1.py). Если вы запустите его, вы должны увидеть, как создается папка с именем dist. Вот как выглядела моя папка после запуска config_1.exe:
Вы заметите, что при запуске исполняемого файла он создает файл конфигурации sampleconfig2.ini. Вы можете увидеть предупреждение о том, что пакет pywin32 не установлен. Вы можете проигнорировать это предупреждение или загрузить и установить pywin32.
Теперь мы готовы двигаться дальше и попытаться создать исполняемый файл из кода, использующего wxPython!
Продвинутой расширенной конфигурации bbfreeze На странице PyPI для bbfreeze (которая также является его домашней страницей) очень мало документации. Однако там говорится, что предпочтительный способ использования bbfreeze - это небольшие скрипты. Мы попробуем создать двоичный файл с помощью примера wxPython, упомянутого ранее. Вот код wx:
import wx class DemoPanel(wx.Panel): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Panel.__init__(self, parent) labels = [\u0026quot;Name\u0026quot;, \u0026quot;Address\u0026quot;, \u0026quot;City\u0026quot;, \u0026quot;State\u0026quot;, \u0026quot;Zip\u0026quot;, \u0026quot;Phone\u0026quot;, \u0026quot;Email\u0026quot;, \u0026quot;Notes\u0026quot;] mainSizer = wx.BoxSizer(wx.VERTICAL) lbl = wx.StaticText(self, label=\u0026quot;Please enter your information here:\u0026quot;) lbl.SetFont(wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD)) mainSizer.Add(lbl, 0, wx.ALL, 5) for lbl in labels: sizer = self.buildControls(lbl) mainSizer.Add(sizer, 1, wx.EXPAND) self.SetSizer(mainSizer) mainSizer.Layout() def buildControls(self, label): \u0026quot;\u0026quot;\u0026quot; Put the widgets together \u0026quot;\u0026quot;\u0026quot; sizer = wx.BoxSizer(wx.HORIZONTAL) size = (80,40) font = wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD) lbl = wx.StaticText(self, label=label, size=size) lbl.SetFont(font) sizer.Add(lbl, 0, wx.ALL|wx.CENTER, 5) if label != \u0026quot;Notes\u0026quot;: txt = wx.TextCtrl(self, name=label) else: txt = wx.TextCtrl(self, style=wx.TE_MULTILINE, name=label) sizer.Add(txt, 1, wx.ALL, 5) return sizer class DemoFrame(wx.Frame): \u0026quot;\u0026quot;\u0026quot; Frame that holds all other widgets \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Frame.__init__(self, None, wx.ID_ANY, \u0026quot;Py2Exe Tutorial\u0026quot;, size=(600,400) ) panel = DemoPanel(self) self.Show() if __name__ == \u0026quot;__main__\u0026quot;: app = wx.App(False) frame = DemoFrame() app.MainLoop() Теперь давайте создадим простой скрипт freezing!
# bb_setup.py from bbfreeze import Freezer f = Freezer(distdir=\u0026quot;bb-binary\u0026quot;) f.addScript(\u0026quot;sampleApp.py\u0026quot;) f() Прежде всего, мы импортируем класс Freezer из пакета bbfreeze. Freezer принимает три аргумента: папку назначения, итерабельную переменную includes и итерабельную переменную excludes (т.е. кортеж или список). Чтобы посмотреть, насколько хорошо работает bbfreeze с настройками по умолчанию, мы опустим кортежи/списки include и excludes. Как только у вас есть объект Freezer, вы можете добавить свой сценарий(и), вызвав метод addScript имени объекта Freezer. Затем вам нужно просто вызвать объект (например, f() ).
Примечание: Вы можете увидеть предупреждение о том, что bb_freeze не может найти \u0026ldquo;MSVCP90.dll\u0026rdquo; или что-то подобное. Если вы увидите такое сообщение, возможно, вам придется включить его явно или добавить в качестве зависимости при создании программы установки. О том, как создать программу установки, мы узнаем в одной из следующих глав.
Чтобы запустить этот сценарий, нужно сделать примерно следующее:
python bb_setup.py Когда я запустил этот скрипт, он создал папку с именем bb-binary, содержащую 19 файлов размером 17,2 МБ. Когда я запустил файл sampleApp.exe, он прекрасно запустился и был правильно оформлен, однако у него также был экран консоли. Нам придется немного отредактировать наш сценарий, чтобы исправить это:
# bb_setup2.py from bbfreeze import Freezer includes = [] excludes = ['_gtkagg', '_tkagg', 'bsddb', 'curses', 'email', 'pywin.debugger', 'pywin.debugger.dbgcon', 'pywin.dialogs', 'tcl', 'Tkconstants', 'Tkinter'] bbFreeze_Class = Freezer('dist', includes=includes, excludes=excludes) bbFreeze_Class.addScript(\u0026quot;sampleApp.py\u0026quot;, gui_only=True) bbFreeze_Class.use_compression = 0 bbFreeze_Class.include_py = True bbFreeze_Class() Если вы запустите его, то в итоге получите папку dist с примерно 19 файлами, но немного другого размера - 19,6 МБ. Обратите внимание, что мы добавили второй аргумент в метод addScript: gui_only=True. Благодаря этому раздражающая консоль исчезнет. Мы также установили сжатие на ноль (без сжатия) и включили интерпретатор Python. Включение сжатия только уменьшило результат до 17,2 МБ.
Пакет bbfreeze также работает с \u0026ldquo;рецептами\u0026rdquo; и включает несколько примеров, однако они не очень хорошо документированы. Не стесняйтесь изучить их самостоятельно в качестве упражнения.
Подведение итогов Теперь вы должны знать основы использования bbfreeze для создания двоичных файлов из ваших программ. Я заметил, что когда я запускал bbfreeze на своей машине, он значительно медленнее создавал исполняемый файл wxPython по сравнению с py2exe. Это одна из тех вещей, с которыми вам придется экспериментировать, когда вы будете определять, какой инструмент использовать для создания двоичных файлов.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter41_bb_freeze/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day41/":{title:"41. Рабочий процесс с открытым исходным кодом",tags:["devops"],content:`Рабочий процесс с открытым исходным кодом Когда мы изучали основы GitHub, мы проходили процесс форка произвольного проекта и внесения изменений в наш локальный репозиторий. Здесь мы хотим сделать еще один шаг вперед и внести свой вклад в проект с открытым исходным кодом. Помните, что вклад не обязательно должен заключаться в исправлении ошибок, кодировании функций, это может быть и документация. Каждая мелочь помогает, и это также позволит вам поработать с некоторыми функциями git, которые мы рассмотрели.
Форк проекта Первое, что нам нужно сделать, это найти проект, в который мы можем внести свой вклад. Я недавно выступал с презентациями в Kanister Project и хотел бы поделиться своими презентациями, которые теперь есть на YouTube, с основным readme.mdf-файлом проекта.
Прежде всего, нам нужно форкнуть проект. Давайте проделаем этот процесс. Я собираюсь перейти по ссылке, указанной выше, и форкнуть репозиторий.
Теперь у нас есть наша копия всего репозитория.
Для справки в файле Readme.mdfile в списке оригинальных Presenations указаны только эти два, поэтому нам нужно исправить это в нашем процессе.
Клонирование на локальную машину Теперь у нас есть собственный форк, который мы можем перенести на локальную машину и начать вносить правки в файлы. Используя кнопку code на нашем репозитории, мы можем получить URL, а затем использовать git clone url в каталоге, куда мы хотим поместить репозиторий.
Вносим изменения У нас есть локальный проект, поэтому мы можем открыть VSCode или IDE или текстовый редактор по вашему выбору, чтобы добавить свои изменения.
Файл readme.mdfile написан на языке markdown, и поскольку я изменяю чужой проект, я собираюсь следовать существующему форматированию проекта для добавления нашего содержимого.
Тестируем свои изменения В качестве лучшей практики мы должны тестировать наши изменения, это совершенно логично, если бы это было изменение кода приложения, вы бы хотели убедиться, что приложение продолжает функционировать после изменения кода, но мы также должны убедиться, что документация отформатирована и выглядит правильно.
В VScode у нас есть возможность добавить множество плагинов, одним из которых является возможность предварительного просмотра страниц в формате markdown.
Верните изменения в наш форкнутый репозиторий У нас нет аутентификации, чтобы отправить наши изменения непосредственно в репозиторий Kanister, поэтому мы должны пойти этим путем. Теперь, когда я доволен нашими изменениями, мы можем выполнить некоторые из этих хорошо известных команд git.
Теперь мы возвращаемся в GitHub, чтобы еще раз проверить изменения и затем внести вклад в мастер-проект.
Теперь мы можем вернуться в верхнюю часть нашего форкнутого репозитория для Kanister и увидеть, что мы на 1 коммит опережаем ветку kanisterio:master.
Далее мы нажимаем на кнопку \u0026ldquo;Внести вклад\u0026rdquo;, выделенную выше. Мы видим опцию \u0026ldquo;Open Pull Request\u0026rdquo;.
Open a pull request На следующем изображении происходит довольно много всего: слева вверху вы видите, что мы находимся в оригинальном или основном репозитории. Затем вы можете увидеть, что мы сравниваем, а это оригинальный основной и наш форкнутый репозиторий. Затем у нас есть кнопка создания запроса на притяжение, к которой мы скоро вернёмся. У нас есть единственный коммит, но если бы изменений было больше, то здесь могло бы быть несколько коммитов. Затем у нас есть изменения, которые мы внесли в readme.mdfile.
Мы просмотрели вышеуказанные изменения и готовы создать pull request, нажав на зеленую кнопку.
Затем, в зависимости от того, как мейнтейнер проекта настроил функциональность Pull Request в своём репозитории, у вас может быть или не быть шаблона, который даст вам указания на то, что хочет видеть мейнтейнер.
Здесь вам снова нужно составить содержательное описание того, что вы сделали, четкое и краткое, но достаточно подробное. Вы можете видеть, что я сделал простой обзор изменений и отметил документацию.
Создайте запрос на исправление Теперь мы готовы к созданию запроса на исправление. После нажатия кнопки \u0026ldquo;Create Pull Request\u0026rdquo; в верхней части страницы вы получите краткое описание вашего запроса.
Прокручивая страницу вниз, вы, вероятно, увидите, что происходит автоматизация, в данном случае нам требуется рецензия, и происходят некоторые проверки. Мы видим, что Travis CI находится в процессе и началась сборка, которая проверит наше обновление и убедится, что перед тем, как что-то будет слито, мы не сломаем что-то своими добавлениями.
Еще одна вещь, которую следует отметить, это то, что красный цвет на снимке экрана выше, может выглядеть немного пугающе и выглядеть так, как будто вы совершили ошибки! Не волнуйтесь, вы ничего не нарушили, мой главный совет - этот процесс поможет вам и сопровождающим проекта. Если вы допустили ошибку, по крайней мере, по моему опыту, сопровождающий свяжется с вами и посоветует, что делать дальше.
Этот запрос на исправление теперь общедоступен для всех added Kanister presentation/resource #1237.
Я собираюсь опубликовать это до того, как слияние и запрос на исправление будут приняты, так что, возможно, мы сможем получить небольшой приз для тех, кто всё ещё следит за развитием событий и сможет добавить картинку к успешному PR?
Форкните этот репозиторий на свой собственный аккаунт GitHub Добавьте свою картинку и, возможно, текст Внесите изменения в свой форкнутый репозиторий. Создайте PR, который я увижу и одобрю. Я придумаю какой-нибудь приз. На этом мы завершаем знакомство с Git и GitHub, далее мы погружаемся в контейнеры, что начинается с рассмотрения общей картины того, как, почему контейнеры, а также с рассмотрения виртуализации и того, как мы к ней пришли.
Ресурсы Learn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners BitBucket Tutorials Playlist What is Version Control? Types of Version Control System Git Tutorial for Beginners Git for Professionals Tutorial Git and GitHub for Beginners - Crash Course Complete Git and GitHub Tutorial Git cheatsheet `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day41/"},"https://romankurnovskii.com/ru/docs/python101/chapter42_cx_freeze/":{title:"42. cx_Freeze",tags:[],content:`В этой главе мы познакомимся с cx_Freeze, кроссплатформенным набором скриптов, предназначенных для freeze скриптов Python в исполняемые файлы, подобно py2exe, PyInstaller и т.д. Мы заморозим один консольный скрипт и один оконный (т.е. GUI) скрипт, используя примеры из предыдущей главы. Инструмент cx_Freeze - единственный инструмент создания двоичных файлов, который на данный момент может работать как с Python 2.x, так и с 3.x на различных операционных системах. В этой главе мы будем использовать его с Python 2.7 только потому, что хотим сравнить его с другими инструментами создания двоичных файлов.
Вы можете установить cx_Freeze с помощью одного из их инсталляторов для Windows, через предоставленные ими RPM для Linux, через исходный RPM или непосредственно из исходного кода. Вы также можете использовать pip для установки cx_Freeze.
Примечание: Я тестировал на Windows 7, используя Python 2.7.3, wxPython 2.9.4.0 (classic) и cx_Freeze 4.3.2.
Начало работы с cx_Freeze Как указано на сайте cx_Freeze, существует три способа использования этого скрипта. Первый - просто использовать прилагаемый скрипт cxfreeze; второй - создать установочный скрипт distutils (думаю о py2exe), который вы можете сохранить для дальнейшего использования; и третий - работать с внутренними компонентами cxfreeze. Мы сосредоточимся на первых двух способах использования cx_Freeze. Начнем с консольного скрипта:
# config_1.py import configobj def createConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Create the configuration file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() inifile = configFile config.filename = inifile config['server'] = \u0026quot;http://www.google.com\u0026quot; config['username'] = \u0026quot;mike\u0026quot; config['password'] = \u0026quot;dingbat\u0026quot; config['update interval'] = 2 config.write() def getConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Open the config file and return a configobj \u0026quot;\u0026quot;\u0026quot; return configobj.ConfigObj(configFile) def createConfig2(path): \u0026quot;\u0026quot;\u0026quot; Create a config file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() config.filename = path config[\u0026quot;Sony\u0026quot;] = {} config[\u0026quot;Sony\u0026quot;][\u0026quot;product\u0026quot;] = \u0026quot;Sony PS3\u0026quot; config[\u0026quot;Sony\u0026quot;][\u0026quot;accessories\u0026quot;] = ['controller', 'eye', 'memory stick'] config[\u0026quot;Sony\u0026quot;][\u0026quot;retail price\u0026quot;] = \u0026quot;$400\u0026quot; config.write() if __name__ == \u0026quot;__main__\u0026quot;: createConfig2(\u0026quot;sampleConfig2.ini\u0026quot;) Все, что делает этот скрипт, это создает действительно простой конфигурационный файл, используя модуль configobj Майкла Фоорда. Вы можете настроить его и на чтение конфигурации, но для данного примера мы это пропустим. Давайте узнаем, как собрать бинарник с помощью cx_Freeze! Согласно документации, для этого достаточно ввести в командную строку следующую строку (при условии, что вы находитесь в правильной директории):
cxfreeze config_1.py --target-dir dirName Это предполагает, что в вашем пути есть C:\\PythonXX\\Scripts. Если это не так, вам придется либо исправить это, либо ввести полный путь. В любом случае, если скрипт cxfreeze запущен правильно, у вас должна быть папка со следующим содержимым:
Как вы можете видеть, общий размер файла должен составлять около 5 мегабайт. Это было довольно просто. Он даже подхватил модуль configobj без нашей просьбы. Есть 18 аргументов командной строки, которые вы можете передать cx_Freeze, чтобы управлять его действиями. Они варьируются от того, какие модули включать или исключать, оптимизировать, сжимать, включать zip-файл, манипулировать путями и многое другое.
Теперь давайте попробуем кое-что более продвинутое.
Продвинутый cx_Freeze - использование файла setup.py Прежде всего, нам нужен скрипт, который мы будем использовать. Мы будем использовать пример формы wxPython из предыдущих глав.
import wx class DemoPanel(wx.Panel): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Panel.__init__(self, parent) labels = [\u0026quot;Name\u0026quot;, \u0026quot;Address\u0026quot;, \u0026quot;City\u0026quot;, \u0026quot;State\u0026quot;, \u0026quot;Zip\u0026quot;, \u0026quot;Phone\u0026quot;, \u0026quot;Email\u0026quot;, \u0026quot;Notes\u0026quot;] mainSizer = wx.BoxSizer(wx.VERTICAL) lbl = wx.StaticText(self, label=\u0026quot;Please enter your information here:\u0026quot;) lbl.SetFont(wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD)) mainSizer.Add(lbl, 0, wx.ALL, 5) for lbl in labels: sizer = self.buildControls(lbl) mainSizer.Add(sizer, 1, wx.EXPAND) self.SetSizer(mainSizer) mainSizer.Layout() def buildControls(self, label): \u0026quot;\u0026quot;\u0026quot; Put the widgets together \u0026quot;\u0026quot;\u0026quot; sizer = wx.BoxSizer(wx.HORIZONTAL) size = (80,40) font = wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD) lbl = wx.StaticText(self, label=label, size=size) lbl.SetFont(font) sizer.Add(lbl, 0, wx.ALL|wx.CENTER, 5) if label != \u0026quot;Notes\u0026quot;: txt = wx.TextCtrl(self, name=label) else: txt = wx.TextCtrl(self, style=wx.TE_MULTILINE, name=label) sizer.Add(txt, 1, wx.ALL, 5) return sizer class DemoFrame(wx.Frame): \u0026quot;\u0026quot;\u0026quot; Frame that holds all other widgets \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Frame.__init__(self, None, wx.ID_ANY, \u0026quot;Py2Exe Tutorial\u0026quot;, size=(600,400) ) panel = DemoPanel(self) self.Show() if __name__ == \u0026quot;__main__\u0026quot;: app = wx.App(False) frame = DemoFrame() app.MainLoop() Теперь давайте создадим файл setup.py в стиле cx_Freeze:
# setup.py from cx_Freeze import setup, Executable setup( name = \u0026quot;wxSampleApp\u0026quot;, version = \u0026quot;0.1\u0026quot;, description = \u0026quot;An example wxPython script\u0026quot;, executables = [Executable(\u0026quot;sampleApp.py\u0026quot;)] ) Как вы можете видеть, это довольно просто. Мы импортируем пару классов из cx_Freeze и передаем в них некоторые параметры. В данном случае мы даем классу setup имя, версию, описание и класс Executable. Класс Executable также получает один параметр - имя скрипта, который он будет использовать для создания бинарного файла.
В качестве альтернативы вы можете создать простой setup.py с помощью команды quickstart cx_Freeze (если она находится в пути вашей системы) в той же папке, что и ваш код:
cxfreeze-quickstart
Чтобы заставить setup.py собрать двоичный файл, вам нужно сделать следующее в командной строке:
python setup.py build После запуска у вас должны появиться следующие папки: buildexe.win32-2.7. Внутри последней папки у меня оказалось 15 файлов общим размером 16,6 МБ. Когда вы запустите файл sampleApp.exe, вы заметите, что мы что-то испортили. В дополнение к нашему графическому интерфейсу загружается окно консоли! Чтобы исправить это, нам нужно немного изменить наш установочный файл. Взгляните на наш новый файл:
from cx_Freeze import setup, Executable exe = Executable( script=\u0026quot;sampleApp.py\u0026quot;, base=\u0026quot;Win32GUI\u0026quot;, ) setup( name = \u0026quot;wxSampleApp\u0026quot;, version = \u0026quot;0.1\u0026quot;, description = \u0026quot;An example wxPython script\u0026quot;, executables = [exe] ) Во-первых, мы отделили класс Executable от класса setup и присвоили класс Executable переменной. Мы также добавили второй параметр к классу Executable, который является ключевым. Этот параметр называется base. Установив base=\u0026ldquo;Win32GUI\u0026rdquo;, мы можем подавить консольное окно. В документации на сайте cx_Freeze показано множество других параметров, которые принимает класс Executable.
Подведение итогов Теперь вы должны знать, как создавать двоичные файлы с помощью cx_Freeze. Это довольно просто сделать, и в моем тестировании он работал намного быстрее, чем bbfreeze. Если у вас есть необходимость создавать двоичные файлы для Python 2.x и 3.x на всех основных платформах, то этот инструмент для вас!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter42_cx_freeze/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day42/":{title:"42. Контейнеры",tags:["devops"],content:`Контейнеры Этот раздел будет посвящен контейнерам. Будем рассматривать Docker, вникая в некоторые ключевые области, чтобы понять больше о контейнерах.
Я также попытаюсь провести практические занятия по созданию контейнера, который мы сможем использовать не только в этом разделе, но и в последующих.
Почему другой способ запуска приложений? Первое, на что мы должны обратить внимание, - зачем нам нужен другой способ запуска программ или приложений? Просто выбор велик, мы можем запускать наши приложения в разных формах, мы можем видеть приложения, развернутые на физическом оборудовании с операционной системой и одним приложением, мы можем видеть виртуальную машину или облачные IaaS экземпляры, запускающие наше приложение, которое затем интегрируется в базу данных снова в виртуальной машине или как PaaS предложение в публичном облаке. Или мы можем увидеть наши приложения, работающие в контейнерах.
Ни один из перечисленных вариантов не является неправильным или правильным, но у каждого из них есть свои причины для существования, и я также твердо уверен, что ни один из них не исчезнет. Я видел много материалов, в которых обсуждаются контейнеры и виртуальные машины, и на самом деле здесь не должно быть спора, поскольку это больше похоже на спор между яблоками и грушами, где они оба являются фруктами (способы запуска наших приложений), но это не одно и то же.
Я бы также сказал, что если вы начинаете и разрабатываете приложение, вам следует склониться к контейнерам просто потому, что мы рассмотрим некоторые из этих областей позже, но речь идет об эффективности, скорости и размере. Но за это тоже приходится платить, если вы не имеете представления о контейнерах, то вам придется учиться, чтобы понять, зачем это нужно, и вжиться в этот образ мышления. Если вы разрабатывали свои приложения особым образом или вы не работаете в новой среде, то у вас может быть больше болевых точек, с которыми нужно справиться, прежде чем рассматривать контейнеры.
У нас есть много различных вариантов, когда нужно загрузить ту или иную часть программного обеспечения, есть множество различных операционных систем, которые мы можем использовать. И конкретные инструкции о том, что нам нужно сделать, чтобы установить наши приложения.
В последнее время я все чаще замечаю, что приложения, для которых раньше требовалась полноценная серверная ОС, виртуальная машина, физический или облачный экземпляр, теперь выпускают версии своего программного обеспечения на основе контейнеров. Я нахожу это интересным, поскольку это открывает мир контейнеров и Kubernetes для всех, а не только для разработчиков приложений.
Как вы уже, наверное, поняли, я не собираюсь утверждать, что ответ - это контейнеры, в чем вопрос! Но я хотел бы обсудить, что это еще один вариант, о котором мы должны знать при развертывании наших приложений.
У нас уже давно существует контейнерная технология, так почему же именно сейчас, за последние 10 лет, она стала популярной, я бы сказал, даже более популярной в последние 5 лет. У нас были контейнеры в течение десятилетий. Все сводится к вызову контейнеров или, лучше сказать, образов, тому, как мы распространяем наше программное обеспечение, потому что если у нас будет только контейнерная технология, то у нас останется много тех же проблем, которые были с управлением программным обеспечением.
Если мы подумаем о Docker как об инструменте, то причина его взлета заключается в экосистеме образов, которые легко найти и использовать. Их легко установить на свои системы и запустить в работу. Важной частью этого является согласованность во всем пространстве, во всех этих различных проблемах, с которыми мы сталкиваемся при работе с программным обеспечением. Неважно, MongoDB это или nodeJS, процесс запуска любого из них будет одинаковым. Процесс остановки любого из них одинаков. Все эти проблемы будут существовать, но самое приятное, что когда мы объединяем хорошие технологии контейнеров и образов, у нас появляется единый набор инструментов для решения всех этих различных проблем. Некоторые из этих проблем перечислены ниже:
Сначала нам нужно найти программное обеспечение в Интернете. Затем мы должны загрузить это программное обеспечение. Доверяем ли мы источнику? Нужна ли нам лицензия? Какая лицензия? Совместима ли она с различными платформами? Что представляет собой пакет? Бинарный? Исполняемый? Менеджер пакетов? Как сконфигурировать программу? Зависимости? Были ли они учтены при загрузке или они нам тоже нужны? Зависимости зависимостей? Как нам запустить приложение? Как мы остановим приложение? Будет ли оно автозапускаться? Запускаться при загрузке? Конфликты ресурсов? Конфликтующие библиотеки? Конфликты портов Безопасность программного обеспечения? Обновления программного обеспечения? Как удалить программное обеспечение? Мы можем разделить вышеперечисленное на 3 области сложности программного обеспечения, с которыми помогают справиться контейнеры и образы.
Распространение Установка Эксплуатация Найти Установить Запустить Скачать Конфигурация Безопасность Лицензия Деинсталляция Порты Пакет Зависимости Конфликты с ресурсами Доверие Платформа Автоперезагрузка Поиск Библиотеки Обновления Контейнеры и образы помогут нам устранить некоторые из этих проблем, с которыми мы сталкиваемся при работе с другими программами и приложениями.
На высоком уровне мы можем перенести установку и эксплуатацию в один список: образы помогут нам с точки зрения распространения, а контейнеры помогут с установкой и эксплуатацией.
Хорошо, возможно, звучит здорово и захватывающе, но нам все еще нужно понять, что такое контейнер, и теперь я упомянул образы, поэтому давайте рассмотрим эти области далее.
Еще одна вещь, которую вы могли часто видеть, когда мы говорили о контейнерах для разработки программного обеспечения, - это аналогия с морскими контейнерами: морские контейнеры используются для перевозки различных товаров по морю с помощью больших судов.
Какое отношение это имеет к нашей теме о контейнерах? Подумайте о коде, который пишут разработчики программного обеспечения, как мы можем перенести этот код с одной машины на другую?
Если мы подумаем о том, что мы уже говорили о распространении программного обеспечения, установке и операциях, то теперь мы начнем выстраивать это в визуальную среду. У нас есть аппаратное обеспечение и операционная система, на которой вы будете запускать несколько приложений. Например, nodejs имеет определенные зависимости и нуждается в определенных библиотеках. Если вы хотите установить MySQL, то ему нужны необходимые библиотеки и зависимости. Каждое программное приложение будет иметь свою библиотеку и зависимость. Нам может крупно повезти, и у нас не будет конфликтов между приложениями, где определенные библиотеки и зависимости сталкиваются, вызывая проблемы, но чем больше приложений, тем больше вероятность или риск конфликтов. Однако речь не идет об одном развертывании, когда все исправления ваших программных приложений будут обновлены, и тогда мы также можем столкнуться с этими конфликтами.
Контейнеры могут помочь решить эту проблему. Контейнеры помогают создать ваше приложение, отправить приложение, развернуть и масштабировать эти приложения с легкостью самостоятельно. Давайте рассмотрим архитектуру, у вас есть аппаратное обеспечение и операционная система, а поверх них - контейнерный движок, такой как docker, который мы рассмотрим позже. Программное обеспечение контейнерного движка помогает создавать контейнеры, которые упаковывают библиотеки и зависимости вместе с ними, так что вы можете легко перемещать этот контейнер с одной машины на другую, не беспокоясь о библиотеках и зависимостях, поскольку они поставляются как часть пакета, который является ничем иным, как контейнером, так что вы можете иметь различные контейнеры, которые можно перемещать между системами, не беспокоясь о базовых зависимостях, которые необходимы приложению. потому что все, что нужно приложению для работы, упаковано как контейнер, который можно перемещать.
Преимущества контейнеров Контейнеры помогают упаковать все зависимости внутри контейнера и изолировать его.
Контейнерами легко управлять
Возможность перехода от одной системы к другой.
Контейнеры помогают упаковать программное обеспечение, и вы можете легко отправить его без каких-либо дублирующих усилий.
Контейнеры легко масштабируются.
Используя контейнеры, вы можете масштабировать независимые контейнеры и использовать балансировщик нагрузки или сервис, который поможет разделить трафик, и вы сможете масштабировать приложения горизонтально. Контейнеры обеспечивают большую гибкость и облегчают управление приложениями.
Что такое контейнер? Когда мы запускаем приложения на нашем компьютере, это может быть веб-браузер или VScode, который вы используете для чтения этого сообщения. Это приложение работает как процесс или то, что известно как процесс. На наших ноутбуках или системах мы обычно запускаем несколько приложений или, как мы сказали, процессов. Когда мы открываем новое приложение или нажимаем на значок приложения, это приложение, которое мы хотим запустить, иногда это приложение может быть службой, которую мы просто хотим запустить в фоновом режиме, наша операционная система полна служб, которые работают в фоновом режиме, предоставляя вам возможность пользоваться системой.
Значок приложения представляет собой ссылку на исполняемый файл в файловой системе, после чего операционная система загружает этот файл в память. Интересно, что этот исполняемый файл иногда называют образом, когда речь идет о процессе.
Контейнеры - это процессы, а контейнер - это стандартная единица программного обеспечения, которая упаковывает код и все его зависимости, чтобы приложение быстро и надежно работало в разных вычислительных средах.
Контейнерное программное обеспечение всегда будет работать одинаково, независимо от инфраструктуры. Контейнеры изолируют программное обеспечение от его окружения и обеспечивают его единообразную работу, несмотря на различия, например, между разработкой и постановкой на хранение.
Я упоминал образы в последнем разделе, когда речь шла о том, как и почему контейнеры и образы вместе сделали контейнеры популярными в нашей экосистеме.
Что такое образ? Образ контейнера - это легкий, автономный, исполняемый пакет программного обеспечения, который включает все необходимое для запуска приложения: код, время выполнения, системные инструменты, системные библиотеки и настройки. Образы контейнеров становятся контейнерами во время выполнения.
Что такое контейнер? Когда мы запускаем приложения на нашем компьютере, это может быть веб-браузер или VScode, который вы используете для чтения этого сообщения. Это приложение работает как процесс или то, что известно как процесс. На наших ноутбуках или системах мы склонны запускать несколько приложений или, как мы сказали, процессов. Когда мы открываем новое приложение или нажимаем на значок приложения, это приложение, которое мы хотели бы запустить, иногда это приложение может быть службой, которую мы просто хотим запустить в фоновом режиме, наша операционная система полна служб, которые работают в фон, предоставляющий вам пользовательский опыт, который вы получаете с вашей системой.
Этот значок приложения представляет собой ссылку на исполняемый файл где-то в вашей файловой системе, затем операционная система загружает этот исполняемый файл в память. Интересно, что этот исполняемый файл иногда называют образом, когда мы говорим о процессе.
Контейнеры — это процессы. Контейнер — это стандартная единица программного обеспечения, которая упаковывает код и все его зависимости, чтобы приложение быстро и надежно запускалось из одной вычислительной среды в другую.
Контейнерное программное обеспечение всегда будет работать одинаково, независимо от инфраструктуры. Контейнеры изолируют программное обеспечение от его среды и обеспечивают его единую работу, несмотря на различия, например, между разработкой и промежуточной стадией.
Я упомянул изображения в предыдущем разделе, когда речь шла о том, как и почему сочетание контейнеров и изображений сделало контейнеры популярными в нашей экосистеме.
Ссылки TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day42/"},"https://romankurnovskii.com/ru/docs/python101/chapter43_pyinstaller/":{title:"43. PyInstaller",tags:[],content:`PyInstaller - это последний инструмент, который мы рассмотрим для создания двоичных файлов. Он поддерживает Python 2.4 - 2.7. Мы продолжим использовать наши простые консольные и wxPython GUI скрипты для тестирования. PyInstaller должен работать на Windows, Linux, Mac, Solaris и AIX. Поддержка Solaris и AIX является экспериментальной. PyInstaller поддерживает подпись кода (Windows), eggs, скрытый импорт, один исполняемый файл, один каталог и многое другое!
Начало работы с PyInstaller Чтобы установить PyInstaller, вы можете скачать исходный код в tarball или zip архиве, распаковать его и запустить его файл setup.py:
python setup.py install Вы также можете установить PyInstaller с помощью pip. Мы начнем с нашего маленького кусочка кода создания конфигурации:
# config_1.py import configobj def createConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Create the configuration file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() inifile = configFile config.filename = inifile config['server'] = \u0026quot;http://www.google.com\u0026quot; config['username'] = \u0026quot;mike\u0026quot; config['password'] = \u0026quot;dingbat\u0026quot; config['update interval'] = 2 config.write() def getConfig(configFile): \u0026quot;\u0026quot;\u0026quot; Open the config file and return a configobj \u0026quot;\u0026quot;\u0026quot; return configobj.ConfigObj(configFile) def createConfig2(path): \u0026quot;\u0026quot;\u0026quot; Create a config file \u0026quot;\u0026quot;\u0026quot; config = configobj.ConfigObj() config.filename = path config[\u0026quot;Sony\u0026quot;] = {} config[\u0026quot;Sony\u0026quot;][\u0026quot;product\u0026quot;] = \u0026quot;Sony PS3\u0026quot; config[\u0026quot;Sony\u0026quot;][\u0026quot;accessories\u0026quot;] = ['controller', 'eye', 'memory stick'] config[\u0026quot;Sony\u0026quot;][\u0026quot;retail price\u0026quot;] = \u0026quot;$400\u0026quot; config.write() if __name__ == \u0026quot;__main__\u0026quot;: createConfig2(\u0026quot;sampleConfig2.ini\u0026quot;) Теперь давайте попробуем создать исполняемый файл! Вам должно быть достаточно сделать это, чтобы PyInstaller заработал:
pyinstaller config_1.py Когда я запустил это, я получил следующую ошибку:
Error: PyInstaller for Python 2.6+ on Windows needs pywin32. Please install from http://sourceforge.net/projects/pywin32/ Чтобы использовать PyInstaller в Windows, вам нужно сначала установить PyWin32! После установки PyWin32 попробуйте повторно запустить эту команду. Вы должны увидеть много вывода на экран, а также две папки рядом с вашим скриптом: build и dist. Если вы перейдете в папку *dist, а затем в ее папку config_1, вы должны увидеть что-то вроде этого:
Когда я запустил исполняемый файл, он создал файл конфигурации, как и должен был. Вы заметите, что PyInstaller смог захватить configobj без вашего указания.
PyInstaller и wxPython Теперь давайте попробуем создать двоичный файл из простого скрипта wxPython. Вот код wxPython, который мы использовали в предыдущих главах:
import wx class DemoPanel(wx.Panel): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Panel.__init__(self, parent) labels = [\u0026quot;Name\u0026quot;, \u0026quot;Address\u0026quot;, \u0026quot;City\u0026quot;, \u0026quot;State\u0026quot;, \u0026quot;Zip\u0026quot;, \u0026quot;Phone\u0026quot;, \u0026quot;Email\u0026quot;, \u0026quot;Notes\u0026quot;] mainSizer = wx.BoxSizer(wx.VERTICAL) lbl = wx.StaticText(self, label=\u0026quot;Please enter your information here:\u0026quot;) lbl.SetFont(wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD)) mainSizer.Add(lbl, 0, wx.ALL, 5) for lbl in labels: sizer = self.buildControls(lbl) mainSizer.Add(sizer, 1, wx.EXPAND) self.SetSizer(mainSizer) mainSizer.Layout() def buildControls(self, label): \u0026quot;\u0026quot;\u0026quot; Put the widgets together \u0026quot;\u0026quot;\u0026quot; sizer = wx.BoxSizer(wx.HORIZONTAL) size = (80,40) font = wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD) lbl = wx.StaticText(self, label=label, size=size) lbl.SetFont(font) sizer.Add(lbl, 0, wx.ALL|wx.CENTER, 5) if label != \u0026quot;Notes\u0026quot;: txt = wx.TextCtrl(self, name=label) else: txt = wx.TextCtrl(self, style=wx.TE_MULTILINE, name=label) sizer.Add(txt, 1, wx.ALL, 5) return sizer class DemoFrame(wx.Frame): \u0026quot;\u0026quot;\u0026quot; Frame that holds all other widgets \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Frame.__init__(self, None, wx.ID_ANY, \u0026quot;Py2Exe Tutorial\u0026quot;, size=(600,400) ) panel = DemoPanel(self) self.Show() if __name__ == \u0026quot;__main__\u0026quot;: app = wx.App(False) frame = DemoFrame() app.MainLoop() Если вы выполните команду pyinstaller против этого сценария, вы увидите, что на экран будет выводиться все больше данных. Будет создано 23 файла общим размером 19,4 МБ. Вы также заметите, что когда вы запускаете sampleApp.exe, он показывает консольное окно в дополнение к вашему графическому интерфейсу, а это не то, что мы хотим. Самый простой способ исправить это - вызвать PyInstaller с командой -w, которая говорит PyInstaller подавить консольное окно:
pyinstaller -w sampleApp.py Пакет PyInstaller имеет множество опций командной строки, которые вы можете использовать, чтобы изменить способ обработки PyInstaller вашей программы. Каждый раз, когда вы запускаете PyInstaller, он создает файл spec, который он использует для обработки вашей программы. Если вы хотите сохранить копию файла спецификации, чтобы лучше понять, что делает PyInstaller, вы можете сделать это с помощью следующей команды:
pyi-makespec sampleApp.py Вы можете передать pyi-makespec те же команды, что и PyInstaller, который изменит спецификацию соответствующим образом. Вот содержимое спецификации, созданной с помощью предыдущей команды:
# -*- mode: python -*- a = Analysis(['sampleApp.py'], pathex=['c:\\\\py101\\\\wxpy'], hiddenimports=[], hookspath=None, runtime_hooks=None) pyz = PYZ(a.pure) exe = EXE(pyz, a.scripts, exclude_binaries=True, name='sampleApp.exe', debug=False, strip=None, upx=True, console=False ) coll = COLLECT(exe, a.binaries, a.zipfiles, a.datas, strip=None, upx=True, name='sampleApp') В ранних версиях PyInstaller вы должны были создать файл спецификации и редактировать его напрямую. Теперь, если вам не нужно что-то действительно особенное, вы можете сгенерировать нужную спецификацию, просто используя флаги. Обязательно прочитайте документацию для получения полной информации, так как флагов много и их описание выходит за рамки этой главы.
Подведение итогов На этом наш краткий экскурс по PyInstaller закончен. Я надеюсь, что вы нашли это полезным в ваших начинаниях по созданию бинарных файлов Python. Проект PyInstaller довольно хорошо задокументирован и стоит того, чтобы потратить на него свое время.
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter43_pyinstaller/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day43/":{title:"43. Установка Docker",tags:["devops"],content:`Что такое Docker и его установка В предыдущей статье я хотя бы раз упомянул Docker, и это потому, что Docker действительно является новатором в создании популярности контейнеров, несмотря на то, что они существуют уже очень давно.
Здесь мы будем использовать и объяснять docker, но мы также должны упомянуть [Open Container Initiative (OCI)] (https://www.opencontainers.org/), которая является организацией по отраслевым стандартам, поощряющей инновации и избегающей опасности блокировки поставщиков. Благодаря OCI у нас есть выбор при выборе инструментария для контейнеров, включая Docker, CRI-O, Podman, LXC и другие.
Docker - это программная среда для создания, запуска и управления контейнерами. Термин \u0026ldquo;docker\u0026rdquo; может относиться как к инструментам (командам и демону), так и к формату файлов Dockerfile.
Мы будем использовать Docker Personal, который является бесплатным (для образования и обучения). Он включает в себя все самое необходимое, что нам нужно для получения хорошего фундамента знаний о контейнерах и инструментах.
Возможно, стоит разделить некоторые инструменты \u0026ldquo;docker\u0026rdquo;, которые мы будем использовать и для чего они нужны. Термин docker может относиться к проекту docker в целом, который является платформой для разработчиков и администраторов для разработки, доставки и запуска приложений. Также это может быть ссылка на процесс docker daeemon, запущенный на хосте, который управляет образами и контейнерами и называется Docker Engine.
Docker Engine Docker Engine - это технология контейнеризации с открытым исходным кодом для создания и контейнеризации приложений. Docker Engine действует как клиент-серверное приложение:
Сервер с долго работающим процессом-демоном dockerd. API, определяющие интерфейсы, которые программы могут использовать для общения и обучения демона Docker. Клиент docker с интерфейсом командной строки (CLI). Вышеизложенное было взято из официальной документации Docker и конкретного Docker Engine Overview
Docker Desktop У нас есть рабочий стол docker для систем Windows и macOS. Простая в установке, легковесная среда разработки docker. Нативное приложение для ОС, использующее возможности виртуализации на хостовой операционной системе.
Это лучшее решение, если вы хотите создавать, отлаживать, тестировать, упаковывать и отправлять Docker-приложения на Windows или macOS.
На Windows мы также можем воспользоваться преимуществами WSL2 и Microsoft Hyper-V. Мы рассмотрим некоторые преимущества WSL2 по ходу дела.
Благодаря интеграции с возможностями гипервизора на хостовой операционной системе docker предоставляет возможность запускать ваши контейнеры с операционными системами Linux.
Docker Compose Docker compose - это инструмент, позволяющий запускать более сложные приложения в нескольких контейнерах. Преимуществом является возможность использования одного файла и команды для запуска приложения.
Docker Hub Централизованный ресурс для работы с Docker и его компонентами. Чаще всего он известен как реестр для размещения образов Docker. Но здесь есть множество дополнительных сервисов, которые можно использовать для автоматизации или интеграции в GitHub, а также для сканирования безопасности.
Dockerfile Dockerfile - это текстовый файл, содержащий команды, которые обычно выполняются вручную для создания образа docker. Docker может собирать образы автоматически, читая инструкции, которые содержатся в нашем dockerfile.
Установка Docker Desktop Документация docker documenation просто потрясающая, и если вы только начинаете в нее погружаться, то вам стоит ее просмотреть и прочитать. Мы будем использовать Docker Desktop на Windows с WSL2. Я уже выполнил установку на своей машине, которую мы используем здесь.
Обратите внимание перед установкой на системные требования, Install Docker Desktop on Windows, если вы используете macOS, включая архитектуру процессора на базе M1, вы также можете взглянуть на Install Docker Desktop on macOS.
Я проведу установку Docker Desktop для Windows на другой машине Windows и запишу процесс ниже.
Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day43/"},"https://romankurnovskii.com/ru/docs/python101/chapter44_creating_an_installer/":{title:"44. Создание программы установки",tags:[],content:`В этой главе мы проведем вас через процесс создания исполняемого файла и его последующей упаковки в программу установки. Для создания исполняемого файла мы будем использовать очень аккуратный пользовательский интерфейс GUI2Exe, написанный Андреа Гаваной. Он основан на wxPython, поэтому для его использования вам потребуется его установить. GUI2Exe поддерживает py2exe, bbfreeze, cx_Freeze, PyInstaller и py2app. После создания папки dist мы используем Inno Setup для создания нашего инсталлятора.
Мы снова будем использовать следующий код:
# sampleApp.py import wx class DemoPanel(wx.Panel): \u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot; def __init__(self, parent): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Panel.__init__(self, parent) labels = [\u0026quot;Name\u0026quot;, \u0026quot;Address\u0026quot;, \u0026quot;City\u0026quot;, \u0026quot;State\u0026quot;, \u0026quot;Zip\u0026quot;, \u0026quot;Phone\u0026quot;, \u0026quot;Email\u0026quot;, \u0026quot;Notes\u0026quot;] mainSizer = wx.BoxSizer(wx.VERTICAL) lbl = wx.StaticText(self, label=\u0026quot;Please enter your information here:\u0026quot;) lbl.SetFont(wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD)) mainSizer.Add(lbl, 0, wx.ALL, 5) for lbl in labels: sizer = self.buildControls(lbl) mainSizer.Add(sizer, 1, wx.EXPAND) self.SetSizer(mainSizer) mainSizer.Layout() def buildControls(self, label): \u0026quot;\u0026quot;\u0026quot; Put the widgets together \u0026quot;\u0026quot;\u0026quot; sizer = wx.BoxSizer(wx.HORIZONTAL) size = (80,40) font = wx.Font(12, wx.SWISS, wx.NORMAL, wx.BOLD) lbl = wx.StaticText(self, label=label, size=size) lbl.SetFont(font) sizer.Add(lbl, 0, wx.ALL|wx.CENTER, 5) if label != \u0026quot;Notes\u0026quot;: txt = wx.TextCtrl(self, name=label) else: txt = wx.TextCtrl(self, style=wx.TE_MULTILINE, name=label) sizer.Add(txt, 1, wx.ALL, 5) return sizer class DemoFrame(wx.Frame): \u0026quot;\u0026quot;\u0026quot; Frame that holds all other widgets \u0026quot;\u0026quot;\u0026quot; def __init__(self): \u0026quot;\u0026quot;\u0026quot;Constructor\u0026quot;\u0026quot;\u0026quot; wx.Frame.__init__(self, None, wx.ID_ANY, \u0026quot;Py2Exe Tutorial\u0026quot;, size=(600,400) ) panel = DemoPanel(self) self.Show() if __name__ == \u0026quot;__main__\u0026quot;: app = wx.App(False) frame = DemoFrame() app.MainLoop() Давайте начнем!
Начало работы с GUI2Exe Чтобы использовать GUI2Exe, нужно просто зайти на его сайт (http://code.google.com/p/gui2exe/) и скачать релиз. Затем распаковать его и запустить скрипт, который называется GUI2Exe.py. Проект GUI2Exe основан на wxPython, поэтому убедитесь, что он у вас тоже установлен. Я успешно запустил свой проект с wxPython 2.9. Вот как его можно вызвать:
python GUI2Exe.py При успешном выполнении вы должны увидеть экран, похожий на этот:
Теперь перейдите в меню Файл -\u0026gt; Новый проект и дайте проекту имя. В данном случае я назвал проект wxForm. Если вы хотите, вы можете добавить вымышленное название компании, авторские права и дать ему название программы. Не забудьте также найти ваш основной Python-скрипт (т.е. sampleApp.py). Согласно сайту Андреа, вы должны установить значение Optimize на 2, Compressed на 2 и Bundled Files на 1. В большинстве случаев это работает, но у меня было несколько ошибок, которые, похоже, возникли из-за установки последнего значения на 1. На самом деле, по словам одного из моих знакомых из списка рассылки py2exe, опция bundle должна быть установлена на 3, чтобы минимизировать ошибки. Приятно, что при установке bundle в \u0026ldquo;1\u0026rdquo; получается всего один файл, но поскольку я собираюсь свернуть его с Inno, я собираюсь использовать вариант 3, чтобы убедиться, что моя программа работает хорошо.
После того, как все будет сделано так, как вы хотите, нажмите кнопку Compile в правом нижнем углу. Это создаст все файлы, которые вы хотите распространять, в папке dist, если вы не изменили название, установив dist checkbox и отредактировав последующее текстовое поле. После завершения компиляции GUI2Exe спросит вас, хотите ли вы протестировать ваш исполняемый файл. Нажимайте \u0026ldquo;Yes\u0026rdquo;. Если вы получите какие-либо ошибки о недостающих модулях, вы можете добавить их в раздел Python Modules или Python Packages в зависимости от ситуации. В данном примере такой проблемы возникнуть не должно.
Теперь мы готовы к изучению создания программы установки!
Давайте создадим программу установки! Теперь, когда у нас есть исполняемый файл и куча зависимостей, как нам сделать программу установки? В этой главе мы будем использовать Inno Setup, но вы также можете использовать NSIS или фирменную программу установки Microsoft. Вам нужно будет перейти на их сайт (http://www.jrsoftware.org/isdl.php), скачать программу и установить ее. Затем запустите программу. Вы должны увидеть главную программу со следующим диалоговым окном поверх нее:
Выберите опцию Create a new script using the Script Wizard и нажмите кнопку OK. Нажмите кнопку Next, и вы должны увидеть что-то вроде этого:
Заполните его, как вам нравится, и нажмите кнопку Next (я назвал свой скрипт wxForm). На следующем экране вы можете выбрать место установки приложения по умолчанию. По умолчанию это *Program Files(), что вполне подходит. Нажмите кнопку Next. Теперь вы должны увидеть следующее окно:
Перейдите к созданному исполняемому файлу, чтобы добавить его. Затем нажмите кнопку **Add file(s)\u0026hellip; **, чтобы добавить остальные. Вы можете выбрать все файлы, кроме exe, и нажать OK. Вот как получилось у меня:
Теперь вы готовы нажать кнопку Next. Убедитесь, что папка меню Пуск имеет правильное имя (в данном случае wxForm) и продолжайте. Вы можете проигнорировать следующие два экрана или поэкспериментировать с ними, если хотите. Однако я не использую лицензию и не помещаю информационные файлы для отображения пользователю. Последний экран перед завершением работы позволяет выбрать каталог, в который будет помещен вывод. Я оставил это поле пустым, так как по умолчанию оно указывает на место расположения исполняемого файла, что вполне подходит для данного примера. Нажмите кнопку Next, Next и Finish. Будет создан полноценный файл .iss, который Inno Setup использует для превращения вашего приложения в программу установки. Он спросит вас, хотите ли вы продолжить компиляцию скрипта сейчас. Сделайте это. Затем он спросит, хотите ли вы сохранить сценарий в формате .iss. Это хорошая идея, поэтому сделайте и это. Надеюсь, вы не получили никаких ошибок и можете опробовать новую программу установки.
Если вам интересно узнать о языке сценариев Inno, не стесняйтесь читать документацию Inno. Вы можете сделать с его помощью довольно много. Если вы случайно внесли изменения в сценарий сборки, вы можете пересобрать программу установки, перейдя в меню build и выбрав пункт меню compile.
Подведение итогов Теперь вы знаете, как создать настоящий, живой инсталлятор, который можно использовать для установки вашего приложения и любых файлов, необходимых для его работы. Это особенно удобно, когда у вас есть множество пользовательских значков для панелей инструментов или база данных по умолчанию, файл конфигурации и т.д., которые нужно распространять вместе с приложением. Вернитесь и попробуйте создать программу установки снова, но выберите другие варианты, чтобы посмотреть, что еще можно сделать. Экспериментирование - отличный способ обучения. Только убедитесь, что у вас всегда есть резервная копия на случай, если что-то пойдет не так!
`,url:"https://romankurnovskii.com/ru/docs/python101/chapter44_creating_an_installer/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day44/":{title:"44. Установка образов Docker в Docker Desktop",tags:["devops"],content:`Образы Docker и практическая работа с Docker Desktop Теперь у нас в системе установлен Docker Desktop. (Если вы используете Linux, у вас все еще есть опции, но нет графического интерфейса, но docker, очевидно, работает на Linux)Install Docker Engine on Ubuntu (Другие дистрибутивы также доступны).
В этом посте мы собираемся начать с развертывания некоторых образов в нашей среде. Напомним, что такое образ Docker - образ Docker - это файл, используемый для выполнения кода в контейнере Docker. Образы Docker действуют как набор инструкций для создания контейнера Docker, как шаблон. Образы Docker также служат отправной точкой при использовании Docker.
Сейчас самое время пойти и создать свой аккаунт на DockerHub
DockerHub - это централизованный ресурс для работы с Docker и его компонентами. Наиболее известен как реестр для размещения образов докеров. Но здесь есть множество дополнительных сервисов, которые можно использовать для автоматизации или интеграции в GitHub, а также для сканирования безопасности.
Если вы прокрутите вниз после входа в систему, вы увидите список образов контейнеров, вы можете увидеть образы баз данных для mySQL, hello-world и т.д. и т.п. Рассматривайте их как отличные базовые образы, или вам может понадобиться просто образ базы данных, и вам лучше всего использовать официальный образ, что означает, что вам не нужно создавать свой собственный.
Мы можем углубиться в просмотр доступных изображений и осуществлять поиск по категориям, операционным системам и архитектурам. Единственное, что я выделил ниже, это Office Image, это должно дать вам уверенность в происхождении этого образа контейнера.
Мы также можем искать конкретное изображение, например, wordpress может быть хорошим базовым изображением, которое нам нужно, мы можем сделать это в верхней части и найти все изображения контейнеров, связанные с wordpress. Ниже обратите внимание, что у нас также есть проверенный издатель.
Официальные образы - Официальные образы Docker - это курируемый набор открытых исходных кодов Docker и репозиториев решений \u0026ldquo;drop-in\u0026rdquo;.
Проверенный издатель - высококачественный контент Docker от проверенных издателей. Эти продукты публикуются и поддерживаются непосредственно коммерческой организацией.
Изучение Docker Desktop У нас в системе установлен Docker Desktop, и если открыть его, то, если он у вас еще не установлен, вы увидите нечто похожее на изображение ниже. Как вы можете видеть, у нас нет запущенных контейнеров, но наш движок docker запущен.
Поскольку это была не свежая установка для меня, у меня есть некоторые изображения, которые уже загружены и доступны в моей системе. Скорее всего, здесь вы ничего не увидите.
В разделе удаленных репозиториев вы найдете все образы контейнеров, которые хранятся в вашем хабе docker. Ниже показано, что у меня нет никаких образов.
Мы также можем уточнить это на нашем сайте dockerhub и подтвердить, что у нас там нет репозиториев.
Далее у нас есть вкладка Volumes, если у вас есть контейнеры, которым требуется постоянство, то здесь мы можем добавить эти тома в вашу локальную файловую систему или общую файловую систему.
На момент написания статьи также существует вкладка Dev Environments, которая поможет вам сотрудничать с вашей командой вместо того, чтобы перемещаться между различными ветками git. Мы не будем ее рассматривать.
Вернувшись на первую вкладку, вы увидите, что там есть команда, которую мы можем запустить - это контейнер для запуска. Давайте запустим docker run -d -p 80:80 docker/getting-started в нашем терминале.
Если мы снова проверим окно рабочего стола docker, то увидим, что у нас есть запущенный контейнер.
Вы могли заметить, что я использую WSL2, и для того, чтобы вы могли использовать его, вам нужно убедиться, что он включен в настройках.
Если теперь мы снова перейдем на вкладку Images, вы должны увидеть используемый образ под названием docker/getting-started.
Вернитесь на вкладку Containers/Apps, нажмите на ваш запущенный контейнер. По умолчанию вы увидите журналы, а в верхней части есть несколько опций на выбор, в нашем случае я уверен, что это будет веб-страница, запущенная в этом контейнере, поэтому мы выберем опцию \u0026ldquo;Открыть в браузере\u0026rdquo;.
Когда мы нажмем на кнопку выше, конечно же, откроется веб-страница на вашем локальном хосте и отобразится что-то похожее на то, что показано ниже.
Этот контейнер также содержит более подробную информацию о том, что такое контейнеры и изображения.
Теперь мы запустили наш первый контейнер. Пока ничего страшного. А что если мы захотим вытащить один из образов контейнера из DockerHub? Может быть, там есть докер-контейнер hello world, который мы могли бы использовать.
Я остановил начальный контейнер, не то чтобы он занимал много ресурсов, но для аккуратности, пока мы проходим еще несколько шагов.
Вернемся в терминал и выполним команду docker run hello-world и посмотрим, что произойдет.
Вы можете видеть, что у нас не было локального образа, поэтому мы стянули его, а затем получили сообщение, записанное в образ контейнера, с информацией о том, что он сделал, чтобы запуститься, и некоторые ссылки на точки отсчета.
Однако, если мы посмотрим в Docker Desktop, у нас нет запущенных контейнеров, но есть вышедший контейнер, который использовал сообщение hello-world, то есть он появился, передал сообщение и затем завершился.
И в последний раз, давайте просто проверим вкладку images и увидим, что у нас есть новый образ hello-world локально в нашей системе, что означает, что если мы снова выполним команду docker run hello-world в нашем терминале, нам не придется ничего вытаскивать, если только версия не изменится.
В сообщении от контейнера hello-world была поставлена задача запустить что-то более амбициозное.
Вызов принят!
Запустив docker run -it ubuntu bash в нашем терминале, мы собираемся запустить контейнерную версию Ubuntu, а не полную копию операционной системы. Вы можете узнать больше об этом конкретном образе на DockerHub.
Вы можете видеть ниже, когда мы выполним команду, у нас появится интерактивная подсказка (-it) и мы запустим оболочку bash в нашем контейнере.
У нас есть оболочка bash, но у нас не так много больше, поэтому образ этого контейнера занимает менее 30 мб.
Но мы все еще можем использовать этот образ, и мы все еще можем установить программное обеспечение, используя наш менеджер пакетов apt, мы можем обновить наш образ контейнера и обновить также.
Или, может быть, мы хотим установить какое-то программное обеспечение в наш контейнер, я выбрал очень плохой пример, поскольку pinta - это редактор изображений, и его размер превышает 200мб, но, надеюсь, вы поняли, к чему я веду. Это значительно увеличит размер нашего контейнера, но все же мы будем находиться в мб, а не в гб.
Я хотел, чтобы вы получили общее представление о Docker Desktop и не таком уж страшном мире контейнеров, когда вы разбиваете его на простые сценарии использования, но нам нужно рассказать о некоторых сетевых возможностях, безопасности и других вариантах, которые у нас есть по сравнению с просто загрузкой образов контейнеров и их использованием таким образом. К концу раздела мы хотим создать что-то, загрузить в наш репозиторий DockerHub и иметь возможность развернуть это.
Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day44/"},"https://romankurnovskii.com/ru/docs/python101/chapter45_hosting/":{title:"45. Хостинг Python приложения",tags:[],content:" Как хостить телеграм-бота (и другие скрипты на Python) на Repl.it бесплатно 24/7 ",url:"https://romankurnovskii.com/ru/docs/python101/chapter45_hosting/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day45/":{title:"45. Что из себя представляет оьбраз Docker",tags:["devops"],content:`Анатомия образа Docker На прошлом занятии мы рассмотрели некоторые основы использования Docker Desktop в сочетании с DockerHub для развертывания и запуска некоторых проверенных образов. Вкратце о том, что такое образ, вы не забудете, если я продолжу упоминать.
Образ Docker - это шаблон, доступный только для чтения, содержащий набор инструкций для создания контейнера, который может работать на платформе Docker. Это удобный способ упаковки приложений и предварительно сконфигурированных серверных сред, которые вы можете использовать для личного пользования или публично делиться ими с другими пользователями Docker. Образы Docker также являются отправной точкой для тех, кто впервые использует Docker.
Что произойдет, если мы захотим создать свой собственный образ Docker? Для этого мы создадим Dockerfile. Вы видели, как мы могли взять образ контейнера Ubuntu и добавить наше программное обеспечение, и у нас получился образ контейнера с программным обеспечением, которое мы хотели, и все хорошо, но если этот контейнер выключить или выбросить, то все эти обновления и установки программного обеспечения пропадут, и не будет повторяющейся версии того, что мы сделали. Это отлично подходит для демонстрации возможностей, но не помогает при транспортировке образов в несколько сред с одним и тем же набором программного обеспечения, устанавливаемого при каждом запуске контейнера.
Что такое Dockerfile Dockerfile - это текстовый файл, содержащий команды, которые обычно выполняются вручную для создания образа docker. Docker может собирать образы автоматически, читая инструкции, содержащиеся в нашем dockerfile.
Каждый из файлов, составляющих образ docker, называется слоем. Эти слои образуют серию образов, поэтапно создаваемых друг над другом. Каждый слой зависит от слоя, расположенного непосредственно под ним. Порядок расположения слоев является ключевым фактором эффективности управления жизненным циклом образов docker.
Мы должны расположить слои, которые меняются чаще всего, как можно выше в стеке, потому что при внесении изменений в слой образа Docker перестраивает не только этот слой, но и все слои, созданные на его основе. Поэтому изменение слоя на самом верху требует наименьшего объема работы по пересборке всего образа.
Каждый раз, когда docker запускает контейнер из образа (как мы делали вчера), он добавляет слой, доступный для записи, известный как слой контейнера. В нем хранятся все изменения, вносимые в контейнер в течение всего времени его работы. Этот слой - единственное различие между работающим контейнером и исходным образом. Любое количество подобных контейнеров может иметь общий доступ к одному и тому же базовому образу, сохраняя при этом свое индивидуальное состояние.
Вернемся к примеру, который мы использовали вчера с образом Ubuntu. Мы можем выполнить одну и ту же команду несколько раз и на первый контейнер установить pinta, а на второй - figlet. Это два разных приложения, разного назначения, разного размера и т.д. и т.п.. Каждый контейнер, который мы установили, имеет один и тот же образ, но не одно и то же состояние, и это состояние исчезает, когда мы удаляем контейнер.
В приведенном выше примере используется образ Ubuntu, но также существует множество других готовых образов контейнеров, доступных на DockerHub и в других сторонних репозиториях. Эти образы обычно называют родительским образом. Это фундамент, на котором строятся все остальные слои, и базовые строительные блоки для наших контейнерных сред.
Наряду с набором отдельных файлов слоев, образ Docker также включает дополнительный файл, известный как манифест. Это, по сути, описание образа в формате JSON, содержащее такую информацию, как теги образа, цифровая подпись и подробные сведения о том, как настроить контейнер для различных типов хост-платформ.
Как создать образ docker Есть два способа создания образа docker. Мы можем сделать это на лету, используя процесс, который мы начали вчера, мы выбираем наш базовый образ, раскручиваем контейнер, устанавливаем все программное обеспечение и депенансы, которые мы хотим иметь на нашем контейнере.
Затем мы можем использовать команду docker commit container name, после чего у нас будет локальная копия этого образа в разделе docker images и на вкладке docker desktop images.
Супер просто, я бы не рекомендовал этот метод, если вы не хотите понять процесс, будет очень сложно управлять жизненным циклом таким образом и много ручной настройки/переконфигурации. Но это самый быстрый и простой способ создания образа docker. Отлично подходит для тестирования, устранения неполадок, проверки зависимостей и т.д.
Мы собираемся создать наш образ с помощью dockerfile. Это дает нам чистый, компактный и повторяемый способ создания образов. Намного проще управлять жизненным циклом и легко интегрировать в процессы непрерывной интеграции и непрерывной доставки. Но, как вы уже поняли, это немного сложнее, чем первый упомянутый процесс.
Использование метода dockerfile гораздо больше соответствует реальным развертываниям контейнеров корпоративного уровня.
Создание dockerfile - это трехэтапный процесс, в ходе которого вы создаете dockerfile и добавляете команды, необходимые для сборки образа.
В следующей таблице приведены некоторые из утверждений dockerfile, которые мы будем использовать или которые вы, скорее всего, будете использовать.
Команда Задача FROM Чтобы указать родительский образ WORKDIR Чтобы задать рабочий каталог для всех последующих команд в Dockerfile. RUN Для установки любых приложений и пакетов, необходимых для контейнера. COPY Для копирования файлов или каталогов из определенного места. ADD Как COPY, но также может работать с удаленными URL и распаковывать сжатые файлы. ENTRYPOINT Команда, которая всегда будет выполняться при запуске контейнера. Если она не указана, по умолчанию используется /bin/sh -c. Аргументы, передаваемые точке входа. Если ENTRYPOINT не задан (по умолчанию /bin/sh -c), .md будут командами, которые выполняет контейнер. EXPOSE Для определения порта, через который будет осуществляться доступ к вашему контейнерному приложению. LABEL Чтобы добавить метаданные к образу. Теперь у нас есть подробная информация о том, как создать наш первый dockerfile, мы можем создать рабочий каталог и создать наш dockerfile. Я создал рабочий каталог в этом репозитории, где вы можете увидеть файлы и папки, которые мне предстоит пройти. Containers
В этом каталоге я собираюсь создать файл .dockerignore, аналогичный .gitignore, который мы использовали в предыдущем разделе. В этом файле будут перечислены все файлы, которые могут быть созданы в процессе сборки Docker и которые вы хотите исключить из окончательной сборки.
Помните, что все, что связано с контейнерами, - это компактность, максимальная скорость и отсутствие лишнего объема.
Создадим простой Dockerfile с приведенной ниже схемой, которую также можно найти в папке по ссылке выше.
# Use the official Ubuntu 18.04 as base FROM ubuntu:18.04 # Install nginx and curl RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y RUN apt-get install -y nginx curl RUN rm -rf /var/lib/apt/lists/* Перейдите в этот каталог в терминале, а затем выполните команду docker build -t 90daysofdevops:0.1 . мы используем -t, а затем задаем имя и тег изображения.
Теперь, когда мы создали наш образ, мы можем запустить его с помощью Docker Desktop или командной строки docker. Я использовал Docker Desktop Я запустил контейнер, и вы можете видеть, что у нас есть curl, доступный нам в cli контейнера.
В Docker Desktop также есть возможность использовать пользовательский интерфейс для выполнения некоторых других задач с этим новым образом.
Мы можем проинспектировать наш образ, при этом очень хорошо виден dockerfile и строки кода, которые мы хотели запустить в нашем контейнере.
У нас есть опция pull, теперь она не работает, потому что это изображение нигде не размещено, поэтому мы получим ошибку. Однако у нас есть Push to hub, который позволит нам отправить наш образ на DockerHub.
Если вы используете ту же docker build, которую мы запустили ранее, то это тоже не сработает, вам понадобится команда сборки docker build -t {{username}}/{{imagename}}:{{version}}.
Если мы посмотрим на наш репозиторий DockerHub, то увидим, что мы только что выложили новый образ. Теперь в Docker Desktop мы сможем использовать эту вкладку pull.
Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started Blog on gettng started building a docker image Docker documentation for building an image `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day45/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day46/":{title:"46. Docker Compose",tags:["devops"],content:`Docker Compose Возможность запуска одного контейнера может быть отличной, если у вас есть самодостаточный образ, в котором есть все, что вам нужно для одного случая использования, но все становится интересным, когда вы ищете возможность создания нескольких приложений между различными образами контейнеров. Например, если у меня есть фронт-энд сайта, но есть потребность в базе данных бэкенда, я могу поместить все в один контейнер, но лучше и эффективнее было бы иметь собственный контейнер для базы данных.
Именно здесь на помощь приходит Docker compose - инструмент, позволяющий запускать более сложные приложения в нескольких контейнерах. Преимущество заключается в том, что для запуска приложения можно использовать один файл и команду. Пример, который я собираюсь рассмотреть в этой заметке, взят из [Docker QuickStart sample apps (Quickstart: Compose and WordPress)] (https://docs.docker.com/samples/wordpress/).
В этом первом примере мы собираемся:
Использовать Docker compose для создания WordPress и отдельного экземпляра MySQL. Использовать YAML файл, который будет называться docker-compose.yml. Соберите проект Настроить WordPress через браузер Выключение и очистка Установка Docker Compose Как уже упоминалось, Docker Compose - это инструмент, если вы работаете на macOS или Windows, то compose включен в вашу установку Docker Desktop. Однако вы можете захотеть запустить свои контейнеры на сервере Windows или Linux, и в этом случае вы можете установить их, используя эти инструкции Install Docker Compose.
Чтобы убедиться, что docker-compose установлен в нашей системе, мы можем открыть терминал и просто ввести приведенную выше команду.
Docker-Compose.yml (YAML) Следующее, о чем нужно поговорить, это docker-compose.yml, который вы можете найти в папке container репозитория. Но что более важно, нам нужно немного обсудить YAML в целом.
YAML можно было бы посвятить отдельную сессию, поскольку вы можете встретить его в самых разных местах. Но по большей части
\u0026ldquo;YAML - это удобный для человека язык сериализации данных для всех языков программирования\u0026rdquo;.
Он обычно используется для файлов конфигурации и в некоторых приложениях, где данные хранятся или передаются. Вы, несомненно, сталкивались с XML-файлами, которые обычно предлагают тот самый файл конфигурации. YAML предоставляет минимальный синтаксис, но нацелен на те же случаи использования.
YAML Ain\u0026rsquo;t Markup Language (YAML) - это язык сериализации, популярность которого неуклонно растет в течение последних нескольких лет. Возможности сериализации объектов делают его реальной заменой таким языкам, как JSON.
Аббревиатура YAML была сокращением от Yet Another Markup Language. Но сопровождающие переименовали его в YAML Ain\u0026rsquo;t Markup Language, чтобы сделать больший акцент на его функциях, ориентированных на данные.
В любом случае, вернемся к файлу docker-compose.yml. Это файл конфигурации того, что мы хотим сделать, когда речь идет о развертывании нескольких контейнеров на нашей единой системе.
Прямо из приведенного выше руководства вы можете увидеть, что содержимое файла выглядит следующим образом:
version: \u0026quot;3.9\u0026quot; services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - wordpress_data:/var/www/html ports: - \u0026quot;8000:80\u0026quot; restart: always environment: WORDPRESS_DB_HOST: db WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: db_data: {} wordpress_data: {} Мы объявляем версию, а затем большая часть этого файла docker-compose.yml состоит из наших служб, у нас есть служба db и служба wordpress. Вы можете видеть, что для каждого из них определено изображение, с которым связан тег версии. В отличие от наших первых прохождений, сейчас мы также вводим состояние в нашу конфигурацию, но теперь мы собираемся создать тома, чтобы мы могли хранить там наши базы данных.
Затем у нас есть некоторые переменные окружения, такие как пароли и имена пользователей. Очевидно, что эти файлы могут стать очень сложными, но конфигурационный файл YAML упрощает то, как они выглядят в целом.
Сборка проекта Далее мы можем вернуться в терминал и использовать некоторые команды с помощью нашего инструмента docker-compose. Перейдите в каталог, где находится ваш файл docker-compose.yml.
В терминале мы можем просто выполнить команду docker-compose up -d, которая запустит процесс извлечения образов и создания вашего многоконтейнерного приложения.
Символ -d в этой команде означает отделенный режим, что означает, что команда Run выполняется или будет выполняться в фоновом режиме.
Если теперь мы выполним команду docker ps, вы увидите, что у нас запущено 2 контейнера, один из которых - wordpress, а другой - mySQL.
Далее мы можем проверить, что у нас запущен WordPress, открыв браузер и перейдя по адресу http://localhost:8000, вы должны увидеть страницу установки wordpress.
Мы можем выполнить настройку WordPress, а затем начать создавать наш сайт по своему усмотрению в консоли ниже.
Если мы откроем новую вкладку и перейдем по тому же адресу, что и раньше http://localhost:8000, то увидим простую тему по умолчанию с названием нашего сайта \u0026ldquo;90DaysOfDevOps\u0026rdquo;, а затем образец поста.
Прежде чем мы сделаем какие-либо изменения, откройте Docker Desktop и перейдите на вкладку volumes, здесь вы увидите два тома, связанных с нашими контейнерами, один для wordpress и один для db.
Моя текущая тема для wordpress - \u0026ldquo;Twenty Twenty-Two\u0026rdquo;, и я хочу изменить ее на \u0026ldquo;Twenty Twenty\u0026rdquo; Вернувшись в панель управления, мы можем внести эти изменения.
Я также собираюсь добавить новый пост на свой сайт, и здесь ниже вы видите последнюю версию нашего нового сайта.
Очищать или нет Если мы сейчас используем команду docker-compose down, это приведет к остановке наших контейнеров. Но наши тома останутся на месте.
Мы можем просто подтвердить в Docker Desktop, что наши тома все еще там.
Если мы захотим вернуть все обратно, мы можем выполнить команду docker up -d из той же директории, и наше приложение снова будет запущено.
Затем мы переходим в браузере по тому же адресу http://localhost:8000 и замечаем, что наш новый пост и смена темы все еще на месте.
Если мы хотим избавиться от контейнеров и этих томов, то выполнение команды docker-compose down --volumes также уничтожит тома.
Теперь, когда мы снова используем docker-compose up -d, мы начнем все сначала, однако образы все еще будут локальными в нашей системе, поэтому вам не нужно будет повторно брать их из репозитория DockerHub.
Я знаю, что когда я начал погружаться в docker-compose и его возможности, я был в замешательстве относительно того, где он находится рядом с инструментами оркестровки контейнеров, такими как Kubernetes, ну, все, что мы сделали здесь в этой короткой демонстрации, сосредоточено на одном хосте, у нас есть wordpress и db, запущенные на локальной настольной машине. У нас нет нескольких виртуальных машин или нескольких физических машин, у нас также нет возможности легко увеличивать и уменьшать требования нашего приложения.
В следующем разделе мы рассмотрим Kubernetes, но сначала у нас есть еще несколько дней, посвященных контейнерам в целом.
Это также отличный ресурс для примеров приложений docker compose с множеством интеграций. Awesome-Compose.
В вышеупомянутом репозитории есть отличный пример, который развернет Elasticsearch, Logstash и Kibana (ELK) на одном узле.
Я загрузил файлы в папку Containers Когда у вас есть эта папка локально, перейдите туда и вы можете просто использовать docker-compose up -d.
Затем мы можем проверить наличие запущенных контейнеров с помощью docker ps.
Теперь мы можем открыть браузер для каждого из контейнеров:
Чтобы удалить все, мы можем использовать команду docker-compose down.
Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started Blog on gettng started building a docker image Docker documentation for building an image YAML Tutorial: Everything You Need to Get Started in Minute `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day46/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day47/":{title:"47. Сетевое взаимодействие Docker и безопасность",tags:["devops"],content:`Docker Networking \u0026amp; Security Во время этой сессии по контейнерам мы уже кое-что сделали, но не рассмотрели, как все работает за кулисами с точки зрения сетевых технологий, а также не затронули безопасность, поэтому мы планируем эту сессию.
Основы сетевого взаимодействия Docker Откройте терминал и введите команду docker network - это основная команда для настройки и управления сетями контейнеров.
Ниже показано, как мы можем использовать эту команду и все доступные подкоманды. Мы можем создавать новые сети, составлять список существующих, проверять и удалять сети.
Давайте посмотрим на существующие сети, которые у нас есть с момента установки, поэтому из коробки Docker networking выглядит как использование команды docker network list.
Каждая сеть получает уникальный ID и NAME. Каждая сеть также связана с одним драйвером. Обратите внимание, что сеть \u0026ldquo;bridge\u0026rdquo; и сеть \u0026ldquo;host\u0026rdquo; имеют те же имена, что и их соответствующие драйверы.
Далее мы можем более детально рассмотреть наши сети с помощью команды docker network inspect.
Запустив команду docker network inspect bridge, я могу получить все детали конфигурации конкретного имени сети. Сюда входят имя, ID, драйверы, подключенные контейнеры и, как вы можете видеть, многое другое.
Docker: Bridge Networking Как вы видели выше, стандартная установка Docker Desktop дает нам предварительно созданную сеть под названием bridge Если вы обратитесь к команде docker network list, то увидите, что сеть под названием bridge связана с драйвером bridge. То, что у них одинаковое имя, не означает, что это одно и то же. Связаны, но не одно и то же.
Вывод выше также показывает, что сеть bridge имеет локальную привязку. Это означает, что сеть существует только на этом хосте Docker. Это справедливо для всех сетей, использующих драйвер моста - драйвер моста обеспечивает работу сети на одном хосте.
Все сети, созданные с помощью драйвера моста, основаны на мосте Linux (он же виртуальный коммутатор).
Подключение контейнера По умолчанию новым контейнерам назначается сеть bridge, то есть, если вы не укажете сеть, все контейнеры будут подключены к сети bridge.
Давайте создадим новый контейнер командой docker run -dt ubuntu sleep infinity.
Команда sleep выше просто будет поддерживать работу контейнера в фоновом режиме, чтобы мы могли возиться с ним.
Если мы затем проверим нашу сеть моста с помощью docker network inspect bridge, вы увидите, что у нас есть контейнер, соответствующий тому, что мы только что развернули, потому что мы не указали сеть.
Мы также можем погрузиться в контейнер, используя docker exec -it 3a99af449ca2 bash, вам придется использовать docker ps, чтобы получить идентификатор контейнера.
Отсюда наш образ не имеет ничего для пинга, поэтому нам нужно выполнить следующую команду.apt-get update \u0026amp;\u0026amp; apt-get install -y iputils-ping затем пингуем внешний адрес интерфеса. ping -c5 www.90daysofdevops.com
Чтобы устранить эту проблему, мы можем запустить docker stop 3a99af449ca2 и снова использовать docker ps для поиска ID вашего контейнера, но это приведет к удалению нашего контейнера.
Настройте NAT для внешнего подключения На этом шаге мы запустим новый контейнер NGINX и назначим порт 8080 на хосте Docker на порт 80 внутри контейнера. Это означает, что трафик, поступающий на хост Docker по порту 8080, будет передаваться на порт 80 внутри контейнера.
Запустите новый контейнер на основе официального образа NGINX, выполнив команду docker run --name web1 -d -p 8080:80 nginx.
Просмотрите состояние контейнера и сопоставление портов, выполнив команду docker ps.
Верхняя строка показывает новый контейнер web1, запущенный NGINX. Обратите внимание на команду, которую запускает контейнер, а также на сопоставление портов - 0.0.0.0:8080-\u0026gt;80/tcp сопоставляет порт 8080 на всех интерфейсах хоста с портом 80 внутри контейнера web1. Это сопоставление портов делает веб-сервис контейнера доступным из внешних источников (через IP-адрес хоста Docker на порту 8080).
Теперь нам нужен IP-адрес нашего реального хоста, мы можем сделать это, зайдя в терминал WSL и используя команду ip addr.
Затем мы можем взять этот IP, открыть браузер и перейти по адресу http://172.25.218.154:8080/ Ваш IP может быть другим. Это подтверждает, что NGINX доступен.
Я взял эти инструкции с этого сайта с далекого 2017 DockerCon, но они актуальны и сегодня. Однако остальная часть руководства посвящена Docker Swarm, и я не собираюсь рассматривать его здесь. Docker Networking - DockerCon 2017
Обеспечение безопасности контейнеров Контейнеры обеспечивают безопасную среду для рабочих нагрузок по сравнению с полной конфигурацией сервера. Они позволяют разбить ваши приложения на более мелкие, слабо связанные компоненты, изолированные друг от друга, что помогает уменьшить поверхность атаки в целом.
Но они не застрахованы от хакеров, которые хотят использовать системы в своих целях. Нам по-прежнему необходимо понимать подводные камни безопасности этой технологии и придерживаться лучших практик.
Откажитесь от прав root Все контейнеры, которые мы развернули, использовали права root для процессов внутри контейнеров. Это означает, что они имеют полный административный доступ к вашим контейнерам и хост-средам. Теперь для целей прохождения мы знали, что эти системы не будут работать долго. Но вы видели, как легко их запустить.
Мы можем добавить несколько шагов к нашему процессу, чтобы дать возможность не root-пользователям быть предпочтительной лучшей практикой. При создании нашего dockerfile мы можем создать учетные записи пользователей. Вы можете найти этот пример также в папке containers в репозитории.
# Используем официальную версию Ubuntu 18.04 в качестве базовой FROM ubuntu:18.04 RUN apt-get update \u0026amp;\u0026amp; apt-get upgrade -y RUN groupadd -g 1000 basicuser \u0026amp;\u0026amp; useradd -r -u 1000 -g basicuser basicuser пользователь basicuser Мы также можем использовать docker run --user 1009 ubuntu Команда Docker run переопределяет любого пользователя, указанного в вашем Dockerfile. Поэтому в следующем примере ваш контейнер всегда будет запускаться с наименьшими привилегиями при условии, что идентификатор пользователя 1009 также имеет самый низкий уровень прав.
Однако этот метод не устраняет основной недостаток безопасности самого образа. Поэтому лучше указать в Dockerfile пользователя, не являющегося root, чтобы ваши контейнеры всегда запускались безопасно.
Частный репозитории Еще одна область, которую мы активно используем, - это публичные реестры в DockerHub, а частный реестр образов контейнеров, созданный вашей организацией, означает, что вы можете размещать их там, где пожелаете, или же для этого существуют управляемые сервисы, но в целом это дает вам полный контроль над образами, доступными для вас и вашей команды.
DockerHub отлично подходит для создания базового уровня, но он предоставляет только базовый сервис, где вам придется во многом доверять издателю образа.
Lean \u0026amp; Clean Мы уже упоминали об этом, хотя это и не связано с безопасностью. Но размер вашего контейнера также может влиять на безопасность с точки зрения поверхности атаки, если у вас есть ресурсы, которые вы не используете в своем приложении, то они не нужны в вашем контейнере.
Это также является моей основной проблемой при использовании последних образов, потому что это может принести много лишнего в ваши образы. DockerHub показывает сжатый размер для каждого образа в хранилище.
docker image - отличная команда для просмотра размера ваших образов.
Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started Blog on gettng started building a docker image Docker documentation for building an image YAML Tutorial: Everything You Need to Get Started in Minute `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day47/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day48/":{title:"48. Альтернативы Docker",tags:["devops"],content:`Альтернативы Docker В самом начале этого раздела я говорил, что мы будем использовать Docker, просто потому, что ресурсов очень много, а сообщество очень большое, но также именно с него начался толчок к популярности контейнеров. Я бы посоветовал вам пойти и посмотреть немного истории о Docker и о том, как он появился, я нашел это очень полезным.
Но, как я уже упоминал, существуют и другие альтернативы Docker. Если мы подумаем о том, что такое Docker и что мы уже рассмотрели. Это платформа для разработки, тестирования, развертывания и управления приложениями.
Я хочу выделить несколько альтернатив Docker, которые вы можете увидеть или увидите в будущем.
Podman Что такое Podman? Podman - это контейнерный движок без демонов для разработки, управления и запуска OCI-контейнеров в вашей системе Linux. Контейнеры могут быть запущены от имени root или в режиме rootless.
Я буду рассматривать это с точки зрения Windows, но знаю, что, как и в случае с Docker, здесь не требуется виртуализация, поскольку он будет использовать базовую ОС, чего нельзя сделать в мире Windows.
Podman может быть запущен под WSL2, хотя и не так гладко, как в случае с Docker Desktop. Существует также удаленный клиент Windows, с помощью которого можно подключиться к виртуальной машине Linux, где будут запущены ваши контейнеры.
Мой Ubuntu на WSL2 - это версия 20.04. Следуя следующим шагам, вы сможете установить Podman на свой экземпляр WSL.
echo \u0026quot;deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_20.04/ /\u0026quot; | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list Добавим ключ GPG
curl -L \u0026quot;https://download.opensuse.org/repositories/devel:/kubic:\\ /libcontainers:/stable/xUbuntu_20.04/Release.key\u0026quot; | sudo apt-key add - Запустите обновление системы с помощью команды sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade. Наконец, мы можем установить podman с помощью команды sudo apt install podman.
Теперь мы можем использовать многие из тех же команд, которые мы использовали для docker, однако обратите внимание, что у нас нет красивого пользовательского интерфейса рабочего стола docker. Вы можете видеть ниже, я использовал podman images и у меня ничего не появилось после установки, затем я использовал podman pull ubuntu для извлечения образа контейнера ubuntu.
Затем мы можем запустить наш образ Ubuntu с помощью podman run -dit ubuntu и podman ps, чтобы увидеть наш запущенный образ.
Чтобы попасть в этот контейнер, мы можем выполнить команду podman attach dazzling_darwin, имя вашего контейнера, скорее всего, будет другим.
Если вы переходите от docker к podman, то обычно также необходимо изменить ваш конфигурационный файл на alias docker=podman, тогда любая команда, запущенная с помощью docker, будет использовать podman.
LXC LXC - это механизм контейнеризации, который позволяет пользователям снова создавать несколько изолированных контейнерных сред Linux. В отличие от Docker LXC действует как гипервизор для создания нескольких Linux-машин с отдельными системными файлами, сетевыми функциями. Появился еще до Docker, а затем сделал короткое возвращение из-за недостатков Docker.
LXC такой же легкий, как и docker, и легко развертывается.
Containerd Автономная среда выполнения контейнеров. Containerd обеспечивает простоту и надежность, а также, конечно, переносимость. Ранее Containerd был инструментом, работающим как часть контейнерных сервисов Docker, пока Docker не решил вывести свои компоненты в самостоятельные.
Проект в Cloud Native Computing Foundation, что ставит его в один ряд с такими популярными контейнерными инструментами, как Kubernetes, Prometheus и CoreDNS.
Другие инструменты Docker Мы могли бы также упомянуть инструменты и опции вокруг Rancher, VirtualBox, но мы можем рассказать о них более подробно в другой раз.
Gradle
Сканирование сборки позволяет командам совместно отлаживать свои скрипты и отслеживать историю всех сборок. Опции выполнения дают командам возможность непрерывной сборки так, чтобы при каждом вводе изменений задание выполнялось автоматически. Настраиваемый макет репозитория дает командам возможность рассматривать любую структуру файловых каталогов как хранилище артефактов. Packer
Возможность параллельного создания нескольких машинных образов для экономии времени разработчиков и повышения эффективности. Команды могут легко отлаживать сборки с помощью отладчика Packer, который проверяет сбои и позволяет командам опробовать решения перед перезапуском сборки. Поддержка многих платформ с помощью плагинов, что позволяет командам настраивать свои сборки. Logspout
Инструмент для ведения логов - настраиваемость инструмента позволяет командам отправлять одни и те же логи в несколько мест назначения. Команды могут легко управлять своими файлами, поскольку инструмент требует только доступа к сокету Docker. Полностью с открытым исходным кодом и прост в развертывании. Logstash
Настройте свой конвейер с помощью подключаемой структуры Logstash. Легко анализируйте и преобразуйте данные для анализа и повышения ценности бизнеса. Разнообразие выходов Logstash позволяет направлять данные туда, куда вам нужно. Portainer
Используйте готовые шаблоны или создавайте свои собственные для развертывания приложений. Создавайте команды и назначайте роли и разрешения для членов команды. Узнайте, что запущено в каждой среде, используя приборную панель инструмента. Ресурсы TechWorld with Nana - Docker Tutorial for Beginners Programming with Mosh - Docker Tutorial for Beginners Docker Tutorial for Beginners - What is Docker? Introduction to Containers WSL 2 with Docker getting started Blog on gettng started building a docker image Docker documentation for building an image YAML Tutorial: Everything You Need to Get Started in Minute Podman | Daemonless Docker | Getting Started with Podman LXC - Guide to building a LXC Lab `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day48/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day49/":{title:"49. Основы Kubernetes",tags:["devops"],content:`Общая картина: Kubernetes В предыдущем разделе мы рассмотрели контейнеры. Контейнеры не справляются с задачей масштабирования и оркестровки. Лучшее, что мы можем сделать, это использовать docker-compose для объединения нескольких контейнеров. Когда речь заходит о Kubernetes, который является оркестратором контейнеров, это дает нам возможность масштабирования в автоматическом режиме или в зависимости от нагрузки ваших приложений и сервисов.
Как платформа Kubernetes предлагает возможность оркестровки контейнеров в соответствии с вашими требованиями и желаемым состоянием. Мы рассмотрим Kubernetes в этом разделе, поскольку она быстро развивается как следующая волна инфраструктуры. С точки зрения DevOps, Kubernetes - это лишь одна из платформ, базовое понимание которой вам понадобится. Вам также потребуется понимание \u0026ldquo;голого металла\u0026rdquo;, виртуализации и, скорее всего, облачных сервисов. Kubernetes - это просто еще один вариант запуска наших приложений.
Что такое оркестровка контейнеров? Я упомянул Kubernetes и упомянул оркестровку контейнеров, Kubernetes - это технология, а оркестровка контейнеров - это концепция или процесс, стоящий за технологией. Kubernetes - не единственная платформа для оркестровки контейнеров, у нас также есть Docker Swarm, HashiCorp Nomad и другие. Но Kubernetes набирает силу, поэтому я хочу рассказать о Kubernetes, но хочу сказать, что она не единственная.
Что такое Kubernetes? Первое, что вам следует прочитать, если вы новичок в Kubernetes, - это официальная документация. Мой опыт глубокого погружения в Kubernetes чуть больше года назад показал, что это будет крутая кривая обучения. Будучи выходцем из сферы виртуализации и хранения данных, я думал о том, насколько пугающим это кажется.
Но на самом деле сообщество, бесплатные учебные ресурсы и документация просто потрясающие. Kubernetes.io
Kubernetes - это портативная, расширяемая платформа с открытым исходным кодом для управления контейнерными рабочими нагрузками и сервисами, которая облегчает как декларативную конфигурацию, так и автоматизацию. Она имеет большую, быстро развивающуюся экосистему. Услуги, поддержка и инструменты Kubernetes широко доступны.
Важные моменты, которые следует отметить из вышеприведенного цитаты: Kubernetes является открытым исходным кодом с богатой историей, восходящей к Google, который передал проект в фонд Cloud Native computing Foundation (CNCF), и в настоящее время он развивается сообществом открытого исходного кода, а также крупными корпоративными поставщиками, которые внесли свой вклад, чтобы сделать Kubernetes тем, чем он является сегодня.
Я уже упоминал, что контейнеры - это здорово, и в предыдущем разделе мы говорили о том, как контейнеры и образы контейнеров изменили и ускорили внедрение облачных нативных систем. Но сами по себе контейнеры не дадут вам готового к производству опыта, который необходим вашему приложению. Kubernetes дает нам следующее:
Обнаружение сервисов и балансировка нагрузки Kubernetes может открыть контейнер, используя DNS-имя или собственный IP-адрес. Если трафик на контейнер высок, Kubernetes может сбалансировать нагрузку и распределить сетевой трафик так, чтобы развертывание было стабильным.
Оркестровка хранилищ Kubernetes позволяет автоматически монтировать системы хранения по вашему выбору, например, локальные хранилища, общедоступные облачные провайдеры и многое другое.
Автоматизированное развертывание и откат Вы можете описать желаемое состояние для развернутых контейнеров с помощью Kubernetes, и он может изменить фактическое состояние на желаемое с контролируемой скоростью. Например, вы можете автоматизировать Kubernetes для создания новых контейнеров для развертывания, удаления существующих контейнеров и переноса всех их ресурсов в новый контейнер.
Автоматическая упаковка контейнеров Вы предоставляете Kubernetes кластер узлов, которые он может использовать для выполнения контейнерных задач. Вы сообщаете Kubernetes, сколько процессора и памяти (RAM) требуется каждому контейнеру. Kubernetes может разместить контейнеры на ваших узлах, чтобы наилучшим образом использовать ваши ресурсы.
Самовосстановление Kubernetes перезапускает вышедшие из строя контейнеры, заменяет контейнеры, уничтожает контейнеры, которые не отвечают на заданную пользователем проверку работоспособности, и не рекламирует их клиентам, пока они не будут готовы к обслуживанию.
Управление секретами и конфигурациями Kubernetes позволяет хранить и управлять конфиденциальной информацией, такой как пароли, токены OAuth и ключи SSH. Вы можете развертывать и обновлять секреты и конфигурацию приложений, не перестраивая образы контейнеров и не раскрывая секреты в конфигурации стека.
Kubernetes предоставляет вам основу для отказоустойчивого запуска распределенных систем.
Container Orchestration управляет развертыванием, размещением и жизненным циклом контейнеров.
На нее также возложено множество других обязанностей:
Управление кластером объединяет узлы в одну цель.
Управление расписанием распределяет контейнеры по узлам с помощью планировщика.
Обнаружение сервисов знает, где находятся контейнеры, и распределяет между ними запросы клиентов.
Репликация обеспечивает наличие необходимого количества узлов и контейнеров для требуемой рабочей нагрузки.
Управление здоровьем обнаруживает и заменяет нездоровые контейнеры и узлы.
Основные компоненты Kubernetes Kubernetes - это контейнерный оркестратор для обеспечения, управления и масштабирования приложений. Вы можете использовать его для управления жизненным циклом контейнерных приложений в кластере узлов, который представляет собой набор рабочих машин, таких как виртуальные машины или физические машины.
Для работы вашим приложениям может понадобиться множество других ресурсов, таких как тома, сети и секреты, которые помогут вам подключаться к базам данных, общаться с бэкграундом и защищать ключи. С помощью Kubernetes вы можете добавить эти ресурсы в свое приложение. Инфраструктурные ресурсы, необходимые вашим приложениям, управляются декларативно.
Ключевой парадигмой Kubernetes является ее декларативная модель. Вы предоставляете нужное вам состояние, а Kubernetes его реализует. Если вам нужно пять экземпляров, вы не запускаете пять отдельных экземпляров самостоятельно. Вместо этого вы сообщаете Kubernetes, что вам нужно пять экземпляров, и Kubernetes автоматически согласовывает состояние. Если с одним из ваших экземпляров что-то пойдет не так и он выйдет из строя, Kubernetes все равно будет знать нужное вам состояние и создаст экземпляры на доступном узле.
Узел План управления
Каждый кластер Kubernetes требует наличия узла Control Plane, компоненты которого принимают глобальные решения относительно кластера (например, планирование), а также обнаруживают и реагируют на события кластера.
Рабочий узел Рабочая машина, на которой выполняются рабочие нагрузки Kubernetes. Это может быть физическая (bare metal) машина или виртуальная машина (VM). На каждом узле может размещаться один или несколько стручков. Узлы Kubernetes управляются плоскостью управления
Существуют и другие типы узлов, но я не буду их здесь рассматривать.
kubelet
Агент, который запускается на каждом узле кластера. Он следит за тем, чтобы контейнеры запускались в Pod.
Куплет принимает набор PodSpecs, которые предоставляются через различные механизмы, и гарантирует, что контейнеры, описанные в этих PodSpecs, запущены и здоровы. Куплет не управляет контейнерами, которые не были созданы Kubernetes.
kube-proxy
kube-proxy - это сетевой прокси, который работает на каждом узле вашего кластера, реализуя часть концепции Kubernetes Service.
kube-proxy поддерживает сетевые правила на узлах. Эти сетевые правила позволяют сетевое взаимодействие с вашими Pods из сетевых сессий внутри или вне вашего кластера.
kube-proxy использует уровень фильтрации пакетов операционной системы, если он есть и доступен. В противном случае kube-proxy сам перенаправляет трафик.
Время выполнения контейнера
Время выполнения контейнеров - это программное обеспечение, которое отвечает за запуск контейнеров.
Kubernetes поддерживает несколько сред выполнения контейнеров: Docker, containerd, CRI-O и любую реализацию Kubernetes CRI (Container Runtime Interface).
​
Кластер Кластер - это группа узлов, где узлом может быть физическая машина или виртуальные машины. На каждом из узлов будет установлена среда выполнения контейнеров (Docker), а также будет запущен сервис kubelet, который является агентом, принимающим команды от главного контроллера (подробнее об этом позже), и прокси, который используется для прокси-соединений с Pods от другого компонента (сервисы, которые мы рассмотрим позже).
На нашей плоскости управления, которую можно сделать высокодоступной, будет несколько уникальных ролей по сравнению с рабочими узлами, самой важной из них будет сервер kube API, именно с ним будет происходить любое взаимодействие для получения информации или отправки информации в наш кластер Kubernetes.
Kube API-Server
Сервер API Kubernetes проверяет и настраивает данные для объектов api, которые включают стручки, сервисы, контроллеры репликации и другие. API-сервер обслуживает REST-операции и предоставляет фронтенд к общему состоянию кластера, через который взаимодействуют все остальные компоненты.
Планировщик
Планировщик Kubernetes - это процесс в плоскости управления, который назначает Pods узлам. Планировщик определяет, какие узлы являются допустимыми для размещения каждого Pod в очереди планирования в соответствии с ограничениями и доступными ресурсами. Затем планировщик ранжирует каждый допустимый узел и привязывает Pod к подходящему узлу.
Менеджер контроллера
Менеджер контроллеров Kubernetes - это демон, который встраивает основные контуры управления, поставляемые с Kubernetes. В приложениях робототехники и автоматизации контур управления - это не завершающийся цикл, который регулирует состояние системы. В Kubernetes контроллер - это контур управления, который следит за общим состоянием кластера через apiserver и вносит изменения, пытаясь переместить текущее состояние в желаемое.
etcd.
Последовательное и высокодоступное хранилище значений ключей, используемое в качестве резервного хранилища Kubernetes для всех данных кластера.
kubectl
Для управления этим с точки зрения CLI у нас есть kubectl, kubectl взаимодействует с сервером API.
Инструмент командной строки Kubernetes, kubectl, позволяет выполнять команды для кластеров Kubernetes. Вы можете использовать kubectl для развертывания приложений, проверки и управления ресурсами кластера, а также для просмотра журналов.
Pods Pod - это группа контейнеров, которые образуют логическое приложение. Например, если у вас есть веб-приложение, в котором запущен контейнер NodeJS, а также контейнер MySQL, то оба этих контейнера будут находиться в одном Pod. Pod также может иметь общие тома данных, а также разделять одно и то же сетевое пространство имен. Помните, что Pods являются эфемерными и могут быть подняты и опущены главным контроллером. Kubernetes использует простое, но эффективное средство идентификации Pods с помощью концепции Labels (имя - значения).
Подсистемы управляют томами, секретами и конфигурацией контейнеров.
Подсистемы являются эфемерными. Они предназначены для автоматического перезапуска после смерти.
Pods реплицируются при горизонтальном масштабировании приложения с помощью ReplicationSet. Каждый Pod будет выполнять один и тот же код контейнера.
Pods живут на рабочих узлах (Worker Nodes).
Развертывания Вы можете просто решить запустить Pods, но когда они умирают, они умирают.
Развертывание позволит вашему стручку работать непрерывно.
Развертывания позволяют вам обновлять работающее приложение без простоя.
Развертывания также определяют стратегию перезапуска стручков, когда они умирают
ReplicaSets Развертывание также может создать набор реплик.
ReplicaSet гарантирует, что ваше приложение имеет необходимое количество Pods.
ReplicaSets будет создавать и масштабировать Pods на основе развертывания
Развертывание, наборы реплик, подсистемы не являются исключительными, но могут быть
StatefulSets Требуется ли вашему приложению хранить информацию о его состоянии?
База данных нуждается в состоянии
Подсистемы StatefulSet не являются взаимозаменяемыми.
Каждый Pod имеет уникальный постоянный идентификатор, который контроллер сохраняет при любом перепланировании.
Каждый Pod имеет уникальный, постоянный идентификатор, который контроллер сохраняет при любом перепланировании.
DaemonSets DaemonSets предназначены для непрерывного процесса.
Они запускают по одному Pod на узел.
Каждый новый узел, добавленный в кластер, получает запущенный pod.
Полезны для фоновых задач, таких как мониторинг и сбор логов.
Каждый Pod имеет уникальный, постоянный идентификатор, который контроллер сохраняет при любом перепланировании.
Сервисы единая конечная точка для доступа к Pods
унифицированный способ маршрутизации трафика к кластеру и, в конечном итоге, к списку Pods.
Используя сервис, Pods можно поднимать и опускать, не затрагивая ничего.
Это лишь краткий обзор и заметки о фундаментальных строительных блоках Kubernetes, мы можем использовать эти знания и добавить некоторые другие области, такие как Storage и Ingress, чтобы улучшить наши приложения, но у нас также есть большой выбор, где будет работать наш кластер Kubernetes. Следующая сессия будет посвящена этим вариантам, где я могу запустить кластер Kubernetes, а также изучению некоторых особенностей хранения данных.
Ресурсы Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day49/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day50/":{title:"50. Выбор платформы Kubernetes для проекта",tags:["devops"],content:`Выбор платформы Kubernetes Я хотел бы использовать эту сессию для разбора некоторых платформ или, может быть, дистрибутивов - более подходящий термин для этого, одна вещь, которая была проблемой в мире Kubernetes - это устранение сложности.
Kubernetes the hard way рассказывает о том, как построить из ничего полноценный функциональный кластер Kubernetes, очевидно, что это крайность, но все больше и больше людей, по крайней мере, тех, с кем я общаюсь, хотят устранить эту сложность и запустить управляемый кластер Kubernetes. Проблема в том, что это стоит больше денег, но преимущества могут быть следующими: если вы используете управляемый сервис, действительно ли вам нужно знать архитектуру узлов и то, что происходит с точки зрения плоскости управления узлов, когда обычно у вас нет к этому доступа.
Затем у нас есть локальные дистрибутивы для разработки, которые позволяют нам использовать наши собственные системы и запускать локальную версию Kubernetes, чтобы разработчики могли иметь полную рабочую среду для запуска своих приложений на платформе, для которой они предназначены.
Общая основа всех этих концепций заключается в том, что все они являются разновидностью Kubernetes, что означает, что мы должны иметь возможность свободно мигрировать и перемещать наши рабочие нагрузки туда, куда нам нужно, в соответствии с нашими требованиями.
Во многом наш выбор будет зависеть от того, какие инвестиции были сделаны. Я уже упоминал об опыте разработчиков, но некоторые из локальных сред Kubernetes, в которых работают наши ноутбуки, отлично подходят для ознакомления с технологией без затрат денег.
Bare-Metal Clusters Вариантом для многих может быть запуск ОС Linux прямо на нескольких физических серверах для создания кластера, это также может быть Windows, но я не слышал о темпах внедрения Windows, контейнеров и Kubernetes. Очевидно, что если вы - компания, и вы приняли решение о покупке физических серверов, то это может быть способом создания кластера Kubernetes, но управление и администрирование здесь означает, что вам придется создавать и управлять всем с нуля.
Виртуализация Независимо от тестовых и учебных сред или готовых корпоративных кластеров Kubernetes виртуализация является отличным способом продвижения, обычно это возможность запускать виртуальные машины в качестве узлов и затем объединять их в кластер. Вы получаете базовую архитектуру, эффективность и скорость виртуализации, а также возможность эффективно использовать существующие затраты. Например, VMware предлагает отличное решение для виртуальных машин и Kubernetes в различных вариантах.
Мой первый кластер Kubernetes был создан на основе виртуализации с использованием Microsoft Hyper-V на старом сервере, который был способен запускать несколько виртуальных машин в качестве узлов.
Варианты локального рабочего стола Существует несколько вариантов запуска локального кластера Kubernetes на вашем настольном компьютере или ноутбуке. Как уже говорилось ранее, это дает разработчикам возможность увидеть, как будет выглядеть их приложение, без необходимости создавать несколько дорогостоящих или сложных кластеров. Лично я часто использую этот кластер, в частности, я использую minikube. Он обладает отличной функциональностью и дополнениями, которые меняют способ создания и запуска приложений.
Kubernetes Managed Services Я уже упоминал о виртуализации, и это может быть достигнуто с помощью гипервизоров локально, но мы знаем из предыдущих разделов, что мы также можем использовать виртуальные машины в публичном облаке в качестве узлов. Я говорю об управляемых сервисах Kubernetes - это предложения, которые мы видим у крупных гипермасштабирующих компаний, а также у MSP, которые убирают уровни управления и контроля от конечного пользователя; это может быть удаление плоскости управления от конечного пользователя, что происходит с Amazon EKS, Microsoft AKS и Google Kubernetes Engine. (GKE)
Непреодолимый выбор Выбор - это здорово, но есть момент, когда он становится чрезмерным, и это не глубокий обзор всех вариантов в каждой из перечисленных выше категорий. В дополнение к вышеперечисленному у нас есть OpenShift от Red Hat, и этот вариант действительно может быть использован во всех вышеперечисленных вариантах у всех основных облачных провайдеров и, вероятно, сегодня обеспечивает наилучшее общее удобство для администраторов независимо от того, где развернуты кластеры.
Итак, с чего вы начнете свое обучение, как я уже сказал, я начал с пути виртуализации, но это было потому, что у меня был доступ к физическому серверу, который я мог использовать для этой цели, я ценю и фактически с тех пор у меня больше нет такой возможности.
Сейчас я бы посоветовал использовать Minikube в качестве первого варианта или Kind (Kubernetes в Docker), но Minikube дает нам некоторые дополнительные преимущества, которые почти абстрагируют сложность, так как мы можем просто использовать дополнительные модули и быстро создавать вещи, а затем разрушать их, когда мы закончим, мы можем запускать несколько кластеров, мы можем запускать их почти везде, кросс-платформенные и аппаратно-агностические.
Я проделал небольшой путь в изучении Kubernetes, поэтому я собираюсь оставить выбор платформы и конкретику здесь, чтобы перечислить варианты, которые я пробовал, чтобы дать мне лучшее понимание платформы Kubernetes и того, где она может работать. Что я мог бы сделать с нижеприведенными записями в блоге, так это еще раз взглянуть на них, обновить их и перенести сюда, вместо того, чтобы они были ссылками на записи в блоге.
Ресурсы Kubernetes playground – How to choose your platform Kubernetes playground – Setting up your cluster Getting started with Amazon Elastic Kubernetes Service (Amazon EKS) Getting started with Microsoft Azure Kubernetes Service (AKS) Getting Started with Microsoft AKS – Azure PowerShell Edition Getting started with Google Kubernetes Service (GKE) Kubernetes, How to – AWS Bottlerocket + Amazon EKS Getting started with CIVO Cloud Minikube - Kubernetes Demo Environment For Everyone `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day50/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day51/":{title:"51. Установка minikube",tags:["devops","minikube"],content:`Развертывание первого кластера Kubernetes В этом посте мы собираемся запустить кластер Kubernetes на нашей локальной машине с помощью minikube, это даст нам базовый кластер Kubernetes для остальной части раздела Kubernetes, хотя позже мы рассмотрим развертывание кластера Kubernetes и в VirtualBox. Причина, по которой мы выбрали этот метод, а не развертывание управляемого кластера Kubernetes в публичном облаке, заключается в том, что это будет стоить денег даже при бесплатном уровне, однако я поделился некоторыми блогами, если вы захотите развернуть такую среду в предыдущем разделе День 50.
Что такое Minikube? Minikube быстро создает локальный кластер Kubernetes на macOS, Linux и Windows.
Для начала, независимо от ОС вашей рабочей станции, вы можете запустить minikube. Сначала перейдите на страницу проекта. Первая опция, которая у вас есть, это выбор метода установки. Я не использовал этот метод, но вы можете выбрать мой способ (о моем способе речь впереди).
Ниже упоминается, что вам необходимо иметь \u0026ldquo;Менеджер контейнеров или виртуальных машин, такой как: Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox или VMware\u0026rdquo; - это то, где будет работать MiniKube, и это простой вариант, и если не указано в репозитории, я использую Docker. Вы можете установить Docker на свою систему, используя следующую ссылку.
Понятное руководство по установке minikube
Мой способ установки minikube Я уже некоторое время использую arkade, чтобы получить все эти инструменты Kubernetes и CLI, вы можете посмотреть шаги установки на этом github репозитории для начала работы с Arkade. Я также упоминал об этом в других записях блога, когда мне нужно было что-то установить. Простота установки: достаточно нажать arkade get и посмотреть, доступен ли ваш инструмент или cli, очень удобна. В разделе Linux мы говорили о менеджере пакетов и процессе получения нашего программного обеспечения, вы можете думать об Arkade как о рынке для всех ваших приложений и clis для Kubernetes. Очень удобный инструмент, который нужно иметь в своих системах, написанный на Golang и кроссплатформенный.
В длинном списке доступных приложений в arkade minikube является одним из них, поэтому с помощью простой команды arkade get minikube мы загружаем бинарник и можем приступать.
Нам также понадобится kubectl как часть нашего инструментария, поэтому вы можете получить его через arkade или, как я полагаю, в документации по minikube он представлен как часть команд curl, упомянутых выше. Подробнее о kubectl мы расскажем позже в этом посте.
Получение и запуск кластера Kubernetes В этом конкретном разделе я хочу рассказать о доступных нам вариантах запуска кластера Kubernetes на вашей локальной машине. Мы можем просто выполнить следующую команду, и она запустит кластер для использования.
minikube используется в командной строке, и, проще говоря, после того как вы все установили, вы можете выполнить команду minikube start для развертывания вашего первого кластера Kubernetes. Ниже вы увидите, что драйвер Docker по умолчанию является местом, где мы будем запускать наш вложенный узел виртуализации. В начале статьи я упомянул о других доступных опциях, которые помогут вам расширить вид локального кластера Kubernetes.
Один кластер Minikube будет состоять из одного контейнера docker, в котором будут находиться узел плоскости управления и рабочий узел в одном экземпляре. Обычно вы разделяете эти узлы по отдельности. Об этом мы расскажем в следующем разделе, где мы рассмотрим домашние лабораторные среды Kubernetes, но немного ближе к производственной архитектуре.
Я уже несколько раз говорил об этом, мне очень нравится minikube из-за доступных дополнений, возможность развернуть кластер с помощью простой команды, включающей все необходимые дополнения с самого начала, действительно помогает мне каждый раз развертывать одну и ту же необходимую установку.
Ниже представлен список этих аддонов, я обычно использую аддоны csi-hostpath-driver и volumesnapshots, но вы можете увидеть длинный список ниже. Конечно, эти аддоны могут быть развернуты с помощью Helm, о чем мы расскажем позже в разделе Kubernetes, но это значительно упрощает работу.
Я также определяю в нашем проекте некоторые дополнительные конфигурации, apiserver установлен на 6433 вместо случайного порта API, я определяю время выполнения контейнера также на containerd, однако docker используется по умолчанию, и CRI-O также доступен. Я также устанавливаю определенную версию Kubernetes.
Теперь мы готовы развернуть наш первый кластер Kubernetes с помощью minikube. Я уже упоминал, что вам также понадобится kubectl для взаимодействия с вашим кластером. Вы можете установить kubectl с помощью arkade, выполнив команду arkade get kubectl.
или вы можете загрузить кросс-платформенную версию со следующих сайтов
Linux macOS Windows После установки kubectl мы можем взаимодействовать с нашим кластером с помощью простой команды kubectl get nodes.
Что такое kubectl? Теперь у нас есть наш кластер minikube | Kubernetes, и я попросил вас установить Minikube, где я объяснил, что он делает, но я не объяснил, что такое kubectl и что он делает.
kubectl - это программа, которая используется или позволяет вам взаимодействовать с кластерами Kubernetes, мы используем ее здесь для взаимодействия с нашим кластером minikube, но мы также используем kubectl для взаимодействия с нашими корпоративными кластерами в публичном облаке.
Мы используем kubectl для развертывания приложений, проверки и управления ресурсами кластера. Гораздо лучший Обзор kubectl можно найти здесь, в официальной документации Kubernetes.
kubectl взаимодействует с сервером API, расположенным на узле Control Plane, о котором мы вкратце рассказывали в одном из предыдущих постов.
kubectl шпаргалка Наряду с официальной документацией я также обнаружил, что при поиске команд kubectl у меня постоянно открыта эта страница. Unofficial Kubernetes
Listing Resources kubectl get nodes List all nodes in cluster kubectl get namespaces List all namespaces in cluster kubectl get pods List all pods in default namespace cluster kubectl get pods -n name List all pods in \u0026ldquo;name\u0026rdquo; namespace kubectl get pods -n name List all pods in \u0026ldquo;name\u0026rdquo; namespace Creating Resources kubectl create namespace name Create a namespace called \u0026ldquo;name\u0026rdquo; kubectl create -f [filename] Create a resource from a JSON or YAML file: Editing Resources kubectl edit svc/servicename To edit a service More detail on Resources kubectl describe nodes display the state of any number of resources in detail, Delete Resources kubectl delete pod Remove resources, this can be from stdin or file Вы захотите узнать краткие названия некоторых команд kubectl, например, -n - это краткое название для namespace, что облегчает ввод команды, а также, если вы пишете скрипты, вы можете получить гораздо более аккуратный код.
Short name Full name csr certificatesigningrequests cs componentstatuses cm configmaps ds daemonsets deploy deployments ep endpoints ev events hpa horizontalpodautoscalers ing ingresses limits limitranges ns namespaces no nodes pvc persistentvolumeclaims pv persistentvolumes po pods pdb poddisruptionbudgets psp podsecuritypolicies rs replicasets rc replicationcontrollers quota resourcequotas sa serviceaccounts svc services В заключение хочу добавить, что я создал еще один проект на основе minikube, чтобы помочь мне быстро развернуть демонстрационные среды для демонстрации сервисов данных и защиты этих рабочих нагрузок с помощью Kasten K10, Project Pace можно найти там и буду рад вашим отзывам или взаимодействию, он также показывает или включает некоторые автоматизированные способы развертывания кластеров minikube и создания различных приложений сервисов данных.
Далее мы перейдем к развертыванию нескольких узлов в виртуальные машины с помощью VirtualBox, но здесь мы будем действовать проще, как мы делали в разделе Linux, где мы использовали vagrant для быстрого запуска машин и развертывания нашего программного обеспечения, как мы хотим.
Я добавил этот список к вчерашнему посту, который представляет собой блоги с описанием развертывания различных кластеров Kubernetes.
Kubernetes playground – How to choose your platform Kubernetes playground – Setting up your cluster Getting started with Amazon Elastic Kubernetes Service (Amazon EKS) Getting started with Microsoft Azure Kubernetes Service (AKS) Getting Started with Microsoft AKS – Azure PowerShell Edition Getting started with Google Kubernetes Service (GKE) Kubernetes, How to – AWS Bottlerocket + Amazon EKS Getting started with CIVO Cloud Minikube - Kubernetes Demo Environment For Everyone Ресурсы Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day51/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day52/":{title:"52. Настройка многоузлового кластера Kubernetes",tags:["devops"],content:`Настройка многоузлового кластера Kubernetes Я хотел назвать эту статью \u0026ldquo;Настройка многоузлового кластера Kubernetes с помощью Vagrant\u0026rdquo;, но подумал, что это будет слишком длинно!
На вчерашней сессии мы использовали классный проект для развертывания нашего первого кластера Kubernetes и немного поработали с самым важным инструментом CLI, с которым вы столкнетесь при использовании Kubernetes (kubectl).
Здесь мы будем использовать VirtualBox в качестве основы, но, как мы уже говорили о Vagrant в разделе Linux, мы можем использовать любой гипервизор или инструмент виртуализации. Это был День 14, когда мы прошли и развернули машину Ubuntu для раздела Linux.
Краткая информация о Vagrant Vagrant - это утилита CLI, которая управляет жизненным циклом ваших виртуальных машин. Мы можем использовать vagrant для запуска и разворачивания виртуальных машин на различных платформах, включая vSphere, Hyper-v, Virtual Box и Docker. У него есть и другие поставщики, но мы будем придерживаться этого, мы используем Virtual Box, так что все готово.
Я собираюсь использовать базовый уровень этого блога и репозитория, чтобы пройтись по конфигурации. Однако я бы посоветовал, если вы впервые развертываете кластер Kubernetes, посмотреть, как это делается вручную, и тогда вы хотя бы будете знать, как это выглядит. Хотя я должен сказать, что эти операции и усилия дня 0 становятся все более эффективными с каждым выпуском Kubernetes. Я сравниваю это с временами VMware и ESX, когда для развертывания 3 серверов ESX требовался по меньшей мере день, а теперь мы можем сделать это за час. Мы движемся в этом направлении, когда речь идет о Kubernetes\u0026quot;.
Лабораторная среда Kubernetes Я загрузил в папку Kubernetes vagrantfile, который мы будем использовать для создания нашей среды. Возьмите его и перейдите в этот каталог в терминале. Я снова использую Windows, поэтому я буду использовать PowerShell для выполнения команд рабочей станции с vagrant. Если у вас нет vagrant, вы можете использовать arkade, о котором мы говорили вчера при установке minikube и других инструментов. Простая команда arkade get vagrant должна заставить вас загрузить и установить последнюю версию vagrant.
Когда вы окажетесь в своей директории, вы можете просто запустить vagrant up, и если все настроено правильно, вы должны увидеть в терминале следующее.
В терминале вы увидите ряд шагов, но тем временем давайте посмотрим, что мы на самом деле создаем.
Из приведенного выше изображения видно, что мы собираемся создать 3 виртуальные машины, у нас будет узел плоскости управления и два рабочих узла. Если вы вернетесь к День 49, вы увидите более подробное описание этих областей, которые мы видим на изображении.
Также на изображении мы указываем, что наш доступ к kubectl будет происходить извне кластера и попадать в kube apiserver, в то время как на самом деле в рамках инициализации vagrant мы развертываем kubectl на каждом из этих узлов, чтобы мы могли получить доступ к кластеру изнутри каждого из наших узлов.
Процесс создания этой лаборатории может занять от 5 до 30 минут в зависимости от вашей установки.
Я собираюсь в ближайшее время рассказать о скриптах, но если вы посмотрите в файл vagrant, то заметите, что мы вызываем 3 скрипта как часть развертывания, и именно здесь создается кластер. Мы видели, как легко использовать vagrant для развертывания наших виртуальных машин и установки ОС с помощью боксов vagrant, но возможность запуска скрипта оболочки как часть процесса развертывания - это то, что становится довольно интересным в автоматизации этих лабораторных сборок.
После завершения мы можем подключиться по ssh к одному из наших узлов vagrant ssh master из терминала должен получить доступ, имя пользователя и пароль по умолчанию - vagrant/vagrant.
Вы также можете использовать vagrant ssh node01 и vagrant ssh node02 для получения доступа к рабочим узлам, если хотите.
Теперь мы находимся на одном из вышеуказанных узлов нашего нового кластера, мы можем выдать команду kubectl get nodes, чтобы показать наш 3-узловой кластер и его статус.
На данный момент у нас есть запущенный 3-узловой кластер, с 1 узлом плоскости управления и 2 рабочими узлами.
Vagrantfile и Shell Script walkthrough Если мы посмотрим на наш vagrantfile, вы увидите, что мы определяем количество рабочих узлов, сетевые IP-адреса для мостовой сети в VirtualBox, а также некоторые именования. Еще вы заметите, что мы также вызываем некоторые скрипты, которые мы хотим запустить на определенных хостах.
NUM_WORKER_NODES=2 IP_NW=\u0026quot;10.0.0.\u0026quot; IP_START=10 Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.provision \u0026quot;shell\u0026quot;, inline: \u0026lt;\u0026lt;-SHELL apt-get update -y echo \u0026quot;$IP_NW$((IP_START)) master-node\u0026quot; \u0026gt;\u0026gt; /etc/hosts echo \u0026quot;$IP_NW$((IP_START+1)) worker-node01\u0026quot; \u0026gt;\u0026gt; /etc/hosts echo \u0026quot;$IP_NW$((IP_START+2)) worker-node02\u0026quot; \u0026gt;\u0026gt; /etc/hosts SHELL config.vm.box = \u0026quot;bento/ubuntu-21.10\u0026quot; config.vm.box_check_update = true config.vm.define \u0026quot;master\u0026quot; do |master| master.vm.hostname = \u0026quot;master-node\u0026quot; master.vm.network \u0026quot;private_network\u0026quot;, ip: IP_NW + \u0026quot;#{IP_START}\u0026quot; master.vm.provider \u0026quot;virtualbox\u0026quot; do |vb| vb.memory = 4048 vb.cpus = 2 vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--natdnshostresolver1\u0026quot;, \u0026quot;on\u0026quot;] end master.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/common.sh\u0026quot; master.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/master.sh\u0026quot; end (1..NUM_WORKER_NODES).each do |i| config.vm.define \u0026quot;node0#{i}\u0026quot; do |node| node.vm.hostname = \u0026quot;worker-node0#{i}\u0026quot; node.vm.network \u0026quot;private_network\u0026quot;, ip: IP_NW + \u0026quot;#{IP_START + i}\u0026quot; node.vm.provider \u0026quot;virtualbox\u0026quot; do |vb| vb.memory = 2048 vb.cpus = 1 vb.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--natdnshostresolver1\u0026quot;, \u0026quot;on\u0026quot;] end node.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/common.sh\u0026quot; node.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/node.sh\u0026quot; end end end Давайте разберем эти выполняемые скрипты. У нас есть три скрипта, перечисленные в вышеуказанном VAGRANTFILE для запуска на определенных узлах.
master.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/common.sh\u0026quot;
Приведенный выше скрипт будет направлен на подготовку узлов, он будет запущен на всех трех наших узлах и удалит все существующие компоненты Docker и переустановит Docker и ContainerD, а также kubeadm, kubelet и kubectl. Этот скрипт также обновит существующие пакеты программного обеспечения в системе.
master.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/master.sh\u0026quot;
Скрипт master.sh будет выполняться только на узле плоскости управления, этот скрипт создаст кластер Kubernetes с помощью команд kubeadm. Он также подготовит контекст конфигурации для доступа к этому кластеру, о чем мы расскажем далее.
node.vm.provision \u0026quot;shell\u0026quot;, path: \u0026quot;scripts/node.sh\u0026quot;
Это просто возьмет конфиг, созданный мастером, и присоединит наши узлы к кластеру Kubernetes, этот процесс присоединения снова использует kubeadm и другой скрипт, который можно найти в папке config.
Доступ к кластеру Kubernetes Теперь у нас есть два развернутых кластера: кластер minikube, который мы развернули в предыдущем разделе, и новый 3-узловой кластер, который мы только что развернули на VirtualBox.
Также в этом конфигурационном файле, к которому у вас будет доступ на машине, с которой вы запускали vagrant, описано, как мы можем получить доступ к нашему кластеру с нашей рабочей станции.
Прежде чем мы покажем это, позвольте мне коснуться контекста.
Контекст важен, необходима возможность доступа к кластеру Kubernetes с рабочего стола или ноутбука. Существует множество различных вариантов, и люди используют различные операционные системы в качестве повседневных драйверов.
По умолчанию клиент Kubernetes CLI (kubectl) использует папку C:\\Users\\username.kube\\config для хранения информации о кластере Kubernetes, такой как конечная точка и учетные данные. Если вы развернули кластер, вы сможете увидеть этот файл в этом месте. Но если вы до сих пор использовали главный узел для выполнения всех команд kubectl через SSH или другими способами, то эта статья, надеюсь, поможет вам освоить возможность подключения к рабочей станции.
Затем нам нужно получить файл kubeconfig из кластера или мы также можем получить его из нашего файла конфигурации после развертывания, получить содержимое этого файла либо через SCP, либо просто открыть консольный сеанс на главном узле и скопировать на локальную машину windows.
Затем мы хотим взять копию этого файла конфигурации и переместить в место $HOME/.kube/config.
Теперь с локальной рабочей станции вы сможете запустить kubectl cluster-info и kubectl get nodes, чтобы убедиться, что у вас есть доступ к вашему кластеру.
Это не только обеспечивает подключение и управление с вашей windows-машины, но и позволяет нам выполнить проброс портов для доступа к определенным сервисам с нашей windows-машины.
Если вам интересно, как управлять несколькими кластерами на рабочей станции, у меня есть более подробное описание здесь.
Я добавил этот список, в котором представлены блоги, посвященные различным развертываемым кластерам Kubernetes.
Kubernetes playground – How to choose your platform Kubernetes playground – Setting up your cluster Getting started with Amazon Elastic Kubernetes Service (Amazon EKS) Getting started with Microsoft Azure Kubernetes Service (AKS) Getting Started with Microsoft AKS – Azure PowerShell Edition Getting started with Google Kubernetes Service (GKE) Kubernetes, How to – AWS Bottlerocket + Amazon EKS Getting started with CIVO Cloud Minikube - Kubernetes Demo Environment For Everyone Ресурсы Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day52/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day53/":{title:"53. Обзор Rancher",tags:["devops"],content:`Обзор Rancher - практическое применение В этом разделе мы рассмотрим Rancher, до сих пор все, что мы делали, было в cli и с использованием kubectl, но у нас есть несколько действительно хороших пользовательских интерфейсов и инструментов управления несколькими кластерами, чтобы дать нашим операционным командам хорошую видимость управления кластером.
Rancher, согласно их сайту
Rancher - это полный программный стек для команд, внедряющих контейнеры. Он решает операционные проблемы и проблемы безопасности при управлении несколькими кластерами Kubernetes в любой инфраструктуре, обеспечивая команды DevOps интегрированными инструментами для запуска контейнерных рабочих нагрузок.
Rancher позволяет нам развертывать кластеры Kubernetes производственного уровня практически из любого места, а затем обеспечивает централизованную аутентификацию, контроль доступа и наблюдаемость. Я упоминал в предыдущем разделе, что существует почти непреодолимый выбор, когда речь идет о Kubernetes и о том, где вы должны или можете их запустить, но с Rancher действительно не имеет значения, где они находятся.
Развертывание Rancher Первое, что нам нужно сделать, это развернуть Rancher на нашей локальной рабочей станции, есть несколько способов и мест, которые вы можете выбрать для выполнения этого шага, я хочу использовать свою локальную рабочую станцию и запустить Rancher как контейнер docker. Выполнив приведенную ниже команду, мы получим образ контейнера и доступ к пользовательскому интерфейсу rancher.
Доступны и другие методы развертывания rancher Rancher Quick-Start-Guide sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher.
Как вы можете видеть на нашем рабочем столе Docker, у нас есть запущенный контейнер rancher.
Доступ к пользовательскому интерфейсу Rancher Запустив вышеуказанный контейнер, мы должны иметь возможность перейти к нему через веб-страницу. По адресу https://localhost откроется страница входа в систему, как показано ниже.
Следуйте инструкциям ниже, чтобы получить требуемый пароль. Поскольку я использую Windows, я решил использовать bash для Windows, так как для этого требуется команда grep.
Затем мы можем взять указанный выше пароль и войти в систему, на следующей странице мы можем задать новый пароль.
После выполнения вышеуказанных действий мы войдем в систему и увидим наш начальный экран. В рамках развертывания Rancher мы также увидим локальный кластер K3s.
Краткий экскурс по rancher Первое, на что мы посмотрим, это наш локально развернутый кластер K3S. Вы можете видеть ниже, что мы получаем хорошее представление о том, что происходит внутри нашего кластера. Это развертывание по умолчанию, и мы еще ничего не развертывали в этом кластере. Видно, что он состоит из 1 узла и имеет 5 развертываний. Также вы можете видеть, что есть некоторые статистические данные по стручкам, ядрам и памяти.
В меню слева есть вкладка Apps \u0026amp; Marketplace, которая позволяет нам выбрать приложения, которые мы хотели бы запустить на наших кластерах. Как уже упоминалось ранее, Rancher дает нам возможность запускать и управлять несколькими различными кластерами. С помощью рынка мы можем очень легко развернуть наши приложения.
Еще одна вещь, о которой стоит упомянуть, это то, что если вам понадобится получить доступ к любому кластеру, управляемому Rancher, в правом верхнем углу есть возможность открыть оболочку kubectl для выбранного кластера.
Создание нового кластера На последних двух занятиях мы создали кластер minikube локально и использовали Vagrant с VirtualBox для создания 3-узлового кластера Kubernetes, с помощью Rancher мы также можем создавать кластеры. В папке Rancher Folder вы найдете дополнительные файлы vagrant, которые создадут те же 3 узла, но без шагов по созданию нашего кластера Kubernetes (мы хотим, чтобы Rancher сделал это за нас).
Тем не менее, мы хотим установить docker и обновить ОС, поэтому вы увидите скрипт common.sh, запускаемый на каждом из наших узлов. Это также установит Kubeadm, Kubectl и т.д. Но он не запустит команды Kubeadm для создания и объединения наших узлов в кластер.
Мы можем перейти в папку vagrant и просто запустить vagrant up, и это начнет процесс создания наших 3 виртуальных машин в virtualbox.
Теперь, когда у нас есть наши узлы или ВМ на месте и готовы, мы можем использовать Rancher для создания нашего нового кластера Kubernetes. Первый экран для создания кластера дает вам несколько вариантов того, где находится ваш кластер, то есть используете ли вы службы Kubernetes, управляемые публичным облаком, vSphere или что-то еще.
Мы выберем \u0026ldquo;custom\u0026rdquo;, так как не используем ни одну из интегрированных платформ. На открывшейся странице вы определяете имя вашего кластера (ниже написано local, но вы не можете использовать local, наш кластер называется vagrant). Здесь вы можете определить версии Kubernetes, сетевых провайдеров и некоторые другие параметры конфигурации, чтобы запустить ваш кластер Kubernetes.
На следующей странице вы найдете регистрационный код, который необходимо запустить на каждом из узлов и включить соответствующие службы: etcd, controlplane и worker. Для нашего главного узла нам нужны etcd и controlplane, поэтому команду можно увидеть ниже.
sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.6.3 --server https://10. 0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --etcd --controlplaneЕсли сетевое взаимодействие настроено правильно, то вы должны довольно быстро увидеть следующее на приборной панели rancher, указывающее на то, что первый мастер-узел сейчас регистрируется и кластер создается.
Затем мы можем повторить процесс регистрации для каждого из рабочих узлов с помощью следующей команды, и через некоторое время вы получите свой кластер, способный использовать рынок для развертывания приложений.
sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.6.3 --server https://10. 0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --workerЗа последние 3 занятия мы использовали несколько различных способов запуска кластера Kubernetes, в оставшиеся дни мы рассмотрим прикладную сторону платформы, вероятно, самую важную. Мы рассмотрим сервисы и возможность предоставления и использования наших сервисов в Kubernetes.
Мне сказали, что требования к загрузке узлов rancher требуют, чтобы эти виртуальные машины имели 4 ГБ оперативной памяти, иначе они будут работать с ошибками, с тех пор я обновил информацию, так как наши рабочие узлы имели 2 ГБ.
Ресурсы Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day53/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day54/":{title:"54. Развертывание приложений Kubernetes",tags:["devops"],content:`Развертывание приложений Kubernetes Теперь мы, наконец, переходим к реальному развертыванию некоторых приложений в наших кластерах, некоторые говорят, что именно для этого существует Kubernetes - для доставки приложений.
Идея заключается в том, что мы можем взять наши образы контейнеров и развернуть их в виде стручков в нашем кластере Kubernetes, чтобы воспользоваться преимуществами Kubernetes как контейнерного оркестратора.
Развертывание приложений в Kubernetes Существует несколько способов развертывания наших приложений в кластере Kubernetes, мы рассмотрим два наиболее распространенных подхода - YAML-файлы и диаграммы Helm.
Для развертывания приложений мы будем использовать кластер minikube. Мы рассмотрим некоторые из ранее упомянутых компонентов или строительных блоков Kubernetes.
На протяжении всего этого раздела и раздела о контейнерах мы говорили об образах и преимуществах Kubernetes, а также о том, как мы можем легко справляться с масштабированием на этой платформе.
В этом первом шаге мы просто создадим приложение без статических данных в нашем кластере minikube. Мы будем использовать дефакто стандартное приложение без статики в нашей первой демонстрации nginx. Мы настроим Deployment, который предоставит нам наши стручки, а затем мы также создадим службу, которая позволит нам перейти к простому веб-серверу, размещенному в стручке nginx. Все это будет содержаться в пространстве имен.
Создание YAML В первом демо мы хотим определить все, что мы делаем с YAML, мы могли бы создать целый раздел о YAML, но я собираюсь пропустить это и оставить некоторые ресурсы в конце, которые расскажут о YAML более подробно.
Мы можем создать следующее как один YAML-файл или разбить его на части для каждого аспекта нашего приложения, то есть это могут быть отдельные файлы для пространства имен, развертывания и создания сервисов, но в этом файле ниже мы разделили их с помощью --- в одном файле. Вы можете найти этот файл, расположенный здесь
apiVersion: v1kind: Namespacemetadata:name: nginx\u0026quot;labels\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;nginx\u0026quot;}---apiVersion: apps/v1kind: Deploymentmetadata:name: nginx-deploymentnamespace: nginxspec:selector:matchLabels:app: nginxreplicas: 1template:metadata:labels:app: nginxspec:containers:- name: nginximage: nginxports:- containerPort: 80---apiVersion: v1kind: Servicemetadata:name: nginx-servicenamespace: nginxspec:selector:app: nginx-deploymentports:- protocol: TCPport: 80targetPort: 80Проверка нашего кластера Перед тем как развернуть что-либо, мы должны убедиться, что у нас нет существующих пространств имен с названием nginx. Мы можем сделать это, выполнив команду kubectl get namespace, и как вы можете видеть ниже, у нас нет пространства имен с названием nginx.
Время развернуть наше приложение Теперь мы готовы развернуть наше приложение на нашем кластере minikube, этот же процесс будет работать на любом другом кластере Kubernetes.
Нам нужно перейти к расположению нашего yaml файла, а затем мы можем выполнить команду kubectl create -f nginx-stateless-demo.yaml, после чего вы увидите, что было создано 3 объекта, у нас есть пространство имен, развертывание и сервис.
Давайте снова выполним команду, чтобы увидеть доступные пространства имен в нашем кластере kubectl get namespace, и теперь вы можете увидеть, что у нас есть наше новое пространство имен.
Если мы затем проверим наше пространство имен на наличие стручков с помощью kubectl get pods -n nginx, вы увидите, что у нас есть 1 стручок в готовом и запущенном состоянии.
Мы также можем проверить, что наш сервис создан, выполнив команду kubectl get service -n nginx.
Наконец, мы можем пойти и проверить наше развертывание, развертывание - это то, где и как мы сохраняем нашу желаемую конфигурацию.
Выше приведено несколько команд, которые стоит знать, но вы также можете использовать kubectl get all -n nginx, чтобы увидеть все, что мы развернули с помощью одного YAML-файла.
Вы можете заметить, что у нас также есть replicaset, в нашем развертывании мы определяем, сколько копий нашего образа мы хотим развернуть. Изначально мы установили значение 1, но если мы хотим быстро масштабировать наше приложение, мы можем сделать это несколькими способами.
Мы можем отредактировать наш файл с помощью команды kubectl edit deployment nginx-deployment -n nginx, которая откроет текстовый редактор в вашем терминале и позволит вам изменить развертывание.
После сохранения в текстовом редакторе в терминале, если не возникло проблем и было использовано правильное форматирование, вы должны увидеть дополнительное развертывание в вашем пространстве имен.
Мы также можем изменить количество реплик с помощью kubectl и команды kubectl scale deployment nginx-deployment --replicas=10 -n nginx.
Мы также можем использовать этот метод для уменьшения масштаба нашего приложения до 1 снова, если захотим, используя любой метод. Я использовал опцию edit, но вы также можете использовать команду scale выше.
Надеюсь, здесь вы можете увидеть пример использования: не только все очень быстро запускается и выключается, но у нас есть возможность быстро увеличивать и уменьшать масштаб наших приложений. Если бы это был веб-сервер, мы могли бы увеличивать масштаб в периоды загруженности и уменьшать, когда нагрузка снижается.
Раскрытие нашего приложения Но как нам получить доступ к нашему веб-серверу?
Если вы посмотрите выше на наш сервис, вы увидите, что там нет внешнего IP, поэтому мы не можем просто открыть веб-браузер и ожидать, что он будет там волшебным образом. Для доступа у нас есть несколько вариантов.
ClusterIP - IP, который вы видите, является кластерным IP, он находится во внутренней сети кластера. Только объекты внутри кластера могут достичь этого IP.
NodePort - Выставляет службу на один и тот же порт каждого из выбранных узлов в кластере с помощью NAT.
LoadBalancer - Создает внешний балансировщик нагрузки в текущем облаке, мы используем minikube, но если вы создали свой собственный кластер Kubernetes, т.е. то, что мы сделали в VirtualBox, вам нужно будет развернуть LoadBalancer, такой как metallb, в вашем кластере, чтобы обеспечить эту функциональность.
Port-Forward - У нас также есть возможность Port Forward, которая позволяет вам получить доступ и взаимодействовать с внутренними процессами кластера Kubernetes с вашего localhost. На самом деле эта опция используется только для тестирования и поиска неисправностей.
Теперь у нас есть несколько вариантов на выбор, Minikube имеет некоторые ограничения или отличия от полноценного кластера Kubernetes.
Мы можем просто выполнить следующую команду, чтобы перенаправить порт для доступа, используя нашу локальную рабочую станцию.
kubectl port-forward deployment/nginx-deployment -n nginx 8090:80.
Обратите внимание, что при выполнении вышеуказанной команды терминал становится непригодным для использования, поскольку он действует как проброс порта на вашу локальную машину и порт.
Наконец, в новом терминале запустите minikube --profile='mc-demo' service nginx-service --url -n nginx, чтобы создать туннель для нашего сервиса.
Откройте браузер или программу управления и нажмите на ссылку в терминале.
Helm Helm - это еще один способ, с помощью которого мы можем развернуть наши приложения. Известен как \u0026ldquo;менеджер пакетов для Kubernetes\u0026rdquo;. Вы можете узнать больше здесь.
Helm - это менеджер пакетов для Kubernetes. Helm можно считать аналогом yum или apt для Kubernetes. Helm развертывает диаграммы, которые можно представить как упакованное приложение. Это чертеж предварительно сконфигурированных ресурсов приложения, которые можно развернуть в виде одной простой в использовании диаграммы. Затем вы можете развернуть другую версию диаграммы с другим набором конфигураций.
У компании есть сайт, на котором можно просмотреть все доступные диаграммы Helm и, конечно, создать свою собственную. Документация также понятна и лаконична и не так пугает, как когда я впервые услышал термин Helm среди всех других новых слов в этой области.
Запустить или установить Helm очень просто. Просто. Здесь вы можете найти двоичные файлы и ссылки на загрузку практически для всех дистрибутивов, включая устройства RaspberryPi arm64.
Или вы можете использовать скрипт установщика, преимущество которого в том, что будет загружена и установлена последняя версия Helm.
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3chmod 700 get_helm.sh./get_helm.shНаконец, есть также возможность использовать менеджер пакетов для менеджера приложений, homebrew для mac, chocolatey для windows, apt с Ubuntu/Debian, snap и pkg также.
Пока что Helm кажется наиболее удобным способом загрузки и установки различных тестовых приложений в кластере.
Хорошим ресурсом для ссылки здесь будет ArtifactHUB, который является ресурсом для поиска, установки и публикации пакетов Kubernetes. Я также порекомендую KubeApps, который представляет собой пользовательский интерфейс для отображения диаграмм штурвала.
Ресурсы Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day54/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day55/":{title:"55. State и Ingress в Kubernetes",tags:["devops"],content:`State и Ingress в Kubernetes В этом заключительном разделе, посвященном Kubernetes, мы рассмотрим State и ingress.
Все, о чем мы говорили до сих пор, касается stateless, stateless - это когда нашим приложениям не важно, какую сеть они используют, и им не нужно постоянное хранение данных. В то время как приложения с состоянием, например, базы данных, чтобы такое приложение функционировало правильно, вам нужно убедиться, что стручки могут обращаться друг к другу через уникальную идентификацию, которая не меняется (имена хостов, IP\u0026hellip; и т.д.). Примерами stateful-приложений являются кластеры MySQL, Redis, Kafka, MongoDB и другие. В принципе, любое приложение, которое хранит данные.
Stateful Application StatefulSets представляют собой набор Pods с уникальными, постоянными идентификаторами и стабильными именами хостов, которые Kubernetes поддерживает независимо от того, где они запланированы. Информация о состоянии и другие устойчивые данные для любого данного StatefulSet Pod хранятся в постоянном дисковом хранилище, связанном с StatefulSet.
Развертывание против StatefulSet Репликация stateful-приложений является более сложной задачей. Репликация наших стручков в развертывании (Stateless Application) идентична и взаимозаменяема. Создаем капсулы в случайном порядке со случайными хэшами Один сервис, который балансирует нагрузку на любой стручок. Когда дело доходит до StatefulSets или Stateful Applications, вышеописанное становится сложнее.
Невозможно одновременно создавать и удалять. Не может быть случайного обращения. реплики Pods не являются идентичными. То, что вы увидите в нашей демонстрации в ближайшее время, заключается в том, что каждая копия имеет свою собственную идентичность. В приложении без статического состояния вы увидите случайные имена. Например, app-7469bbb6d7-9mhxd, в то время как Stateful Application будет иметь имя mongo-0, а затем при масштабировании создаст новую капсулу под названием mongo-1.
Эти стручки создаются на основе одной и той же спецификации, но они не взаимозаменяемы. Каждая капсула StatefulSet имеет постоянный идентификатор при любом повторном планировании. Это необходимо, потому что когда нам требуются нагрузки с учетом состояния, такие как база данных, где требуется запись и чтение в базу данных, мы не можем иметь две капсулы, пишущие в одно и то же время без осведомленности, так как это приведет к несогласованности данных. Нам нужно убедиться, что в любой момент времени только один из наших стручков записывает данные в базу данных, однако мы можем иметь несколько стручков, читающих эти данные.
Каждый стручок в StatefulSet будет иметь доступ к своему собственному постоянному тому и копии базы данных для чтения, которая постоянно обновляется с главного сервера. Также интересно отметить, что каждый pod будет хранить свое состояние pod в этом постоянном томе, если mongo-0 умрет, то при инициализации нового pod он возьмет состояние pod, хранящееся в хранилище.
TLDR; StatefulSets vs Deployments
Predicatable pod name = mongo-0 Fixed individual DNS name Pod Identity - Retain State, Retain Role Replicating stateful apps is complex There are lots of things you must do: Configure cloning and data synchronisation. Make remote shared storage available. Management \u0026amp; backup Как сохранять данные в Kubernetes?
Мы упоминали выше, что когда у нас есть приложение с состоянием, нам нужно где-то хранить состояние, и именно здесь возникает необходимость в томе, поскольку из коробки Kubernetes не обеспечивает постоянство данных.
Нам нужен уровень хранения, который не зависит от жизненного цикла стручка. Это хранилище должно быть доступно со всех наших узлов Kubernetes. Хранилище также должно находиться вне кластера Kubernetes, чтобы иметь возможность выжить, даже если кластер Kubernetes потерпит крах.
Постоянный том Ресурс кластера (например, процессор и оперативная память) для хранения данных. Создается с помощью файла YAML. Требуется реальное физическое хранилище (NAS) Внешняя интеграция в ваш кластер Kubernetes. В вашем хранилище могут быть доступны различные типы хранилищ. PV не имеют пространства имен Локальное хранилище доступно, но оно будет специфично для одного узла в кластере Персистентность базы данных должна использовать удаленное хранилище (NAS) Утверждение о постоянном томе Постоянный том, как описано выше, может существовать и быть доступным, но пока он не заявлен приложением, он не используется.
Создается с помощью файла YAML Утверждение постоянного тома используется в конфигурации стручка (атрибут volumes) PVC находятся в том же пространстве имен, что и pod Том монтируется в капсулу Стручки могут иметь несколько различных типов томов (ConfigMap, Secret, PVC). Другой способ представить PVs и PVCs заключается в следующем
PVs создаются администратором Kubernetes Admin PVC создаются пользователем или разработчиком приложения.
У нас также есть два других типа томов, которые мы не будем подробно описывать, но о которых стоит упомянуть:
ConfigMaps | Secrets Конфигурационный файл для вашего стручка. Файл сертификата для вашей капсулы. StorageClass Создается с помощью файла YAML Предоставляет постоянные тома динамически, когда PVC заявляет об этом. Каждый бэкенд хранилища имеет свой собственный провизор Бэкенд хранилища определяется в YAML (через атрибут provisioner) Абстракции базового провайдера хранения Определяет параметры для этого хранилища Время просмотра Во вчерашней сессии мы рассмотрели создание приложения без статических данных, здесь мы хотим сделать то же самое, но использовать наш кластер minikube для развертывания рабочей нагрузки с статическими данными.
Напомним команду minikube, которую мы используем, чтобы иметь возможность и аддоны для использования персистентности: minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p mc-demo --kubernetes-version=1.21.2.
Эта команда использует драйвер csi-hostpath-driver, который дает нам наш класс хранилища, что я покажу позже.
Сборка приложения выглядит следующим образом:
Вы можете найти файл конфигурации YAML для этого приложения здесь pacman-stateful-demo.yaml
Конфигурация класса хранилища Есть еще один шаг, который мы должны выполнить перед началом развертывания нашего приложения, а именно убедиться, что наш класс хранилища (csi-hostpath-sc) является классом по умолчанию. Сначала мы можем проверить это, выполнив команду kubectl get storageclass, но из коробки кластер minikube будет показывать стандартный класс хранения по умолчанию, поэтому мы должны изменить его с помощью следующих команд.
Первая команда сделает наш класс хранилища csi-hostpath-sc классом по умолчанию.
kubectl patch storageclass csi-hostpath-sc -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;true\u0026quot;}}}'}''
Эта команда удалит аннотацию по умолчанию из стандартного StorageClass.
kubectl patch storageclass standard -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;false\u0026quot;}}}'}''
Начнем с того, что в нашем кластере нет пространства имен pacman. kubectl get namespace
Затем мы развернем наш YAML-файл. kubectl create -f pacman-stateful-demo.yaml Из этой команды видно, что мы создаем ряд объектов в нашем кластере Kubernetes.
Теперь у нас есть наше только что созданное пространство имен.
Из следующего изображения и команды kubectl get all -n pacman видно, что в нашем пространстве имен происходит несколько вещей. У нас есть pods, запускающий наш NodeJS web front end, у нас есть mongo, запускающий нашу backend базу данных. Есть сервисы для pacman и mongo для доступа к этим стручкам. У нас есть развертывание для pacman и statefulset для mongo.
У нас также есть наши постоянные тома и утверждения постоянных томов. Выполнив команду kubectl get pv, мы получим наши постоянные тома, не связанные с именами, а выполнив команду kubectl get pvc -n pacman, мы получим наши утверждения постоянных томов, связанные с именами. Играем в игру | Я имею в виду доступ к нашему критически важному приложению Поскольку мы используем Minikube, как уже упоминалось в приложении без статических данных, нам предстоит преодолеть несколько препятствий, когда дело доходит до доступа к нашему приложению. Однако если бы у нас был доступ к ingress или балансировщику нагрузки в нашем кластере, служба настроена на автоматическое получение IP-адреса от него для получения доступа извне. (Вы можете видеть это выше на изображении всех компонентов в пространстве имен pacman).
В данном демонстрационном примере мы будем использовать метод проброса портов для доступа к нашему приложению. Открыв новый терминал и выполнив следующую команду kubectl port-forward svc/pacman 9090:80 -n pacman, открыв браузер, мы получим доступ к нашему приложению. Если вы запускаете это в AWS или в определенных местах, то это также сообщит об облаке и зоне, а также о хосте, который равен вашему стручку в Kubernetes, опять же, вы можете оглянуться назад и увидеть это имя стручка на наших скриншотах выше.
Теперь мы можем пойти и создать высокий балл, который затем будет сохранен в нашей базе данных.
Хорошо, у нас есть высокий балл, но что произойдет, если мы удалим наш mongo-0 pod? Выполнив команду kubectl delete pod mongo-0 -n pacman, я могу удалить его, и если вы все еще находитесь в приложении, вы увидите, что высокий балл недоступен, по крайней мере, в течение нескольких секунд.
Теперь, если я вернусь в свою игру, я смогу создать новую игру и увидеть свои высокие баллы. Единственный способ поверить мне в это - попробовать и поделиться в социальных сетях своими высокими результатами!
С развертыванием мы можем увеличить масштаб с помощью команд, которые мы рассматривали в предыдущей сессии, но в частности здесь, особенно если вы хотите устроить огромную вечеринку pacman, вы можете увеличить масштаб с помощью kubectl scale deployment pacman --replicas=10 -n pacman.
Ingress объяснено Прежде чем мы закончим с Kubernetes, я также хотел бы затронуть важный аспект Kubernetes, и это - ingress.
Что такое ingress? До сих пор в наших примерах мы использовали port-forward или определенные команды в minikube, чтобы получить доступ к нашим приложениям, но в производстве это не сработает. Нам нужен лучший способ доступа к нашим приложениям в масштабе с множеством пользователей.
Мы также говорили о возможности использования NodePort, но это опять же должно быть только в тестовых целях.
Ingress дает нам лучший способ открыть наши приложения, он позволяет нам определить правила маршрутизации в нашем кластере Kubernetes.
Для ingress мы создадим запрос на внутреннюю службу нашего приложения.
Когда вам нужен ingress? Если вы используете облачный провайдер, управляемое предложение Kubernetes, то, скорее всего, у них будет своя опция ingress для вашего кластера или они предоставят вам свой собственный балансировщик нагрузки. Вам не придется реализовывать это самостоятельно, что является одним из преимуществ управляемого Kubernetes.
Если вы управляете собственным кластером, вам необходимо настроить точку входа.
Настройка Ingress на Minikube На моем конкретном запущенном кластере под названием mc-demo я могу выполнить следующую команду, чтобы включить ingress на моем кластере.
minikube --profile='mc-demo' addons enable ingress.
Если теперь мы проверим наши пространства имен, то увидим, что у нас есть новое пространство имен ingress-nginx. kubectl get ns
Теперь мы должны создать YAML-конфигурацию ingress для запуска нашего сервиса Pacman. Я добавил этот файл в репозиторий pacman-ingress.yaml.
Затем мы можем создать его в нашем пространстве имен ingress с помощью kubectl create -f pacman-ingress.yaml.
Затем, если мы запустим kubectl get ingress -n pacman
Затем мне говорят, что поскольку мы используем minikube, работающий на WSL2 в Windows, мы должны создать туннель minikube, используя minikube tunnel --profile=mc-demo.
Но я все еще не могу получить доступ к 192.168.49.2 и играть в свою игру pacman.
Если у кого-нибудь есть или есть возможность заставить это работать под Windows и WSL, я буду благодарен за отзывы. Я подниму вопрос об этом в репозитории и вернусь к нему, как только у меня появится время и исправление.
UPDATE: Мне кажется, что этот блог помогает определить причину того, что игра не работает на WSL Configuring Ingress to run Minikube on WSL2 using Docker runtime
Ресурсы Kubernetes StatefulSet simply explained Kubernetes Volumes explained Kubernetes Ingress Tutorial for Beginners Kubernetes Documentation TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours] TechWorld with Nana - Kubernetes Crash Course for Absolute Beginners Kunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified! На этом мы завершаем раздел Kubernetes. Существует так много дополнительных материалов, которые мы могли бы осветить на тему Kubernetes, и 7 дней дают нам базовые знания, но есть люди, которые проходят 100DaysOfKubernetes, где вы можете погрузиться в самую гущу событий.
Далее мы рассмотрим инфраструктуру как код и ту важную роль, которую она играет с точки зрения DevOps.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day55/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day56/":{title:"56. Обзор IaC",tags:["devops"],content:`Обзор IaC Люди совершают ошибки! Автоматизация - это путь к успеху!
Как вы строите свои системы сегодня?
Каков был бы ваш план, если бы вы потеряли все, физические машины, виртуальные машины, облачные виртуальные машины, облачные PaaS и т.д. и т.п.?
Сколько времени у вас уйдет на замену всего?
Инфраструктура как код предоставляет решение, позволяющее сделать это и одновременно протестировать, не путайте это с резервным копированием и восстановлением, но что касается вашей инфраструктуры и сред, ваших платформ, мы должны быть в состоянии раскрутить их и обращаться с ними как со скотом и домашними животными.
TLDR; заключается в том, что мы можем использовать код для восстановления всей нашей среды.
Если мы также вспомним, что с самого начала мы говорили о DevOps в целом - это способ преодоления барьеров для безопасной и быстрой доставки систем в производство.
Infrastructure as code помогает нам поставлять системы, мы говорили о множестве процессов и инструментов. IaC предлагает нам больше инструментов, с которыми мы должны быть знакомы, чтобы обеспечить эту часть процесса.
В этом разделе мы сосредоточимся на инфраструктуре как коде. Вы также можете услышать упоминание этого термина как \u0026ldquo;инфраструктура из кода\u0026rdquo; или \u0026ldquo;конфигурация как код\u0026rdquo;. Я думаю, что наиболее известным термином является Инфраструктура как код.
Домашние животные против крупного рогатого скота Если мы посмотрим на до DevOps, то при необходимости создания нового приложения мы должны были подготовить наши серверы вручную.
Развернуть виртуальные машины | физические серверы и установить операционную систему Настроить сеть Создать таблицы маршрутизации Установить программное обеспечение и обновления Настроить программное обеспечение Установка базы данных Это ручной процесс, выполняемый системными администраторами. Чем больше приложение, тем больше ресурсов и серверов требуется, тем больше ручных усилий потребуется для создания этих систем. Это потребует огромного количества человеческих усилий и времени, но, кроме того, как компания, вы должны будете заплатить за эти ресурсы, чтобы создать эту среду. Как я уже говорил в начале раздела \u0026ldquo;Люди совершают ошибки! Автоматизация - это путь к успеху!\u0026rdquo;.
После вышеупомянутой фазы начальной установки вам предстоит обслуживание этих серверов.
Обновление версий Развертывание новых релизов Управление данными Восстановление приложений Добавление, удаление и масштабирование серверов Конфигурация сети Добавьте сюда сложность нескольких сред тестирования и разработки.
Именно здесь на помощь приходит Infrastructure as Code. Выше было время, когда мы заботились об этих серверах, как о домашних животных, люди даже называли их домашними именами или, по крайней мере, давали им какие-то имена, потому что они должны были находиться рядом какое-то время, они должны были стать частью \u0026ldquo;семьи\u0026rdquo; на какое-то время.
С Infrastructure as Code у нас есть возможность автоматизировать все эти задачи от конца до конца. Инфраструктура как код - это концепция, и есть инструменты, которые выполняют автоматическое обеспечение инфраструктуры. На данный момент, если с сервером случается что-то плохое, вы выбрасываете его и запускаете новый. Этот процесс автоматизирован, и сервер точно такой же, как определено в коде. В этот момент нам не важно, как они называются, они находятся в поле и служат своей цели до тех пор, пока их больше нет в поле, и нам нужно заменить их либо из-за сбоя, либо из-за обновления части или всего нашего приложения.
Это может быть использовано практически во всех платформах, виртуализации, облачных рабочих нагрузках, а также в облачной нативной инфраструктуре, такой как Kubernetes и контейнеры.
Обеспечение инфраструктуры Не все IaC охватывают все перечисленное ниже, вы увидите, что инструмент, который мы будем использовать в этом разделе, охватывает только первые две области; Terraform - это тот инструмент, который мы будем рассматривать, и он позволяет нам начать с нуля и определить в коде, как должна выглядеть наша инфраструктура, а затем развернуть ее, он также позволит нам управлять этой инфраструктурой и первоначально развернуть приложение, но в этот момент он потеряет контроль над приложением, и здесь на помощь приходит следующий раздел, и что-то вроде Ansible как инструмент управления конфигурацией может работать лучше на этом фронте.
Без забегания вперед такие инструменты, как chef, puppet и ansible, лучше всего подходят для начальной установки приложений, а затем для управления этими приложениями и их конфигурацией.
Первоначальная установка и настройка программного обеспечения
Развертывание новых серверов Конфигурация сети Создание балансировщиков нагрузки Конфигурация на уровне инфраструктуры Конфигурация инфраструктуры с провизией Установка приложения на серверы Подготовьте серверы для развертывания приложения. Развертывание приложения Развертывание и управление приложением Этап обслуживания Обновления программного обеспечения Реконфигурация Различия инструментов IaC Декларативный и процедурный
Процедурный
Пошаговая инструкция Создайте сервер \u0026gt; Добавьте сервер \u0026gt; Внесите это изменение Декларативный
объявить конечный результат 2 сервера Изменяемые (домашние животные) против неизменяемых (крупный рогатый скот)
Мутабельный
Изменение вместо замены Как правило, долгоживущие Неизменяемые
Замена вместо изменения Возможно, недолговечна Именно поэтому у нас есть множество различных вариантов Infrastructure as Code, потому что не существует одного инструмента, который бы управлял всеми.
Мы будем в основном использовать terraform и работать с ним, поскольку это лучший способ начать видеть преимущества инфраструктуры как кода в действии. Практическая работа - это также лучший способ приобрести навыки, так как вы будете писать код.
Далее мы начнем изучать Terraform со 101-го урока, прежде чем приступим к практическому использованию.
Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day56/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day57/":{title:"57. Введение в Terraform",tags:["devops"],content:`\u0026ldquo;Terraform - это инструмент для безопасного и эффективного создания, изменения и управления версиями инфраструктуры\u0026rdquo;. «Приведенная выше цитата взята из HashiCorp, HashiCorp - это компания, стоящая за Terraform.
\u0026ldquo;Terraform - это программный инструмент \u0026ldquo;инфраструктура как код\u0026rdquo; с открытым исходным кодом, который обеспечивает последовательный рабочий процесс CLI для управления сотнями облачных сервисов. Terraform кодирует облачные API в декларативные конфигурационные файлы\u0026rdquo;.
У HashiCorp есть отличный ресурс HashiCorp Learn, который охватывает все их продукты и дает несколько отличных демонстрационных примеров, когда вы пытаетесь достичь чего-то с помощью инфраструктуры как кода.
Все облачные провайдеры и локальные платформы обычно предоставляют нам доступ к консолям управления, которые позволяют нам создавать наши ресурсы с помощью пользовательского интерфейса, обычно эти платформы также предоставляют доступ к CLI или API для создания тех же ресурсов, но с API у нас есть возможность быстрого предоставления ресурсов.
Инфраструктура как код позволяет нам подключаться к этим API для развертывания наших ресурсов в нужном состоянии.
Ниже перечислены и другие инструменты, но они не являются исключительными или исчерпывающими. Если у вас есть другие инструменты, пожалуйста, поделитесь с нами через PR.
Cloud Specific Cloud Agnostic AWS CloudFormation Terraform Azure Resource Manager Pulumi Google Cloud Deployment Manager Это еще одна причина, почему мы используем Terraform, мы хотим быть независимыми от облаков и платформ, которые мы хотим использовать для наших демонстраций, а также в целом.
Обзор Terraform Terraform - это инструмент, ориентированный на обеспечение, Terraform - это CLI, который предоставляет возможности для обеспечения сложных инфраструктурных сред. С помощью Terraform мы можем определить сложные требования к инфраструктуре, существующей локально или удаленно (облако). Terraform позволяет нам не только создавать вещи на начальном этапе, но и поддерживать и обновлять эти ресурсы в течение всего срока их службы.
Здесь мы рассмотрим основные моменты, но для получения более подробной информации и множества ресурсов вы можете посетить сайт terraform.io.
Запись Terraform позволяет нам создавать декларативные конфигурационные файлы, которые будут создавать наше окружение. Файлы пишутся с помощью языка HashiCorp Configuration Language (HCL), который позволяет кратко описывать ресурсы с помощью блоков, аргументов и выражений. Мы, конечно, будем подробно рассматривать их при развертывании виртуальных машин, контейнеров и в Kubernetes.
План Возможность проверить, что вышеуказанные конфигурационные файлы развернут то, что мы хотим видеть, используя определенные функции terraform cli, чтобы иметь возможность протестировать этот план перед развертыванием чего-либо или изменением чего-либо. Помните, что Terraform - это инструмент для продолжения вашей инфраструктуры, если вы хотите изменить аспект вашей инфраструктуры, вы должны сделать это через terraform, чтобы все это было зафиксировано в коде.
Применить Очевидно, что когда вы будете довольны, вы сможете применить эту конфигурацию к множеству провайдеров, доступных в Terraform. Вы можете увидеть большое количество доступных провайдеров здесь.
Еще одна вещь, о которой следует упомянуть, это то, что также доступны модули, и это похоже на образы контейнеров в том, что эти модули были созданы и выложены в открытый доступ, так что вам не придется создавать их снова и снова, просто используйте лучшую практику развертывания определенного ресурса инфраструктуры одинаковым способом везде. Вы можете найти доступные модули здесь.
Рабочий процесс Terraform выглядит следующим образом: (взято с сайта terraform)
Terraform vs Vagrant Во время этого испытания мы использовали Vagrant, который является еще одним инструментом с открытым исходным кодом от Hashicorp, сконцентрированным на средах разработки.
Vagrant - это инструмент, ориентированный на управление средами разработки.
Terraform - это инструмент для создания инфраструктуры.
Отличное сравнение этих двух инструментов можно найти здесь на официальном сайте Hashicorp
Установка Terraform В установке Terraform нет ничего сложного.
Terraform является кроссплатформенным, и вы можете видеть ниже на моей Linux машине у нас есть несколько вариантов загрузки и установки CLI
Использование arkade для установки Terraform, arkade - это удобный инструмент для получения необходимых инструментов, приложений и clis на вашу систему. Простая команда arkade get terraform позволит обновить terraform, если он доступен, или эта же команда также установит Terraform CLI
Мы собираемся больше узнать о HCL, а также начать использовать Terraform для создания некоторых инфраструктурных ресурсов на различных платформах.
Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day57/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day58/":{title:"58. Язык конфигурации HashiCorp (HCL)",tags:["devops"],content:`Язык конфигурации HashiCorp (HCL) Прежде чем мы начнем создавать вещи с помощью Terraform, мы должны немного погрузиться в язык HashiCorp Configuration Language (HCL). До сих пор в ходе нашей задачи мы рассмотрели несколько различных языков скриптов и программирования, и вот еще один. Мы затронули язык программирования Go, затем скрипты bash, мы даже немного затронули python, когда дело дошло до автоматизации сети.
Теперь мы должны рассмотреть язык конфигурации HashiCorp (HCL), если вы впервые видите этот язык, он может показаться немного пугающим, но он довольно прост и очень мощный.
По мере продвижения по этому разделу мы будем использовать примеры, которые мы можем запустить локально на нашей системе, независимо от того, какую ОС вы используете, мы будем использовать virtualbox, хотя и не инфраструктурную платформу, которую вы обычно используете с Terraform. Тем не менее, запуск этого локально, он бесплатный и позволит нам достичь того, что мы ищем в этой заметке. Мы также можем расширить концепцию этого поста на docker или Kubernetes.
В целом, вы будете или должны использовать Terraform для развертывания инфраструктуры в публичном облаке (AWS, Google, Microsoft Azure), а также в средах виртуализации, таких как (VMware, Microsoft Hyper-V, Nutanix AHV). В публичном облаке Terraform позволяет нам делать гораздо больше, чем просто автоматическое развертывание виртуальных машин, мы можем создавать всю необходимую инфраструктуру, такую как рабочие нагрузки PaaS, и все необходимые сетевые ресурсы, такие как VPC и группы безопасности.
В Terraform есть два важных аспекта: код, который мы рассмотрим в этой статье, и состояние. Оба этих аспекта вместе можно назвать ядром Terraform. Затем у нас есть среда, в которую мы хотим обратиться и развернуть, которая выполняется с помощью провайдеров Terraform, кратко упомянутых на прошлом занятии, но у нас есть провайдеры AWS, есть провайдеры Azure и т.д. Их сотни. Их сотни.
Базовое использование Terraform Давайте посмотрим на файл Terraform .tf, чтобы увидеть, как они создаются. Первый пример, который мы рассмотрим, будет кодом для развертывания ресурсов на AWS, для этого также потребуется установить AWS CLI на вашей системе и настроить его для вашей учетной записи.
Providers В верхней части нашей файловой структуры .tf, обычно называемой main.tf, по крайней мере до тех пор, пока мы не сделаем все более сложным. Здесь мы определим провайдеров, о которых мы упоминали ранее. Наш источник провайдера aws, как вы видите, hashicorp/aws, это означает, что провайдер поддерживается или был опубликован самой компанией hashicorp. По умолчанию вы будете ссылаться на провайдеров, доступных в Terraform Registry, у вас также есть возможность написать свои собственные провайдеры и использовать их локально или самостоятельно опубликовать в Terraform Registry.
terraform {required_providers {aws = {source = \u0026quot;hashicorp/aws\u0026quot;version = \u0026quot;~\u0026gt; 3.0\u0026quot;}}}Здесь мы также можем добавить регион, чтобы определить, какой регион AWS мы хотим предоставить, мы можем сделать это, добавив следующее:
provider \u0026quot;aws\u0026quot; {region = \u0026quot;ap-southeast-1\u0026quot; //region where resources need to be deployed}Resources Другой важный компонент конфигурационного файла terraform, который описывает один или несколько объектов инфраструктуры, таких как EC2, Load Balancer, VPC и т.д.
Блок ресурсов объявляет ресурс заданного типа (\u0026ldquo;aws_instance\u0026rdquo;) с заданным локальным именем (\u0026ldquo;90daysofdevops\u0026rdquo;).
Тип ресурса и имя вместе служат идентификатором для данного ресурса.
resource \u0026quot;aws_instance\u0026quot; \u0026quot;90daysofdevops\u0026quot; {ami = data.aws_ami.instance_id.idinstance_type = \u0026quot;t2.micro\u0026quot;availability_zone = \u0026quot;us-west-2a\u0026quot;security_groups = [aws_security_group.allow_web.name]user_data = \u0026lt;\u0026lt;-EOF#! /bin/bashsudo yum updatesudo yum install -y httpdsudo systemctl start httpdsudo systemctl enable httpdecho \u0026quot;\u0026lt;h1\u0026gt;Deployed via Terraform\u0026lt;/h1\u0026gt;\u0026quot; | sudo tee /var/www/html/index.htmlEOFtags = {Name = \u0026quot;Created by Terraform\u0026quot;}}Из вышеприведенного видно, что мы также запускаем обновление yum и устанавливаем httpd в наш экземпляр ec2.
Если мы теперь посмотрим на полный файл main.tf, он может выглядеть примерно так.
terraform {required_providers {aws = {source = \u0026quot;hashicorp/aws\u0026quot;version = \u0026quot;~\u0026gt; 3.27\u0026quot;}}required_version = \u0026quot;\u0026gt;= 0.14.9\u0026quot;}provider \u0026quot;aws\u0026quot; {profile = \u0026quot;default\u0026quot;region = \u0026quot;us-west-2\u0026quot;}resource \u0026quot;aws_instance\u0026quot; \u0026quot;90daysofdevops\u0026quot; {ami = \u0026quot;ami-830c94e3\u0026quot;instance_type = \u0026quot;t2.micro\u0026quot;availability_zone = \u0026quot;us-west-2a\u0026quot;user_data = \u0026lt;\u0026lt;-EOF#! /bin/bashsudo yum updatesudo yum install -y httpdsudo systemctl start httpdsudo systemctl enable httpdecho \u0026quot;\u0026lt;h1\u0026gt;Deployed via Terraform\u0026lt;/h1\u0026gt;\u0026quot; | sudo tee /var/www/html/index.htmlEOFtags = {Name = \u0026quot;Created by Terraform\u0026quot;tags = {Name = \u0026quot;ExampleAppServerInstance\u0026quot;}}Приведенный выше код позволит развернуть очень простой веб-сервер в качестве экземпляра ec2 в AWS. Самое замечательное в этой и любой другой подобной конфигурации то, что мы можем повторить ее и каждый раз получать один и тот же результат. Кроме вероятности того, что я испортил код, нет никакого взаимодействия с человеком.
Мы можем рассмотреть суперпростой пример, который вы, скорее всего, никогда не будете использовать, но давайте все равно пошутим. Как и во всех хороших скриптах и языках программирования, мы должны начать со скрипта приветствия мира.
terraform {# This module is now only being tested with Terraform 0.13.x. However, to make upgrading easier, we are setting# 0.12.26 as the minimum version, as that version added support for required_providers with source URLs, making it# forwards compatible with 0.13.x code.required_version = \u0026quot;\u0026gt;= 0.12.26\u0026quot;}# website::tag::1:: The simplest possible Terraform module: it just outputs \u0026quot;Hello, World!\u0026quot;output \u0026quot;hello_world\u0026quot; {value = \u0026quot;Hello, 90DaysOfDevOps from Terraform\u0026quot;}Вы найдете этот файл в папке IAC в разделе hello-world, но из коробки он не будет просто работать, есть несколько команд, которые необходимо выполнить, чтобы использовать наш код терраформы.
В терминале перейдите в папку, где был создан файл main.tf, он может быть из этого репозитория или вы можете создать новый, используя код выше.
Находясь в этой папке, выполните команду terraform init.
Мы должны выполнить эту команду в любой директории, где у нас есть или перед запуском любого кода terraform. Инициализация каталога конфигурации загружает и устанавливает провайдеров, определенных в конфигурации, в данном случае у нас нет провайдеров, но в примере выше это загрузит провайдера aws для этой конфигурации.
Следующей командой будет terraform plan.
Команда terraform plan создает план выполнения, который позволяет вам предварительно просмотреть изменения, которые Terraform планирует внести в вашу инфраструктуру.
Вы можете видеть ниже, что на нашем примере hello-world мы увидим результат, если бы это был экземпляр AWS ec2, мы бы увидели все шаги, которые мы будем создавать.
На данном этапе мы инициализировали наш репозиторий, загрузили провайдеров, где это необходимо, запустили тестовый проход, чтобы убедиться, что это то, что мы хотим видеть, теперь мы можем запустить и развернуть наш код.
Команда terraform apply позволяет нам это сделать, в нее встроена мера безопасности, и это снова даст вам представление о том, что произойдет, что требует от вас ответа \u0026ldquo;да\u0026rdquo;, чтобы продолжить.
Когда мы вводим \u0026ldquo;да\u0026rdquo;, чтобы ввести значение, наш код развертывается. Очевидно, это не так интересно, но вы можете видеть, что у нас есть вывод, который мы определили в нашем коде.
Теперь мы ничего не развернули, мы ничего не добавили, не изменили и не уничтожили, но если бы мы это сделали, то мы бы увидели, что это также указано выше. Однако если мы что-то развернули и хотим избавиться от всего, что развернули, мы можем использовать команду terraform destroy. Опять же, это имеет ту безопасность, когда вы должны ввести \u0026ldquo;да\u0026rdquo;, хотя вы можете использовать --auto-approve в конце ваших команд apply и destroy, чтобы обойти это ручное вмешательство. Но я бы посоветовал использовать это сокращение только в процессе обучения и тестирования, так как все будет исчезать иногда быстрее, чем было создано.
Таким образом, мы рассмотрели всего 4 команды из Terraform CLI.
terraform init = подготовить папку проекта с провайдерами terraform plan = показать, что будет создано, изменено во время следующей команды на основе нашего кода. terraform apply = развернет ресурсы, определенные в нашем коде. terraform destroy = уничтожит ресурсы, которые мы создали в нашем проекте. Мы также рассмотрели два важных аспекта наших кодовых файлов.
providers = как terraform общается с конечной платформой через API-интерфейсы resources = что именно мы хотим развернуть с помощью кода Еще одна вещь, которую следует отметить, когда мы запускаем terraform init, посмотрите на дерево в папке до и после, чтобы увидеть, что происходит и где мы храним провайдеры и модули.
Terraform state Нам также необходимо знать о файле состояния, который создается также внутри нашей директории, и для этого примера hello world наш файл состояния прост. Это JSON-файл, который является представлением мира в соответствии с Terraform. Состояние будет радостно демонстрировать ваши конфиденциальные данные, поэтому будьте осторожны и в качестве лучшей практики помещайте файлы .tfstate в папку .gitignore перед загрузкой на GitHub.
По умолчанию файл состояния, как вы видите, находится в том же каталоге, что и код вашего проекта, но его можно хранить и удаленно. В производственной среде это, скорее всего, будет общее место, например, ведро S3.
Другим вариантом может быть Terraform Cloud, это платная управляемая услуга. (Бесплатно до 5 пользователей)
Плюсы хранения состояния в удаленном месте заключаются в том, что мы получаем:
{\u0026quot;version\u0026quot;: 4,\u0026quot;terraform_version\u0026quot;: \u0026quot;1.1.6\u0026quot;,\u0026quot;serial\u0026quot;: 1,\u0026quot;lineage\u0026quot;: \u0026quot;a74296e7-670d-0cbb-a048-f332696ca850\u0026quot;,\u0026quot;outputs\u0026quot;: {\u0026quot;hello_world\u0026quot;: {\u0026quot;value\u0026quot;: \u0026quot;Hello, 90DaysOfDevOps from Terraform\u0026quot;,\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;}},\u0026quot;resources\u0026quot;: []}Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day58/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day59/":{title:"59. Создание виртуальной машины с помощью Terraform",tags:["devops"],content:`Создание виртуальной машины с помощью Terraform и переменных В этой сессии мы будем создавать виртуальную машину или две виртуальные машины с помощью Terraform внутри VirtualBox. Это не совсем обычно, VirtualBox - это вариант виртуализации рабочих станций, и на самом деле это не было бы вариантом использования Terraform, но я сейчас нахожусь на высоте 36 000 футов в воздухе, и как бы я ни развертывал ресурсы публичного облака так высоко в облаках, гораздо быстрее сделать это локально на моем ноутбуке.
Чисто демонстрационная цель, но концепция та же, мы собираемся иметь наш желаемый код конфигурации состояния, а затем мы собираемся запустить его против провайдера virtualbox. В прошлом мы использовали здесь vagrant, и я рассказал о различиях между vagrant и terraform в начале раздела.
Создание виртуальной машины в VirtualBox Первое, что мы сделаем, это создадим новую папку под названием virtualbox, затем мы можем создать файл virtualbox.tf, в котором мы определим наши ресурсы. Приведенный ниже код, который можно найти в папке VirtualBox под названием virtualbox.tf, создаст 2 виртуальные машины в Virtualbox.
Вы можете узнать больше о сообществе провайдера Virtualbox здесь
terraform {required_providers {virtualbox = {source = \u0026quot;terra-farm/virtualbox\u0026quot;version = \u0026quot;0.2.2-alpha.1\u0026quot;}}}# В настоящее время нет никаких опций конфигурации для самого провайдера.resource \u0026quot;virtualbox_vm\u0026quot; \u0026quot;node\u0026quot; {count = 2name = format(\u0026quot;node-%02d\u0026quot;, count.index + 1)image = \u0026quot;https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box\u0026quot;cpus = 2memory = \u0026quot;512 mib\u0026quot;network_adapter {type = \u0026quot;hostonly\u0026quot;host_interface = \u0026quot;vboxnet1\u0026quot;}}output \u0026quot;IPAddr\u0026quot; {value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 1)}output \u0026quot;IPAddr_2\u0026quot; {value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 2)}Теперь, когда мы определили наш код, мы можем выполнить terraform init для нашей папки, чтобы загрузить провайдер для virtualbox.
Очевидно, что в вашей системе также должен быть установлен virtualbox. Затем мы можем запустить terraform plan, чтобы посмотреть, что наш код создаст для нас. Затем следует terraform apply. На рисунке ниже показан завершенный процесс.
Теперь в Virtualbox вы увидите две виртуальные машины.
Изменение конфигурации Давайте добавим еще один узел в наше развертывание. Мы можем просто изменить строку count, чтобы показать новое желаемое количество узлов. Когда мы запустим нашу terraform apply, она будет выглядеть примерно так, как показано ниже.
После завершения работы в virtualbox вы можете увидеть, что у нас теперь есть 3 узла.
Когда мы закончим, мы можем очистить все это с помощью команды terraform destroy, и наши машины будут удалены.
Переменные и выходные данные Мы упоминали о выводах, когда выполняли пример hello-world на прошлом занятии. Но здесь мы можем остановиться на этом более подробно.
Но есть много других переменных, которые мы можем использовать здесь, также есть несколько различных способов, которыми мы можем определить переменные.
Мы можем вручную ввести наши переменные с помощью команды terraform plan или terraform apply.
Мы можем определить их в .tf-файле внутри блока
Мы можем использовать переменные окружения в нашей системе, используя TF_VAR_NAME в качестве формата.
Я предпочитаю использовать файл terraform.tfvars в папке нашего проекта.
Существует опция *auto.tfvars файла
или мы можем определить, когда запускаем terraform plan или terraform apply с помощью var или var-file.
Порядок определения переменных будет начинаться снизу вверх.
Мы также упоминали, что файл состояния будет содержать конфиденциальную информацию. Мы можем определить нашу чувствительную информацию как переменную и определить ее как чувствительную.
variable \u0026quot;some resource\u0026quot; {description = \u0026quot;something important\u0026quot;type: stringsensitive = true}Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day59/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day60/":{title:"60. Контейнеры, провайдеры и модули Docker",tags:["devops"],content:`Контейнеры и модули Docker Вчера мы развернули виртуальную машину с помощью Terraform в нашей локальной среде FREE virtualbox. В этом разделе мы собираемся развернуть контейнер Docker с некоторой конфигурацией в нашей локальной среде Docker.
Docker Demo Для начала мы используем приведенный ниже блок кода, суть которого заключается в том, что мы хотим развернуть простое веб-приложение в docker и опубликовать его, чтобы оно было доступно в нашей сети. Мы будем использовать nginx и сделаем его доступным извне на нашем ноутбуке через localhost и порт 8000. Мы используем провайдера docker из сообщества, и вы можете видеть образ docker, который мы используем, также указанный в нашей конфигурации.
terraform {required_providers {docker = {source = \u0026quot;kreuzwerker/docker\u0026quot;version = \u0026quot;2.16.0\u0026quot;}}}provider \u0026quot;docker\u0026quot; {}resource \u0026quot;docker_image\u0026quot; \u0026quot;nginx\u0026quot; {name = \u0026quot;nginx:latest\u0026quot;keep_locally = false}resource \u0026quot;docker_container\u0026quot; \u0026quot;nginx\u0026quot; {image = docker_image.nginx.latestname = \u0026quot;tutorial\u0026quot;ports {internal = 80external = 8000}}Первой задачей является использование команды terraform init для загрузки провайдера на нашу локальную машину.
Затем мы запускаем команду terraform apply, а затем docker ps, и вы можете увидеть, что у нас есть запущенный контейнер.
Если мы откроем браузер, то перейдем по адресу http://localhost:8000/ и увидим, что у нас есть доступ к нашему контейнеру NGINX.
Вы можете узнать больше информации о Docker Provider.
Выше приведена очень простая демонстрация того, что можно сделать с помощью Terraform плюс Docker и как мы теперь можем управлять этим в состоянии Terraform. Мы рассматривали docker compose в разделе о контейнерах, и есть небольшое пересечение между этим, инфраструктурой как код, а также Kubernetes.
Для демонстрации того, как Terraform может справиться с более сложными задачами, мы возьмем файл docker compose для wordpress и mysql, который мы создали с помощью docker compose, и поместим его в Terraform. Вы можете найти docker-wordpress.tf
terraform {required_providers {docker = {source = \u0026quot;kreuzwerker/docker\u0026quot;version = \u0026quot;2.16.0\u0026quot;}}}provider \u0026quot;docker\u0026quot; {}variable wordpress_port {default = \u0026quot;8080\u0026quot;}resource \u0026quot;docker_volume\u0026quot; \u0026quot;db_data\u0026quot; {name = \u0026quot;db_data\u0026quot;}resource \u0026quot;docker_network\u0026quot; \u0026quot;wordpress_net\u0026quot; {name = \u0026quot;wordpress_net\u0026quot;}resource \u0026quot;docker_container\u0026quot; \u0026quot;db\u0026quot; {name = \u0026quot;db\u0026quot;image = \u0026quot;mysql:5.7\u0026quot;restart = \u0026quot;always\u0026quot;network_mode = \u0026quot;wordpress_net\u0026quot;env = [\u0026quot;MYSQL_ROOT_PASSWORD=wordpress\u0026quot;,\u0026quot;MYSQL_PASSWORD=wordpress\u0026quot;,\u0026quot;MYSQL_USER=wordpress\u0026quot;,\u0026quot;MYSQL_DATABASE=wordpress\u0026quot;]mounts {type = \u0026quot;volume\u0026quot;target = \u0026quot;/var/lib/mysql\u0026quot;source = \u0026quot;db_data\u0026quot;}}resource \u0026quot;docker_container\u0026quot; \u0026quot;wordpress\u0026quot; {name = \u0026quot;wordpress\u0026quot;image = \u0026quot;wordpress:latest\u0026quot;restart = \u0026quot;always\u0026quot;network_mode = \u0026quot;wordpress_net\u0026quot;env = [\u0026quot;WORDPRESS_DB_HOST=db:3306\u0026quot;,\u0026quot;WORDPRESS_DB_USER=wordpress\u0026quot;,\u0026quot;WORDPRESS_DB_NAME=wordpress\u0026quot;,\u0026quot;WORDPRESS_DB_PASSWORD=wordpress\u0026quot;]ports {internal = \u0026quot;80\u0026quot;external = \u0026quot;\${var.wordpress_port}\u0026quot;}}Мы снова помещаем это в новую папку и затем запускаем команду terraform init, чтобы извлечь необходимые нам провайдеры.
Затем мы запускаем команду terraform apply и смотрим на вывод docker ps, мы должны увидеть наши только что созданные контейнеры.
Затем мы можем перейти к нашему фронт-энду WordPress. Точно так же, как мы проходили этот процесс с docker-compose в разделе о контейнерах, теперь мы можем выполнить установку, и наши посты wordpress будут жить в нашей базе данных MySQL.
Очевидно, что теперь мы рассмотрели контейнеры и Kubernetes в некоторых деталях, мы, вероятно, знаем, что это подходит для тестирования, но если бы вы действительно собирались запустить веб-сайт, вы бы не стали делать это только с помощью контейнеров и рассмотрели бы использование Kubernetes для достижения этой цели, Далее мы рассмотрим использование Terraform с Kubernetes.
Provisioners Провайдеры существуют для того, чтобы если что-то не может быть декларировано, у нас был способ разобрать это для нашего развертывания.
Если у вас нет другой альтернативы, и добавление такой сложности в ваш код - это то, что вам нужно, то вы можете сделать это, выполнив что-то похожее на следующий блок кода.
ресурс \u0026quot;docker_container\u0026quot; \u0026quot;db\u0026quot; { # ...provisioner \u0026quot;local-exec\u0026quot; {command = \u0026quot;echo The server's IP address is \${self.private_ip}\u0026quot;}}Удаленный исполнительный провайдер вызывает скрипт на удаленном ресурсе после его создания. Это может быть использовано для чего-то специфического для ОС, или это может быть использовано для обертывания в инструмент управления конфигурацией. Хотя заметьте, что некоторые из них мы уже рассмотрели в собственных провайдерах.
Средство подготовки удаленных исполняемых файлов вызывает скрипт на удаленном ресурсе после его создания. Это может быть использовано для чего-то определенного для ОС или может быть использовано для включения инструмента управления конфигурацией. Хотя обратите внимание, что у нас есть некоторые из них, покрытые их собственными провизорами. Подробнее о провизорах](https://www.terraform.io/language/resources/provisioners/syntax)
file local-exec remote-exec vendor ansible chef puppet Модули Модули - это контейнеры для нескольких ресурсов, которые используются вместе. Модуль состоит из коллекции файлов .tf в одном каталоге.
Модули - это хороший способ разделить ресурсы инфраструктуры, а также возможность использовать уже созданные сторонние модули, чтобы не изобретать колесо.
Например, если бы мы хотели использовать один и тот же проект для создания нескольких виртуальных машин, VPC, групп безопасности, а затем кластера Kubernetes, мы бы, вероятно, захотели разделить наши ресурсы на модули, чтобы лучше определить наши ресурсы и их группировку.
Еще одним преимуществом модулей является то, что вы можете взять эти модули и использовать их в других проектах или публично поделиться ими, чтобы помочь сообществу.
Мы разбиваем нашу инфраструктуру на компоненты, компоненты известны здесь как модули.
Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day60/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day61/":{title:"61. Kubernetes и множественные среды",tags:["devops"],content:`Kubernetes и множественные среды До сих пор в этом разделе, посвященном инфраструктуре как коду, мы рассматривали развертывание виртуальных машин, хотя и с помощью virtualbox, но суть одна и та же: мы определяем в коде, как должна выглядеть наша виртуальная машина, а затем развертываем ее. То же самое касается контейнеров Docker, и на этом занятии мы рассмотрим, как Terraform можно использовать для взаимодействия с ресурсами, поддерживаемыми Kubernetes.
Я использовал Terraform для развертывания своих кластеров Kubernetes в демонстрационных целях на трех основных облачных провайдерах, и вы можете найти репозиторий tf_k8deploy.
Однако вы также можете использовать Terraform для взаимодействия с объектами внутри кластера Kubernetes, это может быть использование Kubernetes provider или Helm provider для управления развертыванием диаграмм.
Теперь мы можем использовать kubectl, как мы показывали в предыдущих разделах. Но есть некоторые преимущества использования Terraform в вашей среде Kubernetes.
Унифицированный рабочий процесс - если вы использовали Terraform для развертывания кластеров, вы можете использовать тот же рабочий процесс и инструмент для развертывания в кластерах Kubernetes.
Управление жизненным циклом - Terraform - это не просто инструмент инициализации, он позволяет вносить изменения, обновления и удаления.
Простая демонстрация Kubernetes Подобно демо, которое мы создали на прошлом занятии, мы можем развернуть nginx в нашем кластере Kubernetes, я снова буду использовать minikube в демонстрационных целях. Мы создаем наш файл Kubernetes.tf, который вы можете найти в папке.
В этом файле мы определим нашего провайдера Kubernetes, укажем на наш файл kubeconfig, создадим пространство имен nginx, затем создадим развертывание, содержащее 2 реплики и, наконец, сервис.
terraform {required_providers {kubernetes = {source = \u0026quot;hashicorp/kubernetes\u0026quot;version = \u0026quot;\u0026gt;= 2.0.0\u0026quot;}}}provider \u0026quot;kubernetes\u0026quot; {config_path = \u0026quot;~/.kube/config\u0026quot;}resource \u0026quot;kubernetes_namespace\u0026quot; \u0026quot;test\u0026quot; {metadata {name = \u0026quot;nginx\u0026quot;}}resource \u0026quot;kubernetes_deployment\u0026quot; \u0026quot;test\u0026quot; {metadata {name = \u0026quot;nginx\u0026quot;namespace = kubernetes_namespace.test.metadata.0.name}spec {replicas = 2selector {match_labels = {app = \u0026quot;MyTestApp\u0026quot;}}template {metadata {labels = {app = \u0026quot;MyTestApp\u0026quot;}}spec {container {image = \u0026quot;nginx\u0026quot;name = \u0026quot;nginx-container\u0026quot;port {container_port = 80}}}}}}resource \u0026quot;kubernetes_service\u0026quot; \u0026quot;test\u0026quot; {metadata {name = \u0026quot;nginx\u0026quot;namespace = kubernetes_namespace.test.metadata.0.name}spec {selector = {app = kubernetes_deployment.test.spec.0.template.0.metadata.0.labels.app}type = \u0026quot;NodePort\u0026quot;port {node_port = 30201port = 80target_port = 80}}}Первое, что мы должны сделать в папке нашего нового проекта, это выполнить команду terraform init.
А затем, прежде чем мы выполним команду terraform apply, позвольте мне показать вам, что у нас нет пространств имен.
Когда мы запустим нашу команду apply, она создаст эти 3 новых ресурса, пространство имен, развертывание и сервис в нашем кластере Kubernetes.
Теперь мы можем взглянуть на развернутые ресурсы в нашем кластере.
Теперь, поскольку мы используем minikube, и вы видели в предыдущем разделе, это имеет свои собственные ограничения, когда мы пытаемся играть с сетью docker для ingress. Но если мы просто выполним команду kubectl port-forward -n nginx svc/nginx 30201:80 и откроем браузер на http://localhost:30201/, мы увидим нашу страницу NGINX.
Если вы хотите попробовать более подробные демонстрации с Terraform и Kubernetes, то на сайте HashiCorp Learn site вы сможете ознакомиться с ними.
Множественные окружения Если мы хотим взять любой из демонстрационных примеров, которые мы проверили, но теперь хотим, чтобы определенные среды производства, постановки и разработки выглядели одинаково и использовали этот код, есть два подхода для достижения этого с помощью Terraform
терраформенные рабочие пространства - несколько именованных разделов в рамках одного бэкенда
файловая структура - расположение каталогов обеспечивает разделение, модули обеспечивают повторное использование.
Каждый из этих подходов имеет свои плюсы и минусы.
terraform workspaces Плюсы
Легко начать работу Удобное выражение terraform.workspace Минимизирует дублирование кода Минусы
Склонность к человеческим ошибкам (мы пытались устранить это, используя TF) Состояние хранится в одном бэкенде Кодовая база не показывает однозначно конфигурации развертывания. Файловая структура Плюсы
Изоляция бэкендов повышенная безопасность снижен потенциал для человеческих ошибок Кодовая база полностью представляет развернутое состояние Минусы
Требуется многократное применение terraform для обеспечения окружения больше дублирования кода, но его можно минимизировать с помощью модулей. Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day61/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day62/":{title:"62. Terraform - Тестирование, инструменты и альтернативы",tags:["devops"],content:`Тестирование, инструменты и альтернативы Завершая этот раздел об инфраструктуре как коде, мы должны упомянуть о тестировании нашего кода, различных доступных инструментах, а также о некоторых альтернативах Terraform для достижения этой цели. Как я уже говорил в начале раздела, я остановился на Terraform, поскольку он, во-первых, бесплатный и с открытым исходным кодом, во-вторых, он кроссплатформенный и не зависит от окружения. Но есть и альтернативы, которые следует рассмотреть, но общая цель состоит в том, чтобы донести до людей, что это способ развертывания инфраструктуры.
Code Rot Первая область, которую я хочу затронуть в этой сессии, - это гниение кода. В отличие от кода приложений, инфраструктура как код может использоваться, а затем не использоваться в течение очень долгого времени. Возьмем пример: мы собираемся использовать Terraform для развертывания нашей среды VM в AWS, все идеально, все работает с первого раза, и у нас есть наша среда, но эта среда не меняется слишком часто, поэтому код остается в состоянии, возможно, или, надеюсь, хранится в центральном месте, но код не меняется.
А что если что-то изменится в инфраструктуре? Но это делается вне диапазона, или другие вещи меняются в нашей среде.
Внеполосные изменения (Out of band changes) Неприкрепленные версии (Unpinned versions) Утратившие актуальность зависимости (Deprecated dependancies) Неприменимые изменения (Unapplied changes) Тестирование Еще одна огромная область, которая следует за гниением кода и в целом, это возможность протестировать ваш IaC и убедиться, что все области работают так, как должны.
Прежде всего, есть несколько встроенных команд тестирования, на которые мы можем взглянуть:
Command Description terraform fmt Rewrite Terraform configuration files to a canonical format and style. terraform validate Validates the configuration files in a directory, referring only to the configuration terraform plan Creates an execution plan, which lets you preview the changes that Terraform plans to make Custom validation Validation of your input variables to ensure they match what you would expect them to be У нас также есть некоторые инструменты тестирования, доступные вне Terraform:
tflint
Найти возможные ошибки (Find possible errors) Предупреждать об устаревшем синтаксисе, неиспользуемых объявлениях. (Warn about deprecated syntax, unused declarations.) Применять лучшие практики, соглашения об именовании. (Enforce best practices, naming conventions.) Инструменты сканирования
checkov - сканирование конфигураций облачной инфраструктуры для поиска неправильных конфигураций до их развертывания. tfsec - сканер безопасности статического анализа для кода Terraform. terrascan - статический анализатор кода для Infrastructure as Code. terraform-compliance - легковесный тестовый фреймворк, ориентированный на безопасность и соответствие требованиям, для terraform, позволяющий проводить негативное тестирование вашей инфраструктуры как кода. snyk - сканирует код Terraform на предмет неправильной конфигурации и проблем безопасности. Управляемое облачное предложение
Terraform Sentinel - встроенный фреймворк политики как кода, интегрированный с продуктами HashiCorp Enterprise. Она позволяет принимать решения о политике на основе логики и может быть расширена для использования информации из внешних источников. Автоматизированное тестирование
Terratest - Terratest - это библиотека Go, которая предоставляет шаблоны и вспомогательные функции для инфраструктуры тестирования. Стоит упомянуть
Terraform Cloud - Terraform Cloud - это управляемый сервис компании HashiCorp. Оно устраняет необходимость в ненужных инструментах и документации для практиков, команд и организаций для использования Terraform в производстве.
Terragrunt - Terragrunt - это тонкая обертка, которая предоставляет дополнительные инструменты для сохранения DRY конфигураций, работы с несколькими модулями Terraform и управления удаленным состоянием.
Atlantis - Terraform Pull Request Automation.
Альтернативы В день 57, когда мы начали этот раздел, мы упоминали, что есть некоторые альтернативы, и я очень планирую изучить их после завершения этой задачи.
Cloud Specific Cloud Agnostic AWS CloudFormation Terraform Azure Resource Manager Pulumi Google Cloud Deployment Manager Я использовал AWS CloudFormation, вероятно, больше всего из вышеперечисленного списка, он является родным для AWS, но я не использовал другие, кроме Terraform. Как вы можете себе представить, версии для конкретных облаков очень хороши для конкретного облака, но если у вас несколько облачных сред, то вам будет сложно перенести эти конфигурации или у вас будет несколько плоскостей управления для ваших усилий IaC.
Я думаю, что следующим интересным шагом для меня будет уделить некоторое время и узнать больше о Pulumi.
Из сравнения Pulumi на их сайте
\u0026ldquo;И Terraform, и Pulumi предлагают модель инфраструктуры желаемого состояния как кода, где код представляет желаемое состояние инфраструктуры, а механизм развертывания сравнивает это желаемое состояние с текущим состоянием стека и определяет, какие ресурсы должны быть созданы, обновлены или удалены\u0026rdquo;.
Самое большое отличие, которое я вижу, заключается в том, что в отличие от HashiCorp Configuration Language (HCL) Pulumi позволяет использовать языки общего назначения, такие как Python, TypeScript, JavaScript, Go и .NET.
Краткий обзор Introduction to Pulumi: Modern Infrastructure as Code Мне нравится простота и возможность выбора, которую вам предлагают, и я хочу разобраться в этом немного подробнее.
На этом мы завершаем раздел \u0026ldquo;Инфраструктура как код\u0026rdquo; и переходим к тому, что немного пересекается с управлением конфигурацией, и, в частности, по мере того, как мы переходим к общей картине управления конфигурацией, мы будем использовать Ansible для некоторых из этих задач и демонстраций.
Ресурсы What is Infrastructure as Code? Difference of Infrastructure as Code Tools Terraform Tutorial | Terraform Course Overview 2021 Terraform explained in 15 mins | Terraform Tutorial for Beginners Terraform Course - From BEGINNER to PRO! HashiCorp Terraform Associate Certification Course Terraform Full Course for Beginners KodeKloud - Terraform for DevOps Beginners + Labs: Complete Step by Step Guide! Terraform Simple Projects Terraform Tutorial - The Best Project Ideas Awesome Terraform Pulumi - IaC in your favorite programming language! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day62/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day63/":{title:"63. Инструменты управления конфигурацией - Ansible/Terraform",tags:["devops"],content:`Введение: Управление конфигурацией Сразу после раздела, посвященного инфраструктуре как коду, мы, вероятно, будем говорить об управлении конфигурацией или управлении конфигурацией приложений.
Управление конфигурацией - это процесс поддержания приложений, систем и серверов в требуемом состоянии. Пересечение с Infrastructure as code заключается в том, что IaC гарантирует, что ваша инфраструктура находится в желаемом состоянии, но после этого, особенно terraform, не будет заботиться о желаемом состоянии настроек вашей ОС или приложений, и именно здесь на помощь приходят инструменты управления конфигурацией. Убедитесь, что система и приложения работают так, как ожидается, поскольку изменения происходят в Deane.
Управление конфигурацией убережет вас от внесения мелких или крупных изменений, которые останутся недокументированными.
Почему вы хотите использовать управление конфигурацией Сценарий или почему вы хотите использовать управление конфигурацией, познакомьтесь с Дином. Он наш системный администратор, и Дин - счастливый турист, который работает над всеми своими системами. работает над всеми системами в своем окружении.
Что произойдет, если их система выйдет из строя, если случится пожар, сервер выйдет из строя? Дин точно знает, что делать, он может легко устранить пожар, но если несколько серверов начнут выходить из строя, особенно если у вас большая и расширяющаяся среда, вот почему Дину действительно необходимо иметь инструмент управления конфигурацией. Инструменты управления конфигурацией могут помочь Дину выглядеть как рок-звезда, все, что ему нужно сделать, это настроить правильные коды, которые позволят ему быстро, эффективно и масштабно передать инструкции по настройке каждого из серверов.
Инструменты управления конфигурацией Существует множество инструментов управления конфигурацией, и каждый из них имеет специфические особенности, которые делают его лучше для одних ситуаций, чем для других.
На этом этапе мы быстро рассмотрим варианты, показанные на рисунке выше, прежде чем сделать выбор, какой из них мы будем использовать и почему.
Chef
Chef обеспечивает последовательное применение конфигурации в любой среде, в любом масштабе с помощью автоматизации инфраструктуры. Chef - это инструмент с открытым исходным кодом, разработанный компанией OpsCode и написанный на Ruby и Erlang. Chef лучше всего подходит для организаций, которые имеют гетерогенную инфраструктуру и ищут зрелые решения. Рецепты и Cookbooks определяют код конфигурации для ваших систем. Pro - Доступна большая коллекция рецептов Pro - Хорошо интегрируется с Git, что обеспечивает надежный контроль версий. Против - Крутая кривая обучения, требуется значительное количество времени. Против - Главный сервер не имеет большого контроля. Архитектура - сервер / клиенты Простота настройки - Умеренная Язык - Процедурный - Указать, как выполнить задачу Puppet
Puppet - это инструмент управления конфигурацией, который поддерживает автоматическое развертывание. Puppet построен на Ruby и использует DSL для написания манифестов. Puppet также хорошо работает с гетерогенной инфраструктурой, где основное внимание уделяется масштабируемости. За - Большое сообщество поддержки. За - Хорошо развитый механизм отчетности. Против - Продвинутые задачи требуют знания языка Ruby. Против - Главный сервер не имеет большого контроля. Архитектура - сервер / клиенты Простота установки - Умеренная Язык - Декларативный - указывать только то, что нужно делать Ansible
Ansible - это инструмент автоматизации ИТ, который автоматизирует управление конфигурацией, предоставление облака, развертывание и оркестровку. Ядро плейбуков Ansible написано на языке YAML. (Следует сделать раздел о YAML, так как мы уже несколько раз сталкивались с этим). Ansible хорошо работает в средах, где основное внимание уделяется быстрой настройке и запуску. Работает на основе плейбуков, которые предоставляют инструкции вашим серверам. Pro - Не нужны агенты на удаленных узлах. Pro - YAML легко изучить. Против - Скорость работы часто ниже, чем у других инструментов (быстрее, чем Дин делает это сам вручную). Против - YAML не такой мощный, как Ruby, но его легче освоить. Архитектура - Только клиент Простота настройки - Очень просто Язык - Процедурный - Указать, как выполнить задачу SaltStack
SaltStack - это инструмент на основе CLI, который автоматизирует управление конфигурацией и удаленное выполнение. SaltStack основан на Python, а инструкции написаны на YAML или собственном DSL. Идеально подходит для сред, где приоритетом является масштабируемость и отказоустойчивость. Плюсы - Простота использования при запуске Плюсы - Хороший механизм отчетности Против - Фаза установки сложная Против - Новый веб-уи, который гораздо менее проработан, чем другие. Архитектура - сервер / клиенты Простота установки - Умеренная Язык - Декларативный - указывайте только то, что нужно делать Ansible vs Terraform Инструментом, который мы будем использовать для этого раздела, будет Ansible. (Простой в использовании и требуются основы языка).
Я думаю, что важно коснуться некоторых различий между Ansible и Terraform, прежде чем мы рассмотрим инструментарий немного подробнее. | |Ansible |Terraform | | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; | |Type |Ansible is a configuration management tool |Terraform is a an orchestration tool | |Infrastructure |Ansible provides support for mutable infrastructure |Terraform provides support for immutable infrastructure | |Language |Ansible follows procedural language |Terraform follows a declartive language | |Provisioning |Ansible provides partial provisioning (VM, Network, Storage) |Terraform provides extensive provisioning (VM, Network, Storage) | |Packaging |Ansible provides complete support for packaging \u0026amp; templating |Terraform provides partial support for packaging \u0026amp; templating | |Lifecycle Mgmt |Ansible does not have lifecycle management |Terraform is heavily dependant on lifecycle and state mgmt |
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day63/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day64/":{title:"64. Ansible Введение",tags:["devops"],content:`Основы Ansible
Ansible: Начало работы Мы немного рассказали о том, что такое Ansible, на вчерашней большой сессии, но здесь мы собираемся начать с более подробной информации. Во-первых, Ansible поставляется компанией RedHat. Во-вторых, это агент, подключается через SSH и выполняет команды. В-третьих, он кроссплатформенный (Linux \u0026amp; macOS, WSL2) и с открытым исходным кодом (есть также платный корпоративный вариант) Ansible толкает конфигурацию по сравнению с другими моделями.
Установка Ansible Как вы можете себе представить, RedHat и команда Ansible проделали фантастическую работу по документированию Ansible. Обычно это начинается с шагов по установке, которые вы можете найти здесь. Помните, мы говорили, что Ansible - это инструмент автоматизации без агентов, инструмент развертывается на системе, называемой \u0026ldquo;узел управления\u0026rdquo;, с этого узла управления осуществляется управление машинами и другими устройствами (возможно, сетевыми) по SSH.
В документации по ссылке выше говорится, что ОС Windows не может использоваться в качестве узла управления.
Для моего узла управления и, по крайней мере, для этой демонстрации я собираюсь использовать виртуальную машину Linux, которую мы создали еще в разделе Linux в качестве узла управления.
Эта система работала под управлением Ubuntu, и для ее установки достаточно выполнить следующие команды.
sudo apt updatesudo apt install software-properties-commonsudo add-apt-repository --yes --update ppa:ansible/ansiblesudo apt install ansibleТеперь у нас должна быть установлена ansible на нашем узле управления, вы можете проверить это, запустив ansible --version, и вы должны увидеть что-то похожее на это ниже.
Прежде чем мы перейдем к управлению другими узлами в нашей среде, мы также можем проверить функциональность ansible, выполнив команду на нашей локальной машине ansible localhost -m ping будет использовать Ansible Module, и это быстрый способ выполнить одну задачу на многих различных системах. Я имею в виду, что это не очень весело только с локальным хостом, но представьте, что вы хотите получить что-то или убедиться, что все ваши системы работают, а у вас 1000+ серверов и устройств.
Или реальное использование модуля в реальной жизни может быть чем-то вроде ansible webservers --m service -a \u0026quot;name=httpd state=started\u0026quot;, это скажет нам, запущена ли служба httpd на всех наших веб-серверах. Я привел термин webservers, используемый в этой команде.
hosts Как я использовал localhost выше для запуска простого модуля ping против системы, я не могу указать другую машину в моей сети, например, в среде, которую я использую, мой хост Windows, на котором работает VirtualBox, имеет сетевой адаптер с IP 10.0.0.1, но вы можете видеть ниже, что я могу связаться с ним с помощью ping, но я не могу использовать ansible для выполнения этой задачи.
Для того чтобы указать наши узлы или узлы, которые мы хотим автоматизировать с помощью этих задач, нам необходимо их определить. Мы можем определить их, перейдя в каталог /etc/ansible в вашей системе.
Файл, который мы хотим отредактировать - это файл hosts, используя текстовый редактор, мы можем зайти в него и определить наши хосты. Файл hosts содержит множество отличных инструкций по использованию и изменению файла. Мы хотим прокрутить вниз и создать новую группу под названием [windows] и добавить наш IP-адрес 10.0.0.1 для этого хоста. Сохраните файл.
Однако помните, я говорил, что вам понадобится SSH, чтобы Ansible мог подключиться к вашей системе. Как вы можете видеть ниже, когда я запускаю ansible windows -m ping, мы получаем недостижимый результат, потому что не удалось подключиться через SSH.
Теперь я также начал добавлять дополнительные хосты в наш инвентарь, другое название для этого файла, так как здесь вы собираетесь определить все ваши устройства, это могут быть сетевые устройства, например, коммутаторы и маршрутизаторы, которые также будут добавлены сюда и сгруппированы. В нашем файле hosts я также добавил свои учетные данные для доступа к группе систем linux.
Теперь, если мы запустим ansible linux -m ping, мы получим успех, как показано ниже.
Далее у нас есть требования к узлам, это целевые системы, на которых вы хотите автоматизировать конфигурацию. Мы не устанавливаем на них ничего для Ansible (то есть, мы можем установить программное обеспечение, но нам не нужен клиент Ansible). Ansible будет устанавливать соединение по SSH и отправлять все по SFTP (если вы хотите и у вас настроен SSH, вы можете использовать SCP против SFTP).
Команды Ansible Вы видели, что мы смогли запустить ansible linux -m ping на нашей Linux машине и получить ответ, в принципе, с Ansible у нас есть возможность запускать множество специальных команд. Но очевидно, что вы можете запустить это против группы систем и получить эту информацию обратно. ad hoc commands
Если вы сталкиваетесь с повторением команд или, что еще хуже, вам приходится входить в отдельные системы для выполнения этих команд, то Ansible может помочь в этом случае. Например, простая команда ниже даст нам вывод всех сведений об операционной системе для всех систем, которые мы добавим в нашу группу linux. ansible linux -a \u0026quot;cat /etc/os-release\u0026quot;.
Другими вариантами использования могут быть перезагрузка систем, копирование файлов, управление упаковщиками и пользователями. Вы также можете объединить специальные команды с модулями Ansible.
Специальные команды используют декларативную модель, рассчитывая и выполняя действия, необходимые для достижения заданного конечного состояния. Они достигают идемпотентности, проверяя текущее состояние перед началом работы и ничего не делая, если текущее состояние не отличается от заданного конечного состояния.
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day64/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day65/":{title:"65. Ansible Playbooks - Часть 1",tags:["devops"],content:`Ansible Playbooks В этом разделе мы рассмотрим основную причину, которую я вижу, по крайней мере, для Ansible. Я имею в виду, что это здорово - взять одну команду и обратиться ко многим различным серверам для выполнения простых команд, таких как перезагрузка длинного списка серверов и избавление от необходимости подключаться к каждому из них по отдельности.
Но как насчет того, чтобы взять голую операционную систему, объявить программное обеспечение и службы, которые мы хотим запустить на этой системе, и убедиться, что все они работают в нужном состоянии.
Здесь на помощь приходят учебники Ansible. Плейбук позволяет нам взять группу серверов и выполнить задачи конфигурации и установки для этой группы.
Формат плейбука Плейбук \u0026gt; Игры \u0026gt; Задачи
Если вы занимаетесь спортом, вы, возможно, сталкивались с термином \u0026ldquo;плейбук\u0026rdquo;. Плейбук рассказывает команде о том, как вы будете играть, состоящий из различных пьес и задач. Если мы считаем пьесы декорациями в спорте или игре, а задачи связаны с каждой пьесой, у вас может быть несколько задач, составляющих пьесу, а в плейбуке может быть несколько различных пьес.
Эти плейбуки написаны на YAML (YAML - это не язык разметки), вы найдете много разделов, которые мы уже рассмотрели, особенно контейнеры и Kubernetes, в которых используются файлы конфигурации в формате YAML.
Давайте рассмотрим простой плейбук под названием playbook.yml.
- name: Simple Playhosts: localhostconnection: localtasks:- name: Ping meping:- name: print osdebug:msg: \u0026quot;{{ ansible_os_family }}\u0026quot;Вы найдете вышеуказанный файл simple_play. Если мы затем используем команду ansible-playbook simple_play.yml, то пройдем следующие шаги.
Вы видите, что первая задача \u0026ldquo;сбор шагов\u0026rdquo; произошла, но мы не вызывали или не просили об этом? Этот модуль автоматически вызывается плейбуками для сбора полезных переменных об удаленных хостах. ansible.builtin.setup
Нашей второй задачей было установить ping, это не ICMP ping, а python скрипт, который сообщает pong об успешном соединении с удаленным или локальным хостом. ansible.builtin.ping
Затем наша третья или на самом деле вторая определенная задача, так как первая будет выполняться, если вы не отключите печать сообщения, сообщающего нам о нашей ОС. В этой задаче мы используем условия, мы можем запустить этот плейбук на всех различных типах операционных систем, и это вернет нам имя ОС. Мы просто передаем этот вывод для удобства, но мы могли бы добавить задачу, чтобы сказать что-то вроде:
tasks: - name: \u0026quot;shut down Debian flavoured systems\u0026quot;command: /sbin/shutdown -t now when: ansible_os_family == \u0026quot;Debian\u0026quot;Vagrant для настройки нашего окружения Мы будем использовать Vagrant для настройки нашего узлового окружения, я собираюсь оставить разумные 4 узла, но вы, надеюсь, увидите, что их может быть 300 или 3000. В этом и заключается сила Ansible и других инструментов управления конфигурацией, чтобы иметь возможность настраивать ваши серверы.
Вы можете найти этот файл здесь (Vagrantfile)
Vagrant.configure(\u0026quot;2\u0026quot;) do |config|servers=[{:hostname =\u0026gt; \u0026quot;db01\u0026quot;,:box =\u0026gt; \u0026quot;bento/ubuntu-21.10\u0026quot;,:ip =\u0026gt; \u0026quot;192.168.169.130\u0026quot;,:ssh_port =\u0026gt; '2210'},{:hostname =\u0026gt; \u0026quot;web01\u0026quot;,:box =\u0026gt; \u0026quot;bento/ubuntu-21.10\u0026quot;,:ip =\u0026gt; \u0026quot;192.168.169.131\u0026quot;,:ssh_port =\u0026gt; '2211'},{:hostname =\u0026gt; \u0026quot;web02\u0026quot;,:box =\u0026gt; \u0026quot;bento/ubuntu-21.10\u0026quot;,:ip =\u0026gt; \u0026quot;192.168.169.132\u0026quot;,:ssh_port =\u0026gt; '2212'},{:hostname =\u0026gt; \u0026quot;loadbalancer\u0026quot;,:box =\u0026gt; \u0026quot;bento/ubuntu-21.10\u0026quot;,:ip =\u0026gt; \u0026quot;192.168.169.134\u0026quot;,:ssh_port =\u0026gt; '2213'}]config.vm.base_address = 600servers.each do |machine|config.vm.define machine[:hostname] do |node|node.vm.box = machine[:box]node.vm.hostname = machine[:hostname]node.vm.network :public_network, bridge: \u0026quot;Intel(R) Ethernet Connection (7) I219-V\u0026quot;, ip: machine[:ip]node.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 22, host: machine[:ssh_port], id: \u0026quot;ssh\u0026quot;node.vm.provider :virtualbox do |v|v.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--memory\u0026quot;, 2048]v.customize [\u0026quot;modifyvm\u0026quot;, :id, \u0026quot;--name\u0026quot;, machine[:hostname]]endendendendИспользуйте команду vagrant up, чтобы запустить эти машины в VirtualBox, Вы можете добавить больше памяти, а также определить разные частные_сетевые адреса для каждой машины, но это работает в моей среде. Помните, что наш блок управления - это рабочий стол Ubuntu, который мы установили в разделе Linux.
Если вы ограничены в ресурсах, вы также можете запустить vagrant up web01 web02, чтобы поднять только веб-серверы, которые мы используем здесь.
Конфигурация хоста Ansible Теперь, когда наша среда готова, мы можем проверить ansible, и для этого мы будем использовать наш рабочий стол Ubuntu (вы можете использовать его, но вы также можете использовать любую машину на базе Linux в вашей сети, доступную для сети ниже) в качестве нашего управления, давайте также добавим новые узлы в нашу группу в файле ansible hosts, Вы можете считать этот файл инвентаризацией, альтернативой этому может быть другой файл инвентаризации, который вызывается как часть вашей команды ansible с -i filename, это может быть полезно по сравнению с использованием файла host, так как вы можете иметь разные файлы для разных сред, например, production, test и staging. Поскольку мы используем стандартный файл hosts, нам не нужно его указывать, так как он будет использоваться по умолчанию.
Я добавил следующее в файл hosts по умолчанию.
[control]ansible-control[proxy] loadbalancer[webservers] web01web02[database] db01Прежде чем двигаться дальше, мы хотим убедиться, что можем выполнить команду для наших узлов, давайте выполним ansible nodes -m command -a hostname, эта простая команда проверит, что у нас есть подключение и сообщит имена наших узлов.
Также обратите внимание, что я добавил эти узлы и IP на мой узел управления Ubuntu в файл /etc/hosts для обеспечения подключения. Нам также может понадобиться выполнить конфигурацию SSH для каждого узла с блока Ubuntu.
192.168.169.140 ansible-control192.168.169.130 db01192.168.169.131 web01192.168.169.132 web02192.168.169.133 loadbalancerНа этом этапе мы хотим выполнить настройку SSH ключей между узлами управления и сервера. Это то, что мы будем делать дальше, другим способом здесь может быть добавление переменных в ваш файл hosts для указания имени пользователя и пароля. Я бы не советовал этого делать, так как это никогда не будет лучшей практикой.
Чтобы настроить SSH и общий доступ между узлами, выполните следующие шаги, вам будет предложено ввести пароль (vagrant), и вам, вероятно, придется нажать y несколько раз, чтобы согласиться.
ssh-keygen
ssh-copy-id localhost
Теперь, если все ваши ВМ включены, вы можете запустить команду ssh-copy-id web01 \u0026amp;\u0026amp; ssh-copy-id web02 \u0026amp;\u0026amp; ssh-copy-id loadbalancer \u0026amp;\u0026amp; ssh-copy-id db01, которая запросит у вас пароль, в нашем случае пароль vagrant.
Я не запускаю все свои виртуальные машины, а запускаю только веб-серверы, поэтому я выдал команду sh-copy-id web01 \u0026amp;\u0026amp; ssh-copy-id web02.
Перед запуском любых плейбуков я хочу убедиться, что у меня есть простое соединение с моими группами, поэтому я запустил ansible webservers -m ping для проверки соединения.
Наш первый \u0026ldquo;настоящий\u0026rdquo; плейбук Ansible Наш первый плейбук Ansible будет настраивать наши веб-серверы, мы сгруппировали их в нашем файле hosts под группировкой [webservers].
Перед запуском нашего плейбука мы можем убедиться, что на web01 и web02 не установлен apache. В верхней части скриншота ниже показано расположение папок и файлов, которые я создал в моей системе управления ansible для запуска этого плейбука, у нас есть playbook1.yml, затем в папке templates у нас есть файлы index.html.j2 и ports.conf.j2. Вы можете найти эти файлы в папке, указанной выше в репозитории.
Затем мы подключаемся по SSH к web01, чтобы проверить, установлен ли у нас apache?
Из вышеприведенного видно, что у нас не установлен apache на web01, поэтому мы можем исправить это, запустив следующий плейбук.
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps\u0026quot;tasks:- name: ensure apache is at the latest versionapt:name: apache2state: latest- name: write the apache2 ports.conf config filetemplate:src: templates/ports.conf.j2dest: /etc/apache2/ports.confnotify:- restart apache- name: write a basic index.html filetemplate:src: templates/index.html.j2dest: /var/www/html/index.htmlnotify:- restart apache- name: ensure apache is runningservice:name: apache2state: startedhandlers:- name: restart apacheservice:name: apache2state: restartedРазбираем вышеприведенный плейбук:
- hosts: webservers означает, что наша группа, на которой будет запущен этот плейбук, называется webservers. become: yes означает, что наш пользователь, запускающий плейбук, станет root на наших удаленных системах. Вам будет предложено ввести пароль root. Затем у нас есть vars, и это определяет некоторые переменные окружения, которые мы хотим использовать на наших веб-серверах. После этого мы приступаем к выполнению наших задач,
Задача 1 - убедиться, что apache работает на последней версии. Задача 2 - написать файл ports.conf из нашего исходного файла, который находится в папке templates. Задача 3 - создание базового файла index.html Задача 4 - убедиться, что apache запущен. Наконец, у нас есть раздел обработчиков, Handlers: Running operations on change
\u0026ldquo;Иногда вы хотите, чтобы задача выполнялась только тогда, когда на машине происходят изменения. Например, вы можете захотеть перезапустить службу, если задача обновляет конфигурацию этой службы, но не перезапускать ее, если конфигурация не изменилась. Для решения этой задачи в Ansible используются обработчики. Обработчики - это задачи, которые выполняются только при получении уведомления. Каждый обработчик должен иметь глобально уникальное имя\u0026rdquo;.
На этом этапе вы можете подумать, но мы развернули 5 виртуальных машин (включая нашу машину Ubuntu Desktop, которая действует как наш Ansible Control) Остальные системы будут задействованы в оставшейся части раздела.
Запуск нашего плейбука Теперь мы готовы запустить наш учебник на наших узлах. Для запуска нашего плейбука мы можем использовать ansible-playbook playbook1.yml Мы определили наши узлы, на которых будет работать наш учебник, и это позволит выполнить наши задачи, которые мы определили.
После завершения команды мы получим результат, показывающий наши пьесы и задачи, это может занять некоторое время, вы можете видеть на изображении ниже, что это заняло некоторое время, чтобы пойти и установить наше желаемое состояние.
Затем мы можем дважды проверить это, зайдя в узел и проверив, что на нашем узле установлено программное обеспечение.
Теперь, когда мы развернули два автономных веб-сервера, мы можем перейти на соответствующие IP, которые мы определили, и получить наш новый веб-сайт.
Мы будем опираться на это руководство по ходу работы над остальной частью этого раздела. Мне также интересно взять наш рабочий стол Ubuntu и посмотреть, сможем ли мы загрузить наши приложения и конфигурацию с помощью Ansible, поэтому мы также можем коснуться этого. Вы видели, что мы можем использовать локальный хост в наших командах, мы также можем запускать плейбуки, например, на нашем локальном хосте.
Еще одна вещь, которую следует добавить, заключается в том, что мы работаем только с виртуальными машинами Ubuntu, но Ansible не зависит от целевых систем. Альтернативы, которые мы уже упоминали ранее для управления системами, могут быть сервер за сервером (не масштабируемый, когда вы получаете большое количество серверов, плюс боль даже с 3 узлами), мы также можем использовать скрипты оболочки, которые мы рассматривали в разделе Linux, но эти узлы потенциально разные, так что да, это можно сделать, но тогда кто-то должен поддерживать и управлять этими скриптами. Ansible бесплатна и позволяет легко справиться с этой задачей по сравнению с необходимостью иметь специализированный скрипт.
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! Your complete guide to Ansible Этот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day65/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day66/":{title:"66. Ansible Playbooks - Часть 2",tags:["devops"],content:`Ansible Playbooks Продолжение\u0026hellip; В нашем последнем разделе мы начали с создания небольшой лаборатории, используя файл Vagrant для развертывания 4 машин, и мы использовали нашу Linux-машину, которую мы создали в этом разделе, в качестве нашей системы управления Ansible.
Мы также проверили несколько скриптов плейбуков, и в конце у нас был плейбук, который сделал наши web01 и web02 отдельными веб-серверами.
Наведение порядка Прежде чем перейти к дальнейшей автоматизации и развертыванию, мы должны рассказать о том, как сохранить наш плейбук аккуратным и опрятным и как мы можем разделить наши такты и обработчики по подпапкам.
В основном мы собираемся копировать наши задачи в их собственный файл в папке.
- name: ensure apache is at the latest versionapt: name=apache2 state=latest- name: write the apache2 ports.conf config filetemplate: src=templates/ports.conf.j2 dest=/etc/apache2/ports.confnotify: restart apache- name: write a basic index.html filetemplate:src: templates/index.html.j2dest: /var/www/html/index.htmlnotify:- restart apache- name: ensure apache is runningservice:name: apache2state: startedи то же для обработчиков.
- name: restart apacheservice:name: apache2state: restartedЗатем в нашем плейбуке, который теперь называется playbook2.yml, мы указываем на эти файлы. Все эти файлы можно найти по адресу ansible-scenario2.
Вы можете проверить это на своей контрольной машине. Если вы скопировали файлы из репозитория, вы должны были заметить, что кое-что изменилось в пункте \u0026ldquo;написать основной файл index.html\u0026rdquo;
Давайте выясним, какое простое изменение я сделал. Использование curl web01:8000
Мы только что привели в порядок наш плейбук и начали разделять области, которые могут сделать плейбук очень перегруженным в масштабе.
Роли и Ansible Galaxy На данный момент мы развернули 4 виртуальные машины и настроили 2 из них как веб-серверы, но у нас есть еще несколько специфических функций, а именно: сервер базы данных и балансировщик нагрузки или прокси. Для того чтобы сделать это и привести в порядок наш репозиторий, мы можем использовать роли в Ansible.
Для этого мы воспользуемся командой ansible-galaxy, которая предназначена для управления ролями Ansible в общих репозиториях.
Мы собираемся использовать ansible-galaxy для создания роли для apache2, где мы собираемся разместить специфику наших веб-серверов.
Приведенная выше команда ansible-galaxy init roles/apache2 создаст структуру папок, которую мы показали выше. Следующим шагом нам нужно переместить существующие задачи и шаблоны в соответствующие папки в новой структуре.
Копировать и вставить легко для перемещения этих файлов, но нам также нужно внести изменения в tasks/main.yml, чтобы указать его на apache2_install.yml.
Нам также нужно изменить наш playbook, чтобы он ссылался на нашу новую роль. В playbook1.yml и playbook2.yml мы определяем наши задачи и обработчики по-разному, так как мы изменили их между двумя версиями. Нам нужно изменить наш плейбук, чтобы использовать эту роль, как показано ниже:
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 66!\u0026quot;roles:- apache2Теперь мы можем запустить наш плейбук снова, на этот раз с новым именем плейбука ansible-playbook playbook3.yml Вы заметите обесценивание, мы можем исправить это дальше.
Хорошо, амортизация хотя наш плейбук запустился, теперь мы должны исправить наши пути, для этого я изменил опцию include в tasks/main.yml на import_tasks, как показано ниже.
Вы можете найти эти файлы в папке ansible-scenario3.
Мы также собираемся создать еще несколько ролей, используя ansible-galaxy, которые мы собираемся создать:
common = for all of our servers (ansible-galaxy init roles/common) nginx = for our loadbalancer (ansible-galaxy init roles/nginx) Я собираюсь оставить этот вариант здесь, а в следующей сессии мы начнем работать над другими узлами, которые мы развернули, но еще ничего не сделали.
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! Your complete guide to Ansible Этот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day66/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day67/":{title:"67. Роли и развертывание балансировщика нагрузки",tags:["devops"],content:`На последнем занятии мы рассмотрели роли и использовали команду ansible-galaxy, чтобы помочь создать структуру папок для некоторых ролей, которые мы будем использовать. В итоге мы получили гораздо более аккуратное рабочее хранилище для нашего кода конфигурации, поскольку все спрятано в папках ролей.
Однако мы использовали только роль apache2 и получили рабочий playbook3.yaml для работы с нашими веб-серверами.
На данном этапе, если вы использовали только vagrant up web01 web02, пришло время запустить vagrant up loadbalancer, который откроет другую систему Ubuntu, которую мы будем использовать в качестве балансировщика нагрузки/прокси.
Мы уже определили эту новую машину в нашем файле hosts, но у нас нет настроенного ssh-ключа, пока он не доступен, поэтому нам нужно также запустить ssh-copy-id loadbalancer, когда система будет запущена и готова.
Общая роль В конце вчерашней сессии я создал роль common, роль common будет использоваться на всех наших серверах, в то время как другие роли специфичны для конкретных случаев использования, сейчас приложения, которые я собираюсь установить в качестве common, не так просты, и я не вижу много причин для этого, но это показывает цель. В структуре папок нашей общей роли перейдите в папку tasks, и у вас появится файл main.yml. В этом yaml нам нужно указать на наш файл install_tools.yml, и мы делаем это, добавляя строку - import_tasks: install_tools.yml. Раньше это был include, но он скоро будет устаревшим, поэтому мы используем import_tasks.
- name: \u0026quot;Install Common packages\u0026quot;apt: name={{ item }} state=latestwith_items:- neofetch- tree- figletЗатем в нашем плейбуке мы добавляем общую роль для каждого блока хоста.
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 66!\u0026quot;roles:- common- apache2nginx Следующим этапом будет установка и настройка nginx на нашем виртуальном компьютере loadbalancer. Как и в общей структуре папок, у нас есть nginx, основанный на последнем сеансе.
Прежде всего, мы добавим блок host в наш playbook. Этот блок будет включать нашу общую роль, а затем нашу новую роль nginx.
Плейбук можно найти здесь. playbook4.yml
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 66!\u0026quot;roles:- common- apache2- hosts: proxy become: yesroles: - common- nginxДля того чтобы это что-то значило, мы должны определить наши задачи, которые мы хотим запустить, таким же образом мы изменим main.yml в задачах, чтобы указать на два файла, один для установки и один для конфигурации.
Есть и другие файлы, которые я изменил в зависимости от желаемого результата, посмотрите в папке ansible-scenario4 все измененные файлы. Вам следует проверить папки tasks, handlers и templates в папке nginx, и вы найдете эти дополнительные изменения и файлы.
Запуск обновленного плейбука Со вчерашнего дня мы добавили роль common, которая теперь будет устанавливать некоторые пакеты в нашей системе, а затем мы также добавили роль nginx, которая включает установку и настройку.
Давайте запустим наш playbook4.yml, используя ansible-playbook playbook4.yml.
Теперь, когда мы настроили наши веб-серверы и loadbalancer, мы должны иметь возможность перейти по адресу http://192.168.169.134/, который является IP-адресом нашего loadbalancer.
Если вы следите за развитием событий и у вас нет такого состояния, то это может быть связано с IP-адресами серверов в вашем окружении. Файл находится в templates\\mysite.j2 и выглядит примерно так, как показано ниже: Вам необходимо обновить IP-адреса ваших веб-серверов.
upstream webservers {server 192.168.169.131:8000;server 192.168.169.132:8000;}server {listen 80;location / { proxy_pass http://webservers;}}Я уверен, что все, что мы установили, в порядке, но давайте воспользуемся специальной командой с помощью ansible, чтобы проверить установку этих общих инструментов.
ansible loadbalancer -m command -a neofetch.
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! Your complete guide to Ansible TЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day67/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day68/":{title:"68. Теги, переменные, инвентаризация и конфигурация сервера базы данных",tags:["devops"],content:`Теги Поскольку мы оставили наш плейбук во время вчерашней сессии, нам нужно будет запустить все задачи и пьесы в рамках этого плейбука. Это означает, что нам придется запустить веб-серверы и балансировщик нагрузки до конца.
Однако теги могут позволить нам отделить их друг от друга, если мы захотим. Это может быть эффективным шагом, если в нашей среде есть очень большие и длинные плейбуки.
В нашем файле плейбука, в данном случае мы используем ansible-scenario5
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 66!\u0026quot;roles:- common- apache2tags: web- hosts: proxy become: yesroles: - common- nginxtags: proxyЗатем мы можем подтвердить это с помощью команды ansible-playbook playbook5.yml --list-tags, а список тегов будет содержать теги, которые мы определили в нашем плейбуке.
Теперь, если мы хотим нацелиться только на прокси, мы можем сделать это, выполнив ansible-playbook playbook5.yml --tags proxy, и это, как вы можете видеть ниже, запустит плейбук только против прокси.
Теги могут быть добавлены и на уровне задач, так что мы можем получить действительно подробную информацию о том, где и что вы хотите, чтобы произошло. Это могут быть теги, ориентированные на приложения, например, мы можем пройтись по задачам и пометить наши задачи на основе установки, настройки или удаления. Еще один очень полезный тег, который вы можете использовать, это
tag: always, который гарантирует, что независимо от того, какие -теги вы используете в вашей команде, если что-то помечено значением always, то оно всегда будет запущено при выполнении команды ansible-playbook.
С помощью тегов мы также можем объединить несколько тегов вместе, и если мы выполним команду ansible-playbook playbook5.yml --tags proxy,web, то будут запущены все элементы с этими тегами. Очевидно, что в нашем случае это будет означать то же самое, что и запуск самого плейбука, но если бы у нас было несколько других плейбуков, то это имело бы смысл.
Вы также можете определить более одного тега.
Переменные В Ansible существует два основных типа переменных.
Созданная пользователем (User created) Факты Ansible (Ansible Facts) Факты Ansible Каждый раз, когда мы запускали наши плейбуки, у нас была задача, которую мы не определяли, называемая \u0026ldquo;Сбор фактов\u0026rdquo;, мы можем использовать эти переменные или факты, чтобы заставить вещи происходить с нашими задачами автоматизации.
Если мы выполним следующую команду ansible proxy -m setup, то увидим много выходных данных в формате JSON. Однако на вашем терминале будет много информации, чтобы действительно использовать ее, поэтому мы хотим вывести ее в файл, используя команду ansible proxy -m setup \u0026gt;\u0026gt; facts.json, вы можете увидеть этот файл в этом репозитории, ansible-scenario5
Если открыть этот файл, то можно увидеть всевозможную информацию для нашей команды. Мы можем получить наши IP-адреса, архитектуру, версию биоса. Много полезной информации, если мы захотим использовать ее в наших плейбуках.
Идея заключается в том, чтобы потенциально использовать одну из этих переменных в шаблоне nginx mysite.j2, где мы жестко закодировали IP-адреса наших веб-серверов. Вы можете сделать это, создав цикл for в вашем mysite.j2, который будет проходить через группу [webservers], что позволит нам иметь более двух веб-серверов, автоматически и динамически созданных или добавленных в эту конфигурацию балансировщика нагрузки.
#Dynamic Config for server {{ ansible_facts['nodename'] }}upstream webservers {{% for host in groups['webservers'] %}server {{ hostvars[host]['ansible_facts']['nodename'] }}:8000;{% endfor %}}server {listen 80;location / { proxy_pass http://webservers;}}Результат вышеописанных действий будет выглядеть так же, как и сейчас, но если мы добавим больше веб-серверов или удалим один, это динамически изменит конфигурацию прокси. Чтобы это работало, необходимо настроить разрешение имен.
Созданные пользователем Переменные, созданные пользователем, - это то, что мы создали сами. Если вы посмотрите в наш playbook, то увидите, что у нас есть vars:, а затем список из трех переменных, которые мы используем.
- hosts: webserversbecome: yesvars:http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 68!\u0026quot;roles:- common- apache2tags: web- hosts: proxy become: yesroles: - common- nginxtags: proxyОднако мы можем очистить наш плейбук от переменных, переместив их в собственный файл. Мы так и сделаем, но перенесем их в папку ansible-scenario6. В корне этой папки мы создадим папку group_vars. Затем мы создадим еще одну папку под названием all (все группы получат эти переменные). В ней мы создадим файл под названием common_variables.yml и скопируем в него наши переменные из нашего плейбука. Удалим их из плейбука вместе с vars:.
http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 68!\u0026quot;Поскольку мы связываем это с глобальной переменной, мы также можем добавить сюда наши серверы NTP и DNS. Переменные устанавливаются из созданной нами структуры папок. Ниже вы можете видеть, как чисто выглядит наш Playbook.
- hosts: webserversbecome: yesroles:- common- apache2tags: web- hosts: proxy become: yesroles: - common- nginxtags: proxyОдной из этих переменных был http_port, мы можем использовать его снова в нашем цикле for в файле mysite.j2, как показано ниже:
#Dynamic Config for server {{ ansible_facts['nodename'] }}upstream webservers {{% for host in groups['webservers'] %}server {{ hostvars[host]['ansible_facts']['nodename'] }}:{{ http_port }};{% endfor %}}server {listen 80;location / { proxy_pass http://webservers;}}Мы также можем определить ansible fact в нашем файле roles/apache2/templates/index.html.j2, чтобы мы могли понять, на каком веб-сервере мы находимся.
\u0026lt;html\u0026gt;\u0026lt;h1\u0026gt;{{ html_welcome_msg }}! I'm webserver {{ ansible_facts['nodename'] }} \u0026lt;/h1\u0026gt;\u0026lt;/html\u0026gt;Результаты выполнения команды ansible-playbook playbook6.yml с нашими изменениями переменных означают, что когда мы нажимаем на наш loadbalancer, вы можете увидеть, что мы нажимаем на любой из веб-серверов, которые есть в нашей группе. Мы также можем добавить папку host_vars и создать web01.yml и иметь определенное сообщение или изменить то, как это выглядит для каждого хоста, если захотим.
Файлы инвентаризации До сих пор мы использовали файл hosts по умолчанию в папке /etc/ansible для определения наших хостов. Однако мы можем иметь разные файлы для разных окружений, например, production и staging. Я не собираюсь создавать больше окружений. Но мы можем создавать свои собственные файлы хостов.
Мы можем создать несколько файлов для нашего различного количества серверов и узлов. Мы будем вызывать их с помощью ansible-playbook -i dev playbook.yml Вы также можете определить переменные в файле hosts и затем распечатать их или использовать эти переменные где-нибудь еще в своих плейбуках. Например, в примере и учебном курсе, за которым я слежу ниже, они добавили переменную окружения, созданную в файле host, в шаблон веб-страницы loadbalancer, чтобы показать окружение как часть сообщения веб-страницы.
Развертывание нашего сервера базы данных У нас осталась еще одна машина, которую мы еще не включили и не настроили. Мы можем сделать это с помощью команды vagrant up db01 из места, где находится наш Vagrantfile. Когда машина будет запущена и доступна, нам нужно убедиться, что SSH-ключ скопирован с помощью ssh-copy-id db01, чтобы мы могли получить доступ.
Мы будем работать из папки ansible-scenario7.
Затем воспользуемся командой ansible-galaxy init roles/mysql, чтобы создать новую структуру папок для новой роли под названием \u0026ldquo;mysql\u0026rdquo;.
В нашем плейбуке мы собираемся добавить новый блок для конфигурации базы данных. В файле /etc/ansible/hosts мы определили нашу группу базы данных. Затем мы указываем нашей группе базы данных роль common и новую роль mysql, которую мы создали в предыдущем шаге. Мы также помечаем нашу группу базы данных тегами database, что означает, как мы обсуждали ранее, что мы можем выбрать запуск только с этими тегами, если захотим.
- hosts: webserversbecome: yesroles:- common- apache2tags:web- hosts: proxybecome: yesroles:- common- nginxtags: proxy- hosts: databasebecome: yesroles:- common- mysqltags: databaseТеперь в структуре папок с нашими ролями автоматически создается дерево, в котором нам нужно заполнить следующее:
Handlers - main.yml
# handlers file for roles/mysql- name: restart mysqlservice:name: mysqlstate: restartedTasks - install_mysql.yml, main.yml \u0026amp; setup_mysql.yml
install_mysql.yml - this task is going to be there to install mysql and ensure that the service is running.
- name: \u0026quot;Install Common packages\u0026quot;apt: name={{ item }} state=latestwith_items:- python3-pip- mysql-client- python3-mysqldb- libmysqlclient-dev- name: Ensure mysql-server is installed latest versionapt: name=mysql-server state=latest- name: Installing python module MySQL-pythonpip:name: PyMySQL- name: Ensure mysql-server is runningservice:name: mysqlstate: startedmain.yml is a pointer file that will suggest that we import_tasks from these files.
# tasks file for roles/mysql- import_tasks: install_mysql.yml- import_tasks: setup_mysql.ymlsetup_mysql.yml - This task will create our database and database user.
- name: Create my.cnf configuration filetemplate: src=templates/my.cnf.j2 dest=/etc/mysql/conf.d/mysql.cnfnotify: restart mysql- name: Create database user with name 'devops' and password 'DevOps90' with all database privilegescommunity.mysql.mysql_user:login_unix_socket: /var/run/mysqld/mysqld.socklogin_user: \u0026quot;{{ mysql_user_name }}\u0026quot; login_password: \u0026quot;{{ mysql_user_password }}\u0026quot; name: \u0026quot;{{db_user}}\u0026quot;password: \u0026quot;{{db_pass}}\u0026quot;priv: '*.*:ALL'host: '%'state: present- name: Create a new database with name '90daysofdevops'mysql_db:login_user: \u0026quot;{{ mysql_user_name }}\u0026quot; login_password: \u0026quot;{{ mysql_user_password }}\u0026quot; name: \u0026quot;{{ db_name }}\u0026quot;state: presentВы можете видеть, что мы используем некоторые переменные для определения некоторых конфигураций, таких как пароли, имена пользователей и базы данных, все это хранится в файле group_vars/all/common_variables.yml.
http_port: 8000https_port: 4443html_welcome_msg: \u0026quot;Hello 90DaysOfDevOps - Welcome to Day 68!\u0026quot;mysql_user_name: rootmysql_user_password: \u0026quot;vagrant\u0026quot;db_user: devopsdb_pass: DevOps90db_name: 90DaysOfDevOpsУ нас также есть файл my.cnf.j2 в папке templates, который выглядит следующим образом:
[mysql] bind-address = 0.0.0.0Запуск плейбука Теперь наша виртуальная машина запущена и работает, и у нас есть наши конфигурационные файлы на месте, теперь мы готовы запустить наш плейбук, который будет включать все, что мы сделали раньше, если мы запустим следующий ansible-playbook playbook7.yml или мы можем выбрать просто развертывание на нашу группу баз данных с помощью команды ansible-playbook playbook7.yml --tags database, которая просто запустит наши новые конфигурационные файлы.
Я запустил только тег database, но наткнулся на ошибку. Эта ошибка говорит мне, что у нас не установлен pip3 (Python). Мы можем исправить это, добавив это в наши общие задачи и установив
Мы исправили вышеуказанное и запустили плейбук снова, и у нас получилось успешное изменение.
Мы должны убедиться, что на нашем новом настроенном сервере db01 все так, как мы хотим. Мы можем сделать это с нашего узла управления с помощью команды ssh db01.
Для подключения к MySQL я использовал команду sudo /usr/bin/mysql -u root -p и указал пароль vagrant для root.
Когда мы подключились, давайте сначала убедимся, что у нас создан пользователь devops. select user, host from mysql.user;
Теперь мы можем выполнить команду SHOW DATABASES;, чтобы увидеть нашу новую базу данных, которая также была создана.
На самом деле я использовал root для подключения, но теперь мы можем войти в систему под учетной записью devops, используя команду sudo /usr/bin/mysql -u devops -p, но пароль здесь будет DevOps90.
Я обнаружил, что в нашем setup_mysql.yml мне пришлось добавить строку login_unix_socket: /var/run/mysqld/mysqld.sock для успешного подключения к моему экземпляру db01 mysql, и теперь каждый раз, когда я запускаю это, он сообщает об изменении при создании пользователя, любые предложения будут очень признательны.
Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! Your complete guide to Ansible Этот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day68/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day69/":{title:"69. Ansible - контроллер автоматизации (Tower), AWX, Vault",tags:["devops"],content:`Завершая раздел об управлении конфигурацией, я хотел бы рассмотреть другие области, с которыми вы можете столкнуться при работе с Ansible.
Существует множество продуктов, составляющих платформу Ansible Automation.
Red Hat Ansible Automation Platform - это основа для создания и эксплуатации автоматизации в организации. Платформа включает в себя все инструменты, необходимые для внедрения автоматизации в масштабах предприятия.
Я постараюсь осветить некоторые из них в этом посте. Но для получения более подробной информации на официальном сайте Red Hat Ansible есть много другой информации. Ansible.com
Ansible Automation Controller | AWX Я объединил эти два продукта вместе, потому что Automation Controller и AWX очень похожи в том, что они предлагают.
Проект AWX или сокращенно AWX - это проект сообщества с открытым исходным кодом, спонсируемый Red Hat, который позволяет вам лучше контролировать ваши проекты Ansible в ваших средах. AWX - это основной проект, из которого взят компонент контроллера автоматизации.
Если вы ищете корпоративное решение, то вам нужен контроллер автоматизации, или вы могли слышать его как Ansible Tower. Контроллер автоматизации Ansible - это плоскость управления для платформы автоматизации Ansible.
И AWX, и контроллер автоматизации обладают следующими характеристиками, превосходящими все, что мы рассмотрели в этом разделе до сих пор.
Пользовательский интерфейс Управление доступом на основе ролей Рабочие процессы Интеграция CI/CD Automation Controller - это корпоративное предложение, в котором вы платите за поддержку.
Мы рассмотрим развертывание AWX в нашей среде minikube Kubernetes.
Развертывание Ansible AWX AWX не нужно развертывать в кластере Kubernetes, github для AWX от ansible даст вам эту подробную информацию. Однако, начиная с версии 18.0, AWX Operator является предпочтительным способом установки AWX.
Прежде всего, нам нужен кластер minikube. Мы можем сделать это, если вы следили за разделом Kubernetes, создав новый кластер minikube с помощью команды minikube start --cpus=4 --memory=6g --addons=ingress.
Официальный Ansible AWX Operator можно найти здесь. Как указано в инструкции по установке, вы должны клонировать этот репозиторий, а затем выполнить развертывание.
Я сделал форк вышеуказанного репозитория, а затем выполнил команду git clone https://github.com/MichaelCade/awx-operator.git. Я советую вам сделать то же самое и не использовать мой репозиторий, так как я могу что-то изменить или его там может не быть.
В клонированном репозитории вы найдете файл awx-demo.yml, в котором нам нужно изменить NodePort на ClusterIP, как показано ниже:
---apiVersion: awx.ansible.com/v1beta1kind: AWXmetadata:name: awx-demospec:service_type: ClusterIPСледующим шагом будет определение нашего пространства имен, в котором мы будем развертывать оператор awx, используя команду export NAMESPACE=awx, а затем команду make deploy, мы начнем развертывание.
При проверке у нас есть наше новое пространство имен, и у нас есть наш awx-operator-controller pod, запущенный в нашем пространстве имен. kubectl get pods -n awx.
В клонированном репозитории вы найдете файл awx-demo.yml. Теперь мы хотим развернуть его в нашем кластере Kubernetes и нашем пространстве имен awx. kubectl create -f awx-demo.yml -n awx.
Вы можете следить за прогрессом с помощью kubectl get pods -n awx -w, который будет визуально следить за происходящим.
У вас должно получиться что-то похожее на изображение, которое вы видите ниже, когда все работает.
Теперь мы должны иметь доступ к нашей awx установке после запуска в новом терминале minikube service awx-demo-service --url -n $NAMESPACE, чтобы открыть ее через minikube ingress.
Если мы откроем браузер по этому адресу [], вы увидите, что нам будет предложено ввести имя пользователя и пароль.
По умолчанию имя пользователя - admin, чтобы получить пароль, мы можем выполнить следующую команду kubectl get secret awx-demo-admin-password -o jsonpath=\u0026quot;{.data.password}\u0026quot; -n awx| base64 --decode.
Очевидно, что это дает вам пользовательский интерфейс для централизованного управления плейбуком и задачами управления конфигурацией, а также позволяет вам работать вместе, в отличие от того, что мы делали до сих пор, когда мы работали с одной станции управления ansible.
Это еще одна из тех областей, где вы, вероятно, могли бы провести еще много времени, изучая возможности этого инструмента.
Я приведу отличный ресурс от Джеффа Гирлинга, который более подробно рассказывает об использовании Ansible AWX. Ansible 101 - Episode 10 - Ansible Tower and AWX
В этом видео он также подробно рассказывает о различиях между Automation Controller (ранее Ansible Tower) и Ansible AWX (Free and Open Source).
Ansible Vault ansible-vault позволяет нам шифровать и расшифровывать файлы данных Ansible. На протяжении всего этого раздела мы пропустили и поместили часть нашей конфиденциальной информации в открытый текст.
Встроенный в двоичный файл Ansible ansible-vault позволяет нам скрыть эту конфиденциальную информацию.
Управление секретами постепенно становится еще одной областью, которой следовало бы уделить больше времени наряду с такими инструментами, как HashiCorp Vault или AWS Key Management Service. Я отмечу эту область как ту, в которую следует погрузиться глубже.
Я собираюсь дать ссылку на отличный ресурс и демонстрационный пример от Jeff Geerling Ansible 101 - Episode 6 - Ansible Vault and Roles
Ansible Galaxy (Docs) Итак, мы уже использовали ansible-galaxy для создания некоторых ролей и файловой структуры для нашего демо-проекта. Но у нас также есть документация по Ansible Galaxy
\u0026ldquo;Galaxy - это центр для поиска и обмена содержимым Ansible\u0026rdquo;.
Тестирование Ansible Ansible Molecule - проект Molecule предназначен для помощи в разработке и тестировании ролей Ansible.
Ansible Lint - CLI-инструмент для линтинга плейбуков, ролей и коллекций.
Другой ресурс Документация Ansible Ресурсы What is Ansible Ansible 101 - Episode 1 - Introduction to Ansible NetworkChuck - You need to learn Ansible right now! Your complete guide to Ansible Этот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате. В этом посте мы завершаем рассмотрение управления конфигурацией, далее мы перейдем к CI/CD Pipelines и некоторым инструментам и процессам, которые мы можем увидеть и использовать для достижения этого рабочего процесса при разработке и выпуске приложений.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day69/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day70/":{title:"70. Конвейеры CI/CD",tags:["devops"],content:`Реализация конвейера CI/CD (Continous Integration/Continous Deployment) является основой современной среды DevOps.
Он устраняет разрыв между разработкой и операциями, автоматизируя сборку, тестирование и развертывание приложений.
Мы много говорили об этой мантре Continous во вступительном разделе задачи. Но повторим еще раз:
Continous Integration (CI) - это более современная практика разработки программного обеспечения, при которой инкрементные изменения кода вносятся чаще и надежнее. Автоматизированные шаги рабочего процесса сборки и тестирования, запускаемые Contininous Integration, обеспечивают надежность изменений кода, сливаемых в репозиторий.
Затем этот код / приложение быстро и беспрепятственно доставляется в рамках процесса непрерывного развертывания.
Важность CI/CD? Доставка программного обеспечения быстро и эффективно Облегчает эффективный процесс вывода приложений на рынок как можно быстрее. Непрерывный поток исправлений ошибок и новых функций без ожидания месяцев или лет выпуска версии. Возможность для разработчиков регулярно вносить небольшие важные изменения означает, что мы быстрее получаем исправления и новые функции.
Хорошо, так что же это значит? В День 5 мы рассмотрели много теории, лежащей в основе DevOps, и, как уже упоминалось здесь, CI/CD Pipeline является основой современной среды DevOps.
Я хочу повторить некоторые ключевые моменты на этом изображении выше, теперь, когда мы немного продвинулись в изучении основ DevOps.
Мы имеем в виду жизненный цикл разработки программного обеспечения (SDLC).
Этапы обычно записываются в бесконечном цикле, поскольку этот цикл повторяется вечно.
The steps in the cycle are, developers write the code then it gets built or all compiled together then it\u0026rsquo;s tested for bugs then it\u0026rsquo;s deployed into production where it\u0026rsquo;s used (Operated) by end users or customers then we monitor and collect feedback and finally we plan improvements around that feedback rinse and repeat.
Давайте немного углубимся в CI/CD CI CI - это практика разработки, которая требует от разработчиков интегрировать код в общий репозиторий несколько раз в день.
Когда код написан и помещен в репозиторий, такой как github или gitlab, вот тут-то и начинается волшебство.
Код проверяется автоматизированной сборкой, что позволяет командам или владельцу проекта обнаружить любые проблемы на ранней стадии.
После этого код анализируется и подвергается серии автоматизированных тестов.
Юнит-тестирование - тестирование отдельных частей исходного кода. тестирование на валидность - проверяется, что программное обеспечение удовлетворяет или соответствует предполагаемому использованию. Тестирование формата проверяет синтаксис и другие ошибки форматирования. Эти тесты создаются как рабочий процесс и затем запускаются каждый раз, когда вы продвигаете мастер-ветку, поэтому практически каждая крупная команда разработчиков имеет какой-то рабочий процесс CI/CD, и помните, что в команде разработчиков новый код может поступать из команд по всему миру в разное время суток от разработчиков, работающих над самыми разными проектами, поэтому эффективнее построить автоматизированный рабочий процесс тестов, которые убеждаются, что все находятся на одной странице, прежде чем код будет принят. Человеку потребуется гораздо больше времени, чтобы сделать это каждый раз.
Как только мы завершили наши тесты и они прошли успешно, мы можем скомпилировать их и отправить в наш репозиторий. Для примера я использую Docker Hub, но это может быть любое другое хранилище, которое затем будет использовано для CD-аспекта конвейера.
Итак, этот процесс, очевидно, очень похож на процесс разработки программного обеспечения: мы создаем наше приложение, добавляем, исправляем ошибки и т.д., затем обновляем контроль исходных текстов и версионируем их, одновременно тестируя.
Переходим к следующему этапу - элементу CD, который на самом деле все больше и больше является тем, что мы обычно видим от любого готового программного обеспечения, я бы утверждал, что мы увидим тенденцию, что если мы получим наше программное обеспечение от такого поставщика, как Oracle или Microsoft, мы будем потреблять его из репозитория типа Docker Hub, а затем мы будем использовать наши конвейеры CD для развертывания этого в наших средах.
CD Теперь у нас есть протестированная версия нашего кода, и мы готовы к развертыванию на природе. Как я уже сказал, поставщик программного обеспечения пройдет через этот этап, но я твердо уверен, что именно так мы все будем развертывать готовое программное обеспечение, которое нам понадобится в будущем.
Теперь пришло время выпустить наш код в среду. Это будет включать в себя производственную среду, но также, вероятно, и другие среды, такие как staging.
Следующим шагом, по крайней мере, в день 1 v1 развертывания программного обеспечения, является то, что нам нужно убедиться, что мы переносим правильную кодовую базу в правильную среду. Это может быть извлечение элементов из репозитория программного обеспечения (DockerHub), но более чем вероятно, что мы также извлечем дополнительную конфигурацию из другого репозитория кода, например, конфигурацию для приложения. На диаграмме ниже мы извлекаем последний релиз программного обеспечения из DockerHub, а затем выпускаем его в нашу среду, при этом, возможно, получая конфигурацию из репозитория Git. Наш CD-инструмент выполняет это и передает все в нашу среду.
Скорее всего, это делается не одновременно, т.е. мы переходим в промежуточную среду и запускаем ее с нашей собственной конфигурацией, чтобы убедиться, что все правильно, и это может быть ручным шагом для тестирования или автоматизированным (давайте остановимся на автоматизированном), прежде чем позволить этому коду быть развернутым в продакшн.
После этого, когда выйдет v2 приложения, мы прополощем и повторим шаги, на этот раз мы убедимся, что наше приложение + конфигурация развернуты в staging, убедимся, что все хорошо, и затем развернем в production.
Зачем использовать CI/CD? Я думаю, мы уже неоднократно рассказывали о преимуществах, но они заключаются в том, что CI/CD автоматизирует то, что в противном случае пришлось бы делать вручную. Он находит небольшие проблемы до того, как они проникнут в основную кодовую базу. Вы, вероятно, можете себе представить, что если вы выкладываете плохой код своим клиентам, то у вас будут плохие времена!
Это также помогает предотвратить то, что мы называем техническим долгом - идею о том, что поскольку основные репозитории кода постоянно дорабатываются с течением времени, то быстрое исправление, сделанное в первый день, становится экспоненциально более дорогим исправлением годы спустя, потому что теперь этот пластырь исправления будет так глубоко переплетен и вплетен во все кодовые базы и логику.
Инструментарий Как и в других разделах, мы будем работать с некоторыми инструментами, которые обеспечивают процесс конвейера CI/CD.
Я считаю важным отметить, что не все инструменты должны делать и CI, и CD. Мы рассмотрим ArgoCD, который, как вы догадались, отлично справляется с CD-элементом развертывания нашего программного обеспечения в кластере Kubernetes. Но что-то вроде Jenkins может работать на разных платформах.
Я планирую рассмотреть следующее:
Jenkins ArgoCD GitHub Actions Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day70/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day71/":{title:"71. Введение в Jenkins",tags:["devops"],content:` Jenkins - это инструмент непрерывной интеграции, который позволяет непрерывно разрабатывать, тестировать и развертывать вновь созданный код.
Этого можно достичь двумя способами: ночные сборки или непрерывная разработка. Первый вариант заключается в том, что наши разработчики в течение дня занимаются своими задачами и в конце рабочего дня вносят свои изменения в репозиторий исходного кода. Затем в течение ночи мы проводим модульные тесты и собираем программное обеспечение. Это можно считать старым способом интеграции всего кода.
Другой вариант и более предпочтительный способ заключается в том, что наши разработчики по-прежнему фиксируют свои изменения в исходном коде, а затем, после фиксации кода, непрерывно запускается процесс сборки.
Приведенные выше методы означают, что при распределении разработчиков по всему миру у нас нет определенного времени каждый день, когда мы должны прекратить фиксацию изменений в коде. Именно здесь на помощь приходит Jenkins, который выступает в роли CI-сервера, контролирующего тесты и процессы сборки.
Я знаю, что мы говорим о Jenkins, но я также хочу добавить еще несколько, которые можно будет рассмотреть позже, чтобы понять, почему я вижу Jenkins как наиболее популярный, почему это так и что другие могут сделать по сравнению с Jenkins.
TravisCI - Размещенный, распределенный сервис непрерывной интеграции, используемый для сборки и тестирования программных проектов, размещенных на GitHub.
Bamboo - может запускать несколько сборок параллельно для более быстрой компиляции, имеет встроенную функциональность для связи с репозиториями и задачи сборки для Ant, Maven.
Buildbot - это фреймворк с открытым исходным кодом для автоматизации процессов сборки, тестирования и выпуска программного обеспечения. Он написан на языке Python и поддерживает распределенное, параллельное выполнение заданий на нескольких платформах.
Apache Gump - специфичен для Java-проектов, разработан с целью сборки и тестирования этих Java-проектов каждую ночь. обеспечивает совместимость всех проектов как на уровне API, так и на уровне функциональности.
Поскольку мы сейчас сосредоточимся на Jenkins - Jenkins, как и все вышеперечисленные инструменты, имеет открытый исходный код и представляет собой сервер автоматизации, написанный на Java. Он используется для автоматизации процесса разработки программного обеспечения посредством непрерывной интеграции и облегчает непрерывную доставку.
Особенности Jenkins Как и следовало ожидать, Jenkins имеет множество функций, охватывающих множество областей.
Простая установка - Jenkins - это самостоятельная программа на базе java, готовая к работе с пакетами для операционных систем Windows, macOS и Linux.
Простая конфигурация - Простая установка и настройка через веб-интерфейс, включающий проверку ошибок и встроенную помощь.
Плагины - Множество плагинов доступно в Центре обновления и интегрируется со многими инструментами в инструментальной цепочке CI / CD.
Расширяемость - В дополнение к доступным плагинам, Jenkins может быть расширен за счет архитектуры плагинов, что обеспечивает практически бесконечное количество вариантов того, для чего он может быть использован.
Распределенность - Jenkins легко распределяет работу по нескольким машинам, помогая ускорить сборку, тестирование и развертывание на различных платформах.
Jenkins Pipeline Вы уже видели этот конвейер, но он используется гораздо шире, и мы не говорили о конкретных инструментах.
Вы собираетесь фиксировать код в Jenkins, который затем будет собирать ваше приложение со всеми автоматизированными тестами, а затем выпускать и развертывать этот код после завершения каждого этапа. Jenkins позволяет автоматизировать этот процесс.
Архитектура Jenkins Во-первых, чтобы не изобретать велосипед, всегда стоит начать с Документации Jenkins, но я собираюсь изложить свои заметки и выводы и здесь.
Jenkins может быть установлен на многих различных операционных системах, Windows, Linux и macOS, а также имеет возможность развертывания в виде контейнера Docker и в Kubernetes. Установка Jenkins
По мере изучения этого вопроса мы, вероятно, рассмотрим установку Jenkins в кластере minikube, имитируя развертывание в Kubernetes. Но это будет зависеть от скриптов, которые мы составим в оставшейся части раздела.
Теперь давайте разберем изображение ниже.
Шаг 1 - Разработчики фиксируют изменения в репозитории исходного кода.
Шаг 2 - Jenkins проверяет репозиторий через регулярные промежутки времени и извлекает любой новый код.
Шаг 3 - Сервер сборки затем собирает код в исполняемый файл, в данном примере мы используем maven как хорошо известный сервер сборки. Еще одна область, которую необходимо охватить.
Шаг 4 - Если сборка не удалась, то разработчикам отправляется обратная связь.
Шаг 5 - Jenkins развертывает собранное приложение на тестовом сервере, в данном примере мы используем selenium как хорошо известный тестовый сервер. Еще одна область, которую необходимо охватить.
Шаг 6 - Если тест не прошел, то обратная связь передается разработчикам.
Шаг 7 - Если тесты прошли успешно, мы можем выпустить продукт в производство.
Этот цикл непрерывен, именно это позволяет обновлять приложения за минуты, а не за часы, дни, месяцы, годы!
Архитектура Jenkins может быть описана гораздо подробнее, если вам это нужно, у них есть возможность работы в режиме master-slave, что позволяет ведущему распределять задачи между подчиненными jenkins.
Для справки, поскольку Jenkins является открытым исходным кодом, будет много предприятий, которым требуется поддержка, CloudBees - это корпоративная версия Jenkins, которая предоставляет поддержку и, возможно, другие функциональные возможности для платного корпоративного клиента.
Примером такого клиента является компания Bosch, вы можете ознакомиться с примером Bosch здесь.
Я собираюсь найти пошаговый пример приложения, которое мы могли бы использовать, чтобы пройтись по Jenkins, а затем использовать его с другими инструментами.
Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day71/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day72/":{title:"72. Работа с Jenkins",tags:["devops"],content:`Сегодня мы планируем немного поработать с Jenkins и сделать что-то в рамках нашего конвейера CI, рассматривая некоторые примеры кодовых баз, которые мы можем использовать.
Что такое конвейер? Прежде чем мы начнем, нам нужно знать, что такое конвейер, когда речь идет о CI, и мы уже рассмотрели это на вчерашнем занятии с помощью следующего изображения.
Мы хотим взять процессы или шаги, описанные выше, и автоматизировать их, чтобы в итоге получить результат, то есть развернутое приложение, которое мы можем отправить нашим клиентам, конечным пользователям и т.д.
Этот автоматизированный процесс позволяет нам иметь контроль версий для наших пользователей и клиентов. Каждое изменение, улучшение функций, исправление ошибок и т.д. проходит через этот автоматизированный процесс, подтверждая, что все в порядке, без излишнего ручного вмешательства, чтобы убедиться, что наш код хорош.
Этот процесс включает в себя создание программного обеспечения надежным и повторяемым способом, а также продвижение созданного программного обеспечения (называемого \u0026ldquo;сборкой\u0026rdquo;) через несколько этапов тестирования и развертывания.
Конвейер jenkins записывается в текстовый файл Jenkinsfile. Который сам должен быть зафиксирован в репозитории контроля исходного кода. Это также известно как Pipeline as code, мы также можем сравнить это с Infrastructure as code, о которой мы рассказывали несколько недель назад.
Jenkins Pipeline Definition
Развертывание Jenkins Я получил некоторое удовольствие от развертывания Jenkins, Вы заметите из документации, что есть много вариантов того, где вы можете установить Jenkins.
Учитывая, что у меня под рукой есть minikube, и мы уже использовали его несколько раз, я хотел использовать его и для этой задачи. (Хотя шаги, описанные в Kubernetes Installation, привели к тому, что я уперся в стену и не смог запустить систему, вы можете сравнить эти два варианта, когда я задокументирую свои шаги здесь.
Первым шагом будет запуск нашего кластера minikube, мы можем сделать это с помощью команды minikube start.
Я добавил папку со всеми конфигурациями и значениями YAML, которые можно найти здесь Теперь, когда у нас есть наш кластер, мы можем выполнить следующие действия для создания пространства имен jenkins. kubectl create -f jenkins-namespace.yml
Мы будем использовать Helm для развертывания jenkins в нашем кластере, о Helm мы рассказывали в разделе Kubernetes. Сначала нам нужно добавить репозиторий jenkinsci в helm helm repo add jenkinsci https://charts.jenkins.io, затем обновить наши таблицы helm repo update.
Идея Jenkins заключается в том, что он будет сохранять состояние для своих пайплайнов, вы можете запустить вышеупомянутую установку helm без персистентности, но если эти pods будут перезагружены, изменены или модифицированы, то все пайплайны или конфигурации, которые вы создали, будут потеряны. Мы создадим том для персистентности, используя файл jenkins-volume.yml с помощью команды kubectl apply -f jenkins-volume.yml.
Нам также нужна учетная запись службы, которую мы можем создать с помощью этого yaml-файла и команды. kubectl apply -f jenkins-sa.yml
На этом этапе мы готовы к развертыванию с помощью схемы helm, сначала мы определим нашу схему с помощью chart=jenkinsci/jenkins, а затем развернем с помощью этой команды, где jenkins-values.yml содержит учетные записи персистентности и сервисов, которые мы ранее развернули на нашем кластере. helm install jenkins -n jenkins -f jenkins-values.yml $chart.
На этом этапе наши капсулы будут извлекать образ, но у капсулы не будет доступа к хранилищу, поэтому никакая конфигурация не может быть начата с точки зрения запуска Jenkins.
Именно здесь документация не помогла мне понять, что должно произойти. Но мы видим, что у нас нет разрешения на запуск установки jenkins.
Для того чтобы исправить вышеописанное или решить проблему, нам нужно убедиться, что мы предоставили доступ или правильное разрешение для того, чтобы наши jenkins pods могли писать в это место, которое мы предложили. Мы можем сделать это, используя minikube ssh, который введет нас в докер-контейнер minikube, на котором мы работаем, а затем, используя sudo chown -R 1000:1000 /data/jenkins-volume, мы можем убедиться, что у нас установлены разрешения на наш том данных.
Вышеописанный процесс должен исправить капсулы, однако если это не так, вы можете заставить капсулы обновиться с помощью команды kubectl delete pod jenkins-0 -n jenkins. На этом этапе у вас должно быть 2/2 запущенных стручка под названием jenkins-0.
Теперь нам нужен наш пароль администратора, и мы можем сделать это с помощью следующей команды. kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password \u0026amp;\u0026amp; echo
Теперь откройте новый терминал, так как мы собираемся использовать команду port-forward, чтобы получить доступ с нашей рабочей станции. kubectl --namespace jenkins port-forward svc/jenkins 8080:8080.
Теперь мы должны быть в состоянии открыть браузер и войти на http://localhost:8080 и аутентифицироваться с именем пользователя: admin и паролем, которые мы собрали в предыдущем шаге.
После аутентификации наша страница приветствия Jenkins должна выглядеть примерно так:
Отсюда я бы предложил перейти к \u0026ldquo;Manage Jenkins\u0026rdquo;, и вы увидите \u0026ldquo;Manage Plugins\u0026rdquo;, где будут доступны некоторые обновления. Выберите все эти плагины и выберите \u0026ldquo;Загрузить сейчас и установить после перезапуска\u0026rdquo;.
Если вы хотите пойти еще дальше и автоматизировать развертывание Jenkins с помощью shell-скрипта, этот замечательный репозиторий был предоставлен мне в twitter mehyedes/nodejs-k8s
Jenkinsfile Теперь у нас есть Jenkins, развернутый в нашем кластере Kubernetes, мы можем вернуться назад и подумать об этом Jenkinsfile.
Каждый Jenkinsfile, скорее всего, будет начинаться примерно так: сначала вы определяете шаги вашего конвейера, в данном случае это Build \u0026gt; Test \u0026gt; Deploy. Но на самом деле мы не делаем ничего, кроме использования команды echo для вызова определенных этапов.
Jenkinsfile (декларативный конвейер)pipeline {agent anystages {stage('Build') {steps {echo 'Building..'}}stage('Test') {steps {echo 'Testing..'}}stage('Deploy') {steps {echo 'Deploying....'}}}}В нашей приборной панели Jenkins выберите \u0026ldquo;New Item\u0026rdquo; дайте элементу имя, я собираюсь \u0026ldquo;echo1\u0026rdquo; Я собираюсь предложить, что это Pipeline.
Нажмите Ok, и у вас появятся вкладки (General, Build Triggers, Advanced Project Options и Pipeline) для простого теста нас интересует только Pipeline. В разделе Pipeline у вас есть возможность добавить скрипт, мы можем скопировать и вставить приведенный выше скрипт в поле.
Как мы уже говорили выше, это не даст многого, но покажет нам этапы нашей сборки \u0026gt; тестирования \u0026gt; развертывания
Нажмите Save, теперь мы можем запустить нашу сборку, используя сборку, показанную ниже.
Мы также должны открыть терминал и выполнить команду kubectl get pods -n jenkins, чтобы посмотреть, что произойдет.
Хорошо, очень просто, но теперь мы можем видеть, что наше развертывание и установка Jenkins работает правильно, и мы можем начать видеть здесь строительные блоки конвейера CI.
В следующем разделе мы будем строить конвейер Jenkins.
Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day72/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day73/":{title:"73. Построение конвейера Jenkins",tags:["devops"],content:`В предыдущем разделе мы развернули Jenkins на нашем кластере Minikube и создали очень простой Jenkins Pipeline, который не делал ничего особенного, кроме как повторял этапы Pipeline.
Вы также могли заметить, что в процессе создания Jenkins Pipeline нам доступны некоторые примеры скриптов для запуска.
Первый демонстрационный скрипт - \u0026ldquo;Declartive (Kubernetes)\u0026rdquo;, и вы можете увидеть его этапы ниже.
// Uses Declarative syntax to run commands inside a container.pipeline {agent {kubernetes {// Rather than inline YAML, in a multibranch Pipeline you could use: yamlFile 'jenkins-pod.yaml'// Or, to avoid YAML:// containerTemplate {// name 'shell'// image 'ubuntu'// command 'sleep'// args 'infinity'// }yaml '''apiVersion: v1kind: Podspec:containers:- name: shellimage: ubuntucommand:- sleepargs:- infinity'''// Can also wrap individual steps:// container('shell') {// sh 'hostname'// }defaultContainer 'shell'}}stages {stage('Main') {steps {sh 'hostname'}}}}Ниже показан результат того, что происходит при выполнении этого конвейера.
Создание задания Цели
Создать простое приложение и сохранить его в публичном репозитории GitHub (https://github.com/scriptcamp/kubernetes-kaniko.git).
С помощью Jenkins собрать образ нашего docker-контейнера и выложить в docker hub. (Для этого мы будем использовать частный репозиторий).
Чтобы добиться этого в нашем кластере Kubernetes, работающем в Minikube или с его помощью, нам нужно использовать нечто под названием Kaniko В общем, если вы используете Jenkins в реальном кластере Kubernetes или запускаете его на сервере, вы можете указать агента, который даст вам возможность выполнять команды сборки docker и загружать их в DockerHub.
Учитывая вышесказанное, мы также собираемся развернуть секрет в Kubernetes с нашими учетными данными GitHub.
kubectl create secret docker-registry dockercred \\--docker-server=https://index.docker.io/v1/ \\--docker-username=\u0026lt;dockerhub-username\u0026gt; \\--docker-password=\u0026lt;dockerhub-password\u0026gt;\\--docker-email=\u0026lt;dockerhub-email\u0026gt;На самом деле я хочу поделиться еще одним замечательным ресурсом от DevOpsCube.com, где рассматривается многое из того, о чем мы будем говорить здесь.
Добавление учетных данных в Jenkins Однако если вы используете систему Jenkins, в отличие от нашей, то вы, скорее всего, захотите определить свои учетные данные в Jenkins, а затем использовать их несколько раз в своих конвейерах и конфигурациях. Мы можем ссылаться на эти учетные данные в конвейерах, используя ID, который мы определили при создании. Я пошел дальше и создал учетные данные для DockerHub и GitHub.
Сначала выберите \u0026ldquo;Manage Jenkins\u0026rdquo;, а затем \u0026ldquo;Manage Credentials\u0026rdquo;.
В центре страницы вы увидите магазины, предназначенные для Jenkins, нажмите на Jenkins здесь.
Теперь выберите Global Credentials (Unrestricted).
Затем в левом верхнем углу у вас есть Добавить учетные данные
Заполните данные вашей учетной записи и затем выберите OK, помните, что ID - это то, на что вы будете ссылаться, когда захотите вызвать эту учетную запись. Мой совет здесь также заключается в том, что вы должны использовать специальные маркеры доступа, а не пароли.
Для GitHub вы должны использовать Personal Access Token.
Лично мне процесс создания этих учетных записей показался не очень интуитивным, поэтому, хотя мы не используем их, я хотел поделиться процессом, так как он не совсем понятен из пользовательского интерфейса.
Построение конвейера У нас есть учетные данные DockerHub, развернутые как секрет в нашем кластере Kubernetes, к которым мы будем обращаться на этапе docker deploy to DockerHub в нашем конвейере.
Сценарий конвейера - это то, что вы видите ниже, это, в свою очередь, может стать нашим Jenkinsfile, расположенным в нашем репозитории GitHub, который, как вы можете видеть, также указан на этапе Get the project в конвейере.
podTemplate(yaml: '''apiVersion: v1kind: Podspec:containers:- name: mavenimage: maven:3.8.1-jdk-8command:- sleepargs:- 99d- name: kanikoimage: gcr.io/kaniko-project/executor:debugcommand:- sleepargs:- 9999999volumeMounts:- name: kaniko-secretmountPath: /kaniko/.dockerrestartPolicy: Nevervolumes:- name: kaniko-secretsecret:secretName: dockercreditems:- key: .dockerconfigjsonpath: config.json''') {node(POD_LABEL) {stage('Get the project') {git url: 'https://github.com/scriptcamp/kubernetes-kaniko.git', branch: 'main'container('maven') {stage('Test the project') {sh '''echo pwd'''}}}stage('Build \u0026amp; Test the Docker Image') {container('kaniko') {stage('Deploy to DockerHub') {sh '''/kaniko/executor --context \`pwd\` --destination michaelcade1/helloworld:latest'''}}}}}Чтобы начать работу на приборной панели Jenkins, нам нужно выбрать \u0026ldquo;Новый элемент\u0026rdquo;
Затем мы дадим нашему элементу имя, выберем Pipeline и нажмем OK.
Мы не будем выбирать общие триггеры или триггеры сборки, но поиграйте с ними, так как здесь есть несколько интересных расписаний и других конфигураций, которые могут быть полезны.
Нас интересует только вкладка Pipeline в конце.
В определении пайплайн мы скопируем и вставим скрипт пайплайна, который мы описали выше, в раздел Script и нажмем кнопку save.
Далее мы выберем опцию \u0026ldquo;Build Now\u0026rdquo; в левой части страницы.
Теперь вам нужно подождать некоторое время, меньше минуты, и вы должны увидеть в статусе этапы, которые мы определили выше в нашем скрипте.
Что еще более важно, если мы теперь перейдем на наш DockerHub и проверим, что у нас есть новая сборка.
В целом это заняло некоторое время, но я хотел придерживаться этого, чтобы получить практический опыт и проработать скрипт, который может выполнить каждый, используя minikube и доступ к github и dockerhub.
Репозиторий DockerHub, который я использовал для этого демо, был частным. Но в следующем разделе я хочу продвинуть некоторые из этих этапов и заставить их действительно что-то делать, а не просто выводить pwd, и действительно запустить некоторые тесты и этапы сборки.
Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day73/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day74/":{title:"74. Hello World - Jenkinsfile App Pipeline",tags:["devops"],content:"Здравствуй мир - Jenkinsfile App Pipeline В предыдущем разделе мы построили простой конвейер в Jenkins, который будет перемещать наш образ докера из нашего dockerfile в публичном репозитории GitHub в наш частный репозиторий Dockerhub.\nВ этом разделе мы хотим сделать еще один шаг вперед и добиться следующего с помощью нашего простого приложения.\nЦель Dockerfile (Hello World) Jenkinsfile Jenkins Pipeline для запуска при обновлении репозитория GitHub Используйте репозиторий GitHub в качестве источника. Запуск - Clone/Get Repository, Build, Test, Deploy Stages Развертывание на DockerHub с инкрементными номерами версий Stretch Goal для развертывания на нашем кластере Kubernetes (для этого потребуется еще одно задание и репозиторий манифеста с использованием учетных данных GitHub). Шаг первый У нас есть наш GitHub репозиторий В настоящее время он содержит наш Dockerfile и наш index.html\nЭто то, что мы использовали в качестве источника в нашем конвейере, теперь мы хотим добавить этот скрипт Jenkins Pipeline в наш репозиторий GitHub.\nТеперь вернемся к нашей приборной панели Jenkins и создадим новый пайплайн, но теперь вместо вставки нашего скрипта мы будем использовать \u0026ldquo;Pipeline script from SCM\u0026rdquo; Мы будем использовать приведенные ниже параметры конфигурации.\nДля справки мы будем использовать https://github.com/MichaelCade/Jenkins-HelloWorld.git в качестве URL репозитория.\nНа этом этапе мы можем нажать кнопку сохранить и применить, после чего мы сможем вручную запустить наш Pipeline для сборки нового образа Docker, загруженного в наш репозиторий DockerHub.\nОднако я также хочу убедиться, что мы установили расписание, по которому при каждом изменении нашего репозитория или исходного кода будет запускаться сборка. Мы можем использовать веб-крючки или запланированное извлечение.\nЭто важный момент, потому что если вы используете дорогостоящие облачные ресурсы для хранения конвейера и у вас много изменений в репозитории кода, то вы понесете большие расходы. Мы знаем, что это демонстрационная среда, поэтому я использую опцию \u0026ldquo;poll scm\u0026rdquo;. (Также я считаю, что при использовании minikube мне не хватает возможности использовать webhooks)\nОдна вещь, которую я изменил со вчерашней сессии, это то, что теперь я хочу загружать изображение в публичный репозиторий, который в данном случае будет michaelcade1\\90DaysOfDevOps, мой Jenkinsfile уже содержит это изменение. И из предыдущих разделов я удалил все существующие образы демо-контейнеров.\nДвигаясь назад, мы создали наш Pipeline, а затем, как было показано ранее, добавили нашу конфигурацию.\nНа данном этапе наш конвейер еще не запущен, и вид сцены будет выглядеть примерно так.\nТеперь нажмем кнопку \u0026ldquo;Build Now\u0026rdquo;. и в представлении этапа будут отображены наши этапы.\nЕсли мы перейдем к нашему репозиторию DockerHub, у нас должно быть 2 новых образа Docker. У нас должен быть идентификатор сборки 1 и последняя версия, потому что каждая сборка, которую мы создаем на основе команды \u0026ldquo;Upload to DockerHub\u0026rdquo;, отправляет версию, используя переменную окружения Jenkins Build_ID, а также выпускает последнюю версию.\nДавайте создадим обновление файла index.html в нашем репозитории GitHub, как показано ниже, я позволю вам пойти и узнать, что говорила версия 1 файла index.html.\nЕсли мы вернемся в Jenkins и снова выберем \u0026ldquo;Build Now\u0026rdquo;. Мы увидим, что наша сборка #2 прошла успешно.\nЗатем быстро взглянув на DockerHub, мы увидим, что у нас есть наш тег версии 2 и наш последний тег.\nЗдесь стоит отметить, что я добавил в свой кластер Kubernetes секрет, который позволяет мне получить доступ и аутентификацию для отправки моих сборок docker в DockerHub. Если вы следуете этому примеру, вам следует повторить этот процесс для своей учетной записи, а также внести изменения в Jenkinsfile, связанный с моим репозиторием и учетной записью.\nРесурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD ",url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day74/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day75/":{title:"75. Обзор GitHub Actions",tags:["devops"],content:`Обзор действий GitHub В этом разделе я хотел бы перейти к рассмотрению, возможно, другого подхода, чем тот, на который мы только что потратили время. На этом занятии мы сосредоточимся на GitHub Actions.
GitHub Actions - это платформа CI/CD, которая позволяет нам строить, тестировать и развертывать, помимо прочих задач, наш конвейер. В ней есть концепция рабочих процессов, которые собираются и тестируются на основе репозитория GitHub. Вы также можете использовать GitHub Actions для управления другими рабочими процессами на основе событий, происходящих в вашем репозитории.
Рабочие процессы В целом, в GitHub Actions наша задача называется рабочий процесс.
Рабочий процесс** - это настраиваемый автоматизированный процесс. Определяется как файлы YAML. Содержит и запускает одно или несколько заданий. Запускается при срабатывании события в вашем хранилище или может быть запущен вручную. Вы можете использовать несколько рабочих процессов для каждого хранилища. рабочий процесс содержит задание, а затем шаги для достижения этого задания. В рамках рабочего процесса у нас также будет запускающий механизм, на котором будет выполняться наш рабочий процесс. Например, у вас может быть один рабочий процесс для создания и тестирования запросов, другой рабочий процесс для развертывания вашего приложения каждый раз, когда создается релиз, и еще один рабочий процесс, который добавляет метку каждый раз, когда кто-то открывает новую проблему.
События События - это определенные события в хранилище, которые запускают рабочий процесс на выполнение.
Задания Задание - это набор шагов рабочего процесса, которые выполняются на бегунке.
Шаги Каждый шаг в задании может быть скриптом оболочки, который выполняется, или действием. Шаги выполняются по порядку и зависят друг от друга.
Действия Повторяющееся пользовательское приложение, используемое для часто повторяющихся задач.
Бегуны Бегунок - это сервер, который запускает рабочий процесс, каждый бегунок выполняет одно задание за раз. GitHub Actions предоставляет возможность запуска бегунов для Ubuntu Linux, Microsoft Windows и macOS. Вы также можете разместить свой собственный на определенной ОС или оборудовании.
Ниже вы можете увидеть, как это выглядит: у нас есть событие, запускающее наш рабочий процесс \u0026gt; наш рабочий процесс состоит из двух заданий \u0026gt; внутри наших заданий есть шаги, а затем действия.
YAML Прежде чем мы приступим к рассмотрению реального случая использования, давайте взглянем на приведенное выше изображение в виде примера YAML-файла.
Я добавил #, чтобы прокомментировать, где мы можем найти компоненты рабочего процесса YAML.
#Workflowname: 90DaysOfDevOps#Eventon: [push]#Jobsjobs:check-bats-version:#Runnersruns-on: ubuntu-latest#Stepssteps:#Actions- uses: actions/checkout@v2- uses: actions/setup-node@v2with:node-version: '14'- run: npm install -g bats- run: bats -vПриступаем к работе с GitHub Actions Я думаю, что у GitHub Actions есть много возможностей, да, они удовлетворят ваши потребности в CI/CD, когда речь идет о сборке, тестировании, развертывании вашего кода и последующих шагах.
Я вижу множество вариантов и других автоматизированных задач, для которых мы могли бы использовать GitHub Actions.
Использование GitHub Actions для линтинга вашего кода Один из вариантов - убедиться, что ваш код чист и аккуратен в вашем репозитории. Это будет наш первый демонстрационный пример.
Я собираюсь использовать некоторый пример кода, связанный в одном из ресурсов для этого раздела, мы будем использовать github/super-linter для проверки нашего кода.
name: Super-Linteron: pushjobs:super-lint:name: Lint code baseruns-on: ubuntu-lateststeps:- name: Checkout codeuses: actions/checkout@v2- name: Run Super-Linteruses: github/super-linter@v3env:DEFAULT_BRANCH: mainGITHUB_TOKEN: \${{ secrets.GITHUB_TOKEN }}github/super-linter Вы можете видеть, что для одного из наших шагов у нас есть действие под названием github/super-linter, которое ссылается на шаг, уже написанный сообществом. Вы можете узнать больше об этом здесь Super-Linter
\u0026ldquo;Этот репозиторий предназначен для GitHub Action для запуска Super-Linter. Это простая комбинация различных линтеров, написанных на bash, чтобы помочь проверить ваш исходный код.\u0026rdquo;
Также в приведенном фрагменте кода упоминается GITHUB_TOKEN, поэтому мне было интересно узнать, зачем и для чего это нужно.
\u0026ldquo;ПРИМЕЧАНИЕ: Если вы передадите переменную окружения GITHUB_TOKEN: \${{ secrets.GITHUB_TOKEN }} в вашем рабочем процессе, то GitHub Super-Linter будет отмечать статус каждого отдельного запуска линтера в разделе \u0026ldquo;Проверки\u0026rdquo; запроса на выгрузку. Без этого вы будете видеть только общий статус всего прогона. Не нужно устанавливать GitHub Secret, так как он автоматически устанавливается GitHub, его нужно только передать в действие.\u0026rdquo;.
Выделенный жирным текст важно отметить на данном этапе. Мы используем его, но нам не нужно устанавливать какую-либо переменную окружения в нашем репозитории.
Для тестирования мы будем использовать наш репозиторий, который мы использовали в нашей демонстрации Jenkins.Jenkins-HelloWorld.
Вот наш репозиторий в том виде, в котором мы оставили его в сессии Jenkins.
Для того, чтобы воспользоваться преимуществами, мы должны использовать вкладку Actions выше, чтобы выбрать из рынка, о котором я расскажу в ближайшее время, или мы можем создать наши собственные файлы, используя наш код супер-лайнера выше, чтобы создать свой собственный, вы должны создать новый файл в вашем репозитории именно в этом месте. .github/workflows/workflow_name, очевидно, убедившись, что имя workflow_name - это что-то полезное для вас, узнаваемое. Здесь мы можем иметь множество различных рабочих процессов, выполняющих различные задания и задачи в нашем репозитории.
Мы создадим .github/workflows/super-linter.yml.
Затем мы можем вставить наш код и зафиксировать его в нашем репозитории, если мы перейдем на вкладку Actions, то увидим наш рабочий процесс Super-Linter в списке, как показано ниже,
Мы определили в нашем коде, что этот рабочий процесс будет запускаться, когда мы будем перемещать что-либо в наш репозиторий, поэтому при перемещении файла super-linter.yml в наш репозиторий мы запустили рабочий процесс.
Как вы можете видеть из вышеприведенного, у нас есть некоторые ошибки, скорее всего, из-за моих способностей к взлому и кодированию.
Хотя на самом деле это был не мой код, по крайней мере пока, запустив его и получив ошибку, я обнаружил вот это issue
Дубль #2 Я изменил версию Super-Linter с версии 3 на 4 и запустил задачу снова.
Как и ожидалось, мой хакерский кодинг вызвал некоторые проблемы, и вы можете увидеть их здесь, в рабочем процессе.
Я хотел показать, как теперь выглядит наш репозиторий, когда что-то в рабочем процессе не сработало или сообщило об ошибке.
Теперь, если мы решим проблему с моим кодом и внесем изменения, наш рабочий процесс снова запустится (как видно из изображения, потребовалось некоторое время, чтобы устранить наши \u0026ldquo;ошибки\u0026rdquo;). Удаление файла, вероятно, не рекомендуется, но это очень быстрый способ показать, что проблема решена.
Если вы нажмете кнопку \u0026ldquo;Новый рабочий процесс\u0026rdquo;, выделенную выше, это откроет вам дверь к огромному количеству действий. Вы, наверное, заметили, что мы не хотим изобретать колесо, мы хотим стоять на плечах гигантов и делиться нашим кодом, автоматизацией и навыками, чтобы сделать нашу жизнь проще.
О, я не показал вам зеленую галочку на репозитории, когда наш рабочий процесс был успешным.
Я думаю, что на этом основы GitHub Actions исчерпаны, но если вы похожи на меня, то вы наверняка видите, как еще можно использовать GitHub Actions для автоматизации множества задач.
Далее мы рассмотрим другую область CD, мы рассмотрим ArgoCD для развертывания наших приложений в наших средах.
Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day75/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day76/":{title:"76. Обзор ArgoCD",tags:["devops"],content:`Обзор ArgoCD \u0026ldquo;Argo CD - это декларативный инструмент непрерывной доставки GitOps для Kubernetes\u0026rdquo;.
Контроль версий - ключевой момент здесь. Вы когда-нибудь вносили изменения в вашу среду на лету и не помните об этих изменениях, а поскольку свет горит и все вокруг зеленое, вы продолжаете упорно двигаться вперед? Вы когда-нибудь вносили изменения и ломали все или часть всего? Вы могли бы знать, что внесли изменение, и вы можете быстро откатить свое изменение, тот плохой скрипт или опечатку. А теперь сделайте это в массовом масштабе, и, возможно, это были не вы, или, возможно, ошибка была обнаружена не сразу, и теперь бизнес страдает. Поэтому контроль версий очень важен. Не только это, но и \u0026ldquo;определения приложений, конфигурации и окружения должны быть декларативными и контролируемыми по версиям\u0026rdquo;. В дополнение к этому (что взято из ArgoCD), они также упоминают, что \u0026ldquo;развертывание приложений и управление жизненным циклом должно быть автоматизировано, проверяемо и просто для понимания\u0026rdquo;.
С точки зрения операционной деятельности, но много играя с Infrastructure as Code, это следующий шаг к обеспечению того, чтобы все эти хорошие вещи были улажены по пути с помощью рабочих процессов непрерывного развертывания/доставки.
Что такое ArgoCD
Развертывание ArgoCD Для этого развертывания мы снова будем использовать наш надежный кластер minikube Kubernetes локально.
kubectl create namespace argocdkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yamlУбедитесь, что все подсистемы ArgoCD запущены и работают с помощью команды kubectl get pods -n argocd.
Также проверим все, что мы развернули в пространстве имен с помощью kubectl get all -n argocd
Когда все выглядит хорошо, мы должны рассмотреть возможность доступа к этому через порт. Используя команду kubectl port-forward svc/argocd-server -n argocd 8080:443. Сделайте это в новом терминале.
Затем откройте новый веб-браузер и перейдите по адресу https://localhost:8080.
Для входа в систему вам понадобится имя пользователя admin, а для получения созданного вами секрета в качестве пароля используйте команду kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026quot;{.data.password}\u0026quot; | base64 -d \u0026amp;\u0026amp; echo
После входа в систему у вас будет чистый холст CD.
Развертывание нашего приложения Теперь у нас есть ArgoCD, и мы можем начать использовать его для развертывания наших приложений из наших Git-репозиториев, а также Helm.
Приложение, которое я хочу развернуть, это Pac-Man, да, именно так, знаменитая игра и то, что я использую во многих демонстрациях, когда речь идет об управлении данными, это не последний раз, когда мы видим Pac-Man.
Вы можете найти репозиторий для Pac-Man здесь.
Вместо того чтобы описывать каждый шаг с помощью снимков экрана, я решил, что будет проще создать видеоролик с описанием шагов, предпринятых для развертывания этого конкретного приложения.
ArgoCD Demo - 90DaysOfDevOps
Примечание - Во время видео есть служба, которая никогда не удовлетворяется как здоровое приложение, это потому, что тип LoadBalancer, установленный для службы pacman, находится в состоянии ожидания, в Minikube у нас нет настроенного loadbalancer. Если вы хотите проверить это, вы можете изменить YAML для службы на ClusterIP и использовать проброс портов для игры.
На этом мы завершаем раздел CICD Pipelines, я считаю, что в настоящее время в индустрии уделяется большое внимание этой области, и вы также услышите термины GitOps, связанные с методологиями, используемыми в CICD в целом.
Следующий раздел, в который мы переходим, посвящен Observability, еще одной концепции или области, которая не является новой, но становится все более важной, поскольку мы смотрим на наши среды по-другому.
Ресурсы Jenkins is the way to build, test, deploy Jenkins.io ArgoCD ArgoCD Tutorial for Beginners What is Jenkins? Complete Jenkins Tutorial GitHub Actions GitHub Actions CI/CD `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day76/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day77/":{title:"77. Мониторинг",tags:["devops"],content:`Введение: Мониторинг В этом разделе мы поговорим о мониторинге, что это такое, зачем он нам нужен?
Что такое мониторинг? Мониторинг - это процесс пристального наблюдения за всей инфраструктурой.
и зачем он нам нужен? Предположим, что мы управляем тысячей серверов, которые включают в себя множество специализированных серверов, таких как серверы приложений, серверы баз данных и веб-серверы. Мы также можем усложнить эту задачу за счет дополнительных сервисов и различных платформ, включая публичные облачные предложения и Kubernetes.
Мы отвечаем за то, чтобы все сервисы, приложения и ресурсы на серверах работали так, как должны.
Как мы это делаем? Есть три способа:
Войти вручную на все наши серверы и проверить все данные, относящиеся к процессам и ресурсам служб. Написать скрипт, который заходит на серверы за нас и проверяет данные. Оба варианта потребуют от нас значительного объема работы,
Третий вариант проще, мы можем использовать решение для мониторинга, которое доступно на рынке.
Nagios и Zabbix - это возможные решения, которые легко доступны и позволяют нам расширить нашу инфраструктуру мониторинга, чтобы включить столько серверов, сколько мы захотим.
Nagios Nagios - это инструмент мониторинга инфраструктуры, созданный одноименной компанией. Версия этого инструмента с открытым исходным кодом называется Nagios core, а коммерческая версия называется Nagios XI. Сайт Nagios
Этот инструмент позволяет нам следить за нашими серверами и видеть, достаточно ли они используются или есть какие-либо задачи, требующие решения.
По сути, мониторинг позволяет нам достичь этих двух целей, проверить состояние наших серверов и сервисов и определить здоровье нашей инфраструктуры. Он также дает нам возможность увидеть всю инфраструктуру с высоты 40 000 метров, чтобы увидеть, работают ли наши серверы, правильно ли работают приложения, доступны или нет веб-серверы.
Он сообщит нам, что объем нашего диска увеличивался на 10 процентов в течение последних 10 недель на определенном сервере, что он будет полностью исчерпан в течение следующих четырех или пяти дней, и мы не сможем ответить в ближайшее время. Он предупредит нас, когда ваш диск или сервер находится в критическом состоянии, чтобы мы могли принять соответствующие меры, чтобы избежать возможных сбоев.
В этом случае мы можем освободить некоторое дисковое пространство и гарантировать, что наши серверы не выйдут из строя и наши пользователи не пострадают.
Сложный вопрос для большинства инженеров по мониторингу - что мы отслеживаем, а что нет?
Каждая система имеет ряд ресурсов, за какими из них мы должны внимательно следить, а на какие можем закрыть глаза, например, нужно ли следить за использованием процессора, ответ \u0026ldquo;да\u0026rdquo; очевиден, тем не менее, это все равно решение, которое нужно принять, нужно ли следить за количеством открытых портов в системе, мы можем следить или не следить в зависимости от ситуации, если это сервер общего назначения, то, вероятно, не нужно, но если это веб-сервер, то, вероятно, нужно.
Постоянный мониторинг Мониторинг не является чем-то новым, и даже непрерывный мониторинг был идеалом, который многие предприятия приняли в течение многих лет.
Есть три ключевых области, на которых необходимо сосредоточиться, когда речь заходит о мониторинге.
Мониторинг инфраструктуры Мониторинг приложений Мониторинг сети Важно отметить, что существует множество доступных инструментов, мы упомянули две общие системы и инструменты в этой сессии, но их очень много. Реальная польза от решения для мониторинга появляется тогда, когда вы действительно потратили время на то, чтобы убедиться, что вы ответили на вопрос, что мы должны отслеживать, а что нет?
Мы можем включить решение мониторинга в любой из наших платформ, и оно начнет собирать информацию, но если этой информации просто слишком много, вам будет трудно извлечь пользу из этого решения, вам придется потратить время на настройку.
На следующем занятии мы попробуем использовать инструмент мониторинга и посмотрим, что мы можем начать отслеживать.
Ресурсы The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day77/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day78/":{title:"78. Hands-On Monitoring Tools",tags:["devops"],content:`Инструменты мониторинга своими руками На последнем занятии я говорил об общей картине мониторинга и рассмотрел Nagios, для этого было две причины. Во-первых, это программное обеспечение, о котором я много слышал на протяжении многих лет, поэтому хотел узнать немного больше о его возможностях.
Сегодня я буду изучать Prometheus, я все больше и больше вижу Prometheus в ландшафте Cloud-Native, но его также можно использовать для присмотра за физическими ресурсами вне Kubernetes и тому подобного.
Prometheus - мониторинг практически всего Прежде всего, Prometheus - это Open-Source, который может помочь вам контролировать контейнеры и системы на базе микросервисов, а также физические, виртуальные и другие сервисы. За Prometheus стоит большое сообщество.
Prometheus имеет большой набор интеграций и экспортеров Ключевым моментом является экспорт существующих метрик в метрики Prometheus. Кроме того, он также поддерживает несколько языков программирования.
Подход Pull - Если вы работаете с тысячами микросервисов или систем и сервисов, то метод push - это метод, при котором сервис, как правило, обращается к системе мониторинга. При этом возникают некоторые проблемы, связанные с переполнением сети, высокой производительностью процессора и единой точкой отказа. Метод Pull дает нам гораздо лучший опыт, когда Prometheus будет получать данные из конечной точки метрики на каждом сервисе.
И снова мы видим YAML для конфигурации Prometheus.
Позже вы увидите, как это выглядит при развертывании в Kubernetes, в частности, у нас есть PushGateway, который получает наши метрики от наших заданий/экспортеров.
У нас есть AlertManager, который рассылает оповещения, и именно здесь мы можем интегрироваться во внешние сервисы, такие как электронная почта, slack и другие инструменты.
Затем у нас есть сервер Prometheus, который управляет получением этих метрик из PushGateway, а затем отправляет эти оповещения в AlertManager. Сервер Prometheus также хранит данные на локальном диске. Хотя можно использовать решения для удаленного хранения данных.
У нас также есть PromQL - язык, используемый для взаимодействия с метриками, который можно увидеть позже в веб-интерфейсе Prometheus, но позже в этом разделе вы также увидите, как он используется в инструментах визуализации данных, таких как Grafana.
Способы развертывания Prometheus Существуют различные способы установки Prometheus, Download Section Также доступны образы Docker.
docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus.
Но мы сосредоточим наши усилия на развертывании в Kubernetes. У которого также есть несколько вариантов.
Создание конфигурационных YAML-файлов Использование оператора (менеджер всех компонентов prometheus) Использование диаграммы helm для развертывания оператора Развертывание в Kubernetes Для этой быстрой и простой установки мы снова будем использовать наш локальный кластер minikube. Как и в предыдущих случаях с minikube, мы будем использовать helm для развертывания диаграммы Prometheus helm.
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts.
Как видно из вышеприведенного, мы также выполнили обновление репо helm, теперь мы готовы развернуть Prometheus в нашей среде minikube с помощью команды helm install stable prometheus-community/prometheus.
Через пару минут вы увидите, что появилось несколько новых подкастов, для этого демо я развернул их в пространство имен по умолчанию, обычно я бы развернул их в собственное пространство имен.
После запуска всех подсистем мы также можем посмотреть на все развернутые аспекты Prometheus.
Теперь, чтобы получить доступ к пользовательскому интерфейсу сервера Prometheus, мы можем использовать следующую команду для проброса портов.
export POD_NAME=$(kubectl get pods --namespace default -l \u0026quot;app=prometheus,component=server\u0026quot; -o jsonpath=\u0026quot;{.items[0].metadata.name}\u0026quot;)kubectl --namespace default port-forward $POD_NAME 9090Когда мы впервые открываем наш браузер на http://localhost:9090, мы видим следующий очень пустой экран.
Поскольку мы развернули наш кластер Kubernetes, мы будем автоматически получать метрики из нашего Kubernetes API, поэтому мы можем использовать некоторые PromQL, чтобы убедиться, что мы получаем метрики container_cpu_usage_seconds_total.
Коротко об изучении PromQL и применении его на практике. Это очень похоже на то, о чем я говорил ранее: получение метрик - это здорово, как и мониторинг, но вы должны знать, что вы отслеживаете и почему, и что вы не отслеживаете и почему!
Я хочу вернуться к Prometheus, но пока я думаю, что нам нужно подумать об управлении журналами и визуализации данных, чтобы позже вернуться к Prometheus.
Ресурсы The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day78/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day79/":{title:"79. Log Management",tags:["devops"],content:`Введение: Управление журналами В продолжение проблем и решений в области мониторинга инфраструктуры, управление журналами - это еще один пазл в общей картине наблюдаемости.
Управление и агрегация журналов Давайте поговорим о двух основных концепциях, первая из которых - агрегация журналов, это способ сбора и маркировки журналов приложений от множества различных служб в единую приборную панель, по которой можно легко осуществлять поиск.
Одной из первых систем, которые должны быть построены в системе управления производительностью приложений, является агрегация журналов. Управление производительностью приложений - это та часть жизненного цикла devops, когда все было создано и развернуто, и вам нужно убедиться, что они постоянно работают, что им выделено достаточно ресурсов и что ошибки не показываются пользователям. В большинстве производственных развертываний существует множество связанных событий, которые передают журналы по сервисам, в google один поиск может попасть в десять различных сервисов, прежде чем будет возвращен пользователю, если вы получили неожиданные результаты поиска, это может означать логическую проблему в любом из десяти сервисов, и агрегация журналов помогает таким компаниям, как google, диагностировать проблемы в производстве.
В этом суть хорошей платформы для агрегации журналов, которая эффективно собирает журналы отовсюду, откуда они исходят, и делает их легко доступными для поиска в случае повторного возникновения неисправности.
Пример приложения Наш пример приложения - это веб-приложение, у нас есть типичный фронт-энд и бэк-энд, хранящий наши важные данные в базе данных MongoDB.
Если бы пользователь сказал нам, что страница стала белой и вывела сообщение об ошибке, мы бы с трудом диагностировали проблему с помощью нашего текущего стека. Пользователь должен вручную отправить нам ошибку, а мы должны сопоставить ее с соответствующими журналами в трех других сервисах.
ELK Давайте посмотрим на ELK, популярный стек агрегации логов с открытым исходным кодом, названный в честь его трех компонентов elasticsearch, logstash и kibana, если мы установим его в той же среде, что и наше приложение.
Веб-приложение подключается к фронтенду, который затем подключается к бэкенду, бэкенд отправляет журналы в logstash, а затем то, как работают эти три компонента.
Компоненты elk Elasticsearch, logstash и Kibana заключается в том, что все сервисы отправляют журналы в logstash, logstash принимает эти журналы, которые являются текстом, испускаемым приложением. Например, веб-приложение, когда вы посещаете веб-страницу, может зарегистрировать доступ посетителя к этой странице в это время, и это пример сообщения журнала, которое будет отправлено в logstash.
Затем Logstash извлекает из них информацию, так что для этого сообщения пользователь сделал что-то, в время. Он извлечет время, извлечет сообщение, извлечет пользователя и включит все это в качестве тегов, так что сообщение будет объектом тегов и сообщений, так что вы можете легко искать по ним, вы можете найти все запросы, сделанные определенным пользователем, но logstash не хранит вещи самостоятельно, он хранит вещи в elasticsearch, который является эффективной базой данных для запроса текста, и elasticsearch раскрывает результаты как Kibana, а Kibana - это веб-сервер, который подключается к elasticsearch и позволяет администраторам, таким как devops или другим людям в вашей команде, дежурному инженеру просматривать журналы в производстве при возникновении серьезных неполадок. Вы, как администратор, подключаетесь к Kibana, Kibana запрашивает elasticsearch на предмет журналов, соответствующих тому, что вы хотите.
Вы можете сказать: \u0026ldquo;Эй, Kibana, в строке поиска я хочу найти ошибки\u0026rdquo;, и Kibana скажет elasticsearch найти сообщения, которые содержат строку error, а затем elasticsearch вернет результаты, которые были заполнены logstash. Logstash получил бы эти результаты от всех других служб.
как бы мы использовали elk для диагностики производственной проблемы Пользователь говорит, что я увидел код ошибки один два три четыре пять шесть семь, когда я попытался сделать это с помощью настройки elk, мы должны зайти в kibana, ввести один два три четыре пять шесть семь в строке поиска, нажать enter, а затем это покажет нам журналы, которые соответствуют этому, и один из журналов может сказать внутреннюю ошибку сервера, возвращающую один два три четыре пять шесть семь, и мы увидим, что служба, которая выдала этот журнал. и мы увидим, что служба, которая выдала этот журнал, была backend, и мы увидим, в какое время был выдан этот журнал, поэтому мы можем перейти ко времени в этом журнале и посмотреть на сообщения выше и ниже него в backend, и тогда мы сможем увидеть лучшую картину того, что произошло для запроса пользователя, и мы сможем повторить этот процесс, переходя к другим службам, пока не найдем, что на самом деле вызвало проблему у пользователя.
Безопасность и доступ к журналам Важной частью головоломки является обеспечение того, чтобы журналы были видны только администраторам (или пользователям и группам, которым абсолютно необходим доступ). Журналы могут содержать конфиденциальную информацию, такую как токены, поэтому важно, чтобы только аутентифицированные пользователи могли получить к ним доступ. Вы не захотите выставлять Kibana в интернет без какого-либо способа аутентификации.
Примеры инструментов управления журналами Примерами платформ для управления журналами являются
Elasticsearch Logstash Kibana Fluentd - популярный вариант с открытым исходным кодом Datadog - хостинговое предложение, обычно используется на крупных предприятиях, LogDNA - хостируемое предложение Splunk Облачные провайдеры также предоставляют протоколирование, например, AWS CloudWatch Logs, Microsoft Azure Monitor и Google Cloud Logging.
Управление журналами является ключевым аспектом общей наблюдаемости ваших приложений и среды инфраструктур для диагностики проблем в производстве. Относительно просто установить готовое решение, такое как ELK или CloudWatch, и это значительно упрощает диагностику и устранение проблем в производстве.
Ресурсы The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples Log Management for DevOps | Manage application, server, and cloud logs with Site24x7 Log Management what DevOps need to know What is ELK Stack? Fluentd simply explained `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day79/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day80/":{title:"80. ELK Stack",tags:["devops"],content:`ELK Stack На этом занятии мы немного подробнее рассмотрим некоторые из упомянутых нами опций.
ELK Stack ELK Stack - это комбинация трех отдельных инструментов:
Elasticsearch - это распределенный, бесплатный и открытый поисковый и аналитический механизм для всех типов данных, включая текстовые, числовые, геопространственные, структурированные и неструктурированные.
Logstash - свободный и открытый конвейер обработки данных на стороне сервера, который получает данные из множества источников, преобразует их, а затем отправляет в ваш любимый \u0026ldquo;тайник\u0026rdquo;.
Kibana - это бесплатный и открытый пользовательский интерфейс, позволяющий визуализировать данные Elasticsearch и перемещаться по стеку Elastic Stack. Делайте все, что угодно: от отслеживания загрузки запросов до понимания того, как запросы проходят через ваши приложения.
Стек ELK позволяет нам надежно и безопасно получать данные из любого источника, в любом формате, затем искать, анализировать и визуализировать их в режиме реального времени.
В дополнение к вышеперечисленным компонентам вы также можете увидеть Beats - легковесные агенты, которые устанавливаются на пограничных узлах для сбора различных типов данных для передачи в стек.
Журналы: Определяются журналы сервера, которые необходимо проанализировать.
Logstash: Собирает журналы и данные о событиях. Он даже анализирует и преобразует данные.
ElasticSearch: Преобразованные данные из Logstash хранятся, ищутся и индексируются.
Kibana использует БД Elasticsearch для изучения, визуализации и обмена данными
Изображение взято с сайта Guru99
Хороший ресурс, объясняющий это The Complete Guide to the ELK Stack
С добавлением битов стек ELK теперь также известен как Elastic Stack.
Для практического скрипта существует множество мест, где можно развернуть Elastic Stack, но мы будем использовать docker compose для локального развертывания в нашей системе.
Start the Elastic Stack with Docker Compose
Оригинальные файлы и руководство, которые я использовал, вы найдете здесь deviantony/docker-elk
Теперь мы можем запустить docker-compose up -d, при первом запуске потребуется вытащить изображения.
Если вы следите за этим репозиторием или за тем, который использовал я, у вас будет пароль \u0026ldquo;changeme\u0026rdquo; или в моем репозитории пароль \u0026ldquo;90DaysOfDevOps\u0026rdquo;. Имя пользователя - \u0026ldquo;elastic\u0026rdquo;.
Через несколько минут мы можем перейти на сайт http://localhost:5601/, который является нашим сервером Kibana / Docker-контейнером.
Ваш начальный главный экран будет выглядеть примерно так.
В разделе \u0026ldquo;Get started by adding integrations\u0026rdquo; есть пункт \u0026ldquo;try sample data\u0026rdquo;, нажмите на него, и мы сможем добавить одну из показанных ниже интеграций.
Я собираюсь выбрать \u0026ldquo;Sample web logs\u0026rdquo;, но это действительно для того, чтобы получить представление о том, какие наборы данных можно получить в стеке ELK.
Когда вы выбрали \u0026ldquo;Добавить данные\u0026rdquo;, требуется некоторое время, чтобы заполнить некоторые из этих данных, а затем у вас появляется опция \u0026ldquo;Просмотр данных\u0026rdquo; и список доступных способов просмотра этих данных в выпадающем списке.
Как указано в представлении приборной панели:
Образцы данных журналов
Эта приборная панель содержит образцы данных, с которыми вы можете поиграть. Вы можете просматривать их, искать и взаимодействовать с визуализациями. Для получения дополнительной информации о Kibana ознакомьтесь с нашей документацией.
Здесь используется Kibana для визуализации данных, которые были добавлены в ElasticSearch через Logstash. Это не единственный вариант, но я лично хотел развернуть и посмотреть на это.
В какой-то момент мы рассмотрим Grafana, и вы увидите некоторые сходства в визуализации данных между ними, вы также видели Prometheus.
Ключевой момент, который я уловил между Elastic Stack и Prometheus + Grafana, заключается в том, что Elastic Stack или ELK Stack сосредоточен на журналах, а Prometheus - на метриках.
Я читал эту статью от MetricFire Prometheus vs. ELK, чтобы лучше понять различные предложения.
Ресурсы Understanding Logging: Containers \u0026amp; Microservices The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples Log Management for DevOps | Manage application, server, and cloud logs with Site24x7 Log Management what DevOps need to know What is ELK Stack? [Fluentd simply explained](https://www.youtube.com/watch?v=5ofsNyHZwWE\u0026amp;t=14s `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day80/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day81/":{title:"81. Fluentd и FluentBit",tags:["devops"],content:`Fluentd и FluentBit Еще одним коллектором данных, который я хотел изучить в рамках раздела о наблюдаемости, был Fluentd. Это унифицированный уровень протоколирования с открытым исходным кодом.
Fluentd имеет четыре ключевые особенности, которые делают его подходящим для создания чистых, надежных конвейеров протоколирования:
Унифицированное протоколирование с JSON: Fluentd старается структурировать данные в виде JSON, насколько это возможно. Это позволяет Fluentd унифицировать все аспекты обработки данных журналов: сбор, фильтрацию, буферизацию и вывод журналов из нескольких источников и мест назначения. Последующая обработка данных намного проще с JSON, так как он имеет достаточную структуру, чтобы быть доступным без принуждения к жестким схемам.
Подключаемая архитектура: Fluentd имеет гибкую систему плагинов, которая позволяет сообществу расширять его функциональность. Более 300 плагинов, созданных сообществом, соединяют десятки источников данных с десятками выходных данных, манипулируя данными по мере необходимости. Используя плагины, вы можете сразу же повысить эффективность использования ваших журналов.
Требуется минимум ресурсов: Коллектор данных должен быть легким, чтобы его можно было легко запустить на загруженной машине. Fluentd написан на комбинации C и Ruby и требует минимальных системных ресурсов. Ванильный экземпляр работает на 30-40 МБ памяти и может обрабатывать 13 000 событий/секунду/ядро.
Встроенная надежность: Потеря данных никогда не должна произойти. Fluentd поддерживает буферизацию на основе памяти и файлов для предотвращения потери данных между узлами. Fluentd также поддерживает надежное восстановление после отказа и может быть настроен на высокую доступность.
Установка Fluentd
Как приложения записывают данные в журнал? Запись в файлы. Файлы .log (трудно анализировать без инструмента и в масштабе) Вести журнал непосредственно в базу данных (каждое приложение должно быть настроено на правильный формат) Сторонние приложения (NodeJS, NGINX, PostgreSQL). Вот почему нам нужен единый уровень логирования.
FluentD позволяет использовать 3 типа данных, показанных выше, и дает нам возможность собирать, обрабатывать и отправлять их по назначению, это может быть отправка логов в базы данных Elastic, MongoDB, Kafka, например.
Любые данные, любой источник данных может быть отправлен в FluentD, и эти данные могут быть отправлены в любое место назначения. FluentD не привязан к какому-либо конкретному источнику или месту назначения.
Изучая Fluentd, я постоянно натыкался на Fluent bit как еще один вариант, и похоже, что если вы хотите развернуть инструмент протоколирования в среде Kubernetes, то Fluent bit даст вам такую возможность, хотя Fluentd также может быть развернут как на контейнерах, так и на серверах.
Fluentd \u0026amp; Fluent Bit
Fluentd и Fluentbit будут использовать входные плагины для преобразования данных в формат Fluent Bit, затем у нас есть выходные плагины для любой цели вывода, например, elasticsearch.
Мы также можем использовать теги и соответствия между конфигурациями.
Я не вижу веских причин для использования Fluentd, и кажется, что Fluent Bit - лучший способ начать работу. Хотя в некоторых архитектурах они могут использоваться вместе.
Fluent Bit в Kubernetes Fluent Bit в Kubernetes развертывается как DaemonSet, что означает, что он будет запущен на каждом узле кластера. Каждая капсула Fluent Bit на каждом узле будет читать каждый контейнер на этом узле и собирать все доступные журналы. Он также будет собирать метаданные с сервера Kubernetes API Server.
Аннотации Kubernetes можно использовать в конфигурационном YAML наших приложений.
Прежде всего, мы можем развернуть приложение из репозитория fluent helm. helm repo add fluent https://fluent.github.io/helm-charts, а затем установить с помощью команды helm install fluent-bit fluent/fluent-bit.
В моем кластере я также запускаю prometheus в моем пространстве имен по умолчанию (в тестовых целях), нам нужно убедиться, что наш fluent-bit pod запущен и работает. Мы можем сделать это с помощью команды kubectl get all | grep fluent, которая покажет нам наш запущенный pod, сервис и набор демонов, о которых мы говорили ранее.
Чтобы Fluentbit знал, откуда получать журналы, у нас есть конфигурационный файл, в этом развертывании Fluentbit на Kubernetes у нас есть configmap, который напоминает конфигурационный файл.
Эта ConfigMap будет выглядеть примерно так:
Name: fluent-bitNamespace: defaultLabels: app.kubernetes.io/instance=fluent-bitapp.kubernetes.io/managed-by=Helmapp.kubernetes.io/name=fluent-bitapp.kubernetes.io/version=1.8.14helm.sh/chart=fluent-bit-0.19.21Annotations: meta.helm.sh/release-name: fluent-bitmeta.helm.sh/release-namespace: defaultData====custom_parsers.conf:----[PARSER]Name docker_no_timeFormat jsonTime_Keep OffTime_Key timeTime_Format %Y-%m-%dT%H:%M:%S.%Lfluent-bit.conf:----[SERVICE]Daemon OffFlush 1Log_Level infoParsers_File parsers.confParsers_File custom_parsers.confHTTP_Server OnHTTP_Listen 0.0.0.0HTTP_Port 2020Health_Check On[INPUT]Name tailPath /var/log/containers/*.logmultiline.parser docker, criTag kube.*Mem_Buf_Limit 5MBSkip_Long_Lines On[INPUT]Name systemdTag host.*Systemd_Filter _SYSTEMD_UNIT=kubelet.serviceRead_From_Tail On[FILTER]Name kubernetesMatch kube.*Merge_Log OnKeep_Log OffK8S-Logging.Parser OnK8S-Logging.Exclude On[OUTPUT]Name esMatch kube.*Host elasticsearch-masterLogstash_Format OnRetry_Limit False[OUTPUT]Name esMatch host.*Host elasticsearch-masterLogstash_Format OnLogstash_Prefix nodeRetry_Limit FalseEvents: \u0026lt;none\u0026gt;Теперь мы можем перенаправить наш pod на наш localhost, чтобы убедиться, что у нас есть соединение. Сначала узнайте имя вашего pod с помощью kubectl get pods | grep fluent и затем используйте kubectl port-forward fluent-bit-8kvl4 2020:2020 откройте веб-браузер на http://localhost:2020/.
Я также нашел эту замечательную статью на Medium, в которой рассказывается о Fluent Bit.
Ресурсы Understanding Logging: Containers \u0026amp; Microservices The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples Log Management for DevOps | Manage application, server, and cloud logs with Site24x7 Log Management what DevOps need to know What is ELK Stack? Fluentd simply explained Fluent Bit explained | Fluent Bit vs Fluentd ) `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day81/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day82/":{title:"82. EFK Stack",tags:["devops"],content:`EFK Stack В предыдущем разделе мы говорили о ELK Stack, который использует Logstash в качестве сборщика логов в стеке, в EFK Stack мы меняем его на FluentD или FluentBit.
Наша задача в этом разделе - отслеживать журналы Kubernetes с помощью EFK.
Обзор EFK Мы развернем следующее в нашем кластере Kubernetes.
Стек EFK представляет собой набор из 3 программ, объединенных вместе, включая:
Elasticsearch : NoSQL база данных используется для хранения данных и предоставляет интерфейс для поиска и журнал запросов.
Fluentd : Fluentd - это сборщик данных с открытым исходным кодом для унифицированного уровня логирования. Fluentd позволяет унифицировать сбор и потребление данных для лучшего использования и понимания данных.
Kibana : Интерфейс для управления и статистики журналов. Отвечает за чтение информации из elasticsearch .
Развертывание EFK на Minikube Мы будем использовать наш надежный кластер minikube для развертывания нашего стека EFK. Давайте запустим кластер с помощью minikube start на нашей системе. Я использую ОС Windows с включенным WSL2.
Я создал efk-stack.yaml, который содержит все необходимое для развертывания стека EFK в нашем кластере, используя команду kubectl create -f efk-stack.yaml мы видим, что все развернуто.
В зависимости от вашей системы и если вы уже выполняли эту процедуру и получили изображения, теперь вам нужно посмотреть, как стручки переходят в состояние готовности, прежде чем мы сможем двигаться дальше, вы можете проверить прогресс с помощью следующей команды. kubectl get pods -n kube-logging -w Это может занять несколько минут.
Приведенная выше команда позволяет нам следить за ситуацией, но я люблю уточнять, все ли в порядке, выполняя следующую команду kubectl get pods -n kube-logging, чтобы убедиться, что все pods теперь работают.
После того, как мы запустили все наши pods, и на этом этапе мы должны увидеть
3 стручка, связанные с ElasticSearch 1 стручок, связанный с Fluentd 1 стручок, связанный с Kibana Мы также можем использовать kubectl get all -n kube-logging, чтобы показать все в нашем пространстве имен, fluentd, как объяснялось ранее, развернут как набор демонов, kibana как развертывание и ElasticSearch как набор состояний.
Теперь все наши pods работают, и мы можем ввести в новом терминале команду port-forward, чтобы мы могли получить доступ к нашей приборной панели kibana. Обратите внимание, что имя вашего pod будет отличаться от команды, которую мы видим здесь. kubectl port-forward kibana-84cf7f59c-v2l8v 5601:5601 -n kube-logging.
Теперь мы можем открыть браузер и перейти по этому адресу, http://localhost:5601 вас встретит либо экран, который вы видите ниже, либо вы можете увидеть экран с примерами данных, либо продолжить и настроить самостоятельно. В любом случае и непременно посмотрите на эти тестовые данные, это то, что мы рассмотрели при изучении стека ELK в предыдущей сессии.
Далее нам нужно перейти на вкладку \u0026ldquo;discover\u0026rdquo; в левом меню и добавить \u0026ldquo;*\u0026rdquo; к нашему шаблону индекса. Перейдите к следующему шагу, нажав кнопку \u0026ldquo;Следующий шаг\u0026rdquo;.
На шаге 2 из 2 мы будем использовать опцию @timestamp из выпадающего списка, так как это позволит отфильтровать наши данные по времени. Когда вы нажмете кнопку создать шаблон, это может занять несколько секунд.
Если через несколько секунд мы вернемся на вкладку \u0026ldquo;discover\u0026rdquo;, вы должны увидеть данные, поступающие с вашего кластера Kubernetes.
Теперь, когда у нас установлен и работает стек EFK и мы собираем журналы с нашего кластера Kubernetes через Fluentd, мы можем взглянуть на другие источники, которые мы можем выбрать. Если вы перейдете на главный экран, нажав на логотип Kibana в левом верхнем углу, вас встретит та же страница, которую мы видели при первом входе в систему.
У нас есть возможность добавить APM, данные журнала, метрические данные и события безопасности из других плагинов или источников.
Если мы выберем \u0026ldquo;Добавить данные журнала\u0026rdquo;, то увидим ниже, что у нас есть большой выбор, откуда мы хотим получать наши журналы, вы можете увидеть, что там упоминается Logstash, который является частью стека ELK.
Под данными метрик вы увидите, что можно добавить источники для Prometheus и многих других сервисов. Переведено с помощью www.DeepL.com/Translator (бесплатная версия)
APM (Мониторинг производительности приложений) Также есть возможность собрать APM (мониторинг производительности приложений), который собирает подробные показатели производительности и ошибки изнутри вашего приложения. Он позволяет отслеживать производительность тысяч приложений в режиме реального времени.
Я не буду здесь углубляться в APM, но вы можете узнать больше на сайте Elastic.
Ресурсы Understanding Logging: Containers \u0026amp; Microservices The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples Log Management for DevOps | Manage application, server, and cloud logs with Site24x7 Log Management what DevOps need to know What is ELK Stack? Fluentd simply explained See you on Day 83
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day82/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day83/":{title:"83. Визуализация данных - Grafana",tags:["devops"],content:`Визуализация данных - Grafana Мы много говорили о Kibana в этом разделе, посвященном Observability. Но мы также должны уделить некоторое время Grafana. Но это не одно и то же, и они не полностью конкурируют друг с другом.
Основной функцией Kibana является запрос и анализ данных. Используя различные методы, пользователи могут искать в данных, проиндексированных в Elasticsearch, определенные события или строки в данных для анализа и диагностики первопричин. На основе этих запросов пользователи могут использовать функции визуализации Kibana, которые позволяют визуализировать данные различными способами, используя графики, таблицы, географические карты и другие виды визуализации.
Grafana фактически началась как форк Kibana, целью Grafana была поддержка метрик и мониторинга, которые в то время Kibana не предоставляла.
Grafana - это бесплатный инструмент визуализации данных с открытым исходным кодом. Обычно мы видим Prometheus и Grafana вместе в полевых условиях, но мы также можем увидеть Grafana вместе с Elasticsearch и Graphite.
Ключевое различие между этими двумя инструментами - это логирование и мониторинг. В начале раздела мы рассмотрели мониторинг с помощью Nagios, затем Prometheus и перешли к логированию, где мы рассмотрели стеки ELK и EFK.
Grafana предназначена для анализа и визуализации таких показателей, как использование системного процессора, памяти, дисков и ввода-вывода. Платформа не позволяет выполнять полнотекстовые запросы данных. Kibana работает поверх Elasticsearch и используется в основном для анализа сообщений журнала.
Как мы уже выяснили, Kibana довольно проста в развертывании, а также в выборе места установки, то же самое можно сказать и о Grafana.
Оба поддерживают установку на Linux, Mac, Windows, Docker или сборку из исходников.
Несомненно, есть и другие, но Grafana - это инструмент, который, по моим наблюдениям, охватывает виртуальные, облачные и облачно-нативные платформы, поэтому я хотел рассказать о нем в этом разделе.
Оператор Prometheus + развертывание Grafana Мы уже рассказывали о Prometheus в этом разделе, но поскольку мы так часто видим эти пары, я хотел создать среду, которая позволила бы нам хотя бы увидеть, какие метрики мы могли бы отображать в визуализации. Мы знаем, что мониторинг наших сред очень важен, но просмотр этих метрик в Prometheus или любом другом метрическом инструменте будет громоздким и не будет масштабироваться. Именно здесь на помощь приходит Grafana, которая предоставляет нам интерактивную визуализацию этих метрик, собранных и сохраненных в базе данных Prometheus.
С помощью этой визуализации мы можем создавать пользовательские графики, диаграммы и оповещения для нашей среды. В этом руководстве мы будем использовать наш кластер minikube.
Для начала мы клонируем его в нашу локальную систему. Используя git clone https://github.com/prometheus-operator/kube-prometheus.git и cd kube-prometheus.
Первая задача - создать наше пространство имен в кластере minikube kubectl create -f manifests/setup, если вы не следили за предыдущими разделами, мы можем использовать minikube start для создания нового кластера.
Далее мы собираемся развернуть все необходимое для нашего демо с помощью команды kubectl create -f manifests/, как вы можете видеть, это развернет множество различных ресурсов в нашем кластере.
Затем нам нужно подождать, пока наши стручки поднимутся, и, находясь в запущенном состоянии, мы можем использовать команду kubectl get pods -n monitoring -w, чтобы следить за стручками.
Когда все запущено, мы можем проверить, что все pods находятся в рабочем и здоровом состоянии, используя команду kubectl get pods -n monitoring.
При развертывании мы развернули ряд сервисов, которые мы будем использовать позже в демо, вы можете проверить их с помощью команды kubectl get svc -n monitoring.
И, наконец, давайте проверим все ресурсы, развернутые в нашем новом пространстве имен мониторинга, используя команду kubectl get all -n monitoring.
Открыв новый терминал, мы готовы получить доступ к нашему инструменту Grafana и начать собирать и визуализировать некоторые из наших метрик, команда для использования - kubectl --namespace monitoring port-forward svc/grafana 3000.
Откройте браузер и перейдите по адресу http://localhost:3000, вам будет предложено ввести имя пользователя и пароль.
По умолчанию имя пользователя и пароль для доступа следующие
Имя пользователя: admin Пароль: adminОднако при первом входе в систему вам будет предложено ввести новый пароль. На начальном экране или домашней странице вы увидите несколько областей для изучения, а также некоторые полезные ресурсы для ознакомления с Grafana и ее возможностями. Обратите внимание на виджеты \u0026ldquo;Добавить свой первый источник данных\u0026rdquo; и \u0026ldquo;Создать свою первую приборную панель\u0026rdquo;, мы будем использовать их позже.
Вы увидите, что источник данных prometheus уже добавлен в источники данных Grafana, однако, поскольку мы используем minikube, нам нужно также перенаправить prometheus, чтобы он был доступен на нашем localhost, открыв новый терминал, мы можем выполнить следующую команду. kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 если на главной странице Grafana мы теперь заходим в виджет \u0026ldquo;Add your first data source\u0026rdquo; и отсюда выбираем Prometheus.
Для нашего нового источника данных мы можем использовать адрес http://localhost:9090, и нам также нужно будет изменить выпадающий список на браузер, как показано ниже.
Внизу страницы мы можем нажать кнопку сохранить и протестировать. Это должно дать нам результат, который вы видите ниже, если проброс порта для prometheus работает.
Вернитесь на главную страницу и найдите опцию \u0026ldquo;Create your first dashboard\u0026rdquo;, выберите \u0026ldquo;Add a new panel\u0026rdquo;.
Ниже вы увидите, что мы уже собираем данные из нашего источника данных Grafana, но мы хотели бы собирать метрики из нашего источника данных Prometheus, выберите выпадающий список источников данных и выберите наш недавно созданный \u0026ldquo;Prometheus-1\u0026rdquo;
Если затем выбрать браузер Metrics, то появится длинный список метрик, собираемых из Prometheus, связанных с нашим кластером minikube.
Для целей демонстрации я собираюсь найти метрику, которая дает нам некоторые данные о наших системных ресурсах, cluster:node_cpu:ratio{} дает нам некоторые подробности об узлах в нашем кластере и доказывает, что эта интеграция работает.
Если вас устраивает такая визуализация, нажмите кнопку \u0026ldquo;Применить\u0026rdquo; в правом верхнем углу, и вы добавите этот график на свою приборную панель. Разумеется, вы можете добавлять дополнительные графики и другие диаграммы, чтобы обеспечить нужную вам визуализацию.
Однако мы можем воспользоваться тысячами ранее созданных приборных панелей, которые мы можем использовать, чтобы не изобретать велосипед.
Если мы выполним поиск по Kubernetes, то увидим длинный список готовых приборных панелей, из которых мы можем выбирать.
Мы выбрали приборную панель Kubernetes API Server и изменили источник данных, чтобы соответствовать нашему недавно добавленному источнику данных Prometheus-1, и мы видим некоторые метрики, отображаемые как показано ниже.
Оповещение Вы также можете использовать развернутый нами alertmanager для отправки оповещений в slack или другие интеграции, для этого вам нужно перенести сервис alertmanager, используя следующие данные.
kubectl --namespace monitoring port-forward svc/alertmanager-main 9093 http://localhost:9093
На этом мы завершаем наш раздел о наблюдаемости. Лично я считаю, что этот раздел подчеркнул, насколько широка эта тема, но в равной степени, насколько она важна для наших ролей, и что будь то метрика, логирование или трассировка, вам необходимо иметь хорошее представление о том, что происходит в наших широких средах в будущем, особенно когда они могут так сильно измениться благодаря автоматизации, которую мы уже рассмотрели в других разделах.
Далее мы рассмотрим управление данными и то, как принципы DevOps также необходимо учитывать, когда речь идет об управлении данными.
Ресурсы Understanding Logging: Containers \u0026amp; Microservices The Importance of Monitoring in DevOps Understanding Continuous Monitoring in DevOps? DevOps Monitoring Tools Top 5 - DevOps Monitoring Tools How Prometheus Monitoring works Introduction to Prometheus monitoring Promql cheat sheet with examples Log Management for DevOps | Manage application, server, and cloud logs with Site24x7 Log Management what DevOps need to know What is ELK Stack? Fluentd simply explained `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day83/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day84/":{title:"84. Управление данными",tags:["devops"],content:`Введение: Управление данными Управление данными - это далеко не новая стена, на которую нужно карабкаться, хотя мы знаем, что данные стали более важными, чем несколько лет назад. Ценные и постоянно меняющиеся, они также могут стать огромным кошмаром, когда мы говорим об автоматизации и непрерывной интеграции, тестировании и развертывании частых выпусков программного обеспечения. Вводим постоянные данные и базовые службы данных, которые часто являются главным виновником, когда что-то идет не так.
Но прежде чем я перейду к управлению данными в облаке, нам нужно подняться на уровень выше. В ходе этой задачи мы затронули множество различных платформ. Будь то физические, виртуальные, облачные и Cloud-Native, включая Kubernetes, ни одна из этих платформ не обеспечивает отсутствие требований к управлению данными.
Каким бы ни был наш бизнес, более чем вероятно, что вы найдете базу данных, скрывающуюся где-то в среде, будь то для самой критически важной системы в бизнесе или, по крайней мере, какой-то винтик в цепи хранит эти постоянные данные на каком-то уровне системы.
DevOps и данные Как и в самом начале этой серии статей, где мы говорили о принципах DevOps, для улучшения процесса работы с данными вам необходимо привлечь нужных людей. Это могут быть DBA, но в равной степени это должны быть и люди, которые заботятся о резервном копировании этих сервисов данных.
Во-вторых, нам также необходимо определить различные типы данных, домены, границы, которые мы связываем с нашими данными. Таким образом, данные не будут рассматриваться изолированно среди администраторов баз данных, инженеров по хранению данных или инженеров, специализирующихся на резервном копировании. Таким образом, вся команда может определить наилучший маршрут действий при разработке и размещении приложений для более широкого бизнеса и сосредоточиться на архитектуре данных, а не на том, о чем подумали позже.
Это может охватывать множество различных областей жизненного цикла данных, мы можем говорить о вводе данных, где и как данные будут вводиться в наш сервис или приложение? Как сервис, приложение или пользователи будут получать доступ к этим данным. Но затем нам также необходимо понять, как мы будем защищать данные, и как мы будем защищать эти данные.
Управление данными 101 Управление данными, согласно Data Management Body of Knowledge, - это \u0026ldquo;разработка, выполнение и контроль планов, политик, программ и практик, которые контролируют, защищают, предоставляют и повышают ценность данных и информационных активов\u0026rdquo;.
Данные - самый важный аспект вашего бизнеса - Данные - это только одна часть вашего бизнеса в целом. Я встречал выражение \u0026ldquo;Данные - это жизненная сила нашего бизнеса\u0026rdquo;, и, скорее всего, это абсолютно верно. Это заставило меня задуматься о том, что кровь очень важна для организма, но сама по себе она ничего не значит, нам все еще нужны аспекты организма, чтобы сделать кровь чем-то другим, кроме жидкости.
Качество данных важно как никогда - Мы должны относиться к данным как к бизнес-активу, что означает, что мы должны уделять им должное внимание, чтобы они работали с нашими принципами автоматизации и DevOps.
Своевременный доступ к данным - Ни у кого не хватит терпения не иметь доступа к нужным данным в нужное время для принятия эффективных решений. Данные должны быть доступны в упорядоченном и своевременном виде независимо от формы представления.
Управление данными должно стать помощником DevOps - я уже упоминал о рационализации, мы должны включить требования к управлению данными в наш цикл и обеспечить не только доступность этих данных, но и другие важные политические меры защиты этих точек данных, а также полностью протестированные модели восстановления.
DataOps DataOps и DevOps применяют лучшие практики разработки и эксплуатации технологий для повышения качества, увеличения скорости, снижения угроз безопасности, восхищения клиентов и обеспечения значимой и сложной работы для квалифицированных специалистов. DevOps и DataOps имеют общие цели - ускорить доставку продукта путем автоматизации как можно большего количества этапов процесса. Для DataOps целью является устойчивый конвейер данных и надежные выводы из аналитики данных.
Некоторые из наиболее распространенных областей более высокого уровня, которые фокусируются на DataOps, - это машинное обучение, большие данные и аналитика данных, включая искусственный интеллект.
Управление данными - это управление информацией В этом разделе я не буду углубляться в машинное обучение или искусственный интеллект, а сосредоточусь на защите данных с точки зрения защиты информации. Этот подраздел называется \u0026ldquo;Управление данными - это управление информацией\u0026rdquo;, и мы можем считать, что информация = данные.
Три ключевые области, которые мы должны рассмотреть на этом пути с данными, следующие:
Точность - Убедитесь в том, что производственные данные точны, также нам необходимо убедиться в том, что наши данные в виде резервных копий также работают и протестированы на восстановление, чтобы быть уверенными в том, что в случае сбоя или возникновения причины нам необходимо иметь возможность восстановить работоспособность как можно быстрее.
Последовательность - Если наши службы данных расположены в нескольких местах, то для производства нам необходимо обеспечить последовательность во всех местах расположения данных, чтобы мы получали точные данные. Это также относится к защите данных, когда речь идет о защите этих служб данных, особенно служб данных, нам необходимо обеспечить последовательность на разных уровнях, чтобы убедиться, что мы делаем хорошую чистую копию этих данных для наших резервных копий, реплик и т. д.
Безопасность - контроль доступа, а также просто хранение данных в целом - актуальная тема в настоящее время во всем мире. Убедиться в том, что нужные люди имеют доступ к вашим данным, - первостепенная задача, и это опять же относится к защите данных, где мы должны убедиться, что только необходимый персонал имеет доступ к резервным копиям и возможность восстановления из них, а также клонирования и предоставления других версий бизнес-данных.
Лучшие данные = лучшие решения
Дни управления данными В течение следующих 6 занятий мы рассмотрим базы данных, резервное копирование и восстановление, аварийное восстановление, мобильность приложений с элементами демонстрации и практической работы.
Ресурсы Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day84/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day85/":{title:"85. Службы данных",tags:["devops"],content:`Службы данных Базы данных являются наиболее распространенными службами данных, с которыми мы сталкиваемся в наших средах. На этом занятии я хотел бы рассмотреть некоторые из этих различных типов баз данных и некоторые случаи их использования. Некоторые из них мы уже использовали и видели в ходе решения задачи.
С точки зрения разработки приложений выбор правильной службы данных или базы данных будет иметь огромное значение для производительности и масштабируемости вашего приложения.
https://www.youtube.com/watch?v=W2Z7fbCLSTw
Ключ-значение База данных \u0026ldquo;ключ-значение\u0026rdquo; - это тип нереляционной базы данных, которая использует простой метод \u0026ldquo;ключ-значение\u0026rdquo; для хранения данных. База данных \u0026ldquo;ключ-значение\u0026rdquo; хранит данные в виде набора пар \u0026ldquo;ключ-значение\u0026rdquo;, в которых ключ служит уникальным идентификатором. И ключи, и значения могут быть любыми, от простых объектов до сложных составных объектов. Базы данных \u0026ldquo;ключ-значение\u0026rdquo; хорошо поддаются разделению и позволяют горизонтальное масштабирование в таких масштабах, которые недостижимы для других типов баз данных.
Примером базы данных типа \u0026ldquo;ключ-значение\u0026rdquo; является Redis.
*Redis - это хранилище структур данных в памяти, используемое как распределенная база данных ключей-значений в памяти, кэш и брокер сообщений с возможностью долговечности. Redis поддерживает различные виды абстрактных структур данных, таких как строки, списки, карты, множества, сортированные множества, HyperLogLogs, растровые изображения, потоки и пространственные индексы.
Как вы можете видеть из описания Redis, это означает, что наша база данных работает быстро, но мы ограничены в пространстве в качестве компромисса. Также нет запросов или объединений, что означает, что возможности моделирования данных очень ограничены.
Лучше всего подходит для:
Кэширование Pub/Sub Лидерборды корзины покупок Обычно используется в качестве кэша над другим постоянным слоем данных.
Широкий столбец База данных с широкими колонками - это база данных NoSQL, которая организует хранение данных в гибких колонках, которые могут быть распределены по нескольким серверам или узлам базы данных, используя многомерное отображение для ссылки на данные по столбцам, строкам и временным меткам.
Cassandra - это бесплатная система управления базами данных NoSQL с открытым исходным кодом, распределенная, с широким хранилищем колонок, разработанная для обработки больших объемов данных на множестве серверов, обеспечивающая высокую доступность без единой точки отказа.
Нет схемы, что означает возможность работы с неструктурированными данными, однако это может рассматриваться как преимущество для некоторых рабочих нагрузок.
Лучше всего подходит для:
Временные ряды Исторические записи Высокая запись, низкий уровень чтения Документ База данных документов (также известная как документо-ориентированная база данных или хранилище документов) - это база данных, которая хранит информацию в документах.
MongoDB - это кросс-платформенная кросс-платформенная программа базы данных, ориентированная на документы. Классифицируемая как NoSQL база данных, MongoDB использует JSON-подобные документы с необязательными схемами. MongoDB разработана компанией MongoDB Inc. и лицензирована по лицензии Server Side Public License..
Документальные базы данных NoSQL позволяют предприятиям хранить простые данные без использования сложных кодов SQL. Быстрое хранение без ущерба для надежности.
Лучше всего подходит для:
Большинство приложений Игры Интернет вещей Реляционная Если вы новичок в области баз данных, но знаете о них, то, скорее всего, вы сталкивались с реляционной базой данных.
Реляционная база данных - это цифровая база данных, основанная на реляционной модели данных, предложенной Э. Ф. Коддом в 1970 году. Система, используемая для ведения реляционных баз данных, - это система управления реляционными базами данных. Многие системы реляционных баз данных имеют возможность использования SQL для запросов и ведения базы данных.
MySQL - это система управления реляционными базами данных с открытым исходным кодом. Ее название представляет собой комбинацию слов \u0026ldquo;My\u0026rdquo;, имя дочери соучредителя Майкла Видениуса, и \u0026ldquo;SQL\u0026rdquo;, аббревиатура для языка структурированных запросов.
MySQL является одним из примеров реляционной базы данных, существует множество других вариантов.
При изучении реляционных баз данных часто упоминается термин или аббревиатура ACID (atomicity, consistency, isolation, durability) - это набор свойств транзакций базы данных, призванных гарантировать достоверность данных, несмотря на ошибки, сбои питания и другие казусы. В контексте баз данных последовательность операций с базой данных, удовлетворяющая свойствам ACID (которую можно воспринимать как одну логическую операцию над данными), называется транзакцией. Например, перевод средств с одного банковского счета на другой, даже включающий несколько изменений, таких как дебетование одного счета и кредитование другого, является одной транзакцией.
Лучше всего подходит для:
Большинство приложений (существует уже много лет, но это не значит, что он лучший). Она не идеальна для неструктурированных данных или способности к масштабированию - некоторые из других NoSQL обеспечивают лучшую способность к масштабированию для определенных рабочих нагрузок.
Graph Графовая база данных хранит узлы и отношения вместо таблиц или документов. Данные хранятся так же, как вы можете набросать идеи на доске. Ваши данные хранятся без ограничения их заранее определенной моделью, что позволяет очень гибко подходить к их осмыслению и использованию.
Neo4j - это система управления графовыми базами данных, разработанная компанией Neo4j, Inc. Разработчики описывают ее как ACID-совместимую транзакционную базу данных со встроенными средствами хранения и обработки графов.
Лучшая для:
Графы Графы знаний Рекомендательные движки Поисковая система В предыдущем разделе мы фактически использовали базу данных поисковой системы на пути к Elasticsearch.
База данных поисковой системы - это тип нереляционной базы данных, предназначенной для поиска данных. Базы данных поисковых систем используют индексы для категоризации схожих характеристик данных и облегчения поиска.
Elasticsearch - это поисковая система, основанная на библиотеке Lucene. Она представляет собой распределенную полнотекстовую поисковую систему с поддержкой многопользовательского доступа, веб-интерфейсом HTTP и документами JSON без схем.
Лучшее для:
Поисковые системы Typeahead Поиск по журналу Мультимодель Многомодельная база данных - это система управления базой данных, разработанная для поддержки нескольких моделей данных на основе единого интегрированного бэкенда. В отличие от этого, большинство систем управления базами данных организованы вокруг одной модели данных, которая определяет, как данные могут быть организованы, храниться и манипулироваться. Документ, граф, реляционная модель и модель ключ-значение - это примеры моделей данных, которые могут поддерживаться многомодельной базой данных.
Fauna - это гибкая, удобная для разработчиков, транзакционная база данных, предоставляемая в виде безопасного и масштабируемого облачного API со встроенным GraphQL..
Лучшее решение для:
Вы не привязаны к выбору модели данных. Соответствует стандарту ACID Быстрая Отсутствие накладных расходов на инициализацию Как вы хотите использовать свои данные и предоставить облаку выполнять всю работу. На этом мы закончим обзор баз данных, независимо от того, в какой отрасли вы работаете, вы обязательно столкнетесь с одной из областей баз данных. Далее в этом разделе мы рассмотрим некоторые из этих примеров и управление данными и, в частности, защиту и хранение этих сервисов данных.
Существует масса ресурсов, ссылки на которые я привел ниже, и вы можете потратить 90 лет на глубокое погружение во все типы баз данных и все, что с этим связано.
Ресурсы Redis Crash Course - the What, Why and How to use Redis as your primary database Redis: How to setup a cluster - for beginners Redis on Kubernetes for beginners Intro to Cassandra - Cassandra Fundamentals MongoDB Crash Course MongoDB in 100 Seconds What is a Relational Database? Learn PostgreSQL Tutorial - Full Course for Beginners MySQL Tutorial for Beginners [Full Course] What is a graph database? (in 10 minutes) What is Elasticsearch? FaunaDB Basics - The Database of your Dreams Fauna Crash Course - Covering the Basics `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day85/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day86/":{title:"86. Резервное копирование всех платформ",tags:["devops"],content:"Резервное копирование всех платформ В ходе всего этого задания мы обсудили множество различных платформ и сред. Всех их объединяет то, что все они нуждаются в определенном уровне защиты данных!\nЗащита данных существует уже много лет, но богатство данных, которые мы имеем сегодня, и ценность, которую эти данные приносят, означает, что мы должны быть уверены не только в устойчивости к сбоям инфраструктуры за счет наличия нескольких узлов и высокой доступности приложений, но мы также должны учитывать, что нам нужна копия этих данных, этих важных данных в безопасном и надежном месте, если произойдет сбой.\nВ наши дни мы часто слышим о киберпреступности и программах-выкупах, и не поймите меня неправильно - это серьезная угроза, и я уверен, что вы подвергнетесь атаке программ-выкупов. Это не вопрос \u0026ldquo;если\u0026rdquo;, это вопрос \u0026ldquo;когда\u0026rdquo;. Поэтому еще больше причин убедиться в том, что ваши данные надежно защищены на тот случай, если такое время настанет. Однако самой распространенной причиной потери данных является не выкупное ПО или киберпреступность, а просто случайное удаление!\nМы все это делали, удаляли то, что не должны были удалять, и тут же сожалели об этом.\nНесмотря на все технологии и автоматизацию, о которых мы говорили в этой статье, требование защищать любые данные с состоянием или даже сложные конфигурации без состояния все еще существует, независимо от платформы.\nНо мы должны быть в состоянии выполнить эту защиту данных с учетом автоматизации и возможности интеграции в наши рабочие процессы.\nЕсли мы посмотрим, что такое резервное копирование:\nВ информационных технологиях резервная копия или резервное копирование данных - это копия компьютерных данных, снятая и сохраненная в другом месте, чтобы ее можно было использовать для восстановления оригинала после потери данных. Глагольная форма, обозначающая процесс создания такой копии, - \u0026ldquo;резервное копирование\u0026rdquo;, а существительное и прилагательное - \u0026ldquo;резервное копирование\u0026rdquo;.\nЕсли мы разберем это в самой простой форме, то резервное копирование - это копирование и вставка данных в новое место. Проще говоря, я могу сделать резервную копию прямо сейчас, скопировав файл с диска C: на диск D:, и у меня будет копия на случай, если что-то случится с диском C: или что-то будет неправильно отредактировано в файлах. Я могу вернуться к копии, которая находится на диске D:. Теперь, если мой компьютер умрет, где находятся оба диска C и D, я не буду защищен, поэтому мне придется искать решение или копировать данные вне моей системы, может быть, на NAS-накопитель у себя дома? Но тогда что произойдет, если что-то случится с моим домом, может быть, мне нужно подумать о хранении данных на другой системе в другом месте, может быть, облако - это вариант. Может быть, я могу хранить копии важных файлов в нескольких местах, чтобы снизить риск сбоя?\n3-2-1 Методика резервного копирования Сейчас самое время поговорить о правиле 3-2-1 или методологии резервного копирования. На самом деле я провел lightening talk, посвященный этой теме.\nМы уже упоминали о некоторых крайностях того, почему нам нужно защищать наши данные, но ниже перечислены еще несколько:\nЭто позволяет мне рассказать о методологии 3-2-1. Моя первая копия или резервная копия данных должна быть как можно ближе к моей производственной системе, причина этого заключается в скорости восстановления и, опять же, возвращаясь к исходному пункту о случайном удалении, это будет наиболее распространенной причиной для восстановления. Но я хочу хранить эти данные на подходящем втором носителе за пределами исходной или рабочей системы.\nЗатем мы хотим убедиться, что мы также отправляем копию наших данных на внешний носитель или за пределы системы, и здесь нам на помощь приходит второе место, будь то другой дом, здание, центр обработки данных или публичное облако.\nОтветственность за резервное копирование\nМы, скорее всего, слышали все мифы о том, что резервное копирование не нужно, например, такие как \u0026ldquo;Все не имеет состояния\u0026rdquo;. Если все не имеет состояния, то что тогда бизнес? Нет баз данных? документов? Очевидно, что каждый человек в компании несет определенную ответственность за обеспечение своей защиты, но, скорее всего, именно операционные команды должны обеспечить процесс резервного копирования критически важных приложений и данных.\nЕще одна хорошая фраза: \u0026ldquo;Высокая доступность - это моя резервная копия, мы встроили несколько узлов в наш кластер, поэтому он ни за что не выйдет из строя!\u0026rdquo;, кроме тех случаев, когда вы допускаете ошибку в базе данных, и она реплицируется на все узлы кластера, или когда происходит пожар, наводнение, что означает, что кластер больше недоступен, а вместе с ним и важные данные. Речь идет не об упрямстве, а о том, чтобы быть в курсе данных и сервисов, абсолютно все должны учитывать высокую доступность и отказоустойчивость в своей архитектуре, но это не заменяет необходимости резервного копирования!\nРепликация также может дать нам копию данных вне офиса, и, возможно, упомянутый выше кластер действительно живет в нескольких местах, однако первая случайная ошибка все равно будет реплицирована туда. Но, опять же, требование резервного копирования должно стоять в одном ряду с репликацией приложений или системной репликацией в среде.\nТеперь, учитывая все вышесказанное, можно впасть в крайность и отправить копии данных в слишком большое количество мест, что приведет не только к большим затратам, но и к увеличению риска подвергнуться атаке, поскольку площадь вашей поверхности теперь значительно увеличилась.\nВ любом случае, кто заботится о резервном копировании? В каждом предприятии это будет по-разному, но кто-то должен понимать требования к резервному копированию. Но также необходимо понимать план восстановления!\nНикому нет дела, пока всем нет дела Резервное копирование является ярким примером: никто не заботится о резервном копировании, пока вам не понадобится что-то восстановить. Наряду с требованием резервного копирования данных нам также необходимо подумать о том, как мы будем восстанавливать данные!\nВ нашем примере с текстовыми документами речь идет об очень маленьких файлах, поэтому возможность копирования туда и обратно является простой и быстрой. Но если речь идет о файлах размером более 100 ГБ, то на это потребуется время. Также необходимо учитывать уровень, на котором требуется восстановление, например, если мы возьмем виртуальную машину.\nУ нас есть вся виртуальная машина, у нас есть операционная система, установка приложений, а если это сервер баз данных, то у нас есть и некоторые файлы баз данных. Если мы допустили ошибку и вставили неправильную строку кода в нашу базу данных, мне, вероятно, не нужно восстанавливать всю виртуальную машину, я хочу быть детальным в том, что я восстанавливаю.\nСценарий резервного копирования Теперь я хочу начать строить скрипт защиты некоторых данных, в частности, я хочу защитить некоторые файлы на моей локальной машине (в данном случае Windows, но инструмент, который я собираюсь использовать, на самом деле не только бесплатный и с открытым исходным кодом, но и кроссплатформенный). Я хочу убедиться, что они защищены на устройстве NAS, которое у меня есть дома, а также в облачном хранилище Object Storage bucket.\nЯ хочу сделать резервную копию этих важных данных, так получилось, что это репозиторий для 90DaysOfDevOps, который, да, также отправляется на GitHub, где вы, вероятно, сейчас это читаете, но что, если моя машина умрет, а GitHub будет закрыт? Как бы кто-нибудь смог прочитать содержимое, а также как бы я мог восстановить эти данные на другом сервисе.\nСуществует множество инструментов, которые могут помочь нам достичь этого, но я собираюсь использовать инструмент под названием Kopia - это инструмент резервного копирования с открытым исходным кодом, который позволит нам шифровать, дедупировать и сжимать наши резервные копии, а также отправлять их во многие места.\nВы найдете релизы для загрузки здесь на момент написания статьи я буду использовать версию 0.10.6.\nУстановка Kopia Существует Kopia CLI и GUI, мы будем использовать GUI, но знайте, что вы можете иметь и CLI версию для тех Linux серверов, которые не дают вам GUI.\nЯ буду использовать KopiaUI-Setup-0.10.6.exe.\nДействительно быстрая установка, а затем, когда вы откроете приложение, вам предложат выбрать тип хранилища, которое вы хотите использовать в качестве хранилища резервных копий.\nНастройка хранилища Сначала мы хотим создать хранилище на локальном NAS-устройстве и собираемся сделать это с помощью SMB, но можно использовать и NFS.\nНа следующем экране мы собираемся определить пароль, этот пароль используется для шифрования содержимого хранилища.\nТеперь, когда хранилище настроено, мы можем запустить adhoc snapshot, чтобы начать запись данных в хранилище. [18:26, 16.06.2022] evgschegolkova: Прежде всего, нам нужно ввести путь к тому, что мы хотим сделать снимок, и в нашем случае мы хотим сделать копию папки 90DaysOfDevOps. Вскоре мы вернемся к аспекту планирования.\nМы можем определить хранение наших снимков.\nВозможно, есть файлы или типы файлов, которые мы хотим исключить.\nЕсли бы мы хотели определить расписание, мы могли бы сделать это на следующем экране, когда вы впервые создаете этот снимок, это начальная страница для определения.\nИ вы увидите ряд других настроек, которые могут быть обработаны здесь.\nВыберите snapshot now, и данные будут записаны в ваше хранилище.\nВнесетевое резервное копирование на S3 С помощью Kopia мы можем настроить только одно хранилище одновременно. Но через пользовательский интерфейс мы можем проявить творческий подход и, по сути, иметь несколько файлов конфигурации хранилища на выбор для достижения нашей цели - иметь локальную и внесетевую копию в Object Storage.\nХранилище Object Storage, в которое я решил отправить свои данные, будет Google Cloud Storage. Сначала я вошел в свой аккаунт Google Cloud Platform и создал себе ведро хранения. В моей системе уже был установлен Google Cloud SDK, но выполнение команды gcloud auth application-default login позволило мне аутентифицироваться в моей учетной записи.\nЗатем я использовал CLI Kopia, чтобы показать мне текущее состояние моего хранилища после того, как мы добавили наше SMB хранилище в предыдущих шагах. Я сделал это с помощью команды \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status.\nТеперь мы готовы заменить конфигурацию хранилища для целей демонстрации. Если бы мы хотели получить долгосрочное решение для обоих хранилищ, мы бы создали файл smb.config и файл object.config и могли бы запускать обе эти команды для отправки наших копий данных в каждое место. Для добавления нашего хранилища мы выполнили команду \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository create gcs --bucket 90daysofdevops.\nПриведенная выше команда учитывает, что ведро Google Cloud Storage, которое мы создали, называется 90daysofdevops.\nТеперь, когда мы создали наше новое хранилище, мы можем снова запустит [18:27, 16.06.2022] evgschegolkova: Прежде всего, нам нужно ввести путь к тому, что мы хотим сделать снимок, и в нашем случае мы хотим сделать копию папки 90DaysOfDevOps. Вскоре мы вернемся к аспекту планирования.\nМы можем определить хранение наших снимков.\nВозможно, есть файлы или типы файлов, которые мы хотим исключить.\nЕсли бы мы хотели определить расписание, мы могли бы сделать это на следующем экране, когда вы впервые создаете этот снимок, это начальная страница для определения.\nИ вы увидите ряд других настроек, которые могут быть обработаны здесь.\nВыберите snapshot now, и данные будут записаны в ваше хранилище.\nВнесетевое резервное копирование на S3 С помощью Kopia мы можем настроить только одно хранилище одновременно. Но через пользовательский интерфейс мы можем проявить творческий подход и, по сути, иметь несколько файлов конфигурации хранилища на выбор для достижения нашей цели - иметь локальную и внесетевую копию в Object Storage.\nХранилище Object Storage, в которое я решил отправить свои данные, будет Google Cloud Storage. Сначала я вошел в свой аккаунт Google Cloud Platform и создал себе ведро хранения. В моей системе уже был установлен Google Cloud SDK, но выполнение команды gcloud auth application-default login позволило мне аутентифицироваться в моей учетной записи.\nЗатем я использовал CLI Kopia, чтобы показать мне текущее состояние моего хранилища после того, как мы добавили наше SMB хранилище в предыдущих шагах. Я сделал это с помощью команды \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status.\nТеперь мы готовы заменить конфигурацию хранилища для целей демонстрации. Если бы мы хотели получить долгосрочное решение для обоих хранилищ, мы бы создали файл smb.config и файл object.config и могли бы запускать обе эти команды для отправки наших копий данных в каждое место. Для добавления нашего хранилища мы выполнили команду \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository create gcs --bucket 90daysofdevops.\nПриведенная выше команда учитывает, что ведро Google Cloud Storage, которое мы создали, называется 90daysofdevops.\nТеперь, когда мы создали наше новое хранилище, мы можем снова запустить команду \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status, которая теперь покажет конфигурацию хранилища GCS.\nСледующее, что нам нужно сделать, это создать снимок и отправить его в наш только что созданный репозиторий. Используя команду \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config kopia snapshot create \u0026quot;C:\\Users\\micha\\demo\\90DaysOfDevOps\u0026quot; мы можем запустить этот процесс. В браузере ниже вы можете увидеть, что в нашем ведре Google Cloud Storage теперь есть файлы kopia, основанные на нашей резервной копии.\nС помощью вышеописанного процесса мы смогли решить нашу задачу по отправке важных данных в 2 разных места, одно из которых находится вне помещения в Google Cloud Storage, и, конечно же, у нас все еще есть наша производственная копия данных на другом типе носителя.\nВосстановление Восстановление - это еще один важный момент, Kopia дает нам возможность не только восстанавливать данные в существующее местоположение, но и в новое.\nЕсли мы выполним команду \u0026quot;C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026quot; --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config snapshot list, это приведет к списку снимков, которые в настоящее время находятся в нашем настроенном хранилище (GCS).\nЗатем мы можем смонтировать эти снимки непосредственно из GCS, используя команду ``C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\u0026rsquo;\u0026rsquo; \u0026ndash;config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config mount all Z:`.\nМы также можем восстановить содержимое снимка с помощью команды kopia snapshot restore kdbd9dff738996cfe7bcf99b45314e193.\nОчевидно, что приведенные выше команды очень длинные, и это потому, что я использовал KopiaUI версию kopia.exe, как объяснялось в верхней части руководства, вы можете скачать kopia.exe и поместить в путь, чтобы вы могли просто использовать команду kopia.\nНа следующем занятии мы сосредоточимся на защите рабочих нагрузок в Kubernetes.\nРесурсы Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility ",url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day86/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day87/":{title:"87. Резервное копирование и восстановление",tags:["devops"],content:`Резервное копирование и восстановление своими руками На прошлом занятии мы рассмотрели Kopia - инструмент резервного копирования с открытым исходным кодом, который мы использовали для переноса важных данных на локальный NAS и в облачное хранилище объектов.
В этом разделе я хочу погрузиться в мир резервного копирования Kubernetes. Это платформа, которую мы рассматривали в The Big Picture: Kubernetes ранее в этой задаче.
Мы снова будем использовать наш кластер minikube, но на этот раз мы воспользуемся некоторыми из доступных аддонов.
Настройка кластера Kubernetes Для настройки нашего кластера minikube мы выполним команду minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p 90daysofdevops --kubernetes-version=1.21.2. Вы заметите, что мы используем volumesnapshots и csi-hostpath-driver, поскольку мы будем использовать их для создания резервных копий.
На данном этапе я знаю, что мы еще не развернули Kasten K10, но мы хотим выполнить следующую команду, когда ваш кластер будет запущен, но мы хотим аннотировать класс volumesnapshotclass, чтобы Kasten K10 мог использовать его.
kubectl annotate volumesnapshotclass csi-hostpath-snapclass \\k10.kasten.io/is-snapshot-class=trueМы также собираемся изменить класс хранения по умолчанию со стандартного класса хранения по умолчанию на класс хранения csi-hostpath, используя следующее.
kubectl patch storageclass csi-hostpath-sc -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;true\u0026quot;}}}}''kubectl patch storageclass standard -p '{\u0026quot;metadata\u0026quot;: {\u0026quot;annotations\u0026quot;:{\u0026quot;storageclass.kubernetes.io/is-default-class\u0026quot;: \u0026quot;false\u0026quot;}}}}''Развертывание Kasten K10 Добавьте репозиторий Kasten Helm
helm repo add kasten https://charts.kasten.io/.
Мы могли бы использовать arkade kasten install k10 и здесь, но для целей демонстрации мы выполним следующие шаги. Подробнее
Создайте пространство имен и разверните K10, обратите внимание, что это займет около 5 минут
helm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace.
Вы можете наблюдать за появлением стручков, выполнив следующую команду.
kubectl get pods -n kasten-io -w
Чтобы получить доступ к приборной панели K10, откройте новый терминал и выполните следующую команду
kubectl --namespace kasten-io port-forward service/gateway 8080:8000.
Приборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/
Для аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.
TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \u0026quot; \u0026quot; -f 1)TOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\u0026quot;{.data.token}\u0026quot; | base64 --decode)echo \u0026quot;Значение токена: \u0026quot;echo $TOKENТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.
Затем мы получаем доступ к приборной панели Kasten K10.
Развертывание нашего stateful-приложения Используйте stateful-приложение, которое мы использовали в разделе Kubernetes.
Вы можете найти конфигурационный файл YAML для этого приложения здесьpacman-stateful-demo.yaml
Мы можем использовать kubectl get all -n pacman, чтобы проверить появление наших стручков.
В новом терминале мы можем перенаправить фронт-енд pacman. kubectl port-forward svc/pacman 9090:80 -n pacman.
Откройте другую вкладку в браузере на http://localhost:9090/
Найдите время, чтобы записать несколько высоких результатов в базе данных backend MongoDB.
Защитите наши высокие баллы Теперь у нас есть некоторые важные данные в нашей базе данных, и мы не хотим их потерять. Мы можем использовать Kasten K10 для защиты всего приложения.
Если мы вернемся на вкладку приборной панели Kasten K10, вы увидите, что количество наших приложений увеличилось с 1 до 2 с добавлением нашего приложения pacman в наш кластер Kubernetes.
Если вы нажмете на карточку Applications, вы увидите автоматически обнаруженные приложения в нашем кластере.
В Kasten K10 у нас есть возможность использовать моментальные снимки на основе хранилища, а также экспортировать наши копии в объектные хранилища.
Для целей демонстрации мы создадим ручной снимок хранилища в нашем кластере, а затем добавим некоторые неавторизованные данные в наши высокие результаты, чтобы имитировать случайную ошибку или нет?
Для начала мы можем воспользоваться приведенным ниже вариантом ручного снапшота.
Для демонстрации я собираюсь оставить все по умолчанию
Вернувшись на приборную панель, вы получите отчет о состоянии задания в процессе его выполнения, а после завершения оно должно выглядеть так же успешно, как и здесь.
Сценарий неудачи Теперь мы можем внести фатальное изменение в наши критически важные данные, просто добавив предписывающее плохое изменение в наше приложение.
Как вы можете видеть ниже, у нас есть два входа, которые мы, вероятно, не хотим видеть в нашей производственной критически важной базе данных.
Восстановление данных Очевидно, что это простая демонстрация и в некотором роде нереалистичная, хотя вы видели, как легко можно сбросить базы данных?
Теперь мы хотим, чтобы список высоких результатов выглядел немного чище и как он выглядел до того, как были допущены ошибки.
Вернемся в карточку приложений и на вкладку pacman, теперь у нас есть 1 точка восстановления, которую мы можем использовать для восстановления.
При выборе восстановления вы можете увидеть все связанные снимки и экспорты для этого приложения.
Выберите восстановление и появится боковое окно, мы сохраним настройки по умолчанию и нажмем восстановить.
Подтвердите, что вы действительно хотите, чтобы это произошло.
Затем вы можете вернуться на приборную панель и просмотреть ход восстановления. Вы должны увидеть что-то вроде этого.
Но более важно то, как выглядит наш список High-Score в нашем критически важном приложении. Вам придется снова запустить проброс портов в pacman, как мы уже рассказывали ранее.
Это очень простая демонстрация, которая лишь слегка касается того, чего Kasten K10 может достичь в области резервного копирования. В будущем я буду создавать более подробные видеоматериалы по некоторым из этих областей. Мы также будем использовать Kasten K10 для освещения некоторых других важных областей управления данными, когда речь идет об аварийном восстановлении и мобильности ваших данных.
Далее мы рассмотрим согласованность приложений.
Ресурсы Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day87/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day88/":{title:"88. Резервное копирование, ориентированное на приложения",tags:["devops"],content:`Резервное копирование, ориентированное на приложения В День 85 мы уже потратили некоторое время на обсуждение служб данных или приложений с интенсивным использованием данных, таких как базы данных. Для этих служб данных мы должны подумать о том, как управлять согласованностью, особенно когда речь идет о согласованности приложений.
В этой статье мы рассмотрим требования к защите данных приложения в последовательной манере.
Для этого мы выберем инструмент Kanister
Представляем Kanister Kanister - это проект с открытым исходным кодом от Kasten, который позволяет нам управлять (резервное копирование и восстановление) данными приложений на Kubernetes. Вы можете развернуть Kanister как helm-приложение в своем кластере Kubernetes.
Kanister использует пользовательские ресурсы Kubernetes, основные пользовательские ресурсы, которые устанавливаются при развертывании Kanister, следующие
Profile - целевое место для хранения резервных копий и восстановления. Чаще всего это объектное хранилище. Blueprint - шаги, которые необходимо предпринять для резервного копирования и восстановления базы данных, должны быть сохранены в Blueprint. ActionSet - действия по перемещению целевой резервной копии в наш профиль, а также действия по восстановлению. Описание выполнения Прежде чем приступить к работе, мы должны рассмотреть рабочий процесс, который использует Kanister для защиты данных приложения. Во-первых, наш контроллер развертывается с помощью helm в нашем кластере Kubernetes, Kanister живет в своем собственном пространстве имен. Мы берем наш Blueprint, для которого существует множество поддерживаемых сообществом Blueprint, мы рассмотрим это более подробно в ближайшее время. Затем у нас есть рабочая нагрузка базы данных.
Затем мы создаем наш ActionSet.
ActionSet позволяет нам запускать действия, определенные в чертеже, против конкретной службы данных.
ActionSet, в свою очередь, использует функции Kanister (KubeExec, KubeTask, Resource Lifecycle) и выталкивает нашу резервную копию в целевое хранилище (Profile).
Если действие выполнено/не выполнено, соответствующий статус обновляется в наборе действий.
Развертывание Kanister И снова мы будем использовать кластер minikube для создания резервной копии приложения. Если у вас он все еще работает с предыдущей сессии, то мы можем продолжать использовать его.
На момент написания статьи мы имеем версию образа 0.75.0. С помощью следующей команды helm мы установим kanister в наш кластер Kubernetes.
helm install kanister --namespace kanister kanister/kanister-operator --set image.tag=0.75.0 --create-namespace.
Мы можем использовать kubectl get pods -n kanister, чтобы убедиться, что pod запущен и работает, а также проверить, что наши пользовательские определения ресурсов теперь доступны (Если вы только установили Kanister, то вы увидите выделенные 3) Развертывание базы данных Развертывание mysql через helm:
APP_NAME=my-production-appkubectl create ns \${APP_NAME}helm repo add bitnami https://charts.bitnami.com/bitnamihelm install mysql-store bitnami/mysql --set primary.persistence.size=1Gi,volumePermissions.enabled=true --namespace=\${APP_NAME}kubectl get pods -n \${APP_NAME} -wЗаполните базу данных mysql исходными данными, выполнив следующее:
MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace \${APP_NAME} mysql-store -o jsonpath=\u0026quot;{.data.mysql-root-password}\u0026quot; | base64 --decode)MYSQL_HOST=mysql-store.\${APP_NAME}.svc.cluster.localMYSQL_EXEC=\u0026quot;mysql -h \${MYSQL_HOST} -u root --password=\${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\u0026quot;echo MYSQL_ROOT_PASSWORD=\${MYSQL_ROOT_PASSWORD}Создание MySQL CLIENT Мы запустим другой образ контейнера, который будет выступать в качестве нашего клиента
APP_NAME=my-production-appkubectl run mysql-client --rm --env APP_NS=\${APP_NAME} --env MYSQL_EXEC=\u0026quot;\${MYSQL_EXEC}\u0026quot; --env MYSQL_ROOT_PASSWORD=\${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=\${MYSQL_HOST} --namespace \${APP_NAME} --tty -i --restart='Never' --image docker.io/bitnami/mysql:latest --command -- bashПримечание: если у вас уже запущен существующий mysql client pod, удалите его с помощью командыkubectl delete pod -n \${APP_NAME} mysql-clientДобавление данных в MySQL echo \u0026quot;create database myImportantData;\u0026quot; | mysql -h \${MYSQL_HOST} -u root --password=\${MYSQL_ROOT_PASSWORD}MYSQL_EXEC=\u0026quot;mysql -h \${MYSQL_HOST} -u root --password=\${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\u0026quot;echo \u0026quot;drop table Accounts\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;create table if not exists Accounts(name text, balance integer); insert into Accounts values('nick', 0);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('albert', 112);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('alfred', 358);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('beatrice', 1321);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('bartholomew', 34);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('edward', 5589);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('edwin', 144);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('edwina', 233);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;insert into Accounts values('rastapopoulos', 377);\u0026quot; | \${MYSQL_EXEC}echo \u0026quot;select * from Accounts;\u0026quot; | \${MYSQL_EXEC}exitВы должны увидеть некоторые данные, как показано ниже.
Создание профиля Kanister Kanister предоставляет CLI, kanctl и другую утилиту kando, которая используется для взаимодействия с провайдером объектного хранилища из blueprint и обе эти утилиты.
CLI Download
Я пошел и создал AWS S3 Bucket, который мы будем использовать в качестве цели профиля и места восстановления. Я буду использовать переменные окружения, чтобы иметь возможность показать вам команды, которые я выполняю с помощью kanctl для создания нашего профиля kanister.
kanctl create profile s3compliant --access-key $ACCESS_KEY --secret-key $SECRET_KEY --bucket $BUCKET --region eu-west-2 --namespace my-production-app.
Время чертежа Не волнуйтесь, вам не нужно создавать свой собственный с нуля, если только ваш сервис данных не указан в Примерах Канистера, но, конечно, вклад сообщества - это то, как этот проект становится известным.
Мы будем использовать следующую схему.
apiVersion: cr.kanister.io/v1alpha1kind: Blueprintmetadata:name: mysql-blueprintactions:backup:outputArtifacts:mysqlCloudDump:keyValue:s3path: \u0026quot;{{ .Phases.dumpToObjectStore.Output.s3path }}\u0026quot;phases:- func: KubeTaskname: dumpToObjectStoreobjects:mysqlSecret:kind: Secretname: '{{ index .Object.metadata.labels \u0026quot;app.kubernetes.io/instance\u0026quot; }}'namespace: '{{ .StatefulSet.Namespace }}'args:image: ghcr.io/kanisterio/mysql-sidecar:0.75.0namespace: \u0026quot;{{ .StatefulSet.Namespace }}\u0026quot;command:- bash- -o- errexit- -o- pipefail- -c- |s3_path=\u0026quot;/mysql-backups/{{ .StatefulSet.Namespace }}/{{ index .Object.metadata.labels \u0026quot;app.kubernetes.io/instance\u0026quot; }}/{{ toDate \u0026quot;2006-01-02T15:04:05.999999999Z07:00\u0026quot; .Time | date \u0026quot;2006-01-02T15-04-05\u0026quot; }}/dump.sql.gz\u0026quot;root_password=\u0026quot;{{ index .Phases.dumpToObjectStore.Secrets.mysqlSecret.Data \u0026quot;mysql-root-password\u0026quot; | toString }}\u0026quot;mysqldump --column-statistics=0 -u root --password=\${root_password} -h {{ index .Object.metadata.labels \u0026quot;app.kubernetes.io/instance\u0026quot; }} --single-transaction --all-databases | gzip - | kando location push --profile '{{ toJson .Profile }}' --path \${s3_path} -kando output s3path \${s3_path}restore:inputArtifactNames:- mysqlCloudDumpphases:- func: KubeTaskname: restoreFromBlobStoreobjects:mysqlSecret:kind: Secretname: '{{ index .Object.metadata.labels \u0026quot;app.kubernetes.io/instance\u0026quot; }}'namespace: '{{ .StatefulSet.Namespace }}'args:image: ghcr.io/kanisterio/mysql-sidecar:0.75.0namespace: \u0026quot;{{ .StatefulSet.Namespace }}\u0026quot;command:- bash- -o- errexit- -o- pipefail- -c- |s3_path=\u0026quot;{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\u0026quot;root_password=\u0026quot;{{ index .Phases.restoreFromBlobStore.Secrets.mysqlSecret.Data \u0026quot;mysql-root-password\u0026quot; | toString }}\u0026quot;kando location pull --profile '{{ toJson .Profile }}' --path \${s3_path} - | gunzip | mysql -u root --password=\${root_password} -h {{ index .Object.metadata.labels \u0026quot;app.kubernetes.io/instance\u0026quot; }}delete:inputArtifactNames:- mysqlCloudDumpphases:- func: KubeTaskname: deleteFromBlobStoreargs:image: ghcr.io/kanisterio/mysql-sidecar:0.75.0namespace: \u0026quot;{{ .Namespace.Name }}\u0026quot;command:- bash- -o- errexit- -o- pipefail- -c- |s3_path=\u0026quot;{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\u0026quot;kando location delete --profile '{{ toJson .Profile }}' --path \${s3_path}Чтобы добавить его, мы воспользуемся командой kubectl create -f mysql-blueprint.yml -n kanister.
Создаем наш ActionSet и защищаем наше приложение Теперь мы создадим резервную копию данных MySQL с помощью ActionSet, определяющего резервное копирование для этого приложения. Создайте ActionSet в том же пространстве имен, что и контроллер.
kubectl get profiles.cr.kanister.io -n my-production-app Эта команда покажет нам профиль, который мы ранее создали, здесь может быть настроено несколько профилей, поэтому мы можем захотеть использовать определенные профили для разных ActionSet\u0026rsquo;ов.
Затем мы создадим наш ActionSet следующей командой с помощью kanctl.
kanctl create actionset --action backup --namespace kanister --blueprint mysql-blueprint --statefulset my-production-app/mysql-store --profile my-production-app/s3-profile-dc5zm --secrets mysql=my-production-app/mysql-store.
Из приведенной выше команды видно, что мы определяем blueprint, который мы добавили в пространство имен, statefulset в нашем пространстве имен my-production-app, а также секреты для входа в приложение MySQL.
Проверьте состояние ActionSet, взяв имя ActionSet и используя эту команду kubectl --namespace kanister describe actionset backup-qpnqv.
Наконец, мы можем пойти и подтвердить, что теперь у нас есть данные в нашем ведре AWS S3.
Восстановление Нам нужно нанести некоторый ущерб, прежде чем мы сможем что-либо восстановить, мы можем сделать это, уронив нашу таблицу, возможно, это был несчастный случай, а возможно и нет.
Подключитесь к нашему MySQL pod.
APP_NAME=my-production-appkubectl run mysql-client --rm --env APP_NS=\${APP_NAME} --env MYSQL_EXEC=\u0026quot;\${MYSQL_EXEC}\u0026quot; --env MYSQL_ROOT_PASSWORD=\${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=\${MYSQL_HOST} --namespace \${APP_NAME} --tty -i --restart='Never' --image docker.io/bitnami/mysql:latest --command -- bashВы можете увидеть, что наша база данных importantdata находится там с помощью echo \u0026quot;SHOW DATABASES;\u0026quot; | \${MYSQL_EXEC}.
Затем для удаления мы запустили echo \u0026quot;DROP DATABASE myImportantData;\u0026quot; | \${MYSQL_EXEC}.
И подтвердили, что все исчезло, сделав несколько попыток показать нашу базу данных.
Теперь мы можем использовать Kanister, чтобы вернуть наши важные данные в рабочее состояние, используя команду kubectl get actionset -n kanister, чтобы узнать имя нашего ActionSet, который мы взяли ранее. Затем мы создадим ActionSet восстановления для восстановления наших данных, используя kanctl create actionset -n kanister --action restore --from \u0026quot;backup-qpnqv\u0026quot;.
Мы можем подтвердить, что наши данные восстановлены, используя следующую команду для подключения к нашей базе данных.
APP_NAME=my-production-appkubectl run mysql-client --rm --env APP_NS=\${APP_NAME} --env MYSQL_EXEC=\u0026quot;\${MYSQL_EXEC}\u0026quot; --env MYSQL_ROOT_PASSWORD=\${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=\${MYSQL_HOST} --namespace \${APP_NAME} --tty -i --restart='Never' --image docker.io/bitnami/mysql:latest --command -- bashТеперь мы находимся внутри клиента MySQL, мы можем выполнить команду echo \u0026quot;SHOW DATABASES;\u0026quot; | \${MYSQL_EXEC} и мы увидим, что база данных восстановлена. Мы также можем выполнить команду echo \u0026quot;select * from Accounts;\u0026quot; | \${MYSQL_EXEC} для проверки содержимого базы данных, и наши важные данные будут восстановлены.
В следующем посте мы рассмотрим аварийное восстановление в Kubernetes.
Ресурсы Kanister Overview - An extensible open-source framework for app-lvl data management on Kubernetes Application Level Data Operations on Kubernetes Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day88/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day89/":{title:"89. Аварийное восстановление",tags:["devops"],content:`Аварийное восстановление Мы уже упоминали о том, что различные скрипты сбоев требуют различных требований к восстановлению. Когда речь идет о скриптах пожара, наводнения и крови, мы можем рассматривать их как аварийные ситуации, в которых нам может потребоваться, чтобы наши рабочие нагрузки были запущены в совершенно другом месте как можно быстрее или, по крайней мере, с почти нулевым временем восстановления (RTO).
Этого можно достичь только в масштабе, если автоматизировать репликацию всего стека приложений в резервную среду.
Это позволяет быстро переходить от одного облачного региона к другому, облачным провайдерам или между локальной и облачной инфраструктурой.
Продолжая тему, мы сосредоточимся на том, как этого можно достичь с помощью Kasten K10, используя наш кластер minikube, который мы развернули и настроили несколько занятий назад.
Затем мы создадим еще один кластер minikube с установленным Kasten K10 в качестве резервного кластера, который теоретически может находиться в любом месте.
Kasten K10 также имеет встроенную функциональность для обеспечения того, что если что-то случится с кластером Kubernetes, на котором он работает, данные каталога будут реплицированы и доступны на новом K10 Disaster Recovery.
Добавление объектного хранилища в K10 Первое, что нам нужно сделать, это добавить ведро объектного хранилища в качестве целевого местоположения для наших резервных копий. Это не только выступает в качестве удаленного хранилища, но мы также можем использовать его в качестве исходных данных для аварийного восстановления.
Я очистил ведро S3, которое мы создали для демонстрации Kanister на прошлом занятии.
Чтобы получить доступ к приборной панели K10, откройте новый терминал и выполните следующую команду:
kubectl --namespace kasten-io port-forward service/gateway 8080:8000.
Приборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/
Для аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.
TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \u0026quot; \u0026quot; -f 1)TOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\u0026quot;{.data.token}\u0026quot; | base64 --decode)echo \u0026quot;Token value: \u0026quot;echo $TOKENТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.
Затем мы получаем доступ к приборной панели Kasten K10.
Теперь, когда мы вернулись в приборную панель Kasten K10, мы можем добавить наш профиль местоположения, выберите \u0026ldquo;Настройки\u0026rdquo; в верхней части страницы и \u0026ldquo;Новый профиль\u0026rdquo;.
На изображении ниже видно, что у нас есть выбор, где будет находиться этот профиль местоположения, мы выбираем Amazon S3, и добавляем наши учетные данные доступа, регион и имя ведра.
Если мы прокрутим окно создания нового профиля вниз, то увидим, что у нас также есть возможность включить неизменяемое резервное копирование, которое использует API блокировки объектов S3. В данном демо мы не будем использовать эту возможность.
Нажмите \u0026ldquo;Сохранить профиль\u0026rdquo;, и теперь вы можете увидеть наш только что созданный или добавленный профиль местоположения, как показано ниже.
Создание политики для защиты приложения Pac-Man в объектном хранилище В предыдущем сеансе мы создали только специальный снимок нашего приложения Pac-Man, поэтому нам нужно создать политику резервного копирования, которая будет отправлять резервные копии нашего приложения в наше недавно созданное объектное хранилище.
Если вы вернетесь на приборную панель и выберете карточку Policy, вы увидите окно, как показано ниже. Выберите \u0026ldquo;Создать новую политику\u0026rdquo;.
Во-первых, мы можем дать нашей политике полезное имя и описание. Мы также можем определить частоту резервного копирования, для демонстрационных целей я использую \u0026ldquo;по требованию\u0026rdquo;.
Далее мы хотим включить резервное копирование через Snapshot exports, что означает, что мы хотим отправлять наши данные в наш профиль местоположения. Если у вас их несколько, вы можете выбрать, в какой из них вы хотите отправлять резервные копии.
Далее выбираем приложение по имени или по меткам, я собираюсь выбрать по имени и все ресурсы.
В разделе Advanced settings мы не будем использовать ничего из этого, но, основываясь на нашем вчерашнем walkthrough of Kanister, мы можем использовать Kanister как часть Kasten K10 для создания согласованных с приложением копий наших данных.
Наконец, выберите \u0026ldquo;Создать политику\u0026rdquo;, и теперь вы увидите политику в нашем окне политики.
В нижней части созданной политики появится \u0026ldquo;Show import details\u0026rdquo;, нам нужна эта строка, чтобы иметь возможность импортировать в наш резервный кластер. Скопируйте ее в безопасное место.
Прежде чем двигаться дальше, нам нужно выбрать \u0026ldquo;run once\u0026rdquo;, чтобы получить резервную копию, отправленную нашему ведру объектного хранилища.
Ниже, на скриншоте просто показано успешное резервное копирование и экспорт наших данных.
Создание нового кластера MiniKube и развертывание K10 Затем нам нужно развернуть второй кластер Kubernetes, и где это может быть любая поддерживаемая версия Kubernetes, включая OpenShift, в целях обучения мы будем использовать очень бесплатную версию MiniKube с другим названием.
Используя minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p standby --kubernetes-version=1.21.2 мы можем создать наш новый кластер.
Затем мы можем развернуть Kasten K10 в этом кластере, используя:
helm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace.
Это займет некоторое время, но тем временем мы можем использовать kubectl get pods -n kasten-io -w, чтобы наблюдать за прогрессом перехода наших pods в статус запущенных.
Стоит отметить, что поскольку мы используем MiniKube, наше приложение будет запущено, когда мы запустим политику импорта, наш класс хранилища будет таким же на этом резервном кластере. Однако то, что мы рассмотрим на последнем занятии, касается мобильности и трансформации.
Когда капсулы запущены, мы можем выполнить шаги, которые мы проделали в предыдущих шагах на другом кластере.
Перенесите порт вперед для доступа к приборной панели K10, откройте новый терминал и выполните следующую команду
kubectl --namespace kasten-io port-forward service/gateway 8080:8000.
Приборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/
Для аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.
TOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \u0026quot; \u0026quot; -f 1)TOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\u0026quot;{.data.token}\u0026quot; | base64 --decode)echo \u0026quot;Token value: \u0026quot;echo $TOKENТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.
Затем мы получаем доступ к приборной панели Kasten K10.
Импортируем Pac-Man в новый кластер MiniKube На данном этапе мы можем создать политику импорта в резервном кластере, подключиться к резервным копиям объектного хранилища и определить, что и как мы хотим, чтобы выглядело.
Во-первых, мы добавляем наш профиль местоположения, который мы рассмотрели ранее на другом кластере, используя темный режим, чтобы показать разницу между нашей производственной системой и резервным местоположением DR.
Теперь вернемся к приборной панели и перейдем на вкладку политик, чтобы создать новую политику.
Создайте политику импорта в соответствии с приведенным ниже изображением. После завершения мы можем создать политику. Здесь есть опция восстановления после импорта, и некоторые люди могут захотеть воспользоваться этой опцией, которая будет восстановлена в нашем резервном кластере по завершении. У нас также есть возможность изменить конфигурацию приложения при восстановлении, и это то, что я описал в Day 90.
Я выбрал импорт по требованию, но вы, очевидно, можете установить расписание, когда вы хотите, чтобы этот импорт происходил. В связи с этим я собираюсь выполнить один раз.
Ниже вы можете видеть успешное выполнение задания политики импорта.
Если мы теперь вернемся на приборную панель и зайдем в карточку Applications, мы можем выбрать выпадающий список, где вы видите ниже \u0026ldquo;Removed\u0026rdquo;, здесь вы увидите наше приложение. Выберите \u0026ldquo;Восстановить
Здесь мы видим доступные нам точки восстановления; это было задание резервного копирования, которое мы выполнили на первичном кластере для нашего приложения Pac-Man.
Я не буду менять никаких настроек по умолчанию, так как хочу рассмотреть это более подробно на следующем занятии.
Когда вы нажмете кнопку \u0026ldquo;Восстановить\u0026rdquo;, появится запрос на подтверждение.
Ниже мы видим, что мы находимся в резервном кластере, и если мы проверим наши pods, мы увидим, что у нас есть наше запущенное приложение.
Затем мы можем перенаправить порт (в реальной жизни/производственной среде вам не понадобится этот шаг для доступа к приложению, вы будете использовать ingress)
Далее мы рассмотрим мобильность и трансформацию приложений.
Ресурсы Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility `,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day89/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/day90/":{title:"90. Мобильность данных и приложений",tags:["devops"],content:`Мобильность данных и приложений День 90 из #90DaysOfDevOps Challenge! В этой заключительной сессии я собираюсь рассказать о мобильности наших данных и приложений. Я сосредоточусь конкретно на Kubernetes, но потребность в мобильности между платформами и между платформами - это то, что является постоянно растущей потребностью и встречается на практике.
Сценарий использования таков: \u0026ldquo;Я хочу переместить рабочую нагрузку, приложение и данные из одного места в другое\u0026rdquo; по разным причинам, будь то стоимость, риск или предоставление бизнесу более качественных услуг.
На этом занятии мы возьмем нашу рабочую нагрузку и рассмотрим перемещение рабочей нагрузки Kubernetes с одного кластера на другой, но при этом мы изменим то, как наше приложение находится в целевом месте.
Фактически, здесь используются многие характеристики, которые мы рассмотрели в статье Аварийное восстановление
Требование Наш текущий кластер Kubernetes не справляется со спросом, а наши затраты стремительно растут, поэтому мы хотим переместить наш производственный кластер Kubernetes в место аварийного восстановления, расположенное в другом публичном облаке, которое обеспечит возможность расширения, но при этом будет дешевле. Мы также сможем воспользоваться некоторыми собственными облачными сервисами, доступными в целевом облаке.
Наше текущее критически важное приложение (Pac-Man) имеет базу данных (MongoDB) и работает на медленном хранилище, мы хотели бы перейти на новый более быстрый уровень хранения.
Текущий фронтенд Pac-Man (NodeJS) не очень хорошо масштабируется, и мы хотели бы увеличить количество доступных стручков в новом месте.
Приступаем к ИТ У нас есть бриф, и на самом деле мы уже импортировали наши импорты в кластер Disaster Recovery Kubernetes.
Первое, что нам нужно сделать, это удалить операцию восстановления, которую мы выполнили в день 89 для тестирования Disaster Recovery.
Мы можем сделать это с помощью команды kubectl delete ns pacman на \u0026ldquo;резервном\u0026rdquo; кластере minikube.
Чтобы начать работу, зайдите в Kasten K10 Dashboard, выберите карточку Applications. Из выпадающего списка выберите \u0026ldquo;Удаленные\u0026rdquo;
Затем мы получим список доступных точек восстановления. Мы выберем ту, которая доступна, так как она содержит важные данные. (В этом примере у нас только одна точка восстановления).
Когда мы работали над процессом аварийного восстановления, мы оставили все по умолчанию. Однако эти дополнительные опции восстановления существуют, если у вас есть процесс Disaster Recovery, который требует преобразования вашего приложения. В данном случае нам требуется изменить хранилище и количество реплик. Выберите опцию \u0026ldquo;Применить преобразования к восстановленным ресурсам\u0026rdquo;.
Так получилось, что два встроенных примера преобразования, которые мы хотим выполнить, соответствуют нашим требованиям.
Первое требование заключается в том, что на нашем основном кластере мы использовали класс хранения под названием csi-hostpath-sc, а в нашем новом кластере мы хотим использовать standard, поэтому мы можем сделать это изменение здесь.
Выглядит хорошо, нажимаем кнопку create transform внизу.
Следующее требование заключается в том, что мы хотим масштабировать развертывание нашего фронтенда Pac-Man до \u0026ldquo;5\u0026rdquo;
Если вы следите за развитием событий, вы должны увидеть оба наших преобразования, как показано ниже.
Теперь вы можете видеть на изображении ниже, что мы собираемся восстановить все артефакты, перечисленные ниже, если бы мы захотели, мы могли бы также детализировать то, что мы хотим восстановить. Нажмите кнопку \u0026ldquo;Восстановить\u0026rdquo;
Снова нам будет предложено подтвердить действия.
И последнее, что мы покажем, если мы вернемся в терминал и посмотрим на наш кластер, вы увидите, что у нас теперь 5 стручков для стручков pacman и наш класс хранения теперь установлен на стандартный, а не на csi-hostpath-sc
Существует множество различных вариантов, которые могут быть достигнуты с помощью трансформации. Это может охватывать не только миграцию, но и аварийное восстановление, скрипты типа тестирования и разработки и т.д.
API и автоматизация Я не говорил о возможности использовать API и автоматизировать некоторые из этих задач, но эти опции присутствуют, и во всем пользовательском интерфейсе есть хлебные крошки, которые предоставляют наборы команд для использования API для задач автоматизации.
Важно отметить, что при развертывании Kasten K10 развертывается внутри кластера Kubernetes и затем может быть вызван через API Kubernetes.
На этом мы завершаем раздел о хранении и защите данных.
Ресурсы Kubernetes Backup and Restore made easy! Kubernetes Backups, Upgrades, Migrations - with Velero 7 Database Paradigms Disaster Recovery vs. Backup: What\u0026rsquo;s the difference? Veeam Portability \u0026amp; Cloud Mobility Закрытие Заканчивая эту задачу, я хочу продолжить просить об обратной связи, чтобы убедиться, что информация всегда актуальна.
Я также ценю, что есть много тем, которые я не смог охватить или не смог глубже погрузиться в тему DevOps.
Это означает, что мы всегда можем предпринять еще одну попытку в следующем году и найти еще 90 дней контента и прохождений для работы.
Что дальше? Во-первых, немного отдохнем от писанины, я начал этот вызов 1 января 2022 года и закончил 31 марта 2022 года в 19:50 BST! Это был тяжелый труд. Но, как я говорю и говорил уже давно, если этот контент поможет одному человеку, то всегда стоит учиться публично! У меня есть несколько идей, куда двигаться дальше, и я надеюсь, что у него будет жизнь за пределами репозитория GitHub, и мы сможем рассмотреть возможность создания электронной книги и, возможно, даже физической книги.
Я также знаю, что нам нужно пересмотреть каждый пост и убедиться, что все грамматически правильно, прежде чем делать что-то подобное. Если кто-то знает о том, как использовать формат markdown для печати или создания электронной книги, я буду очень признателен за ответ.
Как всегда, продолжайте обсуждать вопросы и PR.
Спасибо!
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/day90/"},"https://romankurnovskii.com/ru/docs/disser/utils/text_2_short/":{title:"Генерация аннотации",tags:[],content:" Сгенерировать ",url:"https://romankurnovskii.com/ru/docs/disser/utils/text_2_short/"},"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/ec2/":{title:"EC2",tags:["aws","ec2"],content:`Amazon EC2 Документация Amazon EC2 - 1 Документация Amazon EC2 - 2 Amazon Elastic Compute Cloud (EC2) - одна из самых популярных служб AWS. EC2 позволяет запускать различные типы облачных экземпляров и оплачивать их по модели \u0026ldquo;оплата за использование\u0026rdquo;. EC2 позволяет контролировать вычислительные ресурсы на уровне операционной системы, работая в вычислительной среде Amazon.
Цены Актуальный прайс
Практика Создание EC2 инстанса Заходим на страницу EC2 -\u0026gt; Launch Instance
Образ EC2 Выбираем нужный нам образ Создание ключей Создадим ключ, чтобы использовать его для подключения к инстансу извне Вводим любое имя. Остальные параметры оставляю по умолчанию После создания ключа начнется автоматическое скачивание. Ключ понадобится далее для подключения к EC2 с локального терминала
Сетевые настройки В разделе Network Settings оставляю включенным Allow SSH traffic from Создание Нажимаем Launch Instance
Инстанс создан и доступен для подключения
Подключение к EC2 с терминала Подключимся к EC2 с локального терминала
Перенесем созданный и скачанный ранее ключ mykey в папку home текущего пользователя и дадим права на файл CHMOD 400
cd ~ cd Downloads/ mv mykey.pem $HOME cd .. chmod 400 mykey.pem Для подключения нам необходим публичный iPv4 адрес. Находим на странице с инстансом
Подключаемся с помощью команды ssh
ssh -i mykey.pem ec2-user@52.24.109.78 Вопросы Вопрос 1 A company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.
Which approach meets these requirements?
A) Store the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI. B) Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials. C) Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances. D) Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials Ответ Правильный ответ: D D – AWS Secrets Manager helps to protect the credentialsneeded to access databases, applications,services, and other IT resources. The service enables users to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to the Secrets Manager APIs, eliminating the need to hardcode sensitive information in plaintext. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.
`,url:"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/ec2/"},"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/elasticbeanstalk/":{title:"Elastic Beanstalk",tags:["aws","Elastic Beanstalk"],content:`AWS Elastic Beanstalk Документация AWS Elastic Beanstalk Документация AWS Elastic Beanstalk Цены Дополнительная плата за AWS Elastic Beanstalk не взимается. Оплате подлежат только ресурсы AWS, необходимые для хранения и работы приложений.
Практика Контролируемое развертывание с AWS Elastic Beanstalk Ссылка на лабораторную работу В этой лабораторной работе развернем несколько обновлений версий приложения в среде с балансировкой нагрузки и автоматическим масштабированием.
Первое обновление развертывается с помощью простого развертывания. Второе обновление развертывается с помощью blue-green развертывания, когда создается отдельная среда для запуска новой версии приложения, а DNS свитчер переключает входящий трафик на новую среду.
Итоговая архитектура развертывания будет выглядеть следующим образом Загрузка приложения В данном обзоре я использую код, который предоставил мне Cloudacademy, но у меня есть готовый скрипт для запуска, который вы можете загрузить в Elastic Beanstalk: скачать
Создание Заходим на страницу Elastic Beanstalk и нажимаем Create Application Название Указываем название нового приложения Выбор платформы В разделе Platform выбираем нужную платформу приложения. В нашем случае - Node.js Загрузка исходников В разделе Source code origin указываем версию приложения и загружаем архив с приложением. Пример
Конфигурация приложения Изменяем предустановку Configuration на Custom configuration: Нажимаем Edit в разделе Rolling updates and deployments
В конфигурации по умолчанию обновления распространяются на все экземпляры одновременно. Это приводит к простою приложения, что неприемлемо для производственных сред.
Мы установим Rolling и Batch size 30% Сеть Вернувшись к основной форме приложения, нажмите Edit в конфигурации Network.
На форме Modify network настроим следующие значения, затем Save. VPC: Выберите VPC с блоком CIDR 10.0.0.0/16. Это не будет VPC по умолчанию. Load balancer settings: Load balancer subnets: Выберите подсети с блоками CIDR **10.0.100.0/24 **(us-west-2a)и 10.0.101.0/24 (us-west-2b). Это публичные подсети. Балансировщику нагрузки приложений требуется как минимум две подсети в разных зонах доступности Instance settings: Instance subnets: Выберите подсеть с блоком CIDR 10.0.1.0/24. Это частная подсеть. Подтверждение Нажимаем Create app
Процесс создания приложения занимает от 5 минут.
Далее переходим в раздел Dasboard На этом этап загрузки приложения в Elastic Beanstalk закончен. Далее разберем как переключать загрузку новой версии приложения клиентам.
Загрузка 2-й версии приложения Загрузка версии 2.0 Нажимаем Upload and deploy и загружаем обновленный код. _Например, можно том же исходнике изменить текст для сравнения. Указываем новую версию и настройки публикации Сравнение версий Теперь можем сверить обе версии, пройдя по ссылкам. В моем случае приложения выглядят следующим образом Смена url у приложений Теперь поменяем приложения местами. Чтобы пользователь, который ранее заходил по одному адресу, теперь видел 2-ю версию приложения.
В разделе Actions нажимаем Swap environment URLs и далее выбираем приложение с которым происходит смена Удаление ресурсов Elastic Beanstalk Elastic Beanstalk для развертывания приложений запускает EC2 инстансы, а также прочие сервисы. Но удалить все сервисы можно из одного окна.
Идем в раздел Applications Выбираем приложение Нажимаем Actions -\u0026gt; Terminate environment Ресурсы https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html
`,url:"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/elasticbeanstalk/"},"https://romankurnovskii.com/ru/":{title:"Roman Kurnovskii",tags:[],content:"",url:"https://romankurnovskii.com/ru/"},"https://romankurnovskii.com/ru/posts/rugpt-3-notes/":{title:"ChatGPT/ruGPT-3",tags:[],content:" https://chat.openai.com/chat/ https://russiannlp.github.io/rugpt-demo/ Краткий экскурс в ruGPT-3. Инструкция и демонстрация ",url:"https://romankurnovskii.com/ru/posts/rugpt-3-notes/"},"https://romankurnovskii.com/ru/authors/":{title:"Authors",tags:[],content:"",url:"https://romankurnovskii.com/ru/authors/"},"https://romankurnovskii.com/ru/docs/disser/canditate-minimum/languages-requirements/":{title:"Требования по иностранным языкам",tags:[],content:`Необходимые документы:
Заявление на имя заведующего кафедрой. В з-и указать название и шифр специальности Монография на иностранном языке. Реферат на русском языке (объем 21-8 стр) по прочитанной лит-ре объемом 300 стр. Реферат, подписанный автором, должен иметь заключения, а также библиографию (список использованной литературы). Глосарий (словарь специальных терминов) - ен менее 300 единиц. Отзыв от научного руководителя или специалиста по данной дисциплине. Содержание экзамена:
Вышеуказанные документы сдаются н а кафедру иностраных яызков за 10 дней до экзамена. Чтение, перевод со (словарем) на руский язык оп специальности оригинального текста. Объем 2500 печ знаков. Время на подготовку 45 мин. Форма проверки - чтение части текста вслух, выборочная проверка подготовленного перевода (Если не выполнен минимум 2тыс знаков - экзамен не продолжается). Чтение (просмотровое без словаря) оригинального газетного публицистического текста по специальности. Объем 2500 печ знаков. Время на подготовку - 5 мин. Форма проверки - передача содержания текста на русском языке (реферирование). Чтение оригинального газетно-публицистического текста без словаря. Объем - 2500 печатных знаков. Время н а подготовку - 15-20 мин. Форма проверки - передача содержания текста на иностранном языке и беседа на иностранном языке по прочитанному тексту. Беседа на иностранном по вопросам, связынным со специальностью и научной работой аспиранта (защита реферата по теме исследования; требования к реферату см. выше) Примечание: вышеуказанные документы иностранных языков за 14 дней до экзамена тексту.
`,url:"https://romankurnovskii.com/ru/docs/disser/canditate-minimum/languages-requirements/"},"https://romankurnovskii.com/ru/categories/":{title:"Categories",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/"},"https://romankurnovskii.com/ru/categories/roadmaps/":{title:"Roadmaps",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/roadmaps/"},"https://romankurnovskii.com/ru/docs/disser/":{title:"Диссертация",tags:[],content:`План Публикационный профиль Следует вовремя обновлять идентификаторы автора (ORCID, ResearcherID, Scopus AuthorId) Зарегистрироваться как автор:
Science Index Scopus - Профиль создается автоматически при первой публикации в изданиях, индексируемых Scopus https://scienceadmin.rudn.ru/ru/auth/ - (с корпоративной почты @rudn.ru) Ресурсы:
https://esystem.rudn.ru/pluginfile.php/1402513/mod_resource/content/3/_%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%208%20%D0%9F%D1%83%D0%B1%D0%BB%D0%B8%D0%BA%D0%B0%D1%86.%20%D0%BF%D1%80%D0%BE%D1%84%D0%B8%D0%BB%D1%8C%20%D0%B0%D0%B2%D1%82%D0%BE%D1%80%D0%B0.%20%D0%A0%D0%98%D0%9D%D0%A6.%20%D0%A0%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B%20%D0%A0%D0%A3%D0%94%D0%9D%20%281%29.pdf Оформление ссылок / источников литературы ПО, редназначенное для хранения и систематизации библиографических данных, полных текстов документов, а также оформления ссылок в тексте и списков литературы.
Автоматический подбор/выбор стилей цитирования из базы:
Mendeley - Бесплатно - идеально совместим с базами Scopus, ScienceDirect Zotero - Бесплатно - раcпознает публикации с eLibrary Задачи:
• создание личной электронной библиотеки публикаций • хранение и систематизация библиографической информации и полных текстов публикаций • возможность настраивать нужный стиль цитирования (библиографических ссылок) • синхронизация работы со всех устройств • возможность интегрировать библиографические ссылки в текст исследовательской работы в Word
Варианты расположения Алфавитное. Как правило, сначала приводится перечень документов на кириллице, затем на латинице, далее – на других иностранных языках (при наличии)
Систематическое. Источники подбираются по типам документа (сначала – официальные документы, нормативно-правовые акты, затем – учебная и научная литература (монографии, статьи), потом – интернет-ресурсы
Хронологическое: применяют, как правило, в исследованиях историографического плана, посвященных развитию науки, проблемы и т.д. В пределах каждого года записи дают в алфавитном порядке
Комбинированное. например, внутри систематического списка источники располагаются в алфавитном порядке
Официальные документы ставятся в начале списка и располагаются по значимости
Библиографическая ссылка оформляется в соответствии с ГОСТ Р 7.0.5-2008 и ГОСТ Р 7.0.108-2022
Зарубежные стили цитирования Требования к оформлению статей в зарубежных изданиях, как правило, основаны на руководствах (Manuals of Styles) различных профессиональных сообществ.
Наиболее распространены следующие стили:
Chicago - используется для социально-научных публикаций и в большинстве исторических журналов Vancouver - популярен в медицине и физических науках APA (American Psychological Association) Style - используется для оформления публикаций по психологии, образованию, социальным наукам MLA (Modern Language Association) Style - используется при написании работ в области гуманитарных наук Institute of Electrical \u0026amp; Electronics Engineers Style (IEEE) Style - широко используется во всех областях техники, компьютерных наук и других технологических областях Библиографические ссылки служат для выделения в тексте цитат При цитировании наибольшего внимания должна заслуживать современная литература и первоисточники.
Возможны два способа цитирования:
прямое — в этом случае в кавычках дословно повторяется текст из соответствующего источника (в ссылке на источник через запятую необходимо указать страницу, на которой находится цитируемый текст); косвенное — когда одна или несколько мыслей из источника излагаются автором своии словами, но близко к оригинальному тексту. Все источники транслитерируются с помощью сайта http://translit-online.ru/.
Источники https://esystem.rudn.ru/pluginfile.php/1402512/mod_resource/content/5/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F%207%20%D0%9E%D1%84%D0%BE%D1%80%D0%BC%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BF%D1%83%D0%B1%D0%BB%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D0%B8.pdf `,url:"https://romankurnovskii.com/ru/docs/disser/"},"https://romankurnovskii.com/ru/docs/disser/articles-notes/":{title:"Написание статей",tags:[],content:`В аннотацию прописывать предложения, которые могут быть запрошены как поисковый запрос в поисковиках. Анотации индексируются поисковиками яндекс, гугл.
https://www.semanticscholar.org/ https://www.researchgate.net/ После выхода в сборнике:
Добавить на сайт pdf из журнала Добавить в ResearchGate IMF Report: September 16, 2022: West Bank and Gaza: Report to the AD HOC Liaison Committee
Конференции https://sciencen.org/konferencii/grafik-konferencij/ https://na-konferencii.ru/ https://konferencii.ru/ https://www.kon-ferenc.ru/econom.html https://www.hse.ru/science/HSEconf `,url:"https://romankurnovskii.com/ru/docs/disser/articles-notes/"},"https://romankurnovskii.com/ru/docs/disser/canditate-minimum/":{title:"Кандидатский минимум 08.00.14 «Мировая экономика»",tags:[],content:` РУДН - https://econ.rudn.ru/general_information/cathedras/ekonomicheskih_otnosheniy/kandidatskiy_minimum_2016_vesna_osen_voprosy/ ВШЭ - https://we.hse.ru/phd_programm МГИМО - https://mgimo.ru/upload/2021/04/progr_kand-ehkz-08-00-14-meo-i-ves.pdf Перечень вопросов, выносимых на кандидатский минимум по специальности 08.00.14 Мировая экономика Раздел 1. Экономическая теория.
Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле. Адам Смит: теория абсолютного преимущества в торговле. Давид Рикардо: теория сравнительного преимущества в торговле. Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары. Раздел 2. Международная торговля.
Понятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России. Особенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры. Сущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием. Место и роль России в международной торговле. Конкурентные преимущества России. Стоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры. Международный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами. 3. Международная торговая политика. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)
Политика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики. Таможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение. Нетарифные барьеры в международной торговле. Всемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров. Россия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО. Система внешнеторгового регулирования в ЕАЭС и в России. Раздел 4. Международное движение капитала. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Формы, структура и масштабы международного движения капитала. Масштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны. Международная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций. Раздел 5. Международный валютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)
Понятие иностранной валюты. Валютный курс и паритет покупательной способности валюты. Мировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития. Виды операций на валютном рынке. Хеджирование валютных рисков. Факторы, влияющие на формирование валютного курса. Раздел 6. Эволюция мировой валютной системы.
Структурные принципы и функции мировой валютной системы. Механизм золотого стандарта. Бреттон-Вудская валютная система: основные принципы построения. Роль и функции МВФ. Причины и особенности кризиса Бреттон-Вудской системы. Принципы Ямайской валютной системы. Стандарт СДР. Современные режимы валютных курсов. Роль доллара и евро в современной валютной системе. Европейская валютная система: этапы создания и структурные принципы. Развитие еврозоны. Либерализация валютной политики России. Раздел 7. Евровалютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)
Основные характеристики рынка евровалют: размеры, валютная структура, виды операций, процентные ставки. Основные характеристики рынка еврооблигаций: валютная структура, основные эмитенты, виды облигаций. Россия на рынке еврооблигаций Раздел 8. Внешняя задолженность.(ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Внешняя задолженность как общемировая проблема. Особенности внешней задолженности развитых и развивающихся стран. Долговой характер экономики США: показатели, структура, динамика и способы урегулирования американской внешней задолженности. Роль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран. Внешний долг России: динамика, структура и особенности управления. Задолженность иностранных государств перед Россией. Раздел 9. Международная трудовая миграция.(МАНЬШИН РОМАН ВЛАДИМИРОВИЧ)
Основные направления и структура международной трудовой миграции. Международная трудовая миграция в России. Раздел 10. Региональная экономическая интеграция. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)
Характеристика этапов экономической интеграции (на примере Европейского союза). Особенности интеграционные процессов на постсоветском пространстве. Современные цели, направления, проблемы и перспективы развития интеграционных процессов в ЕАЭС. НАФТА (ЮСМКА): особенности интеграции и влияние на экономики стран-участниц. Особенности интеграционных процессов в Азии (на примере АСЕАН). Особенности интеграционных процессов в Африке (на примере Западноафриканского экономического и валютного союза). Раздел 11. Международные корпорации (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)
Роль ТНК в мировой экономике. Показатели транснационализации и рейтинг крупнейших финансовых и нефинансовых ТНК мира. Особенности ТНК развивающихся стран (на примере Китая). Деятельность зарубежных ТНК в экономике современной России. Раздел 12. Международные финансовые организации. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)
МВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях. Группа Всемирного Банка: цели, направления, динамика и география предоставления финансирования. Роль региональных банков развития в финансировании развивающихся стран. Раздел 13. Свободные экономические зоны.
Свободные экономические зоны в мировой экономике: эволюция и современное состояние. Особые экономические зоны в РФ как фактор привлечения иностранных инвестиций. Раздел 14. Платежный баланс. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Понятие платежного баланса и основные принципы его составления. Классификация статей платежного баланса по методике МВФ. Состояние платежного баланса России в 2020 г. Основные факторы, определяющие платежный баланс. Раздел 15. Внешнеэкономическая безопасность. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)
Внешнеэкономическая безопасность: характер и типология угроз, показатели и их пороговые значения для России. Внешнеэкономическая безопасность: механизмы обеспечения национальных интересов в странах и регионах мира (на примере США и РФ). Экономические санкции как механизм реализации национальных интересов в международных экономических отношениях. Раздел 1. Экономическая теория. ✔️ Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле. ✔️ Адам Смит: теория абсолютного преимущества в торговле. ✔️ Давид Рикардо: теория сравнительного преимущества в торговле. Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары. Раздел 2. Международная торговля ✔️ Понятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России. Раздел 3. Международная торговая политика Раздел 4. Международное движение капитала Раздел 5. Международный валютный рынок Раздел 6. Эволюция мировой валютной системы Раздел 7. Евровалютный рынок Раздел 8. Внешняя задолженность Роль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран. Раздел 9. Международная трудовая миграция Раздел 10. Региональная экономическая интеграция Раздел 11. Международные корпорации Раздел 12. Международные финансовые организации МВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях. Раздел 13. Свободные экономические зоны Раздел 14. Платежный баланс Раздел 15. Внешнеэкономическая безопасность Раздел 2. Международная торговля.
Понятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России. Особенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры. Сущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием. Место и роль России в международной торговле. Конкурентные преимущества России. Стоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры. Международный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами. Раздел 3. Международная торговая политика. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)
Политика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики. Таможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение. Нетарифные барьеры в международной торговле. Всемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров. Россия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО. Система внешнеторгового регулирования в ЕАЭС и в России. Раздел 4. Международное движение капитала. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Формы, структура и масштабы международного движения капитала. Масштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны. Международная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций. Раздел 5. Международный валютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)
Понятие иностранной валюты. Валютный курс и паритет покупательной способности валюты. Мировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития. Виды операций на валютном рынке. Хеджирование валютных рисков. Факторы, влияющие на формирование валютного курса. Раздел 6. Эволюция мировой валютной системы.
Структурные принципы и функции мировой валютной системы. Механизм золотого стандарта. Бреттон-Вудская валютная система: основные принципы построения. Роль и функции МВФ. Причины и особенности кризиса Бреттон-Вудской системы. Принципы Ямайской валютной системы. Стандарт СДР. Современные режимы валютных курсов. Роль доллара и евро в современной валютной системе. Европейская валютная система: этапы создания и структурные принципы. Развитие еврозоны. Либерализация валютной политики России. Раздел 7. Евровалютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)
Основные характеристики рынка евровалют: размеры, валютная структура, виды операций, процентные ставки. Основные характеристики рынка еврооблигаций: валютная структура, основные эмитенты, виды облигаций. Россия на рынке еврооблигаций Раздел 8. Внешняя задолженность.(ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Внешняя задолженность как общемировая проблема. Особенности внешней задолженности развитых и развивающихся стран. Долговой характер экономики США: показатели, структура, динамика и способы урегулирования американской внешней задолженности. Роль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран. Внешний долг России: динамика, структура и особенности управления. Задолженность иностранных государств перед Россией. Раздел 9. Международная трудовая миграция.(МАНЬШИН РОМАН ВЛАДИМИРОВИЧ)
Основные направления и структура международной трудовой миграции. Международная трудовая миграция в России. Раздел 10. Региональная экономическая интеграция. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)
Характеристика этапов экономической интеграции (на примере Европейского союза). Особенности интеграционные процессов на постсоветском пространстве. Современные цели, направления, проблемы и перспективы развития интеграционных процессов в ЕАЭС. НАФТА (ЮСМКА): особенности интеграции и влияние на экономики стран-участниц. Особенности интеграционных процессов в Азии (на примере АСЕАН). Особенности интеграционных процессов в Африке (на примере Западноафриканского экономического и валютного союза). Раздел 11. Международные корпорации (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)
Роль ТНК в мировой экономике. Показатели транснационализации и рейтинг крупнейших финансовых и нефинансовых ТНК мира. Особенности ТНК развивающихся стран (на примере Китая). Деятельность зарубежных ТНК в экономике современной России. Раздел 12. Международные финансовые организации. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)
МВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях. Группа Всемирного Банка: цели, направления, динамика и география предоставления финансирования. Роль региональных банков развития в финансировании развивающихся стран. Раздел 13. Свободные экономические зоны.
Свободные экономические зоны в мировой экономике: эволюция и современное состояние. Особые экономические зоны в РФ как фактор привлечения иностранных инвестиций. Раздел 14. Платежный баланс. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)
Понятие платежного баланса и основные принципы его составления. Классификация статей платежного баланса по методике МВФ. Состояние платежного баланса России в 2020 г. Основные факторы, определяющие платежный баланс. Раздел 15. Внешнеэкономическая безопасность. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)
Внешнеэкономическая безопасность: характер и типология угроз, показатели и их пороговые значения для России. Внешнеэкономическая безопасность: механизмы обеспечения национальных интересов в странах и регионах мира (на примере США и РФ). Экономические санкции как механизм реализации национальных интересов в международных экономических отношениях. Раздел 1. Экономическая теория. (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)
✔️ Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле. Меркантилизм (15-17 вв) - это экономическая политика, цель которой — накопление в стране драгоценных металлов, средство достижения цели – активный торговый баланс, то есть превышение экспорта над импортом.
Необходимость активного вмешательства государства в хозяйственную деятельность, в основном в форме протекционизма: установления высоких импортных пошлин, выдачи субсидий национальным производителям и так далее. 1. Ранний меркантилизм (конец XV — середина XVI века).
Представители: У. Стаффорд, Де Сантис, Г. Скаруффи. В этот период в учении преобладает теория денежного баланса, в рамках которой было закреплено увеличение национального благосостояния законодательным путем: устанавливался запрет на вывоз золота и серебра за границу. Деньги выполняли только функцию средства накопления.
2. Поздний меркантилизм (вторая половина XVI — начало XVII века).
Представители: Т. Ман, А. Серра, А. де Монкретьен.
Ими была создана теория торгового баланса, который обеспечивался путем активной внешней торговли. Главенствовал принцип: покупать дешевле в одной стране и продавать дороже в другой. Вывоз денежных средств за границу был разрешен. Деньгам отводились функции средства накопления и средства обращения — поздний меркантилизм трактовал деньги как капитал и признавал их товаром.
Поздний меркантилизм был прогрессивным. Он содействовал развитию торговли, судостроения, экспортной промышленности, международного разделения труда.
Основные принципы:
— регулирование внешней торговли с целью притока в страну золота и серебра; — поддержка промышленности путем импорта дешевого сырья; — протекционизм; — поощрение экспорта готовой продукции; — рост населения для поддержания низкого уровня зарплаты; — рассмотрение проблем сферы обращения в отрыве от сферы производства; — достижение экономического роста путем приумножения денежного богатства страны через государственное регулирование внешней торговли и достижение положительного сальдо торгового баланса.
Преобладал в странах Западной Европы (преимущественно Франции, Италии и Англии). В России одним из приверженцев идей меркантилизма был выдающийся государственный деятель А. Л. Ордын-Нащекин (1605—1680).
Игра с нулевой суммой — это противоположность беспроигрышным ситуациям — таким как торговое соглашение, которое значительно увеличивает торговлю между двумя странами — или проигрышным ситуациям, таким как война, например. В реальной жизни, однако, не всегда все так очевидно, и зачастую сложно измерить прибыли и убытки.
Игра с нулевой суммой — это ситуация, когда, если одна сторона проигрывает, другая сторона выигрывает, а чистое изменение богатства равно нулю.
Источники:
https://www.banki.ru/wikibank/merkantilizm/ Меркантелизм / годы /страны ✔️ Адам Смит: теория абсолютного преимущества в торговле. Исследование о природе и причинах богатства народов (1776 г.) - основная работа шотландского экономиста Адама Смита.
А. Смит (1723—1790) распространил и на мирохозяйственную сферу, впервые теоретически обосновав принцип абсолютных преимуществ (или абсолютных издержек)
«Основное правило каждого благоразумного главы семьи состоит в том, чтобы не пытаться изготовить дома такие предметы, изготовление которых обойдется дороже, чем при покупке их на стороне\u0026hellip; То, что представляется разумным в образе действия любой частной семьи, вряд ли может оказаться неразумным для всего королевства. Если какая-либо чужая страна может снабдить нас каким-нибудь товаром по более дешевой цене, чем мы в состоянии изготовить его, гораздо лучше покупать его у нее на некоторую часть продукта нашего собственного промышленного труда, прилагаемого в той области, в которой мы обладаем некоторым преимуществом»
Основой развития международной торговли служит различие в абсолютных издержках. Торговля будет приносить экономический эффект, если товары будут ввозиться из страны, где издержки абсолютно меньше, а вывозиться те товары, издержки которых в данной стране ниже, чем за рубежом.
Благосостояние наций зависит не столько от количества накопленного ими золота, сколько от их способностей производить конечные товары и услуги.
Основные положения А.Смита в теории международной торговли:
правительствам не следует вмешиваться во внешнюю торговлю, поддерживая режим открытых рынков и свободы торговли; нации, так же как и частные лица, должны специализироваться на изготовлении товаров, в производстве которых у них есть абсолютные преимущества, и торговать ими в обмен на товары, абсолютным преимуществом в производстве которых обладают другие нации; концентрация усилий (ресурсов) стран на производстве товаров, по которым страны имеют абсолютное преимущество, приводит к увеличению общих объемов производства, росту обмена между странами продуктами своего труда; свободная торговля между странами обусловливает эффективное распределение мировых ресурсов, обеспечивая прибыль любой и каждой торгующей стране. ✔️ Давид Рикардо: теория сравнительного преимущества в торговле. Теория сформулированна Давидом Рикардо (1772-1823) (классик политической экономии, последователь и одновременно оппонент Адама Смита) в начале XIX века.
Давид Рикардо развил теорию абсолютных преимуществ Адама Смита и показал, что торговля выгодна каждой из двух стран, даже если одна из них не обладает абсолютным преимуществом в производстве любых конкретных товаров. Специализация на производстве товара, имеющего максимальные сравнительные преимущества, выгодна, даже если нет абсолютных преимуществ.
Теория сравнительных преимуществ на примере двух стран и двух товаров
Временные затраты на производство единицы товара:
Сыр (в ед. Вина) Вино (в ед. Сыра) Франция 2 1 Испания 4 3 В данном случае во Франции затраты времени в производстве обоих товаров меньше (она обладает абсолютным преимуществом). Согласно А. Смиту, торговля между странами принесёт выгоды только Франции. Однако, с точки зрения теории сравнительных преимуществ Д. Рикардо, при определённом соотношении цен между товарами, торговля может приводить к взаимной выгоде обеих стран даже при абсолютном преимуществе только одной из них.
Рассчитаем альтернативные цены производства каждого из товаров в каждой стране:
Альтернативная цена производства единицы товара:
Сыр (в ед. Вина) Вино (в ед. Сыра) Франция 2 / 1 1 / 2 Испания 4 / 3 3 / 4 В данном случае одна единица сыра (например, килограмм) во Франции стоит 2 единицы вина (2 бутылки), а в Испании единица сыра стоит дешевле (4 / 3 единицы вина). В то же время единица вина в Испании стоит 3 / 4 единицы сыра, что дороже чем во Франции. Таким образом, если Франция будет производить вино для Испании, а Испания — сыр для Франции, то обе страны выиграют трудовые ресурсы. На каждой закупленной единице сыра Франция будет экономить 2 - 4 /3 = 2/3 единицы вина, а Испания 3/4-1/2=1/4 единицы сыра на каждой закупленной единице вина.
Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары. Теория Хекшера — Олина (теория соотношения факторов производства) - каждая страна экспортирует товары, для производства которых она обладает относительно избыточными факторами производства, и импортирует товары, для производства которых она испытывает относительный недостаток факторов производства.
Источники:
https://studfile.net/preview/9266193/page:7/
Раздел 2. Международная торговля БЕЛОВА ИРИНА НИКОЛАЕВНА
✔️ Понятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России. Статья макроэкономика. Вопрос 3 Статистика по нешней торговле РФ 2021 Открытая национальная экономика (полностью открытое хозяйство) характеризуется полностью открытыми внутренними рынками природных ресурсов, товаров, услуг, капиталов, рабочей силы, идей, информации. Такая экономика способна обеспечить углубление специализации и кооперации в национальной экономике, рост ее конкурентоспособности за счет постоянного соперничества с иностранными фирмами на внутреннем рынке, использование позитивного мирового опыта через систему международных экономических отношений, эффективное использование принципа сравнительных преимуществ в международном разделении труда.
Критерии:
факт наличия влияния внешней среды на динамику основных показателей национального экономического развития, а именно: на объем и темпы роста производства, состояние внутренних товарных рынков, занятости населения. В настоящее время стремление к открытости национальных экономик в большей степени обусловливается объективными процессами интернационализации и глобализации производства, обмена, капиталов, потребления, чем экспансией американских корпораций.
Степень открытости экономики
Рассчитывается как показатель экспортной квоты, который понимается как отношение объема экспорта (за год) к ВВП в следующей формуле: Эк = Экспорт / ВВП * 100%. Чем больше показатель Эк, тем более высока степень открытости экономики. показатель импортной квоты. Рассчитывается как отношение объема импорта (за год) к ВВП. Ик = Импорт / ВВП * 100% Наиболее точный показатель - внешнеторговой квоты. отношение суммы объемов экспорта и импорта страны (за год) к ее ВВП: ВТк = (Экспорт+Импорт) / ВВП * 100%. Чем больше ВТК, тем более открытой является экономика страны Показатели Экспорт+Импорт по странам
###Особенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры. ###Сущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием. ###Место и роль России в международной торговле. Конкурентные преимущества России. ###Стоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры. ##3Международный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами.
Раздел 3. Международная торговая политика Политика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики. Таможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение. Нетарифные барьеры в международной торговле. Всемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров. Россия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО. Система внешнеторгового регулирования в ЕАЭС и в России. Раздел 4. Международное движение капитала Формы, структура и масштабы международного движения капитала. Масштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны. Международная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций. Раздел 5. Международный валютный рынок Понятие иностранной валюты. Валютный курс и паритет покупательной способности валюты. Мировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития. Виды операций на валютном рынке. Хеджирование валютных рисков. Факторы, влияющие на формирование валютного курса. Раздел 6. Эволюция мировой валютной системы Раздел 7. Евровалютный рынок Раздел 8. Внешняя задолженность Роль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран. МВФ Три основные функции МВФ:
Экономический надзор
Предоставление государствам-членам рекомендаций относительно принятия мер политики для достижения макроэкономической стабильности, ускорения экономического роста и уменьшения бедности
Кредитование
Предоставление финансирования государствам-членам, чтобы помочь им в решении проблем платежного баланса, включая ситуации нехватки иностранной валюты, возникающие, когда их внешние платежи превышают их поступления в иностранной валюте
Развитие потенциала
Обеспечение развития потенциала (включая техническую помощь и подготовку кадров), по просьбе государствчленов, для оказания им помощи в укреплении экономических институтов в целях разработки и проведения обоснованной экономической политики.
Раздел 9. Международная трудовая миграция Раздел 10. Региональная экономическая интеграция Раздел 11. Международные корпорации Раздел 12. Международные финансовые организации МВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях. Буклет о МФВ (pdf) Термины (pdf 500стр) Годовой отчет МВФ 2022 (pdf) МВФ — это организация, представляющая 184 страны. Целями егоработы являются укреплениемеждународного сотрудничествав валютно-финансовой сфере, обеспечение финансовой стабильности, развитие международной торговли, содействие высокой занятостии устойчивому экономическому росту, а также сокращение бедности.
Был учрежден на основе международногодоговора в1945 году для содействияоздоровлению мировой экономики.
Одновременно с МВФ был образован Международный банк реконструкции и развития (МБРР), болеешироко известный как Всемирныйбанк, для содействия долгосрочному экономическому развитию, в частности с помощью финансирования инфраструктурных проектов, таких как строительство дорог и улучшение системыводоснабжения.
Цели:
Проводит наблюдение за изменениями в экономической и финансовой ситуации и политикев государствах-членах и на глобальном уровне идает государствам-членам рекомендации по вопросам экономической политики Предоставляет кредиты государствам-членам, испытывающим проблемы платежного баланса Основная деятельность МВФ: макроэкономическая политика и политика в отношении финансового сектора
В центре внимания МВФ находится макроэкономическая политика страны — то есть политика, связанная с государственным бюджетом, регулированием процентных ставок, денежнокредитной сферы и обменного курса, а также политика в отношении финансового сектора, включая регулирование банков и других финансовых учреждений и надзор за ними.
в МВФ существует система взвешенного распределения голосов: чем больше квота государства в МВФ, — определяемая в основном размером его экономики, — тем большимчислом голосов оно располагает.
Кредитные механизмы МВФ:
Договоренности о кредите «стэнд-бай» составляют ядро политики кредитования МВФ. Договоренность о кредите «стэнд-бай» гарантирует государству-члену возможность получения средств по кредиту в пределах установленной суммы, обычно в течение 12–18 месяцев, для урегулирования краткосрочной проблемы платежного баланса. Механизм расширенного кредитования МВФ. Поддержка, оказываемая МВФ государствам-членам в рамках механизма расширенного кредитования, гарантирует государству-члену возможностьполучения средств по кредиту в пределах установленной суммы, обычно в течение трех-четырех лет, с тем чтобы помочь ему в решении экономических проблем структурного характера, которые существенно ослабляют состояние его платежного баланса. Механизм финансирования на цели сокращения бедностии содействия экономическому росту (введенный в ноябре 1999 годана смену механизму расширенного финансирования структурной перестройки). Это механизм кредитования с низкими процентными став-ками, призванный помочь беднейшим государствам-членам, испытывающим долговременные проблемы в области платежного баланса. Проценты для заемщиков субсидируются за счет средств, полученных ранее от продажи принадлежащего МВФ золота, а такжезаймов и грантов, предоставленных МВФ на эти цели государствами-членами. Механизм финансирования дополнительных резервов. Предоставляет дополнительное краткосрочное финансирование государствам-членам, испытывающим чрезвычайные трудности в области платежного баланса вследствие внезапной и дестабилизирующей утраты доверия рынка, проявляющейся в оттоках капитала. Процентная ставка по кредитам СРФ включает надбавку к обычнойставке по кредитам МВФ. Чрезвычайная помощь. Форма помощи, введенная в 1962 годудля содействия государствам-членам в преодолении проблем платежного баланса, вызванных внезапными и непредвиденными стихийными бедствиями, в 1995 году была распространена наситуации, когда государства-члены пережили военные конфликты, подорвавшие их институциональный и административный потенциал Ссылки:
IMF Regional Reports Раздел 13. Свободные экономические зоны Раздел 14. Платежный баланс Раздел 15. Внешнеэкономическая безопасность `,url:"https://romankurnovskii.com/ru/docs/disser/canditate-minimum/"},"https://romankurnovskii.com/ru/series/aws-exam-quizz/":{title:"AWS exam quizz",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/aws-exam-quizz/"},"https://romankurnovskii.com/ru/apps/cloud-exam-quizz/":{title:"Cloud exam Quizz",tags:[],content:`Goal: Check if you are ready to pass Cloud exam
Goal: Check if you are ready to pass the Cloud exam
The application calculates progress after each answered question. Ability to answer at least one question and get a comment at the same time. No need to pass all questions before. It is convenient to spend 20 min a day Works from web/tablet/mobile Link: https://www.cloud-exam-prepare.com
`,url:"https://romankurnovskii.com/ru/apps/cloud-exam-quizz/"},"https://romankurnovskii.com/ru/series/":{title:"Series",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/"},"https://romankurnovskii.com/ru/tags/interactivebrokers/":{title:"interactivebrokers",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/interactivebrokers/"},"https://romankurnovskii.com/ru/tags/invest/":{title:"invest",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/invest/"},"https://romankurnovskii.com/ru/tags/":{title:"Tags",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/"},"https://romankurnovskii.com/ru/posts/interactivebrokers-deposit/":{title:"Пополнение Interactive Brokers с Израильского счета",tags:["interactivebrokers","invest"],content:`Web Создание заявки в IB Заходим на сайт https://www.interactivebrokers.co.uk/portal/#/ Нажимаем Deposit Нажимаем Use a new deposit method если ранее шаблон не был создан Bank Wire -\u0026gt; Get instructions Account Number: Номер банковского счета.
Получаем инструкции с реквизитами для пополнения Bank Wire Instructions Эти данные Вам нужны для оплаты в Discount Bank
Отправить деньги из Discount Bank Заходим в личный кабинет банка start.telebank.co.il Нажимаем: ביצוע העברה
Заполняем форму
Нажимаем המשך и жмем далее. Приходит смс с подверждением, вводим и жмем далее `,url:"https://romankurnovskii.com/ru/posts/interactivebrokers-deposit/"},"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/lambda/":{title:"Lambda",tags:[],content:`https://docs.aws.amazon.com/lambda/?id=docs_gateway
https://aws.amazon.com/lambda/
AWS Lambda – это сервис бессерверных вычислений, который запускает программный код в ответ на определенные события и отвечает за автоматическое выделение необходимых вычислительных ресурсов.
AWS Lambda автоматически запускает программный код в ответ на различные события, такие как HTTP‑запросы через Amazon API Gateway, изменение объектов в корзинах Amazon Simple Storage Service (Amazon S3), обновление таблиц в Amazon DynamoDB или смена состояний в AWS Step Functions.
Поддержка языков Java, Go, PowerShell, Node.js, C#, Python и Ruby
Цены Актуальный прайс
Цена x86
0,0000166667 USD за каждую гигабайт-секунду 0,20 USD за 1 млн запросов Цена Arm
0,0000133334 USD за каждую гигабайт-секунду 0,20 USD за 1 млн запросов Практика В строке поиска Консоли управления AWS введите Lambda и выбираем Lambda в разделе «Services»:
https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#
На странице Functions нажимаем Create a function
Author from scratch is selected and enter the following values in the bottom form:
Function name: MyCustomFunc Runtime: Node.js 16.X Я выбираю этот раздел, потому что использую аккаунт cloudacademy. Данная роль дает разрешение на создание функций
Permissions: Change default execution role Execution Role: Select Use an existing role Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole → Create function
Пишу функцию, чтобы просмотреть лог, добавлю печать в терминал. А также добавлю обработку получаемого сообщения (В следующем шаге в разделе тестирования)
функция принимает в качестве объекта event который содержит массив Records. На 1-й (0) позиции Объект Sns (название сервиса SNS Notifications).
В самом объекте будет 2 значения:
cook_secs - время варки (микроволновки) req_secs - время приготовления console.log('Loading function'); exports.handler = function(event, context) { console.log(JSON.stringify(event, null, 2)); const message = JSON.parse(event.Records[0].Sns.Message); if (message.cook_secs \u0026lt; message.req_secs) { if (message.pre) { context.succeed(\u0026quot;User ended \u0026quot; + message.pre + \u0026quot; preset early\u0026quot;); } else { context.succeed(\u0026quot;User ended custom cook time early\u0026quot;); } } context.succeed(); }; → Deploy
→ Test
Данная функциональность позволяет протестировать как функция реагирует на определенные события. Попробуем добавить событие от SNS Notifications.
Выберем из списка
Получаем шаблон, в котором внесем правки, подправим поле Message - то самое, которое мы будем обрабатывать в нашей функции.
Поле Message- строка, поэтому наш объект надо будет обернуть в кавычки
Чтобы обработчик понимал, что мы ставим кавычки внутри кавычек, необходимо поставить специальный символ \\ перед кавычкой.
В итоге обновляем одну строку и сохраняем → Create
Теперь нажимаем на кнопку Test
Так как cook_secs в нашем евенте был меньше, чем req_secs, то функция распечатала первое условие, а ниже в разделе Function Logs видим сообщение, которые мы распечатываем при инициализации функции Loading function
`,url:"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/lambda/"},"https://romankurnovskii.com/ru/tags/aws/":{title:"AWS",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/aws/"},"https://romankurnovskii.com/ru/categories/aws/":{title:"AWS",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/aws/"},"https://romankurnovskii.com/ru/tags/ec2/":{title:"ec2",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/ec2/"},"https://romankurnovskii.com/ru/categories/ec2/":{title:"ec2",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/ec2/"},"https://romankurnovskii.com/ru/tags/elastic-beanstalk/":{title:"Elastic Beanstalk",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/elastic-beanstalk/"},"https://romankurnovskii.com/ru/categories/elastic-beanstalk/":{title:"Elastic Beanstalk",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/elastic-beanstalk/"},"https://romankurnovskii.com/ru/tags/iam/":{title:"iam",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/iam/"},"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/iam/":{title:"IAM",tags:["aws","iam"],content:`AWS Identity and Access Management Документация AWS IAM Документация AWS IAM AWS Identity and Access Management (IAM) позволяет безопасно контролировать доступ пользователей к службам и ресурсам AWS. Эта услуга предназначена для организаций с множеством пользователей или систем, использующих такие продукты AWS, как Amazon EC2, Amazon RDS и AWS Management Console. С помощью IAM вы можете централизованно управлять пользователями, учетными данными безопасности, такими как ключи доступа, и разрешениями, контролирующими доступ пользователей к ресурсам AWS.
Практика Переходим на страницу IAM
Создание групп IAM На странице User Groups нажимаем Create group
Указываем имя группы. Мое: DevOps Добавляем разрешение на просмотр EC2: AmazonEC2ReadOnlyAccess Create Группа создана
Создание пользователей IAM На странице Users нажимаем Create user Вводим имя пользователя(логин) Permissions Добавляем пользователя в созданную группу Tags Пропускаем раздел или ставим tags. Полезно и популярно устанавливать теги ресурсам в компаниях, где много подключено ресурсов AWS
Логин/Пароль На последнем этапе скачиваем .csv файл с логином, ключами и паролем. Пароль понадобится далее, чтобы войти в систему под данным пользователем. На данной странице имеется ссылка для входа. Ею воспользуемся на следующем шаге Вход новым пользователем Проверка прав У данного пользователя есть доступ на просмотр EC2 инстансов. Проверим наличие/отсутствие доступа к S3 корзинам.
Попробуем создать S3 корзину После попытки создать корзину получаем окно с указанием на отсутствие прав `,url:"https://romankurnovskii.com/ru/docs/aws-certified-developer-associate/iam/"},"https://romankurnovskii.com/ru/apps/npm/cognito-token-observer/":{title:"cognito-token-observer",tags:["npm"],content:"",url:"https://romankurnovskii.com/ru/apps/npm/cognito-token-observer/"},"https://romankurnovskii.com/ru/tags/npm/":{title:"npm",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/npm/"},"https://romankurnovskii.com/ru/categories/npm-packages/":{title:"npm packages",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/npm-packages/"},"https://romankurnovskii.com/ru/apps/npm/hugo-lunr-ml/":{title:"hugo-lunr-ml",tags:["npm"],content:"",url:"https://romankurnovskii.com/ru/apps/npm/hugo-lunr-ml/"},"https://romankurnovskii.com/ru/tags/hugo/":{title:"hugo",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/hugo/"},"https://romankurnovskii.com/ru/categories/hugo/":{title:"Hugo",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/hugo/"},"https://romankurnovskii.com/ru/posts/hugo-add-image-zoomin/":{title:"Увеличение картинки по нажатию в Hugo",tags:["hugo"],content:`Введение В Hugo по умолчанию используется парсинг markdown файлов. Т.е. мы получаем html код в том виде, как он написан в markdown.
Для того, чтобы нам понимать какие именно изображения мы можем увеличивать, добавим к этим изображениям отдельный тег/ключ/id
Инструменты Для реализации функционала нам необходимо:
написать/подключить скрипт/обработчик, который будет выполнять эффект zoomin к нужным нам изображениям Добавить необходимые метаданные к изображениям, чтобы скрипт их смог найти Скрипт zoomin Для добавления возможности увеличивать картинку при нажатии воспользуемся пакетом medium-zoom.
Данный покет реализовывает данную функциональность в ненагруженном удобном стиле.
Демо сайт
Логика скрипта Скрипт находит изображения с id и так понимает, что нужно применить свойство zoomin к этим изображениям
Возможные id:
zoom-default zoom-margin zoom-background zoom-scrollOffset zoom-trigger zoom-detach zoom-center Подключение скриптов Для работы скрипта, нам необходимо подключить логику, а также обработчик.
В Hugo в корне проекта есть папка static, которую можно использовать для хранения статических файлов (стиле, скриптов) и использовать для подключения на сайте. Если такой папки нет, то можно создать.
В папке static создадим папку zoom-image и добавим в нее 2 скрипта
static/js/zoom-image/index.js const zoomDefault = mediumZoom('#zoom-default') const zoomMargin = mediumZoom('#zoom-margin', { margin: 48 }) const zoomBackground = mediumZoom('#zoom-background', { background: '#212530' }) const zoomScrollOffset = mediumZoom('#zoom-scrollOffset', { scrollOffset: 0, background: 'rgba(25, 18, 25, .9)', }) // Trigger the zoom when the button is clicked const zoomToTrigger = mediumZoom('#zoom-trigger') const button = document.querySelector('#button-trigger') button.addEventListener('click', () =\u0026gt; zoomToTrigger.open()) // Detach the zoom after having been zoomed once const zoomToDetach = mediumZoom('#zoom-detach') zoomToDetach.on('closed', () =\u0026gt; zoomToDetach.detach()) // Observe zooms to write the history const observedZooms = [ zoomDefault, zoomMargin, zoomBackground, zoomScrollOffset, zoomToTrigger, zoomToDetach, ] // Log all interactions in the history const history = document.querySelector('#history') observedZooms.forEach(zoom =\u0026gt; { zoom.on('open', event =\u0026gt; { const time = new Date().toLocaleTimeString() history.innerHTML += \`\u0026lt;li\u0026gt;Image \u0026quot;\u0026lt;em\u0026gt;\${event.target.alt }\u0026lt;/em\u0026gt;\u0026quot; was zoomed at \${time}\u0026lt;/li\u0026gt;\` }) zoom.on('detach', event =\u0026gt; { const time = new Date().toLocaleTimeString() history.innerHTML += \`\u0026lt;li\u0026gt;Image \u0026lt;em\u0026gt;\u0026quot;\${event.target.alt }\u0026quot;\u0026lt;/em\u0026gt; was detached at \${time}\u0026lt;/li\u0026gt;\` }) }) static/js/zoom-image/placeholders.js // Show placeholders for paragraphs const paragraphs = [].slice.call(document.querySelectorAll('p.placeholder')) paragraphs.forEach(paragraph =\u0026gt; { // eslint-disable-next-line no-param-reassign paragraph.innerHTML = paragraph.textContent .split(' ') .filter(text =\u0026gt; text.length \u0026gt; 4) .map(text =\u0026gt; \`\u0026lt;span class=\u0026quot;placeholder__word\u0026quot;\u0026gt;\${text}\u0026lt;/span\u0026gt;\`) .join(' ') }) CDN скрипт Скрипт можно скачать, а можно подгружать
Ссылка на скрипт
Добавление в шаблон Для того, чтобы данные скрипты работали в шаблоне сайта, их необходимо подключить.
Я использую для этого шаблон baseof.html. Просто добавляю ссылки на скрипта в body шаблона.
# baseof.html ... \u0026lt;/footer\u0026gt; \u0026lt;script src=\u0026quot;https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;/js/zoom-image/placeholders.js\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;/js/zoom-image/index.js\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ID изображения Hugo позволяет изменить поведение при парсинге markdown файлов с помощью хуков. Подробнее о рендер-хуках можно прочитать на сайте.
В папке *layouts
Добавим файл render-image.html по следующему пути layouts -\u0026gt; _default -\u0026gt; _markup код файла:
\u0026lt;p class=\u0026quot;md__image\u0026quot;\u0026gt; \u0026lt;img src=\u0026quot;{{ .Destination | safeURL }}\u0026quot; id=\u0026quot;zoom-default\u0026quot; alt=\u0026quot;{{ .Text }}\u0026quot; {{ with .Title}} title=\u0026quot;{{ . }}\u0026quot; {{ end }} /\u0026gt; \u0026lt;/p\u0026gt; Мы добавили только id=\u0026quot;zoom-default\u0026quot; в код по умолчанию
Итоги Your browser does not support the video tag. Процесс `,url:"https://romankurnovskii.com/ru/posts/hugo-add-image-zoomin/"},"https://romankurnovskii.com/ru/categories/photos/":{title:"Photos",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/photos/"},"https://romankurnovskii.com/ru/posts/photos/22-07-02-israel-haifa-bahai-gardens/":{title:"Израиль - Хайфа - Бахайские сады",tags:[],content:`Маршрут
`,url:"https://romankurnovskii.com/ru/posts/photos/22-07-02-israel-haifa-bahai-gardens/"},"https://romankurnovskii.com/ru/docs/webrtc/":{title:"Карманная книга по WebRTC",tags:[],content:`Создание нового приложения на базе WebRTC-технологий может стать непосильной задачей, если вы не знакомы с API. В этом разделе мы покажем, как начать работать с различными API в стандарте WebRTC, на большом количестве примеров и фрагментов кода, решающих эти задачи.
WebRTC API Стандарт WebRTC работает с двумя различными технологиями: мультимедиа-устройства и P2P-соединение. Мультимедиа-устройства включают в себя не только камеры и микрофоны, но также и «устройства» захвата экрана. Для камер и микрофонов мы используем navigator.mediaDevices.getUserMedia() для захвата MediaStreams. Для записи же мы используем navigator.mediaDevices.getDisplayMedia().
P2P соединение настраивается через интерфейс RTCPeerConnection. Это ключевой пункт для установления и управления соединением между двумя узлами в WebRTC.
Ресурсы: https://webrtc.org/getting-started/overview https://codelabs.developers.google.com/codelabs/webrtc-web `,url:"https://romankurnovskii.com/ru/docs/webrtc/"},"https://romankurnovskii.com/ru/series/%D1%80%D1%83%D0%BA%D0%BE%D0%B2%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%B0/":{title:"руководства",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/%D1%80%D1%83%D0%BA%D0%BE%D0%B2%D0%BE%D0%B4%D1%81%D1%82%D0%B2%D0%B0/"},"https://romankurnovskii.com/ru/tags/devops/":{title:"devops",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/devops/"},"https://romankurnovskii.com/ru/authors/michael-driscoll/":{title:"michael-driscoll",tags:[],content:"",url:"https://romankurnovskii.com/ru/authors/michael-driscoll/"},"https://romankurnovskii.com/ru/categories/programming/":{title:"programming",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/programming/"},"https://romankurnovskii.com/ru/categories/python/":{title:"Python",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/python/"},"https://romankurnovskii.com/ru/docs/python101/":{title:"Карманная книга по Python",tags:[],content:` Python 101 - адаптированный перевод книги Python 101 с применением новых стандартов
Узнайте, как программировать на Python 3 от начала до конца. Книга Python 101 начинается с основ Python, а затем развивает полученные знания. Аудитория этой книги - в основном люди, которые программировали в прошлом, но хотят изучить Python. В дополнение к материалам для начинающих в книге содержится достаточное количество материала среднего уровня.
Чтобы охватить всю эту информацию, книга разделена на пять частей.
Часть первая Первая часть - это раздел для начинающих. В ней вы изучите все основы языка Python. От типов Python (строки, списки, словари) до условных операторов и циклов. Вы также узнаете о генераторах, функциях и классах и обо всем, что с ними связано!
Часть вторая Эта часть будет представлять собой экскурс в стандартную библиотеку Python. Цель не в том, чтобы рассказать обо всем, а в том, чтобы показать читателю, что с помощью Python можно многое сделать прямо из коробки. Мы рассмотрим модули, которые я считаю наиболее полезными в повседневных задачах программирования, такие как os, sys, logging, threads и другие.
Часть третья Промежуточная часть, охватывающая лямбды, декораторы, свойства, отладку, тестирование и профилирование.
Часть четвертая Теперь все становится действительно интересным! В четвертой части мы научимся устанавливать сторонние библиотеки (т.е. пакеты) из Python Package Index и других мест. Мы рассмотрим easy_install и pip. Эта часть также будет представлять собой серию обучающих уроков, в которых вы узнаете, как использовать загруженные пакеты. Например, вы узнаете, как загрузить файл, разобрать XML, использовать Object Relational Mapper для работы с базой данных и т.д.
Часть пятая В последней части книги мы расскажем о том, как поделиться своим кодом с друзьями и всем миром! Вы узнаете, как упаковать его и поделиться им в Python Package Index (т.е. как создать egg или wheel). Вы также узнаете, как создавать исполняемые файлы с помощью py2exe, bb_freeze, cx_freeze и PyInstaller. Наконец, вы узнаете, как создать программу установки с помощью Inno Setup.
Начните читать Python 101 Введение
Часть I - Изучение основ
Глава 1 Программирование IDLE
Глава 2 - Все о строках
Глава 3 - Списки, кортежи и словари
Глава 4 - Условные утверждения
Глава 5 - Циклы
Глава 6 - Генераторы Python
Глава 7 - Обработка исключений
Глава 8 - Работа с файлами
Глава 9 - Импортирование
Глава 10 - Функции
Глава 11 - Классы
Часть II - Обучение с помощью библиотеки
Глава 12 - Интроспекция
Глава 13 - Модуль csv
Глава 14 - configparser
Глава 15 - Логирование
Глава 16 - Модуль os
Глава 17 - Модуль email / smtplib
Глава 18 - Модуль sqlite
Глава 19 - Модуль subprocess
Глава 20 - Модуль sys
Глава 21 - Модуль протоколов
Глава 22 - Работа с датами и временем
Глава 23 - Модуль xml
Часть III - Промежуточные вопросы и ответы
Глава 24 - Отладчик Python
Глава 25 - Декораторы
Глава 26 - Лямбда
Глава 27 - Профилирование кода
Глава 28 - Введение в тестирование
Часть IV - Советы, рекомендации и учебные пособия
Глава 29 - Установка пакетов
Глава 30 - ConfigObj
Глава 31 - Разбор XML с помощью lxml
Глава 32 - Анализ кода Python
Глава 33 - Пакет requests
Глава 34 - SQLAlchemy
Глава 35 - virtualenv
Часть V - Упаковка и распространение
Глава 36 - Создание модулей и пакетов
Глава 37 - Как добавить свой код в PyPI
Глава 38 - Python egg
Глава 39 - Python wheels
Глава 40 - py2exe
Глава 41 - bbfreeze
Глава 42 - cx_Freeze
Глава 43 - PyInstaller
Глава 44 - Создание программы установки
https://animator.github.io/learn-python/
https://www.scaler.com/topics/python/
`,url:"https://romankurnovskii.com/ru/docs/python101/"},"https://romankurnovskii.com/ru/tags/linux/":{title:"linux",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/linux/"},"https://romankurnovskii.com/ru/categories/linux/":{title:"Linux",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/linux/"},"https://romankurnovskii.com/ru/categories/os/":{title:"OS",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/os/"},"https://romankurnovskii.com/ru/tags/ubuntu/":{title:"ubuntu",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/ubuntu/"},"https://romankurnovskii.com/ru/posts/howto-install-ubuntu-desktop-on-arm/":{title:"Установка Ubuntu Desktop 22.10 (Kinetic Kudu) на ARM CPU",tags:["linux","ubuntu"],content:`Ubuntu - одна из популярных Linux систем и достаточно много обзоров по установке Ubuntu. В этой статье мы будем устанавливать образ Ubuntu для ARM процессора на виртуальную машину UTM. Вся установка будет проходить на Mac OS.
Загрузка установочного образа На сайте Ubuntu доступен для скачивания только образ Ubuntu Server ARM версии 22.04 - без графического интерфейса. Но можно скачать обновленный релиз Ubuntu Desktop для ARM - Daily Build по ссылке.
Находим 64-bit ARM (ARMv8/AArch64) desktop image и скачиваем Виртуальная машина В качестве виртуальной машины для установки RHEL 9 использую бесплатную виртуальную машину UTM. Установить можно с помощью Homebrew, выполнив команду brew install --cask utm.
Установка Ubuntu Desktop Настройка виртуальной машины UTM В UTM нажимаем Create a New Virtual Machine -\u0026gt; Virtualize Выбираем скачанный образ и нажимаем Continue, далее оставляем опции по умолчанию Запуск Live версии Выбираем Try or Install Ubuntu. Запустится live образ ubuntu. Такой образ не сохраняет свое состояние после перезагрузки. Входим под пользователем ubuntu
Видим рабочий стол и можем пользоваться.
Установка Внизу справа есть ярлык для стандартной установки Ubuntu. Нажимаем и запускаем обычную установку на диск. Выбираем нужный язык Я выбираю минимальную установку, т.к. мне не нужны будут предустановленные игры и прочие приложения. Графический интерфейс, браузер, терминал остается со всеми базовыми настройками. Оставляем по умолчанию стирание виртуального диска перед установкой Создаем пользователя, под которым будем входить в систему Как только установка закончится, нажимаем Restart. У меня после перезагрузки черный экран. Поэтому я просто закрываю и снова запускаю вирутальную машину.
Вход в систему После запуска системы выбираем *Boot from next volume. Первым по умолчанию будет запуск с вирутального образа, но у нас уже есть система на диске, поэтому выбираем запуск со следующего по очереди диска. Входим под своим пользователем Система предлагает скачать обновления для системы. Нажимаю установить. Теперь можно пользоваться системой и все данные будут сохраняться после перезагрузки. Ссылки Kinetic Kudu Release Schedule `,url:"https://romankurnovskii.com/ru/posts/howto-install-ubuntu-desktop-on-arm/"},"https://romankurnovskii.com/ru/series/%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-%D0%BE%D1%81/":{title:"Установка ОС",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0-%D0%BE%D1%81/"},"https://romankurnovskii.com/ru/tags/react/":{title:"react",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/react/"},"https://romankurnovskii.com/ru/categories/react/":{title:"React",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/react/"},"https://romankurnovskii.com/ru/posts/integrate-hugo-react/":{title:"Как подключить React .jsx в проект на Hugo",tags:["react","hugo"],content:`Hugo предлагает подключение различных JS библиотек в проект. Такие изменения влекут за собой полное обновление проекта. Сегодня мы подключим компонент react без внесения больших изменений.
React - это библиотека. Чтобы она заработала на сайте, необходимо ее подклчюить, а далее воспользоваться внутренними функциями.
Подключить можно двумя способоами. С помощью подгрузки скрипта с CDN или загрузки пакета в package.json, чтопозволит использовать .jsx
package.json План:
Импорт пакета в package.json Создание .jsx скрипта Загрузка/build пакета в Hugo Импорт В корне проекта запускаем команду
npm i react react-dom Создание jsx скрипта В папке с темой assets создадим файл my-react-script.jsx import React from 'react'; import * as ReactDOM from 'react-dom'; import { createRoot } from 'react-dom/client'; const App = () =\u0026gt; { function sayHello () { alert('Hello, World!') } return ( \u0026lt;button onClick={sayHello}\u0026gt;Click me!\u0026lt;/button\u0026gt; ) } ReactDOM.render( React.createElement(App, null), document.getElementById('root') ) const container = document.getElementById('my_render_block'); const root = createRoot(container); root.render(\u0026lt;App /\u0026gt;); Добавим блок div в место в шаблоне для отрисовки react приложения \u0026lt;div id=\u0026quot;my_render_block\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; Подключение в HUGO В файле head.html или другом файте шаблона Hugo импортируем скрипт
{{ with resources.Get \u0026quot;my-react-script.jsx\u0026quot; }} {{ $options := dict \u0026quot;defines\u0026quot; (dict \u0026quot;process.env.NODE_ENV\u0026quot; \u0026quot;\\\u0026quot;development\\\u0026quot;\u0026quot; \u0026quot;process.env.BaseURL\u0026quot; (printf \`\u0026quot;%s\u0026quot;\` $.Site.BaseURL)) }} {{ $script := . | js.Build $options }} \u0026lt;script src=\u0026quot;{{ $script.RelPermalink }}\u0026quot; defer\u0026gt;\u0026lt;/script\u0026gt; {{ end }} CDN Второй способ
Подключение библиотеки React В проекте Hugo в шаблонах обновим файл head.html. В моем проекте это шаблон, который содержит основные теги html и head. Открываем layouts/partials/head.html и добавляем скрипт в раздел \u0026lt;head\u0026gt;:
\u0026lt;!-- ... \u0026lt;head\u0026gt; ... --\u0026gt; \u0026lt;!-- Примечание: при деплое на продакшен замените «development.js» на «production.min.js» --\u0026gt; \u0026lt;script src=\u0026quot;https://unpkg.com/react@17/umd/react.development.js\u0026quot; crossorigin\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://unpkg.com/react-dom@17/umd/react-dom.development.js\u0026quot; crossorigin\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- ... \u0026lt;/head\u0026gt; ... --\u0026gt; Выбор места для отрисовки компонента Создадим div блок в любом шаблоне Hugo, где будем отрисоывать React компонент. Например файл layouts/partials/footer.html
\u0026lt;div id=\u0026quot;my_react_app\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; React будет искать данный блок и отрисует внутри него компонент
Создание компонента Вынесем создание компонента в отдельный js файл. В Hugo есть директория static в корне проекта. Если нету, то можно создать. Подробнее о static folder
Создадим файл static/js/my_react_component.js и запишем код:
Важно: сркипт должен подключиться в проекте после блока \u0026lt;div id=\u0026quot;my_react_app\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;
const e = React.createElement; const MyCountButton = () =\u0026gt; { const [count, setCount] = React.useState(100); return e( 'button', { onClick: () =\u0026gt; setCount(count + 1) }, count ); } // Выведем на экран компонент // ищем блок my_react_app и отрисовываем внутри него компонент ReactDOM.render(React.createElement(MyCountButton), document.getElementById(\u0026quot;my_react_app\u0026quot;)); Подключение скрипта с React компонентами Так как скрипт будет искать div \u0026ldquo;my_react_app\u0026rdquo;, данный div блок должен быть загружен до исполнения скрипта. Поэтому в файле layouts/partials/footer.html добавляем скрипт в конец раздела \u0026lt;body\u0026gt;:
Пример Нажми на счетчик: 100
`,url:"https://romankurnovskii.com/ru/posts/integrate-hugo-react/"},"https://romankurnovskii.com/ru/tags/rhel/":{title:"rhel",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/rhel/"},"https://romankurnovskii.com/ru/posts/howto-install-rhel-9-free/":{title:"Установка Linux RHEL 9",tags:["linux","rhel"],content:`Red Hat Enterprise Linux 9 (RHEL 9) под кодовым названием Plow стал общедоступным (GA). Компания Red Hat объявила об этом 18 мая 2022 года. Она сменила бета-версию, которая существовала с 3 ноября 2021 года.
RHEL 9 - это несколько первых релизов в семействе Red Hat. Это первый крупный релиз после приобретения Red Hat компанией IBM в июле 2019 года, а также первая крупная версия после отказа от проекта CentOS в пользу CentOS Stream, который теперь является предшественником RHEL.
RHEL 9 является последней основной версией RHEL и поставляется с ядром 5.14, множеством новых пакетов программного обеспечения и массой усовершенствований. В ней особое внимание уделяется безопасности, стабильности, гибкости и надежности.
Описание RHEL 9 поставляется с новыми версиями программного обеспечения, включая Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0 и многие другие.
Подготовка к установке Регистрация на портале Red Hat Подписка Red Hat Developer Subscription - это бесплатное предложение программы Red Hat Developer, предназначенное для индивидуальных разработчиков, которые хотят воспользоваться всеми преимуществами Red Hat Enterprise Linux.
Она дает разработчикам доступ ко всем версиям Red Hat Enterprise Linux, а также к другим продуктам Red Hat, таким как дополнения, обновления программного обеспечения и ошибки безопасности.
Прежде всего, убедитесь, что у вас есть активная учетная запись Red Hat. Если у вас еще нет учетной записи, перейдите на портал Red Hat Customer Portal, нажмите на кнопку \u0026ldquo;Регистрация\u0026rdquo; и заполните свои данные для создания учетной записи Red Hat. Загрузка установочного образа После создания учетной записи Red Hat вы можете приступать к загрузке RHEL 9. Чтобы загрузить Red Hat Enterprise Linux 9 абсолютно бесплатно, зайдите на Red Hat Developer Portal и войдите в систему, используя учетные данные своей учетной записи. Затем перейдите на страницу загрузки RHEL 9 и нажмите на кнопку загрузки, показанную ниже.
Я использую MacBook M1, поэтому скачиваю образ RHEL 9 для M1 процессора aarch64 Виртуальная машина В качестве вирутальной машины для установки RHEL 9 использую бесплатную виртуальную машину UTM. Установить можно с помощью Homebrew, выполнив команду brew install --cask utm.
Установка Red Hat Enterprise Linux 9 Настройка виртуальной машины UTM В UTM нажимаем Create a New Virtual Machine -\u0026gt; Virtualize Выбираем скачанный образ RHEL 9 и нажимаем Continue Главное меню Помеченные поля необходимо заполнить
Создаем Root Password User Creation. Создаем пользователя, под которым будет осуществляться вход в систему. Connect to Red Hat. Здесь используем учетную запись, созданную выше.
Вводим данные аккаунта, нажимаем Register Нажимаем Done
В разделе Installation Destination выбираем диск по умолчанию
Теперь можем продолижть установку. На главном экране появилась кнопка Begin installation
После завершения установки перезагружаем систему. Иногда после перезагрузки запускается загрузка с установочного образа опять. Неоьбходимо либо отключить диск в настройка вирутальной машины либо перезагрузить UTM.
Запуск Red Hat Enterprise Linux 9 Вводим пароль и видим рабочий стол RHEL 9 Для доступа к приложениям нажимаем кнопку Activities в верхнем левом углу
Настройка Red Hat Enterprise Linux 9 Проверка пользователя ROOT В системе Linux пользователи относятся к разным группам, у которых есть определенные права. Если в процессе установки мы не поставили галку сделать пользователя администратором, то по умолчанию он не сможет устанавливать некоторые системные программы.
Выходим из системы и заходим в систему под пользователем root (тем самым, которого создавали ранее на главном экране). Нажимаем Log out Теперь входим под root. Пользователя может не быть в списке. Жмем Not listed и вводим данные аккаунта. Открываем терминал и проверяем Настройка параметров системы Кнопки сворачивания приложения Первое, что кажется непривычным при использовании GUI, отсутствие кнопок сворачивания окон Устанавливаем необходимый пакет
yum install gnome-tweaks -y После установки появится приложение Tweaks. Найдем его через поиск. В приложении множество и других настроек. Мы отобразим кнопки сворачивания приложений.
Идем в раздел Windows titlebars и включаем параметры Maximize, Minimize Доступ пользователю на установку приложений Чтобы постоянно не переключаться на root пользователя для устновки приложений, мы можем предоставить обычному пользвоателю доступ к установке приложений. Действия продолжаем делать под пользователем root. Открываем файл /etc/sudoers и добавляем пользователя
sudo vi /etc/sudoers Добавляем в конец файла данные пользователя. Имя моего пользователя: rhel-user
rhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf Установим Visual Studio Code под обычным пользователем Установка состоит из следующих шагов:
добавление нужного репозитория. Права на добавление репозитория (изменение файлов в директории по прежнему только у root пользователя) загрузка и Установка Первый шаг делаем под пользователем root Идем на сайт https://code.visualstudio.com/docs/setup/linux
Копируем код и запускаем в терминале
sudo rpm --import https://packages.microsoft.com/keys/microsoft.asc sudo sh -c 'echo -e \u0026quot;[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\u0026quot; \u0026gt; /etc/yum.repos.d/vscode.repo' Переключаемся на пользователя rhel-user. Это можно сделать и в терминале. Обновим репозитории Установим VSCode su rhel-user dnf check-update sudo dnf install code Ссылки https://developers.redhat.com/products/rhel/getting-started https://www.redhat.com/sysadmin/install-linux-rhel-9 `,url:"https://romankurnovskii.com/ru/posts/howto-install-rhel-9-free/"},"https://romankurnovskii.com/ru/series/cheatsheet/":{title:"CheatSheet",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/cheatsheet/"},"https://romankurnovskii.com/ru/categories/cheatsheet/":{title:"CheatSheet",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/cheatsheet/"},"https://romankurnovskii.com/ru/tags/python/":{title:"python",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/python/"},"https://romankurnovskii.com/ru/posts/python-snippets/":{title:"Сниппеты Python",tags:["python"],content:"Дата Текущая import datetime x = datetime.datetime.now() # 2022-08-04 21:41:24.871910 Формат YYYY-MM-DD import datetime x = datetime.datetime.now().strftime(\u0026quot;%Y-%m-%d\u0026quot;) # 2022-08-04 Создать папку import os if not os.path.exists(name): os.makedirs(name) Поиск наиболее часто встречаемого значения в списке import collections x = [1, 2, 7, 4, 5, 6, 7, 10] print(collections.Counter(x).most_common(1)[0][0]) # 7 def most_freq(list): return max(set(list), key=list.count) test = [10, 10, 20, 20, 10, 30, 30, 30, 20, 10] print(most_freq(test)) # 10 Случайное целое число import random x = random.randint(1, 10) # 9 Одновременная проверка нескольких флагов в Python x, y, z = 0, 1, 0 if x == 1 or y == 1 or z == 1: print('passed') if 1 in (x, y, z): print('passed') # These only test for truthiness: if x or y or z: print('passed') if any((x, y, z)): print('passed') Pdf -\u0026gt; Audio import PyPDF2, pyttsx3 # PDF file path = open('clcoding.pdf', 'rb') # creating a PdfFileReader object pdfReader = PyPDF2.PdfFileReader(path) # Get an engine instance for the speech synthesis speak = pyttsx3.init() for pages in range(pdfReader.numPages): text = pdfReader.getPage(pages).extractText() speak.say(text) speak.runAndWait() speak.stop() Полный обзор # Однострочные комментарии начинаются с символа решётки. \u0026quot;\u0026quot;\u0026quot; Многострочный текст может быть записан, используя 3 знака \u0026quot; и обычно используется в качестве встроенной документации \u0026quot;\u0026quot;\u0026quot; #################################################### ## 1. Примитивные типы данных и операторы #################################################### # У вас есть числа 3 # =\u0026gt; 3 # Математика работает вполне ожидаемо 1 + 1 # =\u0026gt; 2 8 - 1 # =\u0026gt; 7 10 * 2 # =\u0026gt; 20 35 / 5 # =\u0026gt; 7.0 # Результат целочисленного деления округляется в меньшую сторону # как для положительных, так и для отрицательных чисел. 5 // 3 # =\u0026gt; 1 -5 // 3 # =\u0026gt; -2 5.0 // 3.0 # =\u0026gt; 1.0 # работает и для чисел с плавающей запятой -5.0 // 3.0 # =\u0026gt; -2.0 # # Результат деления возвращает число с плавающей запятой 10.0 / 3 # =\u0026gt; 3.3333333333333335 # Остаток от деления 7 % 3 # =\u0026gt; 1 # Возведение в степень 2**3 # =\u0026gt; 8 # Приоритет операций указывается скобками 1 + 3 * 2 # =\u0026gt; 7 (1 + 3) * 2 # =\u0026gt; 8 # Булевы значения - примитивы (Обратите внимание на заглавную букву) True # =\u0026gt; True False # =\u0026gt; False # Для отрицания используется ключевое слово not not True # =\u0026gt; False not False # =\u0026gt; True # Булевы операторы # Обратите внимание: ключевые слова \u0026quot;and\u0026quot; и \u0026quot;or\u0026quot; чувствительны к регистру букв True and False # =\u0026gt; False False or True # =\u0026gt; True # True и False на самом деле 1 и 0, но с разными ключевыми словами True + True # =\u0026gt; 2 True * 8 # =\u0026gt; 8 False - 5 # =\u0026gt; -5 # Операторы сравнения обращают внимание на числовое значение True и False 0 == False # =\u0026gt; True 1 == True # =\u0026gt; True 2 == True # =\u0026gt; False -5 != False # =\u0026gt; True # Использование булевых логических операторов на типах int превращает их в булевы значения, но возвращаются оригинальные значения # Не путайте с bool(ints) и bitwise and/or (\u0026amp;,|) bool(0) # =\u0026gt; False bool(4) # =\u0026gt; True bool(-6) # =\u0026gt; True 0 and 2 # =\u0026gt; 0 -5 or 0 # =\u0026gt; -5 # Равенство — это == 1 == 1 # =\u0026gt; True 2 == 1 # =\u0026gt; False # Неравенство — это != 1 != 1 # =\u0026gt; False 2 != 1 # =\u0026gt; True # Ещё немного сравнений 1 \u0026lt; 10 # =\u0026gt; True 1 \u0026gt; 10 # =\u0026gt; False 2 \u0026lt;= 2 # =\u0026gt; True 2 \u0026gt;= 2 # =\u0026gt; True # Проверка, находится ли значение в диапазоне 1 \u0026lt; 2 and 2 \u0026lt; 3 # =\u0026gt; True 2 \u0026lt; 3 and 3 \u0026lt; 2 # =\u0026gt; False # Сравнения могут быть записаны цепочкой 1 \u0026lt; 2 \u0026lt; 3 # =\u0026gt; True 2 \u0026lt; 3 \u0026lt; 2 # =\u0026gt; False # (is vs. ==) ключевое слово is проверяет, относятся ли две переменные к одному и тому же объекту, но == проверяет если указанные объекты имеют одинаковые значения. a = [1, 2, 3, 4] # a указывает на новый список, [1, 2, 3, 4] b = a # b указывает на то, что указывает a b is a # =\u0026gt; True, a и b относятся к одному и тому же объекту b == a # =\u0026gt; True, Объекты a и b равны b = [1, 2, 3, 4] # b указывает на новый список, [1, 2, 3, 4] b is a # =\u0026gt; False, a и b не относятся к одному и тому же объекту b == a # =\u0026gt; True, Объекты a и b равны # Строки определяются символом \u0026quot; или ' \u0026quot;Это строка.\u0026quot; 'Это тоже строка.' # И строки тоже могут складываться! Хотя лучше не злоупотребляйте этим. \u0026quot;Привет \u0026quot; + \u0026quot;мир!\u0026quot; # =\u0026gt; \u0026quot;Привет мир!\u0026quot; # Строки (но не переменные) могут быть объединены без использования '+' \u0026quot;Привет \u0026quot; \u0026quot;мир!\u0026quot; # =\u0026gt; \u0026quot;Привет мир!\u0026quot; # Со строкой можно работать, как со списком символов \u0026quot;Привет мир!\u0026quot;[0] # =\u0026gt; 'П' # Вы можете найти длину строки len(\u0026quot;Это строка\u0026quot;) # =\u0026gt; 10 # Вы также можете форматировать, используя f-строки (в Python 3.6+) name = \u0026quot;Рейко\u0026quot; f\u0026quot;Она сказала, что ее зовут {name}.\u0026quot; # =\u0026gt; \u0026quot;Она сказала, что ее зовут Рейко\u0026quot; # Вы можете поместить любой оператор Python в фигурные скобки, и он будет выведен в строке. f\u0026quot;{name} состоит из {len(name)} символов.\u0026quot; # =\u0026gt; \u0026quot;Рэйко состоит из 5 символов.\u0026quot; # None является объектом None # =\u0026gt; None # Не используйте оператор равенства \u0026quot;==\u0026quot; для сравнения # объектов с None. Используйте для этого \u0026quot;is\u0026quot; \u0026quot;etc\u0026quot; is None # =\u0026gt; False None is None # =\u0026gt; True # None, 0 и пустые строки/списки/словари/кортежи приводятся к False. # Все остальные значения равны True bool(0) # =\u0026gt; False bool(\u0026quot;\u0026quot;) # =\u0026gt; False bool([]) # =\u0026gt; False bool({}) # =\u0026gt; False bool(()) # =\u0026gt; False #################################################### ## 2. Переменные и Коллекции #################################################### # В Python есть функция Print print(\u0026quot;Я Python. Приятно познакомиться!\u0026quot;) # =\u0026gt; Я Python. Приятно познакомиться! # По умолчанию функция, print() также выводит новую строку в конце. # Используйте необязательный аргумент end, чтобы изменить последнюю строку. print(\u0026quot;Привет мир\u0026quot;, end=\u0026quot;!\u0026quot;) # =\u0026gt; Привет мир! # Простой способ получить входные данные из консоли input_string_var = input(\u0026quot;Введите данные: \u0026quot;) # Возвращает данные в виде строки # Примечание: в более ранних версиях Python метод input() назывался raw_input() # Объявлять переменные перед инициализацией не нужно. # По соглашению используется нижний_регистр_с_подчёркиваниями some_var = 5 some_var # =\u0026gt; 5 # При попытке доступа к неинициализированной переменной выбрасывается исключение. # Об исключениях см. раздел \u0026quot;Поток управления и итерируемые объекты\u0026quot;. some_unknown_var # Выбрасывает ошибку NameError # if можно использовать как выражение # Эквивалент тернарного оператора '?:' в C \u0026quot;да!\u0026quot; if 0 \u0026gt; 1 else \u0026quot;нет!\u0026quot; # =\u0026gt; \u0026quot;нет!\u0026quot; # Списки хранят последовательности li = [] # Можно сразу начать с заполненного списка other_li = [4, 5, 6] # Объекты добавляются в конец списка методом append() li.append(1) # [1] li.append(2) # [1, 2] li.append(4) # [1, 2, 4] li.append(3) # [1, 2, 4, 3] # И удаляются с конца методом pop() li.pop() # =\u0026gt; возвращает 3 и li становится равен [1, 2, 4] # Положим элемент обратно li.append(3) # [1, 2, 4, 3]. # Обращайтесь со списком, как с обычным массивом li[0] # =\u0026gt; 1 # Обратимся к последнему элементу li[-1] # =\u0026gt; 3 # Попытка выйти за границы массива приведёт к ошибке индекса li[4] # Выбрасывает ошибку IndexError # Можно обращаться к диапазону, используя так называемые срезы # (Для тех, кто любит математику, это называется замкнуто-открытый интервал). li[1:3] # Вернуть список из индекса с 1 по 3 =\u0026gt; [2, 4] li[2:] # Вернуть список, начиная с индекса 2 =\u0026gt; [4, 3] li[:3] # Вернуть список с начала до индекса 3 =\u0026gt; [1, 2, 4] li[::2] # Вернуть список, выбирая каждую вторую запись =\u0026gt; [1, 4] li[::-1] # Вернуть список в обратном порядке =\u0026gt; [3, 4, 2, 1] # Используйте сочетания всего вышеназванного для выделения более сложных срезов # li[начало:конец:шаг] # Сделать однослойную глубокую копию, используя срезы li2 = li[:] # =\u0026gt; li2 = [1, 2, 4, 3], но (li2 is li) вернет False. # Удаляем произвольные элементы из списка оператором del del li[2] # [1, 2, 3] # Удалить первое вхождение значения li.remove(2) # [1, 3] li.remove(2) # Выбрасывает ошибку ValueError поскольку 2 нет в списке # Вставить элемент по определенному индексу li.insert(1, 2) # [1, 2, 3] # Получить индекс первого найденного элемента, соответствующего аргументу li.index(2) # =\u0026gt; 1 li.index(4) # Выбрасывает ошибку ValueError поскольку 4 нет в списке # Вы можете складывать, или, как ещё говорят, конкатенировать списки # Обратите внимание: значения li и other_li при этом не изменились. li + other_li # =\u0026gt; [1, 2, 3, 4, 5, 6] # Объединять списки можно методом extend() li.extend(other_li) # Теперь li содержит [1, 2, 3, 4, 5, 6] # Проверить элемент на наличие в списке можно оператором in 1 in li # =\u0026gt; True # Длина списка вычисляется функцией len len(li) # =\u0026gt; 6 # Кортежи похожи на списки, только неизменяемые tup = (1, 2, 3) tup[0] # =\u0026gt; 1 tup[0] = 3 # Выбрасывает ошибку TypeError # Обратите внимание, что кортеж длины 1 должен иметь запятую после последнего элемента, но кортежи другой длины, даже 0, не должны. type((1)) # =\u0026gt; \u0026lt;class 'int'\u0026gt; type((1,)) # =\u0026gt; \u0026lt;class 'tuple'\u0026gt; type(()) # =\u0026gt; \u0026lt;class 'tuple'\u0026gt; # Всё то же самое можно делать и с кортежами len(tup) # =\u0026gt; 3 tup + (4, 5, 6) # =\u0026gt; (1, 2, 3, 4, 5, 6) tup[:2] # =\u0026gt; (1, 2) 2 in tup # =\u0026gt; True # Вы можете распаковывать кортежи (или списки) в переменные a, b, c = (1, 2, 3) # a == 1, b == 2 и c == 3 # Вы также можете сделать расширенную распаковку a, *b, c = (1, 2, 3, 4) # a теперь 1, b теперь [2, 3] и c теперь 4 # Кортежи создаются по умолчанию, если опущены скобки d, e, f = 4, 5, 6 # кортеж 4, 5, 6 распаковывается в переменные d, e и f # соответственно, d = 4, e = 5 и f = 6 # Обратите внимание, как легко поменять местами значения двух переменных e, d = d, e # теперь d == 5, а e == 4 # Словари содержат ассоциативные массивы empty_dict = {} # Вот так описывается предзаполненный словарь filled_dict = {\u0026quot;one\u0026quot;: 1, \u0026quot;two\u0026quot;: 2, \u0026quot;three\u0026quot;: 3} # Обратите внимание, что ключи для словарей должны быть неизменяемыми типами. Это # сделано для того, чтобы ключ может быть преобразован в хеш для быстрого поиска. # Неизменяемые типы включают целые числа, числа с плавающей запятой, строки, кортежи. invalid_dict = {[1,2,3]: \u0026quot;123\u0026quot;} # =\u0026gt; Выбрасывает ошибку TypeError: unhashable type: 'list' valid_dict = {(1,2,3):[1,2,3]} # Однако значения могут быть любого типа. # Поиск значений с помощью [] filled_dict[\u0026quot;one\u0026quot;] # =\u0026gt; 1 # Все ключи в виде списка получаются с помощью метода keys(). # Его вызов нужно обернуть в list(), так как обратно мы получаем # итерируемый объект, о которых поговорим позднее. Примечание - для Python # версии \u0026lt;3.7, порядок словарных ключей не гарантируется. Ваши результаты могут # не точно соответствовать приведенному ниже примеру. Однако, начиная с Python 3.7 # элементы в словаре сохраняют порядок, в котором они вставляются в словарь. list(filled_dict.keys()) # =\u0026gt; [\u0026quot;three\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;one\u0026quot;] в Python \u0026lt;3.7 list(filled_dict.keys()) # =\u0026gt; [\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;three\u0026quot;] в Python 3.7+ # Все значения в виде списка можно получить с помощью values(). # И снова нам нужно обернуть вызов в list(), чтобы превратить # итерируемый объект в список. # То же самое замечание насчёт порядка ключей справедливо и здесь list(filled_dict.values()) # =\u0026gt; [3, 2, 1] в Python \u0026lt;3.7 list(filled_dict.values()) # =\u0026gt; [1, 2, 3] в Python 3.7+ # При помощи ключевого слова in можно проверять наличие ключей в словаре \u0026quot;one\u0026quot; in filled_dict # =\u0026gt; True 1 in filled_dict # =\u0026gt; False # Попытка получить значение по несуществующему ключу выбросит ошибку KeyError filled_dict[\u0026quot;four\u0026quot;] # Выбрасывает ошибку KeyError # Чтобы избежать этого, используйте метод get() filled_dict.get(\u0026quot;one\u0026quot;) # =\u0026gt; 1 filled_dict.get(\u0026quot;four\u0026quot;) # =\u0026gt; None # Метод get поддерживает аргумент по умолчанию, когда значение отсутствует filled_dict.get(\u0026quot;one\u0026quot;, 4) # =\u0026gt; 1 filled_dict.get(\u0026quot;four\u0026quot;, 4) # =\u0026gt; 4 # Метод setdefault() вставляет пару ключ-значение, только если такого ключа нет filled_dict.setdefault(\u0026quot;five\u0026quot;, 5) # filled_dict[\u0026quot;five\u0026quot;] возвращает 5 filled_dict.setdefault(\u0026quot;five\u0026quot;, 6) # filled_dict[\u0026quot;five\u0026quot;] по-прежнему возвращает 5 # Добавление элементов в словарь filled_dict.update({\u0026quot;four\u0026quot;:4}) # =\u0026gt; {\u0026quot;one\u0026quot;: 1, \u0026quot;two\u0026quot;: 2, \u0026quot;three\u0026quot;: 3, \u0026quot;four\u0026quot;: 4} filled_dict[\u0026quot;four\u0026quot;] = 4 # Другой способ добавления элементов # Удаляйте ключи из словаря с помощью ключевого слова del del filled_dict[\u0026quot;one\u0026quot;] # Удаляет ключ \u0026quot;one\u0026quot; из словаря # После Python 3.5 вы также можете использовать дополнительные параметры распаковки {'a': 1, **{'b': 2}} # =\u0026gt; {'a': 1, 'b': 2} {'a': 1, **{'a': 2}} # =\u0026gt; {'a': 2} # Множества содержат... ну, в общем, множества empty_set = set() # Инициализация множества набором значений. # Да, оно выглядит примерно как словарь. Ну извините, так уж вышло. filled_set = {1, 2, 2, 3, 4} # =\u0026gt; {1, 2, 3, 4} # Similar to keys of a dictionary, elements of a set have to be immutable. # Как и ключи словаря, элементы множества должны быть неизменяемыми. invalid_set = {[1], 1} # =\u0026gt; Выбрасывает ошибку TypeError: unhashable type: 'list' valid_set = {(1,), 1} # Множеству можно назначать новую переменную filled_set = some_set filled_set.add(5) # {1, 2, 3, 4, 5} # В множествах нет повторяющихся элементов filled_set.add(5) # {1, 2, 3, 4, 5} # Пересечение множеств: \u0026amp; other_set = {3, 4, 5, 6} filled_set \u0026amp; other_set # =\u0026gt; {3, 4, 5} # Объединение множеств: | filled_set | other_set # =\u0026gt; {1, 2, 3, 4, 5, 6} # Разность множеств: - {1, 2, 3, 4} - {2, 3, 5} # =\u0026gt; {1, 4} # Симметричная разница: ^ {1, 2, 3, 4} ^ {2, 3, 5} # =\u0026gt; {1, 4, 5} # Проверить, является ли множество слева надмножеством множества справа {1, 2} \u0026gt;= {1, 2, 3} # =\u0026gt; False # Проверить, является ли множество слева подмножеством множества справа {1, 2} \u0026lt;= {1, 2, 3} # =\u0026gt; True # Проверка на наличие в множестве: in 2 in filled_set # =\u0026gt; True 10 in filled_set # =\u0026gt; False # Сделать однослойную глубокую копию filled_set = some_set.copy() # {1, 2, 3, 4, 5} filled_set is some_set # =\u0026gt; False #################################################### ## 3. Поток управления и итерируемые объекты #################################################### # Для начала создадим переменную some_var = 5 # Так выглядит выражение if. Отступы в python очень важны! # Конвенция заключается в использовании четырех пробелов, а не табуляции. # Pезультат: \u0026quot;some_var меньше, чем 10\u0026quot; if some_var \u0026gt; 10: print(\u0026quot;some_var точно больше, чем 10.\u0026quot;) elif some_var \u0026lt; 10: # Выражение elif необязательно. print(\u0026quot;some_var меньше, чем 10.\u0026quot;) else: # Это тоже необязательно. print(\u0026quot;some_var равно 10.\u0026quot;) \u0026quot;\u0026quot;\u0026quot; Циклы For проходят по спискам. Выводит: собака — это млекопитающее кошка — это млекопитающее мышь — это млекопитающее \u0026quot;\u0026quot;\u0026quot; for animal in [\u0026quot;собака\u0026quot;, \u0026quot;кошка\u0026quot;, \u0026quot;мышь\u0026quot;]: # Можете использовать format() для интерполяции форматированных строк print(\u0026quot;{} — это млекопитающее\u0026quot;.format(animal)) \u0026quot;\u0026quot;\u0026quot; \u0026quot;range(число)\u0026quot; возвращает список чисел от нуля до заданного числа Выводит: 0 1 2 3 \u0026quot;\u0026quot;\u0026quot; for i in range(4): print(i) \u0026quot;\u0026quot;\u0026quot; \u0026quot;range(нижнее, верхнее)\u0026quot; возвращает список чисел от нижнего числа к верхнему Выводит: 4 5 6 7 \u0026quot;\u0026quot;\u0026quot; for i in range(4, 8): print(i) \u0026quot;\u0026quot;\u0026quot; \u0026quot;range(нижнее, верхнее, шаг)\u0026quot; возвращает список чисел от нижнего числа к верхнему, от нижнего числа к верхнему, увеличивая шаг за шагом. Если шаг не указан, значение по умолчанию - 1. Выводит: 4 6 \u0026quot;\u0026quot;\u0026quot; for i in range(4, 8, 2): print(i) \u0026quot;\u0026quot;\u0026quot; Чтобы перебрать список и получить индекс и значение каждого элемента в списке Выводит: 0 собака 1 кошка 2 мышь \u0026quot;\u0026quot;\u0026quot; animals = [\u0026quot;собака\u0026quot;, \u0026quot;кошка\u0026quot;, \u0026quot;мышь\u0026quot;] for i, value in enumerate(animals): print(i, value) \u0026quot;\u0026quot;\u0026quot; Циклы while продолжаются до тех пор, пока указанное условие не станет ложным. Выводит: 0 1 2 3 \u0026quot;\u0026quot;\u0026quot; x = 0 while x \u0026lt; 4: print(x) x += 1 # Краткая запись для x = x + 1 # Обрабатывайте исключения блоками try/except try: # Чтобы выбросить ошибку, используется raise raise IndexError(\u0026quot;Это ошибка индекса\u0026quot;) except IndexError as e: pass # pass — это просто отсутствие оператора. Обычно здесь происходит восстановление после ошибки. except (TypeError, NameError): pass # Несколько исключений можно обработать вместе, если нужно. else: # Необязательное выражение. Должно следовать за последним блоком except print(\u0026quot;Всё хорошо!\u0026quot;) # Выполнится, только если не было никаких исключений finally: # Выполнить при любых обстоятельствах print(\u0026quot;Мы можем очистить ресурсы здесь\u0026quot;) # Вместо try/finally чтобы очистить ресурсы, можно использовать оператор with with open(\u0026quot;myfile.txt\u0026quot;) as f: for line in f: print(line) # Запись в файл contents = {\u0026quot;aa\u0026quot;: 12, \u0026quot;bb\u0026quot;: 21} with open(\u0026quot;myfile1.txt\u0026quot;, \u0026quot;w+\u0026quot;) as file: file.write(str(contents)) # Записывает строку в файл with open(\u0026quot;myfile2.txt\u0026quot;, \u0026quot;w+\u0026quot;) as file: file.write(json.dumps(contents)) # Записывает объект в файл # Чтение из файла with open('myfile1.txt', \u0026quot;r+\u0026quot;) as file: contents = file.read() # Читает строку из файла print(contents) # print: {\u0026quot;aa\u0026quot;: 12, \u0026quot;bb\u0026quot;: 21} with open('myfile2.txt', \u0026quot;r+\u0026quot;) as file: contents = json.load(file) # Читает объект json из файла print(contents) # print: {\u0026quot;aa\u0026quot;: 12, \u0026quot;bb\u0026quot;: 21} # Python предоставляет фундаментальную абстракцию, # которая называется итерируемым объектом (Iterable). # Итерируемый объект — это объект, который воспринимается как последовательность. # Объект, который возвратила функция range(), итерируемый. filled_dict = {\u0026quot;one\u0026quot;: 1, \u0026quot;two\u0026quot;: 2, \u0026quot;three\u0026quot;: 3} our_iterable = filled_dict.keys() print(our_iterable) # =\u0026gt; dict_keys(['one', 'two', 'three']). Это объект, реализующий интерфейс Iterable # Мы можем проходить по нему циклом. for i in our_iterable: print(i) # Выводит one, two, three # Но мы не можем обращаться к элементу по индексу. our_iterable[1] # Выбрасывает ошибку TypeError # Итерируемый объект знает, как создавать итератор. our_iterator = iter(our_iterable) # Итератор может запоминать состояние при проходе по объекту. # Мы получаем следующий объект, вызывая функцию next(). next(our_iterator) # =\u0026gt; \u0026quot;one\u0026quot; # Он сохраняет состояние при вызове next(). next(our_iterator) # =\u0026gt; \u0026quot;two\u0026quot; next(our_iterator) # =\u0026gt; \u0026quot;three\u0026quot; # Возвратив все данные, итератор выбрасывает исключение StopIterator next(our_iterator) # Выбрасывает исключение StopIteration # Мы можем проходить по нему циклом. our_iterator = iter(our_iterable) for i in our_iterator: print(i) # Выводит one, two, three # Вы можете получить сразу все элементы итератора, вызвав на нём функцию list(). list(our_iterable) # =\u0026gt; Возвращает [\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;three\u0026quot;] list(our_iterator) # =\u0026gt; Возвращает [] потому что состояние сохраняется #################################################### ## 4. Функции #################################################### # Используйте def для создания новых функций def add(x, y): print(\u0026quot;x равен %s, а y равен %s\u0026quot; % (x, y)) return x + y # Возвращайте результат с помощью ключевого слова return # Вызов функции с аргументами add(5, 6) # =\u0026gt; Выводит \u0026quot;x равен 5, а y равен 6\u0026quot; и возвращает 11 # Другой способ вызова функции — вызов с именованными аргументами add(y=6, x=5) # Именованные аргументы можно указывать в любом порядке. # Вы можете определить функцию, принимающую переменное число аргументов def varargs(*args): return args varargs(1, 2, 3) # =\u0026gt; (1,2,3) # А также можете определить функцию, принимающую переменное число # именованных аргументов def keyword_args(**kwargs): return kwargs # Вызовем эту функцию и посмотрим, что из этого получится keyword_args(big=\u0026quot;foot\u0026quot;, loch=\u0026quot;ness\u0026quot;) # =\u0026gt; {\u0026quot;big\u0026quot;: \u0026quot;foot\u0026quot;, \u0026quot;loch\u0026quot;: \u0026quot;ness\u0026quot;} # Если хотите, можете использовать оба способа одновременно def all_the_args(*args, **kwargs): print(args) print(kwargs) \u0026quot;\u0026quot;\u0026quot; all_the_args(1, 2, a=3, b=4) выводит: (1, 2) {\u0026quot;a\u0026quot;: 3, \u0026quot;b\u0026quot;: 4} \u0026quot;\u0026quot;\u0026quot; # Вызывая функции, можете сделать наоборот! # Используйте символ * для распаковки кортежей и ** для распаковки словарей args = (1, 2, 3, 4) kwargs = {\u0026quot;a\u0026quot;: 3, \u0026quot;b\u0026quot;: 4} all_the_args(*args) # эквивалентно all_the_args(1, 2, 3, 4) all_the_args(**kwargs) # эквивалентно all_the_args(a=3, b=4) all_the_args(*args, **kwargs) # эквивалентно all_the_args(1, 2, 3, 4, a=3, b=4) # Возврат нескольких значений (с назначением кортежей) def swap(x, y): return y, x # Возвращает несколько значений в виде кортежа без скобок. # (Примечание: скобки исключены, но могут быть включены) x = 1 y = 2 x, y = swap(x, y) # =\u0026gt; x = 2, y = 1 # (x, y) = swap(x,y) # Снова, скобки были исключены, но могут быть включены. # Область определения функций x = 5 def set_x(num): # Локальная переменная x — это не то же самое, что глобальная переменная x x = num # =\u0026gt; 43 print(x) # =\u0026gt; 43 def set_global_x(num): global x print(x) # =\u0026gt; 5 x = num # Глобальная переменная x теперь равна 6 print(x) # =\u0026gt; 6 set_x(43) set_global_x(6) # Python имеет функции первого класса def create_adder(x): def adder(y): return x + y return adder add_10 = create_adder(10) add_10(3) # =\u0026gt; 13 # Также есть и анонимные функции (lambda x: x \u0026gt; 2)(3) # =\u0026gt; True (lambda x, y: x ** 2 + y ** 2)(2, 1) # =\u0026gt; 5 # Есть встроенные функции высшего порядка list(map(add_10, [1, 2, 3])) # =\u0026gt; [11, 12, 13] list(map(max, [1, 2, 3], [4, 2, 1])) # =\u0026gt; [4, 2, 3] list(filter(lambda x: x \u0026gt; 5, [3, 4, 5, 6, 7])) # =\u0026gt; [6, 7] # Для удобного отображения и фильтрации можно использовать списочные интерпретации # Интерпретация списка сохраняет вывод в виде списка, который сам может быть вложенным списком [add_10(i) for i in [1, 2, 3]] # =\u0026gt; [11, 12, 13] [x for x in [3, 4, 5, 6, 7] if x \u0026gt; 5] # =\u0026gt; [6, 7] # Вы также можете создавать интерпретации множеств и словарей. {x for x in 'abcddeef' if x not in 'abc'} # =\u0026gt; {'d', 'e', 'f'} {x: x**2 for x in range(5)} # =\u0026gt; {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} #################################################### ## 5. Модули #################################################### # Вы можете импортировать модули import math print(math.sqrt(16)) # =\u0026gt; 4.0 # Вы можете получить определенные функции из модуля from math import ceil, floor print(ceil(3.7)) # =\u0026gt; 4.0 print(floor(3.7)) # =\u0026gt; 3.0 # Вы можете импортировать все функции из модуля. # Предупреждение: это не рекомендуется from math import * # Вы можете сократить имена модулей import math as m math.sqrt(16) == m.sqrt(16) # =\u0026gt; True # Модули Python - это обычные файлы Python. Вы # можете писать свои собственные и импортировать их. Имя # модуля совпадает с именем файла. # Вы можете узнать, какие функции и атрибуты # определены в модуле. import math dir(math) # Если у вас есть скрипт Python с именем math.py в той же папке, # что и ваш текущий скрипт, файл math.py будет # будет загружен вместо встроенного модуля Python. # Это происходит потому, что локальная папка имеет приоритет # над встроенными библиотеками Python. #################################################### ## 6. Классы #################################################### # Мы используем оператор class для создания класса class Human: # Атрибут класса. Он используется всеми экземплярами этого класса species = \u0026quot;Гомосапиенс\u0026quot; # Обычный конструктор, вызывается при инициализации экземпляра класса # Обратите внимание, что двойное подчёркивание в начале и в конце имени # означает объекты и атрибуты, которые используются Python, но находятся # в пространствах имён, управляемых пользователем. # Методы (или объекты или атрибуты), например: # __init__, __str__, __repr__ и т. д. называются специальными методами. # Не придумывайте им имена самостоятельно. def __init__(self, name): # Присваивание значения аргумента атрибуту self.name = name # Инициализация свойства self._age = 0 # Метод экземпляра. Все методы принимают self в качестве первого аргумента def say(self, msg): return \u0026quot;{name}: {message}\u0026quot;.format(name=self.name, message=msg) # Другой метод экземпляра def sing(self): return 'йо... йо... проверка микрофона... раз, два... раз, два...' # Метод класса разделяется между всеми экземплярами # Они вызываются с указыванием вызывающего класса в качестве первого аргумента @classmethod def get_species(cls): return cls.species # Статический метод вызывается без ссылки на класс или экземпляр @staticmethod def grunt(): return \u0026quot;*grunt*\u0026quot; # property похоже на геттер. # Оно превращает метод age() в одноименный атрибут только для чтения. # Однако нет необходимости писать тривиальные геттеры и сеттеры в Python. @property def age(self): return self._age # Это позволяет установить свойство @age.setter def age(self, age): self._age = age # Это позволяет удалить свойство @age.deleter def age(self): del self._age # Когда интерпретатор Python читает исходный файл, он выполняет весь его код. # Проверка __name__ гарантирует, что этот блок кода выполняется только тогда, когда # этот модуль - это основная программа. if __name__ == '__main__': # Инициализация экземпляра класса i = Human(name=\u0026quot;Иван\u0026quot;) i.say(\u0026quot;привет\u0026quot;) # Выводит: \u0026quot;Иван: привет\u0026quot; j = Human(\u0026quot;Пётр\u0026quot;) j.say(\u0026quot;привет\u0026quot;) # Выводит: \u0026quot;Пётр: привет\u0026quot; # i и j являются экземплярами типа Human, или другими словами: они являются объектами Human # Вызов метода класса i.say(i.get_species()) # \u0026quot;Иван: Гомосапиенс\u0026quot; # Изменение разделяемого атрибута Human.species = \u0026quot;Неандертальец\u0026quot; i.say(i.get_species()) # =\u0026gt; \u0026quot;Иван: Неандертальец\u0026quot; j.say(j.get_species()) # =\u0026gt; \u0026quot;Пётр: Неандертальец\u0026quot; # Вызов статического метода print(Human.grunt()) # =\u0026gt; \u0026quot;*grunt*\u0026quot; # Невозможно вызвать статический метод с экземпляром объекта # потому что i.grunt() автоматически поместит \u0026quot;self\u0026quot; (объект i) в качестве аргумента print(i.grunt()) # =\u0026gt; TypeError: grunt() takes 0 positional arguments but 1 was given # Обновить свойство для этого экземпляра i.age = 42 # Получить свойство i.say(i.age) # =\u0026gt; \u0026quot;Иван: 42\u0026quot; j.say(j.age) # =\u0026gt; \u0026quot;Пётр: 0\u0026quot; # Удалить свойство del i.age # i.age # =\u0026gt; это выбрасило бы ошибку AttributeError #################################################### ## 6.1 Наследование #################################################### # Наследование позволяет определять новые дочерние классы, которые наследуют методы и # переменные от своего родительского класса. # Используя класс Human, определенный выше как базовый или родительский класс, мы можем # определить дочерний класс Superhero, который наследует переменные класса, такие как # \u0026quot;species\u0026quot;, \u0026quot;name\u0026quot; и \u0026quot;age\u0026quot;, а также методы, такие как \u0026quot;sing\u0026quot; и \u0026quot;grunt\u0026quot; из класса Human, # но также может иметь свои уникальные свойства. # Чтобы воспользоваться преимуществами модульности по файлам, вы можете поместить # вышеперечисленные классы в их собственные файлы, например, human.py # Чтобы импортировать функции из других файлов, используйте следующий формат # from \u0026quot;имя-файла-без-расширения\u0026quot; import \u0026quot;функция-или-класс\u0026quot; from human import Human # Укажите родительский класс(ы) как параметры определения класса class Superhero(Human): # Если дочерний класс должен наследовать все определения родителя без каких-либо # изменений, вы можете просто использовать ключевое слово pass (и ничего больше), # но в этом случае оно закомментировано, чтобы разрешить уникальный дочерний класс: # pass # Дочерние классы могут переопределять атрибуты своих родителей species = 'Сверхчеловек' # Дочерние классы автоматически наследуют конструктор родительского класса, включая # его аргументы, но также могут определять дополнительные аргументы или определения # и переопределять его методы, такие как конструктор класса. # Этот конструктор наследует аргумент \u0026quot;name\u0026quot; от класса \u0026quot;Human\u0026quot; # и добавляет аргументы \u0026quot;superpower\u0026quot; и \u0026quot;movie\u0026quot;: def __init__(self, name, movie=False, superpowers=[\u0026quot;сверхсила\u0026quot;, \u0026quot;пуленепробиваемость\u0026quot;]): # добавить дополнительные атрибуты класса: self.fictional = True self.movie = movie # помните об изменяемых значениях по умолчанию, # поскольку значения по умолчанию являются общими self.superpowers = superpowers # Функция \u0026quot;super\u0026quot; позволяет вам получить доступ к методам родительского класса, # которые переопределяются дочерним, в данном случае, методом __init__. # Это вызывает конструктор родительского класса: super().__init__(name) # переопределить метод sing def sing(self): return 'Бам, бам, БАМ!' # добавить дополнительный метод экземпляра def boast(self): for power in self.superpowers: print(\u0026quot;Я обладаю силой '{pow}'!\u0026quot;.format(pow=power)) if __name__ == '__main__': sup = Superhero(name=\u0026quot;Тик\u0026quot;) # Проверка типа экземпляра if isinstance(sup, Human): print('Я человек') if type(sup) is Superhero: print('Я супергерой') # Получить порядок поиска разрешения метода (MRO), # используемый как getattr(), так и super() # Этот атрибут является динамическим и может быть обновлен print(Superhero.__mro__) # =\u0026gt; (\u0026lt;class '__main__.Superhero'\u0026gt;, # =\u0026gt; \u0026lt;class 'human.Human'\u0026gt;, \u0026lt;class 'object'\u0026gt;) # Вызывает родительский метод, но использует свой собственный атрибут класса print(sup.get_species()) # =\u0026gt; Сверхчеловек # Вызов переопределенного метода print(sup.sing()) # =\u0026gt; Бам, бам, БАМ! # Вызывает метод из Human sup.say('Ложка') # =\u0026gt; Тик: Ложка # Метод вызова, существующий только в Superhero sup.boast() # =\u0026gt; Я обладаю силой 'сверхсила'! # =\u0026gt; Я обладаю силой 'пуленепробиваемость'! # Атрибут унаследованного класса sup.age = 31 print(sup.age) # =\u0026gt; 31 # Атрибут, который существует только в Superhero print('Достоин ли я Оскара? ' + str(sup.movie)) #################################################### ## 6.2 Множественное наследование #################################################### # Eще одно определение класса # bat.py class Bat: species = 'Летучая мышь' def __init__(self, can_fly=True): self.fly = can_fly # В этом классе также есть метод say def say(self, msg): msg = '... ... ...' return msg # И свой метод тоже def sonar(self): return '))) ... (((' if __name__ == '__main__': b = Bat() print(b.say('привет')) print(b.fly) # И еще одно определение класса, унаследованное от Superhero и Bat # superhero.py from superhero import Superhero from bat import Bat # Определите Batman как дочерний класс, унаследованный от Superhero и Bat class Batman(Superhero, Bat): def __init__(self, *args, **kwargs): # Обычно для наследования атрибутов необходимо вызывать super: # super(Batman, self).__init__(*args, **kwargs) # Однако здесь мы имеем дело с множественным наследованием, а super() # работает только со следующим базовым классом в списке MRO. # Поэтому вместо этого мы вызываем __init__ для всех родителей. # Использование *args и **kwargs обеспечивает чистый способ передачи # аргументов, когда каждый родитель \u0026quot;очищает слой луковицы\u0026quot;. Superhero.__init__(self, 'анонимный', movie=True, superpowers=['Богатый'], *args, **kwargs) Bat.__init__(self, *args, can_fly=False, **kwargs) # переопределить значение атрибута name self.name = 'Грустный Бен Аффлек' def sing(self): return 'на на на на на бэтмен!' if __name__ == '__main__': sup = Batman() # Получить порядок поиска разрешения метода (MRO), # используемый как getattr(), так и super() # Этот атрибут является динамическим и может быть обновлен print(Batman.__mro__) # =\u0026gt; (\u0026lt;class '__main__.Batman'\u0026gt;, # =\u0026gt; \u0026lt;class 'superhero.Superhero'\u0026gt;, # =\u0026gt; \u0026lt;class 'human.Human'\u0026gt;, # =\u0026gt; \u0026lt;class 'bat.Bat'\u0026gt;, \u0026lt;class 'object'\u0026gt;) # Вызывает родительский метод, но использует свой собственный атрибут класса print(sup.get_species()) # =\u0026gt; Сверхчеловек # Вызов переопределенного метода print(sup.sing()) # =\u0026gt; на на на на на бэтмен! # Вызывает метод из Human, потому что порядок наследования имеет значение sup.say('Я согласен') # =\u0026gt; Грустный Бен Аффлек: Я согласен # Вызов метода, существующий только во втором родителе print(sup.sonar()) # =\u0026gt; ))) ... ((( # Атрибут унаследованного класса sup.age = 100 print(sup.age) # =\u0026gt; 100 # Унаследованный атрибут от второго родителя, # значение по умолчанию которого было переопределено. print('Могу ли я летать? ' + str(sup.fly)) # =\u0026gt; Могу ли я летать? False #################################################### ## 7. Дополнительно #################################################### # Генераторы помогут выполнить ленивые вычисления def double_numbers(iterable): for i in iterable: yield i + i # Генераторы эффективны с точки зрения памяти, потому что они загружают только данные, # необходимые для обработки следующего значения в итерации. # Это позволяет им выполнять операции с недопустимо большими диапазонами значений. # ПРИМЕЧАНИЕ: \u0026quot;range\u0026quot; заменяет \u0026quot;xrange\u0026quot; в Python 3. for i in double_numbers(range(1, 900000000)): # \u0026quot;range\u0026quot; - генератор. print(i) if i \u0026gt;= 30: break # Так же, как вы можете создать интерпретации списков, вы можете создать и # интерпретации генераторов. values = (-x for x in [1,2,3,4,5]) for x in values: print(x) # Выводит -1 -2 -3 -4 -5 # Вы также можете преобразовать интерпретацию генератора непосредственно в список. values = (-x for x in [1,2,3,4,5]) gen_to_list = list(values) print(gen_to_list) # =\u0026gt; [-1, -2, -3, -4, -5] # Декораторы # В этом примере \u0026quot;beg\u0026quot; оборачивает \u0026quot;say\u0026quot;. # Если say_please равно True, он изменит возвращаемое сообщение. from functools import wraps def beg(target_function): @wraps(target_function) def wrapper(*args, **kwargs): msg, say_please = target_function(*args, **kwargs) if say_please: return \u0026quot;{} {}\u0026quot;.format(msg, \u0026quot;Пожалуйста! Спасибо :)\u0026quot;) return msg return wrapper @beg def say(say_please=False): msg = \u0026quot;Вы не купите мне сока?\u0026quot; return msg, say_please print(say()) # Вы не купите мне сока? print(say(say_please=True)) # Вы не купите мне сока? Пожалуйста! Спасибо :) ",url:"https://romankurnovskii.com/ru/posts/python-snippets/"},"https://romankurnovskii.com/ru/tags/minikube/":{title:"minikube",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/minikube/"},"https://romankurnovskii.com/ru/tags/cheatsheet/":{title:"cheatsheet",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/cheatsheet/"},"https://romankurnovskii.com/ru/tags/docker/":{title:"docker",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/docker/"},"https://romankurnovskii.com/ru/categories/docker/":{title:"Docker",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/docker/"},"https://romankurnovskii.com/ru/posts/docker-commands/":{title:"Популярные команды Docker",tags:["docker","cheatsheet"],content:`Full in english
Топ 10 комманд Docker docker ps — смотрим список запущенных контейнеров docker pull — загрузка образа docker build — собирает образ docker logs — смотрим логи docker run — запускаем контейнер docker stop — останавливает контейнер docker kill — «убивает» контейнер docker rm — удаляет контейнер docker rmi — удаляет образ docker volume ls — список томов docker build Документация Построить образ из Dockerfile.
docker build [DOCKERFILE PATH] Флаги --file -f Путь, где находится Dockerfile --force-rm Всегда удалять временные контейнеры. --no-cache Не использовать кэш при построении образа. --rm Удалить временные контейнеры после успешного построения. --tag -t Название и возможный тег в формате name:tag или просто тег my_tag (опционально) Примеры Построить образ с меткой my-org/my-image, используя Dockerfile в /tmp/Dockerfile.
docker build -t my-org:my-image -f /tmp/Dockerfile docker run Документация
Создает и запускает контейнер за один операционный шаг
Примеры docker run -it ubuntu:latest /bin/bash Данная команда запустит контейнер ubuntu и при старте сразу запустит /bin/bash. Если образ ubuntu не был загружен ранее, он загрузится перед запуском.
Флаги -it This will not make the container you started shut down immediately, as it will create a pseudo-TTY session (-t) and keep STDIN open (-i) --rm Automatically remove the container when it exit. Otherwise it will be stored and visible running docker ps -a. --detach -d Run container in background and print container ID --volume -v Bind mount a volume. Useful for accessing folders on your local disk inside your docker container, like configuration files or storage that should be persisted (database, logs etc.). docker exec Документация Выполнить команду внутри запущенного контейнера.
docker exec [CONTAINER ID] Флаги --detach -d Detached mode: запуск в фоновом режиме -it запуск в интерактивном режиме. запуск псевдотерминала pseudo-TTY (-t) и перенаправление ввода-вывода (STDIN) (-i). Даёт доступ к выполнению команд в терминале контейнера. Примеры docker exec [CONTAINER ID] touch /tmp/exec_works docker images Документация Вывести список всех загруженных/созданных образов
docker images Флаги -q показать только ID образов docker inspect Документация
Показать всю информацию о контейнере.
docker inspect [CONTAINER ID] docker logs Документация
Вывести логи контейнера.
docker logs [CONTAINER ID] Флаги --details Показывает дополнительную информацию в логе. --follow -f Следить за выводом журнала --timestamps -t Показать журналы с меткой времени docker ps Документация
Показывает информацию о всех запущенных контейнерах.
docker ps Флаги --all -a Show all containers (default shows just running) --filter -f Filter output based on conditions provided, docker ps -f=\u0026quot;name=\u0026quot;example\u0026quot; --quiet -q Only display numeric IDs docker rmi Документация
Удалить один или несколько образов.
docker rmi [IMAGE ID] Флаги --force -f Force removal of the image Советы и рекомендации по докеру Сборник полезных советов по Docker.
Удалить все контейнеры NOTE: Удалить ВСЕ контенеры.
docker container prune или
docker rm $(docker ps -a -q) Удалить все непомеченные контейнеры docker image prune Вывести сколько памяти занимает Docker docker system df Получить IP-адрес работающего контейнера docker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d '\u0026quot;' -f 4 Сгенерировать образ на основе файла Dockerfile и добавить этому образу имя и версию docker build -t new_image_name:v1 . . означает текущую директорию, где расположен файл Dockerfile.
Сгенерировать из запущенного контейнера новый образ docker commit [CONTAINER ID] [NEW IMAGE NAME] \u0026ldquo;Убить\u0026rdquo; все запущенные контейнеры docker kill $(docker ps -q) Ссылки docs.docker.com docker-cheat-sheet https://sourabhbajaj.com/mac-setup/Docker/ `,url:"https://romankurnovskii.com/ru/posts/docker-commands/"},"https://romankurnovskii.com/ru/series/%D1%88%D0%BF%D0%B0%D1%80%D0%B3%D0%B0%D0%BB%D0%BA%D0%B8/":{title:"шпаргалки",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/%D1%88%D0%BF%D0%B0%D1%80%D0%B3%D0%B0%D0%BB%D0%BA%D0%B8/"},"https://romankurnovskii.com/ru/tags/javascript/":{title:"JavaScript",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/javascript/"},"https://romankurnovskii.com/ru/categories/javascript/":{title:"JavaScript",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/javascript/"},"https://romankurnovskii.com/ru/tags/redirect-url/":{title:"redirect url",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/redirect-url/"},"https://romankurnovskii.com/ru/posts/howto-redirect-to-url/":{title:"Как сделать редирект на другой URL в JavaScript",tags:["JavaScript","redirect url"],content:`Пользователя можно перенаправлять с одной веб-страницы на любую другую несколькими способами.
с помощью обновления мета-данных HTML. Перенаправления на стороне сервера. Например, используя файл .htaccess, PHP с помощью перенаправления на стороне клиента через JavaScript. Для перенаправления на другой URL с помощью JavaScript используем window.location.href или window.location.replace(). Передать второй аргумент, чтобы произвести клик по ссылке (true - по умолчанию) или перенаправление по HTTP (false).
JavaScript функции Логика const newUrl = 'https://www.google.com/'; window.location.href = newUrl; // 1 window.location.replace(newUrl); // 2 window.location.assign(newUrl) // 3 Пример функции const redirect = (url, asLink = true) =\u0026gt; asLink ? (window.location.href = url) : window.location.replace(url); JavaScript в html \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;script\u0026gt; const newUrl = 'https://www.google.com/'; window.location.href = newUrl; \u0026lt;/script\u0026gt; \u0026lt;!--...--\u0026gt; redirect('https://google.com'); метатег HTML \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;0; url=https://example.com/newlocation\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;/html\u0026gt; После того как загрузится ткущая страница, браузер перенаправит на новую страницу, ожидая при этом 0 content=\u0026quot;0 секунд.
Чтобы выполнялась отложенная переадресация, укажите нужное количество секунд в атрибуте content:
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026quot;refresh\u0026quot; content=\u0026quot;10; url=https://example.com/newlocation\u0026quot; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;/html\u0026gt; `,url:"https://romankurnovskii.com/ru/posts/howto-redirect-to-url/"},"https://romankurnovskii.com/ru/tags/cli/":{title:"CLI",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/cli/"},"https://romankurnovskii.com/ru/categories/cli/":{title:"CLI",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/cli/"},"https://romankurnovskii.com/ru/categories/macos/":{title:"MacOS",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/macos/"},"https://romankurnovskii.com/ru/tags/tar/":{title:"tar",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/tar/"},"https://romankurnovskii.com/ru/posts/cheat-sheet-command-tar/":{title:"Шпаргалка tar архиватор",tags:["Linux","CLI","tar","cheatsheet"],content:`Кратко Создать:
tar cf archive.tar directory Распаковать:
tar xf archive.tar Создание mkdir my_dir # Создаем папку tar cf dir_archive.tar my_dir # Создаем архив с папкой ll # Проверяем содержимое текущего каталога # -rw-r--r-- 1 r staff 1.5K Jun 4 14:42 dir_archive.tar # drwxr-xr-x 2 r staff 64B Jun 4 14:42 my_dir Распаковка tar xf dir_archive.tar Сжатие tar czf dir_archive.tar.gz dir_archive.tar Распаковка сжатого файла tar xzf dir_archive.tar.gz Сжатие с помощью bzip2 tar cjf dir_archive.tar.bz2 my_dir Распаковка с помощью bzip2 tar xjf dir_archive.tar.bz2 Просмотр содержимого архива tar -tvf dir_archive.tar `,url:"https://romankurnovskii.com/ru/posts/cheat-sheet-command-tar/"},"https://romankurnovskii.com/ru/posts/howto-rename-files-in-python/":{title:"Как переименовать файлы в Python",tags:["Python"],content:`os.rename Если имеется весь путь до пути файла:
old_source = '/Users/r/Desktop/old_source.txt' new_source = '/Users/r/Desktop/new_source.txt' os.rename(\u0026quot;old_source\u0026quot;, \u0026quot;new_source\u0026quot;) Если имеется только имя файла, воспользуемся os.path.splitext(), который возвращает кортеж из имени файла и расширения:
import os for file in os.listdir(): name, ext = os.path.splitext(file) # return ('путь до файла без расщирения', '.txt') new_name = f\u0026quot;{name}_new{ext}\u0026quot; os.rename(file, new_name) pathlib С помощью встроенного модуля pathlib
Path.rename(new_name) from pathlib import Path for file in os.listdir(): f = Path(file) new_name = f\u0026quot;{f.stem}_new{f.suffix}\u0026quot; f.rename(new_name) shutil.move Модуль Shutil предлагает ряд высокоуровневых операций с файлами и коллекциями файлов. В частности, предусмотрены функции, поддерживающие копирование и удаление файлов.
import shutil old_source = '/Users/r/Desktop/old_source.txt' new_source = '/Users/r/Desktop/new_source.txt' newFileName = shutil.move(old_source, new_source) print(\u0026quot;Новый файл:\u0026quot;, newFileName) # Новый файл: /Users/r/Desktop/new_source.txt `,url:"https://romankurnovskii.com/ru/posts/howto-rename-files-in-python/"},"https://romankurnovskii.com/ru/categories/%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5/":{title:"программирование",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5/"},"https://romankurnovskii.com/ru/posts/howto-create-deepclone-js/":{title:"Как сделать глубокое клонирование объекта в JavaScript",tags:["JavaScript"],content:`В JavaScript объекты копируются по ссылке. Это означает, что фактически две(или более) ссылок ссылается на один объект Для глубокого клонирования мы можем воспользоваться рекурсией
Воспользуемся методом Object.assign() и возьмем пустой объект ({}), чтобы создать клон оригинального объекта. Используем Object.keys() и Array.prototype.forEach() для определения ключей-значений, которые нужно полностью клонировать (не ссылаться на них).
const deepClone = obj =\u0026gt; { let clone = Object.assign({}, obj); Object.keys(clone).forEach( key =\u0026gt; (clone[key] = typeof obj[key] === 'object' ? deepClone(obj[key]) : obj[key]) ); return Array.isArray(obj) \u0026amp;\u0026amp; obj.length ? (clone.length = obj.length) \u0026amp;\u0026amp; Array.from(clone) : Array.isArray(obj) ? Array.from(obj) : clone; }; const a = { foo: 'bar', obj: { a: 1, b: 2 } }; const b = deepClone(a); // a !== b, a.obj !== b.obj `,url:"https://romankurnovskii.com/ru/posts/howto-create-deepclone-js/"},"https://romankurnovskii.com/ru/tags/git/":{title:"git",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/git/"},"https://romankurnovskii.com/ru/tags/pyodide/":{title:"Pyodide",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/pyodide/"},"https://romankurnovskii.com/ru/tags/pyscript/":{title:"pyscript",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/pyscript/"},"https://romankurnovskii.com/ru/posts/pyscript-python-embedded-in-html/":{title:"PyScript - Python, встроенный в HTML",tags:["pyscript","Pyodide","python"],content:`PyScript PyScript - средство запуска Python в браузере, встроенное в HTML, был анонсирован на мероприятии PyCon в Солт-Лейк-Сити, США. Кнопка Instl здесь для шутки, так как установка не требуется
PyScript зависит от существующего проекта Pyodide, который является скомпилированным в WebAssembly интерпретатором CPython 3.8, позволяющим запускать Python в браузере, а также скомпилированных научных пакетов Python.
Связывание с файлами библиотеки CSS и JavaScript PyScript позволяет разработчикам встраивать код Python с помощью тега \u0026lt;py-script\u0026gt;, а также компонент \u0026lt;py-repl\u0026gt; (Read, Evaluate, Print, Loop), который позволяет Python печатать и выполняться динамически.
PyScript является открытым исходным кодом с использованием лицензии Apache 2.0.
Согласно сайту проекта, цели включают в себя включение Python в браузере без настройки на стороне сервера, запуск популярных пакетов Python, двунаправленную связь между JavaScript и Python и визуальную разработку с использованием «легкодоступных контролируемых компонентов пользовательского интерфейса, таких как кнопки, контейнеры, текстовые поля и многое другое».
Упрощение использования в браузере порадует не только ученых, разрабатывающих аналитические приложения, но и программистов любого профиля, ищущих альтернативу JavaScript — хотя разработчики проекта предупреждают, что это «чрезвычайно экспериментальный проект» и что он только проверен в веб-браузере Google Chrome.
Please be advised that PyScript is very alpha and under heavy development. There are many known issues, from usability to loading times, and you should expect things to change often. We encourage people to play and explore with PyScript, but at this time we do not recommend using it for production.
Туториал PyScript Попробуем скачать, настроить и запустить приложение PyScript в браузере.
Рабочая среда Разработчики PyScript пишут, что для работы не требуется никакой среды разработки, кроме веб-браузера. Попробуем запустить в Chrome.
Установка Можно скачать весь пакет с сайта, но будем использовать скрипт, с сервера pyscript.net
Hello World Создаем файл hello.html
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;py-script\u0026gt; print('Hello, World!') \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Тег \u0026lt;py-script\u0026gt; расположен внутри HTML body. Внутри этого тега будем пиcать Python код.
Откроем файл в браузере Работает!
Тег py-script Тег \u0026lt;py-script\u0026gt; позволяет писать многострочный код
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;py-script\u0026gt; print(\u0026quot;Let's compute π:\u0026quot;) def compute_pi(n): pi = 2 for i in range(1,n): pi *= 4 * i ** 2 / (4 * i ** 2 - 1) return pi pi = compute_pi(100000) s = f\u0026quot;π is approximately {pi:.3f}\u0026quot; print(s) \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Важно соблюдать отступы в самом блоке Python. Но Начальную строку кода можно начинать и с начала строки
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;py-script\u0026gt; print(\u0026quot;Let's compute π:\u0026quot;) def compute_pi(n): pi = 2 for i in range(1,n): pi *= 4 * i ** 2 / (4 * i ** 2 - 1) return pi pi = compute_pi(100000) s = f\u0026quot;π is approximately {pi:.3f}\u0026quot; print(s) \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Запись внутри HTML элементов В приведенном выше примере у нас был один тег \u0026lt;py-script\u0026gt;, выводящий одну или несколько строк на страницу по порядку. Внутри \u0026lt;py-script\u0026gt; есть доступ к модулю pyscript, который предоставляет метод .write() для отправки строк в помеченные элементы на странице.
Например, мы добавим некоторые элементы стиля и предоставим заполнители для тега \u0026lt;py-script\u0026gt; для записи.
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;link href=\u0026quot;https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\u0026quot; rel=\u0026quot;stylesheet\u0026quot; crossorigin=\u0026quot;anonymous\u0026quot;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;b\u0026gt;\u0026lt;p\u0026gt;Today is \u0026lt;u\u0026gt;\u0026lt;label id='today'\u0026gt;\u0026lt;/label\u0026gt;\u0026lt;/u\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/b\u0026gt; \u0026lt;br\u0026gt; \u0026lt;div id=\u0026quot;my-custom-pi\u0026quot; class=\u0026quot;alert alert-primary\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;py-script\u0026gt; import datetime as dt pyscript.write('today', dt.date.today().strftime('%A %B %d, %Y')) def compute_pi(n): pi = 2 for i in range(1,n): pi *= 4 * i ** 2 / (4 * i ** 2 - 1) return pi pi = compute_pi(100000) pyscript.write('my-custom-pi', f'π is approximately {pi:.3f}') \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Тег py-env В дополнение к стандартной библиотеке Python и модулю pyscript, многие сторонние пакеты работают с PyScript. Чтобы их использовать, нужно объявить зависимости с помощью тега \u0026lt;py-env\u0026gt; в заголовке HTML. Вы также можете ссылаться на файлы .whl прямо на диске
\u0026lt;py-env\u0026gt; - './static/wheels/travertino-0.1.3-py3-none-any.whl' - './static/wheels/my-other-package-0.0.1-py3-none-any.whl' \u0026lt;/py-env\u0026gt; \u0026lt;py-script\u0026gt; #my python code ... \u0026lt;/py-script\u0026gt; Пример с NumPy
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;py-env\u0026gt; - numpy - matplotlib \u0026lt;/py-env\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Let's plot random numbers\u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026quot;plot\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;py-script output=\u0026quot;plot\u0026quot;\u0026gt; import matplotlib.pyplot as plt import numpy as np x = np.random.randn(1000) y = np.random.randn(1000) fig, ax = plt.subplots() ax.scatter(x, y) fig \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Импорт локальный модулей Мы также можем использовать собсвтенные модули, которые импортируем внутри тега \u0026lt;py-script\u0026gt;
Например, создадим файл \`data.py\u0026rsquo; и запишем в него собственную функцию
# data.py import numpy as np def make_x_and_y(n): x = np.random.randn(n) y = np.random.randn(n) return x, y Внутри тега \u0026lt;py-env\u0026gt; добавим стандартные модули и путь до нашего локального модуля
\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://pyscript.net/latest/pyscript.css\u0026quot; /\u0026gt; \u0026lt;script defer src=\u0026quot;https://pyscript.net/latest/pyscript.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;py-env\u0026gt; - numpy - matplotlib - paths: - /data.py \u0026lt;/py-env\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Let's plot random numbers\u0026lt;/h1\u0026gt; \u0026lt;div id=\u0026quot;plot\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;py-script output=\u0026quot;plot\u0026quot;\u0026gt; import matplotlib.pyplot as plt from data import make_x_and_y x, y = make_x_and_y(n=1000) fig, ax = plt.subplots() ax.scatter(x, y) fig \u0026lt;/py-script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Тег py-repl Тег \u0026lt;py-repl\u0026gt; создает компонент REPL(Read–eval–print loop), который отображается на странице как редактор кода, что позволяет писать исполняемый код в строке.
Тег py-config Тег \u0026lt;py-config\u0026gt; используется для установки и настройки общих метаданных о вашем приложении PyScript в формате YAML.
\u0026lt;py-config\u0026gt; - autoclose_loader: false - runtimes: - src: \u0026quot;https://cdn.jsdelivr.net/pyodide/v0.20.0/full/pyodide.js\u0026quot; name: pyodide-0.20 lang: python \u0026lt;/py-config\u0026gt; Тег py-title Тег визуального отображения. Добавляет компонент заголовка статического текста, который стилизует текст внутри тега как заголовок страницы.
Тег py-box Создает объект-контейнер, который можно использовать для размещения одного или нескольких визуальных компонентов, определяющих, как элементы \u0026lt;py-box\u0026gt; должны выравниваться и отображаться на странице.
Тег py-inputbox Позволяет вставить окно с текстовым полем
Тег py-button Добавляет кнопку, к которой авторы могут добавлять метки и обработчики событий для действий на кнопке, таких как on_focus или on_click.
Ресурсы Примеры использования PyScript Вопросы по PyScript `,url:"https://romankurnovskii.com/ru/posts/pyscript-python-embedded-in-html/"},"https://romankurnovskii.com/ru/tags/mac/":{title:"mac",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/mac/"},"https://romankurnovskii.com/ru/posts/mac-setup-development/":{title:"Mac Setup 2022",tags:["mac","mac setup web developer","mac setup javascript"],content:`MacBook Pro Specification 13-inch Apple M1 Pro M1 2020 16 GB RAM 512 GB SSD QWERTY = English/Hebrew macOS Monterey (Update always) Homebrew Install Homebrew as package manager for macOS:
## paste in terminal and follow the instructions /bin/bash -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026quot; Update everything in Homebrew to recent version:
brew update Add additional source for casks:
brew tap homebrew/cask-versions Install GUI applications (read more about these in GUI Applications):
brew install --cask \\ all-in-one-messenger \\ deepl \\ canva \\ dbeaver-community \\ deepl \\ discord \\ disk-inventory-x \\ docker \\ figma \\ firefox \\ google-chrome \\ google-drive \\ grammarly \\ iterm2 \\ kap \\ macx-youtube-downloader \\ mongodb-compass \\ notion \\ obs \\ postman \\ rectangle \\ reverso \\ spark-ar-studio spotify \\ sublime-text \\ syncthing \\ tor-browser \\ transmission \\ utm \\ viber \\ visual-studio-code \\ vlc \\ yandex-disk \\ zettlr \\ zoom Install terminal applications (read more about these in Terminal Applications):
brew install \\ git \\ ffmpeg \\ nvm \\ jupyterlab Additional GUI Applications Kotatogram Kotatogram - Experimental fork of Telegram Desktop. Folders with features
GUI Applications Google Chrome Google Chrome (web development, web browsing)
Preferences set default browser always show bookmarks import bookmarks from previous machine Chrome Developer Tools Network -\u0026gt; only \u0026ldquo;Fetch/XHR\u0026rdquo; Search Shortcuts. Add Shortucts for different search engines. chrome://settings/searchEngines Yandex, search only in Russia. Shortcut: vv url: https://yandex.ru/{yandex:searchPath}?text=%s\u0026amp;{yandex:referralID}\u0026amp;lr=101443\u0026amp;rstr=-225 Youtube Shortcut: yy url: https://www.youtube.com/results?search_query=%s\u0026amp;page={startPage?}\u0026amp;utm_source=opensearch Chrome Extensions Google Translate DeepL Translate - AI translator React Developer Tools Pocket Session Buddy (Manage Browser Tabs and Bokmarks) LanguageTool (multilingual grammar, style, and spell checker) RSS Feed Reader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) Inoreader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android) 30 Seconds of Knowledge (random code snippet on a new tab) JSON Formatter picture-in-picture (yutube/video above other screens) Visual CSS Editor (Customize any website visually) Opus●Guide (Step-by-step for instructions) Disk Inventory X Disk Inventory X (disk usage utility for macOS)
Docker Docker (Docker, see setup)
used for running databases (e.g. PostgreSQL, MongoDB) in container without cluttering the Mac Preferences enable \u0026ldquo;Use Docker Compose\u0026rdquo; Firefox Firefox (web development)
itsycal Itsycal is a tiny menu bar calendar.
Visual Studio Code (VSCode) Visual Studio Code (web development IDE)
Settings
Sublime Text Sublime Text (editor)
Kap Screen recorder / gif maker
Maccy Maccy (clipboard manager)
enable \u0026ldquo;Launch at Login\u0026rdquo; OBS OBS (for video recording and live streaming)
for Native Mac Screen recorder Base (Canvas) 2880x1800 (Ratio: 16:10) Output 1728x1080 Spotify Spotify
Syncthing syncthing - Sync folders/files between devices. I use to backup all photos/video from mobile to PC
Transmission Transmission (A torrent client that I use. Very minimal in its UI but very powerful and has all the features that I need)
UTM UTM (Virtual machines UI using QEMU)
download ubuntu for arm, doc On error with shared folder: Could not connect: Connection refused open in browser: http://127.0.0.1:9843/ For Debian install spice-webdavd for shared folder. https://packages.debian.org/search?keywords=spice-webdavd, https://github.com/utmapp/UTM/issues/1204 sudo apt install spice-vdagent spice-webdavd -y VLC VLC (video player)
use as default for video files Terminal Applications nvm nvm (node version manager)
jupyterlab jupyterlab (Jupyter - python development, fast code snippets)
jupyter notebook - to start jupyter notebook ffmpeg ffmpeg (Converting video and audio)
compress video: ffmpeg -i input.mp4 -c:v libx264 -crf 23 -preset slow -c:a aac -b:a 192k output.mp4 # or ffmpeg -i input.mp4 output.avi convert video to .gif: - ffmpeg \\ -i input.mp4 \\ -ss 00:00:00.000 \\ -pix_fmt rgb24 \\ -r 10 \\ -s 960x540 \\ -t 00:00:10.000 \\ output.gif NVM for Node/npm The node version manager (NVM) is used to install and manage multiple Node versions. After you have installed it via Homebrew in a previous step, type the following commands to complete the installation:
echo \u0026quot;source $(brew --prefix nvm)/nvm.sh\u0026quot; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc ## or alias ## zshsource Now install the latest LTS version on the command line:
nvm install \u0026lt;latest LTS version from https://nodejs.org/en/\u0026gt; Afterward, check whether the installation was successful and whether the node package manager (npm) got installed along the way:
node -v \u0026amp;\u0026amp; npm -v Update npm to its latest version:
npm install -g npm@latest And set defaults for npm:
npm set init.author.name \u0026quot;your name\u0026quot; npm set init.author.email \u0026quot;you@example.com\u0026quot; npm set init.author.url \u0026quot;example.com\u0026quot; If you are a library author, log in to npm too:
npm adduser That\u0026rsquo;s it. If you want to list all your Node.js installation, type the following:
nvm list If you want to install a newer Node.js version, then type:
nvm install \u0026lt;version\u0026gt; --reinstall-packages-from=$(nvm current) nvm use \u0026lt;version\u0026gt; nvm alias default \u0026lt;version\u0026gt; Optionally install yarn if you use it as alternative to npm:
npm install -g yarn yarn -v If you want to list all globally installed packages, run this command:
npm list -g --depth=0 That\u0026rsquo;s it. You have a running version of Node.js and its package manager.
OH MY ZSH MacOS already comes with zsh as default shell. Install Oh My Zsh for an improved (plugins, themes, \u0026hellip;) experience. Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.
Install:
sh -c \u0026quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026quot; Update everything (e.g. plugins) in Oh My Zsh to recent version:
omz update Install fonts for themes:
brew tap homebrew/cask-fonts brew install --cask font-hack-nerd-font iTerm2 Install theme Theme description brew install romkatv/powerlevel10k/powerlevel10k echo \u0026quot;source $(brew --prefix)/opt/powerlevel10k/powerlevel10k.zsh-theme\u0026quot; \u0026gt;\u0026gt;~/.zshrc Enable suggestions git clone https://github.com/zsh-users/zsh-autosuggestions \${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions echo \u0026quot;plugins=(zsh-autosuggestions)\u0026quot; \u0026gt;\u0026gt;~/.zshrc Open new tab(CMD+T)/restart iTerm to proceed with theme setup
Terminal Script and Aliases Update .zprofile. Еhe changes will take effect after restarting the terminal
vi ~/.zprofile Automatic software updates Add script to zprofile that updates everything:
Update, upgrade all and cleanup softwareupdate - system software update tool We can execute this command on strartup, but i prefer handle it. When I kick of upd command in terminal, it will update everythin I need:
alias upd='brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i' Add aliases to latest versions pip \u0026amp; python
alias pip=pip3 alias python=python3 Final view of .zprofile
... alias pip=pip3 alias python=python3 alias upd='omz update; brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i' Links https://www.robinwieruch.de/mac-setup-web-development/ https://sourabhbajaj.com/mac-setup/iTerm/ack.html https://www.engineeringwithutsav.com/blog/spice-up-your-macos-terminal `,url:"https://romankurnovskii.com/ru/posts/mac-setup-development/"},"https://romankurnovskii.com/ru/tags/mac-setup-javascript/":{title:"mac setup javascript",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/mac-setup-javascript/"},"https://romankurnovskii.com/ru/tags/mac-setup-web-developer/":{title:"mac setup web developer",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/mac-setup-web-developer/"},"https://romankurnovskii.com/ru/tags/ansible/":{title:"Ansible",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/ansible/"},"https://romankurnovskii.com/ru/tags/bash/":{title:"bash",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/bash/"},"https://romankurnovskii.com/ru/tags/ssh/":{title:"ssh",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/ssh/"},"https://romankurnovskii.com/ru/tags/nano/":{title:"nano",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/nano/"},"https://romankurnovskii.com/ru/tags/vim/":{title:"vim",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/vim/"},"https://romankurnovskii.com/ru/tags/%D1%84%D0%B0%D0%B9%D0%BB%D0%BE%D0%B2%D0%B0%D1%8F-%D1%81%D0%B8%D1%82%D0%B5%D0%BC%D0%B0-linux/":{title:"файловая ситема linux",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/%D1%84%D0%B0%D0%B9%D0%BB%D0%BE%D0%B2%D0%B0%D1%8F-%D1%81%D0%B8%D1%82%D0%B5%D0%BC%D0%B0-linux/"},"https://romankurnovskii.com/ru/tags/vagrant/":{title:"vagrant",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/vagrant/"},"https://romankurnovskii.com/ru/tags/virtualbox/":{title:"virtualbox",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/virtualbox/"},"https://romankurnovskii.com/ru/tags/golang/":{title:"golang",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/golang/"},"https://romankurnovskii.com/ru/tags/hello-world/":{title:"hello-world",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/hello-world/"},"https://romankurnovskii.com/ru/tags/cicd/":{title:"cicd",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/cicd/"},"https://romankurnovskii.com/ru/tags/learning/":{title:"learning",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/learning/"},"https://romankurnovskii.com/ru/tags/tests/":{title:"tests",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/tests/"},"https://romankurnovskii.com/ru/tags/agile/":{title:"agile",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/agile/"},"https://romankurnovskii.com/ru/tags/kanban/":{title:"kanban",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/kanban/"},"https://romankurnovskii.com/ru/tags/scrum/":{title:"scrum",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/scrum/"},"https://romankurnovskii.com/ru/tags/90daysofdevops/":{title:"90daysofdevops",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/90daysofdevops/"},"https://romankurnovskii.com/ru/docs/90daysofdevops/":{title:"90 дней DevOps",tags:[],content:` Оригинальный репозиторий на GitHub. Автор версии на английском - Michael Cade Цель данного цикла статей - быстрый обзор и прохождение по всему тьюториалу по DevOps.
Progress [✔️] ♾️ 1 \u0026gt; Введение Что такое и почему нам нужен DevOps [✔️] ♾️ 2 \u0026gt; Задачи DevOps-инженера [✔️] ♾️ 3 \u0026gt; DevOps Lifecycle - Ориентированность на приложения [✔️] ♾️ 4 \u0026gt; DevOps \u0026amp; Agile [✔️] ♾️ 5 \u0026gt; Plan \u0026gt; Code \u0026gt; Build \u0026gt; Testing \u0026gt; Release \u0026gt; Deploy \u0026gt; Operate \u0026gt; Monitor \u0026gt; [✔️] ♾️ 6 \u0026gt; DevOps - The real stories Изучение языка программирования [✔️] ⌨️ 7 \u0026gt; The Big Picture: DevOps \u0026amp; Learning a Programming Language [✔️] ⌨️ 8 \u0026gt; Setting up your DevOps environment for Go \u0026amp; Hello World [✔️] ⌨️ 9 \u0026gt; Let\u0026rsquo;s explain the Hello World code [✔️] ⌨️ 10 \u0026gt; The Go Workspace \u0026amp; Compiling \u0026amp; running code [✔️] ⌨️ 11 \u0026gt; Variables, Constants \u0026amp; Data Types [✔️] ⌨️ 12 \u0026gt; Getting user input with Pointers and a finished program [✔️] ⌨️ 13 \u0026gt; Tweet your progress with our new App Knowing Linux Basics [✔️] 🐧 14 \u0026gt; The Big Picture: DevOps and Linux [✔️] 🐧 15 \u0026gt; Linux Commands for DevOps (Actually everyone) [✔️] 🐧 16 \u0026gt; Managing your Linux System, Filesystem \u0026amp; Storage [✔️] 🐧 17 \u0026gt; Text Editors - nano vs vim [✔️] 🐧 18 \u0026gt; SSH \u0026amp; Web Server(LAMP) [✔️] 🐧 19 \u0026gt; Automate tasks with bash scripts [✔️] 🐧 20 \u0026gt; Dev workstation setup - All the pretty things Understand Networking [✔️] 🌐 21 \u0026gt; The Big Picture: DevOps and Networking [✔️] 🌐 22 \u0026gt; The OSI Model - The 7 Layers [✔️] 🌐 23 \u0026gt; Network Protocols [✔️] 🌐 24 \u0026gt; Network Automation [✔️] 🌐 25 \u0026gt; Python for Network Automation [✔️] 🌐 26 \u0026gt; Building our Lab [✔️] 🌐 27 \u0026gt; Getting Hands-On with Python \u0026amp; Network Stick to one Cloud Provider [✔️] ☁️ 28 \u0026gt; The Big Picture: DevOps \u0026amp; The Cloud [✔️] ☁️ 29 \u0026gt; Microsoft Azure Fundamentals [✔️] ☁️ 30 \u0026gt; Модули безопасности Microsoft Azure [✔️] ☁️ 31 \u0026gt; Microsoft Azure Compute Models [✔️] ☁️ 32 \u0026gt; Microsoft Azure Storage \u0026amp; Database Models [✔️] ☁️ 33 \u0026gt; Microsoft Azure Networking Models + Azure Management [✔️] ☁️ 34 \u0026gt; Microsoft Azure Hands-On Scenarios Use Git Effectively [✔️] 📚 35 \u0026gt; The Big Picture: Git - Version Control [✔️] 📚 36 \u0026gt; Installing \u0026amp; Configuring Git [✔️] 📚 37 \u0026gt; Gitting to know Git [✔️] 📚 38 \u0026gt; Staging \u0026amp; Changing [✔️] 📚 39 \u0026gt; Viewing, unstaging, discarding \u0026amp; restoring [✔️] 📚 40 \u0026gt; Social Network for code [✔️] 📚 41 \u0026gt; The Open Source Workflow Containers [✔️] 🏗️ 42 \u0026gt; The Big Picture: Containers [✔️] 🏗️ 43 \u0026gt; What is Docker \u0026amp; Getting installed [✔️] 🏗️ 44 \u0026gt; Docker Images \u0026amp; Hands-On with Docker Desktop [✔️] 🏗️ 45 \u0026gt; The anatomy of a Docker Image [✔️] 🏗️ 46 \u0026gt; Docker Compose [✔️] 🏗️ 47 \u0026gt; Docker Networking \u0026amp; Security [✔️] 🏗️ 48 \u0026gt; Alternatives to Docker Kubernetes [✔️] ☸ 49 \u0026gt; The Big Picture: Kubernetes [✔️] ☸ 50 \u0026gt; Choosing your Kubernetes platform [✔️] ☸ 51 \u0026gt; Deploying your first Kubernetes Cluster [✔️] ☸ 52 \u0026gt; Setting up a multinode Kubernetes Cluster [✔️] ☸ 53 \u0026gt; Rancher Overview - Hands On [✔️] ☸ 54 \u0026gt; Kubernetes Application Deployment [✔️] ☸ 55 \u0026gt; State and Ingress in Kubernetes Learn Infrastructure as Code [✔️] 🤖 56 \u0026gt; The Big Picture: IaC [✔️] 🤖 57 \u0026gt; An intro to Terraform [✔️] 🤖 58 \u0026gt; HashiCorp Configuration Language (HCL) [✔️] 🤖 59 \u0026gt; Create a VM with Terraform \u0026amp; Variables [✔️] 🤖 60 \u0026gt; Docker Containers, Provisioners \u0026amp; Modules [✔️] 🤖 61 \u0026gt; Kubernetes \u0026amp; Multiple Environments [✔️] 🤖 62 \u0026gt; Testing, Tools \u0026amp; Alternatives Automate Configuration Management [✔️] 📜 63 \u0026gt; The Big Picture: Configuration Management [✔️] 📜 64 \u0026gt; Ansible: Getting Started [✔️] 📜 65 \u0026gt; Ansible Playbooks [✔️] 📜 66 \u0026gt; Ansible Playbooks Continued\u0026hellip; [✔️] 📜 67 \u0026gt; Using Roles \u0026amp; Deploying a Loadbalancer [✔️] 📜 68 \u0026gt; Tags, Variables, Inventory \u0026amp; Database Server config [✔️] 📜 69 \u0026gt; All other things Ansible - Automation Controller, AWX, Vault Create CI/CD Pipelines [✔️] 🔄 70 \u0026gt; The Big Picture: CI/CD Pipelines [✔️] 🔄 71 \u0026gt; What is Jenkins? [✔️] 🔄 72 \u0026gt; Getting hands on with Jenkins [✔️] 🔄 73 \u0026gt; Building a Jenkins pipeline [✔️] 🔄 74 \u0026gt; Hello World - Jenkinsfile App Pipeline [✔️] 🔄 75 \u0026gt; GitHub Actions Overview [✔️] 🔄 76 \u0026gt; ArgoCD Overview Monitoring, Log Management, and Data Visualisation [✔️] 📈 77 \u0026gt; The Big Picture: Monitoring [✔️] 📈 78 \u0026gt; Hands-On Monitoring Tools [✔️] 📈 79 \u0026gt; The Big Picture: Log Management [✔️] 📈 80 \u0026gt; ELK Stack [✔️] 📈 81 \u0026gt; Fluentd \u0026amp; FluentBit [✔️] 📈 82 \u0026gt; EFK Stack [✔️] 📈 83 \u0026gt; Data Visualisation - Grafana Store \u0026amp; Protect Your Data [✔️] 🗃️ 84 \u0026gt; The Big Picture: Data Management [✔️] 🗃️ 85 \u0026gt; Data Services [✔️] 🗃️ 86 \u0026gt; Backup all the platforms [✔️] 🗃️ 87 \u0026gt; Hands-On Backup \u0026amp; Recovery [✔️] 🗃️ 88 \u0026gt; Application Focused Backups [✔️] 🗃️ 89 \u0026gt; Disaster Recovery [✔️] 🗃️ 90 \u0026gt; Data \u0026amp; Application Mobility License Shield: This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
`,url:"https://romankurnovskii.com/ru/docs/90daysofdevops/"},"https://romankurnovskii.com/ru/tags/css/":{title:"css",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/css/"},"https://romankurnovskii.com/ru/tags/html/":{title:"html",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/html/"},"https://romankurnovskii.com/ru/tags/markdown/":{title:"markdown",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/markdown/"},"https://romankurnovskii.com/ru/authors/michael-cade/":{title:"michael-cade",tags:[],content:"",url:"https://romankurnovskii.com/ru/authors/michael-cade/"},"https://romankurnovskii.com/ru/series/roadmaps/":{title:"Roadmaps",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/roadmaps/"},"https://romankurnovskii.com/ru/categories/syntax/":{title:"syntax",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/syntax/"},"https://romankurnovskii.com/ru/tags/themes/":{title:"themes",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/themes/"},"https://romankurnovskii.com/ru/categories/themes/":{title:"themes",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/themes/"},"https://romankurnovskii.com/ru/series/themes-guide/":{title:"Themes Guide",tags:[],content:"",url:"https://romankurnovskii.com/ru/series/themes-guide/"},"https://romankurnovskii.com/ru/posts/markdown-syntax/":{title:"Руководство по оформлению Markdown файлов",tags:["markdown","css","html","themes"],content:`Эта статья предлагает пример базового синтаксиса Markdown, который можно использовать в файлах содержимого Hugo, а также показывает, украшаются ли основные элементы HTML с помощью CSS в теме Hugo.
Рекомендации по оформления статьи
Заголовки Заголовки первого и второго уровней, выполненные с помощью подчеркивания, выглядят следующим образом:
Заголовок первого уровня ======================== Заголовок второго уровня ------------------------- Заголовок первого уровня Заголовок второго уровня Заголовки всех шести уровней можно обозначать и с помощью символа («#»)
# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 H1 H2 H3 H4 H5 H6 Параграфы Для оформления абзацев в html используются теги \u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;, но в Markdown блок текста автоматически преобразуется в параграф.
Для вставки пустой строки необходимо два раза поставить символ переноса строки (нажать на Enter)
Lorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit! Lorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae. Цитаты Элемент blockquote представляет содержимое, которое цитируется из другого источника, по желанию с цитатой, которая должна находиться в элементе footer или cite, и по желанию с изменениями в строке, такими как аннотации и сокращения.
Блок-цитата без указания авторства Tiam, ad mint andaepu dandae nostion secatur sequo quae. Обратите внимание, что вы можете использовать синтаксис Markdown внутри блочной цитаты.
Блок-цитата с указанием авторства Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.
— Rob Pike1
\u0026gt;Это пример цитаты, \u0026gt;в которой перед каждой строкой \u0026gt;ставится угловая скобка. \u0026gt;Это пример цитаты, в которой угловая скобка ставится только перед началом нового параграфа. \u0026gt;Второй параграф. Это пример цитаты, в которой перед каждой строкой ставится угловая скобка.
Это пример цитаты, в которой угловая скобка ставится только перед началом нового параграфа. Второй параграф.
\u0026gt; Первый уровень цитирования \u0026gt;\u0026gt; Второй уровень цитирования \u0026gt;\u0026gt;\u0026gt; Третий уровень цитирования \u0026gt; \u0026gt;Первый уровень цитирования Первый уровень цитирования
Второй уровень цитирования
Третий уровень цитирования
Первый уровень цитирования
Таблицы Таблицы не являются частью основной спецификации Markdown, но Hugo поддерживает их из коробки.
| Name | Age | | ----- | --- | | Bob | 27 | | Alice | 23 | Name Age Bob 27 Alice 23 В ячейках разделительной строки используются только символы - и :. Символ : ставится в начале, в конце или с обеих сторон содержимого ячейки разделительной строки, чтобы обозначить выравнивание текста в соответствующем столбце по левой стороне, по правой стороне или по центру.
Колонка по левому краю | Колонка по правому краю | Колонка по центру :--- | ---: | :---: Текст | Текст | Текст Колонка по левому краю Колонка по правому краю Колонка по центру Текст Текст Текст Markdown внутри таблицы | Italics | Bold | Code | | --------- | -------- | ------ | | *italics* | **bold** | \`code\` | Italics Bold Code italics bold code Блоки кода Блок кода с обратными кавычками \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Блок кода с отступом в четыре пробела \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Блок кода с внутренним шорткодом подсветки Hugo \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Списки Оформляйте заголовки единообразно. В конце заголовка точку не ставьте.
Правильно Неправильно Получение сертификата Создание кластера Получить сертификат Создание кластера Получить сертификат Создать кластер Если требуется описать последовательность действий, используйте нумерованный список. В конце строк ставьте точку.
Если порядок пунктов неважен, используйте маркированный список. Оформляйте его одним из способов:
Если элементы списка — отдельные предложения, начинайте их с заглавной буквы и ставьте точку в конце. Если вводная фраза и список составляют одно предложение, то элементы списка должны начинаться со строчной буквы и завершаться точкой с запятой. Последний элемент списка завершается точкой. Если список состоит из названий или значений параметров (без пояснений), знаки в конце строк не ставьте. Упорядоченный список First item Second item Third item Чтобы оформить упорядоченный нумерованный список, используйте цифры с символом . или ). Рекомендованный формат разметки: цифра 1 и символ ..
1. Первый пункт 1. Второй пункт 1. Третий пункт будет отображаться как:
Первый пункт Второй пункт Третий пункт Чтобы оформить вложенный упорядоченный список, добавьте отступ для элементов дочернего списка. Допустимый размер отступа — от двух до пяти пробелов. Рекомендуемый размер отступа — четыре пробела.
Например, разметка:
1. Первый пункт 1. Вложенный пункт 1. Вложенный пункт 1. Второй пункт будет отображаться как:
Первый пункт Вложенный пункт Вложенный пункт Второй пункт Неупорядоченный список List item Another item And another item Вложенный список Fruit Apple Orange Banana Dairy Milk Cheese Другие элементы - abbr, sub, sup, kbd, mark GIF is a bitmap image format.
H2O
Xn + Yn = Zn
Press CTRL+ALT+Delete to end the session.
Most salamanders are nocturnal, and hunt for insects, worms, and other small creatures.
💡 Структура данных — это контейнер, который хранит данные в определённом формате. Этот контейнер решает, каким образом внешний мир может эти данные считать или изменить.
Приведенная выше цитата взята из книги Роба Пайка talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;
`,url:"https://romankurnovskii.com/ru/posts/markdown-syntax/"},"https://romankurnovskii.com/ru/categories/%D1%82%D1%80%D0%B5%D0%BA%D0%B8/":{title:"треки",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/%D1%82%D1%80%D0%B5%D0%BA%D0%B8/"},"https://romankurnovskii.com/ru/tags/google-script/":{title:"google script",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/google-script/"},"https://romankurnovskii.com/ru/tags/google-sheets/":{title:"google sheets",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/google-sheets/"},"https://romankurnovskii.com/ru/posts/google-sheets-2-json/":{title:"Отображение таблицы Google Sheets в JSON",tags:["google sheets","google script"],content:`Задача Есть таблица google. Необходимо конвертировать ее в JSON и не делать каждый раз ручной экспорт.
Условия таблица закрыта для общего просмотра json отображение читать по ссылке План Использовать webapps от google. Парсить google таблицу и выдавать готовый url с json.
Подготовка Открываем Таблицу Google Extensions → Apps Script Создаем скрипт Как работает endpoint. Документация
Когда пользователь посещает приложение или программа отправляет приложению HTTP-запрос GET, Apps Script запускает функцию doGet(e).
Когда отправляется приложению HTTP-запрос POST, вместо этого Apps Script запускает doPost(e).
В обоих случаях аргумент e представляет собой параметр события, который может содержать информацию о любых параметрах запроса.
Дополнительные условия в запрос сейчас посылать не буду.
Итого функция с получением массива и функция с выдачей результата:
const sheetName = 's1' // название листа const sheetRange = 'A:J' // диапазон const sheet = SpreadsheetApp.getActive().getSheetByName(sheetName) function getData(){ const result = [] const values = sheet.getRange(sheetRange).getValues() const lastRow = parseInt(sheet.getLastRow()) for (let i = 1; i \u0026lt; lastRow; i++) { result.push(values[i]) } return result } function doGet() { const data = getData() return ContentService.createTextOutput( JSON.stringify( {'result': data} ) ).setMimeType(ContentService.MimeType.JSON) } Публикуем приложение Результат `,url:"https://romankurnovskii.com/ru/posts/google-sheets-2-json/"},"https://romankurnovskii.com/ru/p/":{title:"Ps",tags:[],content:"",url:"https://romankurnovskii.com/ru/p/"},"https://romankurnovskii.com/ru/p/privacy_ru/":{title:"Политика конфиденциальности",tags:[],content:`Список приложений: ФСФР - Базовый экзамен
Настоящая Политика конфиденциальности персональных данных (далее – Политика конфиденциальности) действует в отношении всей информации, которую приложения из раздела: \u0026ldquo;Список приложений\u0026rdquo; могут получить о Пользователе во время использования.
Общие положения 1.1. Целью Политики конфиденциальности является реализация требований законодательства в области обработки и защиты персональных данных. 1.2. Настоящий Регламент разработан на основании Конституции Российской Федерации, Трудового кодекса Российской Федерации, Гражданского кодекса Российской Федерации, Уголовного кодекса Российской Федерации, Кодекса об административных правонарушениях Российской Федерации, Федерального закона Российской Федерации «О персональных данных» № 152-ФЗ от 27 июля 2006 года.
Основные понятия На основании законодательства Российской Федерации в целях настоящего Политики конфиденциальности используются следующие понятия 2.1. Администратор Приложений (далее – Администратор) – уполномоченные сотрудник, который организуют и (или) осуществляет обработку персональных данных, а также определяет цели обработки персональных данных, состав персональных данных, подлежащих обработке, действия (операции), совершаемые с персональными данными.
2.2. Пользователь – лицо, являющееся субъектом персональных данных и сообщающее свои персональные данные посредством Приложений.
2.3. «Персональные данные» - любая информация, относящаяся к прямо или косвенно к определяемому физическому лицу (субъекту персональных данных).
2.4. «Обработка персональных данных» - любое действие (операция) или совокупность действий (операций), совершаемых с использованием средств автоматизации или без использования таких средств с персональными данными, включая сбор, запись, систематизацию, накопление, хранение, уточнение (обновление, изменение), извлечение, использование, передачу (распространение, предоставление, доступ), обезличивание, блокирование, удаление, уничтожение персональных данных.
Общие положения 3.1. Использование Пользователем Приложений означает согласие с настоящей Политикой конфиденциальности и условиями обработки персональных данных Пользователя. 3.2. В случае несогласия с условиями Политики конфиденциальности Пользователь должен прекратить использование Приложений.
3.3. Настоящая Политика конфиденциальности применяется только к Приложениям.
Предмет политики конфиденциальности 4.1. Настоящая Политика конфиденциальности устанавливает обязательства по неразглашению и обеспечению режима защиты конфиденциальности персональных данных, которые Пользователь. 4.2. Персональные данные, разрешённые к обработке в рамках настоящей Политики конфиденциальности, предоставляются Пользователем путём заполнения и включают в себя следующую информацию:
фамилию, имя, отчество;
контактный телефон Пользователя;
e-mail
4.3. Любая иная персональная информация неоговоренная выше подлежит надежному хранению и нераспространению.
4.4. Обработка персональных данных осуществляется с использованием интернет-сервисов сторонних организаций, в том числе с использованием интернет-сервиса Google Analitics. С порядком обработки данных с помощью интернет-сервиса Google Analitics можно ознакомиться, перейдя по ссылке https://www.google.ru/policies/privacy/partners/
Цели сбора персональных данных 5.1. Запрещено обрабатывать персональные данные Пользователя о его политических, религиозных и иных убеждениях и частной жизни. 5.2. При передаче персональных данных Пользователя, Администратор предупреждает лиц, получающих персональные данные Пользователя, о том, что эти данные могут быть использованы лишь в целях, для которых они сообщены. Данная норма не распространяется на обмен персональными данными Пользователей в порядке, установленном федеральными законами.
5.3. Защита персональных данных Пользователя от неправомерного их использования или утраты обеспечивается в порядке, установленном законодательством Российской Федерации.
5.4. Пользователь вправе в любое время по своему усмотрению отозвать свое согласие на обработку своих персональных данных путем отправки сообщения об удалении персональных данных по следующему e-mail: r.kurnovskii@gmail.com.
Способы и сроки обработки персональных данных 6.1. Обработка персональных данных Пользователя осуществляется без ограничения срока, любым законным способом, в том числе в информационных системах персональных данных с использованием средств автоматизации или без использования таких средств. 6.2. При утрате или разглашении персональных данных Администрация сайта информирует Пользователя об утрате или разглашении персональных данных.
Обязательства сторон 7.1. Администратор обязан: 7.1.1. Использовать полученную информацию исключительно для целей, указанных в п. 5 настоящей Политики конфиденциальности.
7.1.2. Обеспечить хранение конфиденциальной информации в тайне.
7.1.3. Принимать меры предосторожности для защиты конфиденциальности персональных данных Пользователя согласно порядку, обычно используемого для защиты такого рода информации в существующем деловом обороте.
7.1.4. Осуществить блокирование, удаление персональных данных, относящихся к соответствующему Пользователю, с момента обращения или запроса Пользователя или его законного представителя либо уполномоченного органа по защите прав субъектов персональных данных на период проверки, в случае выявления недостоверных персональных данных или неправомерных действий
7.2. Администратор не несет ответственности за возможное нецелевое использование персональных данных Пользователей, произошедшее из-за:
7.2.1. технических неполадок в программном обеспечении, серверах или компьютерных сетях, находящихся вне контроля Администратора;
Дополнительные условия 8.1. Администратор вправе вносить изменения в настоящую Политику конфиденциальности без согласия Пользователя. 8.2. Новая Политика конфиденциальности вступает в силу с момента ее размещения на Сайте https://romankurnovskii.github.io/p/privacy_ru/, если иное не предусмотрено новой редакцией Политики конфиденциальности.
`,url:"https://romankurnovskii.com/ru/p/privacy_ru/"},"https://romankurnovskii.com/ru/tags/deploy/":{title:"deploy",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/deploy/"},"https://romankurnovskii.com/ru/tags/github/":{title:"github",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/github/"},"https://romankurnovskii.com/ru/posts/nextjs-to-github-pages-ations/":{title:"Публикация next.js приложения на github pages",tags:["github","deploy"],content:`Подготовка коммит все предыдущего состояния на случай вынужденного отката
Для того чтобы Actions имели доступ к репозиторию нужно подключить ключи шифрования
Настройка репозитория Создаю ключи
ssh-keygen -t rsa -b 4096 -C \u0026quot;$(git config user.email)\u0026quot; -f gh-pages -N \u0026quot;\u0026quot; Создалось 2 файла с ключами:
gh-pages - приватный gh-pages.pub - публичный в Репозитории (не профиле)
https://github.com/romankurnovskii/notion-project/settings/keys
Settings → Deploy keys →Add new
из файла gh-pages.pub вставляю текст публичного ключа
Settings → Secrets
Имя: ACTIONS_DEPLOY_KEY
Вставляю приватный ключ из приватного файла gh-pages
https://github.com/romankurnovskii/notion-project/settings/secrets/actions/new
Удаляю ключи файлы чтобы случайно не закоммитить
на гитхабе создаю экшн
https://github.com/romankurnovskii/notion-project/new/main?filename=.github%2Fworkflows%2Fmain.yml\u0026amp;workflow_template=blank
Actions → Create
Создание Actions Выбираю стандартный action (Deploy\u0026hellip;)
Редактирую нижнюю часть кода
- name: Build run: | npm i npm run build npm run export - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: \${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir: ./out npm run export - для создания статических файлов (добавлю позже)
ACTIONS_DEPLOY_KEY - название ключа, что создал ранее
peaceiris/actions-gh-pages@v3 - action из другого популярного репозитория. Ссылаюсь на него.
Итого код:
name: Deploy to Github Pages on: push: branches: - main workflow_dispatch: jobs: deployment: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Setup Node uses: actions/setup-node@v3 with: node-version: \u0026quot;lts/*\u0026quot; cache: \u0026quot;npm\u0026quot; - name: Build run: | npm i npm run build npm run export - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: \${{ secrets.ACTIONS_DEPLOY_KEY }} publish_dir: ./out открыть package.json
найти поле scripts, если нет создать:
{ ..., \u0026quot;scripts\u0026quot;: { \u0026quot;dev\u0026quot;: \u0026quot;next dev\u0026quot;, \u0026quot;build\u0026quot;: \u0026quot;next build\u0026quot;, \u0026quot;start\u0026quot;: \u0026quot;next start\u0026quot;, \u0026quot;deploy\u0026quot;: \u0026quot;vercel --prod\u0026quot;, \u0026quot;export\u0026quot;: \u0026quot;next export\u0026quot; }, ... } Если npm run build \u0026amp;\u0026amp; npm run export отработала, то хорошо
Отладка Не отработала, ошибка:
info - Copying \u0026quot;static build\u0026quot; directory info - No \u0026quot;exportPathMap\u0026quot; found in \u0026quot;next.config.js\u0026quot;. Generating map from \u0026quot;./pages\u0026quot; Error: Image Optimization using Next.js' default loader is not compatible with \`next export\`. Possible solutions: - Use \`next start\` to run a server, which includes the Image Optimization API. - Use any provider which supports Image Optimization (like Vercel). - Configure a third-party loader in \`next.config.js\`. - Use the \`loader\` prop for \`next/image\`. Read more: https://nextjs.org/docs/messages/export-image-api https://nextjs.org/docs/api-reference/next.config.js/exportPathMap
пример кода из документации
module.exports = { exportPathMap: async function ( defaultPathMap, { dev, dir, outDir, distDir, buildId } ) { return { \u0026quot;/\u0026quot;: { page: \u0026quot;/\u0026quot; }, \u0026quot;/about\u0026quot;: { page: \u0026quot;/about\u0026quot; }, \u0026quot;/p/hello-nextjs\u0026quot;: { page: \u0026quot;/post\u0026quot;, query: { title: \u0026quot;hello-nextjs\u0026quot; } }, \u0026quot;/p/learn-nextjs\u0026quot;: { page: \u0026quot;/post\u0026quot;, query: { title: \u0026quot;learn-nextjs\u0026quot; } }, \u0026quot;/p/deploy-nextjs\u0026quot;: { page: \u0026quot;/post\u0026quot;, query: { title: \u0026quot;deploy-nextjs\u0026quot; } }, }; }, }; мой:
module.exports = withBundleAnalyzer({ images: { domains: [\u0026quot;pbs.twimg.com\u0026quot;], }, }); Редактирую **next.config.js**
Добавляю:
const repoName = '/notion-project' module.exports = { basePath: repoName, assetPrefix: repoName, ... https://github.com/romankurnovskii/notion-project/blob/main/next.config.js
Проблема с установкой зависимости вовремя использованя npm установщика. Буду использовать yarn потому что он пропускает минорные уведомления для меня кажется более стабильным.
Пока разбирался с проблемы запуска экшенов и настройками нашёл новые экшены и без использования ключа. Обновлю код
После того как я редактирую данные нужен они не меняются на сайте. Не меняются потому что гитхаб создаёт статические файлы, то есть нужно заново сделать новый билд. Для меня моментальные изменения не критичны поэтому я поставлю задачу билда повторяться каждый день в 7:00 утра
Добавляю код в yaml файл
on: push: branches: [main] schedule: - cron: \u0026quot;0 7 * * *\u0026quot; ## every day 7 am Итоговый результат
https://github.com/romankurnovskii/notion-project/blob/main/.github/workflows/main.yml
lines (32 sloc) 867 Bytes name: Deploy to GitHub Pages on: push: branches: [main] schedule: - cron: \u0026quot;0 7 * * *\u0026quot; ## every day 7 am jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [14.x] steps: - name: Get files uses: actions/checkout@v2 - name: Use Node.js \${{ matrix.node-version }} uses: actions/setup-node@v2 with: node-version: \${{ matrix.node-version }} - name: Install packages run: yarn install - name: Build project run: yarn run build - name: Export static files run: yarn run export - name: Add .nojekyll file run: touch ./out/.nojekyll - name: Deploy uses: JamesIves/github-pages-deploy-action@4.1.1 #third party github actions / ok to use with: branch: gh-pages folder: out После тестового комитета и билда получаю 2 проблемы:
Стилей нет, картинки не подгружены Ссылки не работают Next.js ожидает адрес вида https://username.github.io/
А у меня в конце ещё добавляется репозиторий. Т.е. добавился ещё один уровень в пути
- name: Deploy uses: JamesIves/github-pages-deploy-action@4.1.1 #third party github actions / ok to use with: branch: gh-pages folder: out - name: Add .nojekyll file run: touch ./out/.nojekyll Источники https://wallis.dev/blog/deploying-a-next-js-app-to-github-pages https://gregrickaby.blog/article/nextjs-github-pages https://medium.com/@anotherplanet/git-tips-next-js-github-pages-2dbc9a819cb8 https://www.linkedin.com/pulse/deploy-nextjs-app-github-pages-federico-antu%C3%B1a `,url:"https://romankurnovskii.com/ru/posts/nextjs-to-github-pages-ations/"},"https://romankurnovskii.com/ru/p/publications/":{title:"Печатные публикации",tags:[],content:` Печатные журналы / сборники Для цитирования:
Факторы и условия, определяющие становления финансовой экосистемы в современных условиях, // Экономика и предпринимательство // д.э.н., проф. Коновалова М.Е., Курновский Р.М., Ширяева Д.В. // Номер: 8 (145) 2022 г. Страницы: 928-931
Статья [pdf] / ResearchGate
«Ключевые подходы к разработке доступного, интуитивно понятного интерфейса статистического пакета», // Научный журнал // к.т.н., профессор Суханова Е. И., канд. физ.-мат., доцент Ширяева Л. К., Курновский Р. М. // 2014 г. //
«Мобильность платформы 1С на базе приложения 1С:Монитор ERP» // Известия Института Систем Управления Самарского государственного экономического университета. // Курновский Р. М., Нечаев А. Н. // 2013 г. // Номер: 2 (8) // Страницы: 243-247 //
«Современные инструменты моделирования архитектуры предприятия» // Известия Института Систем Управления Самарского государственного экономического университета. // 2012 г. // Номер: 3 (6) // Страницы: 256-260 //
«Стволовая клетка — миф или реальность» // Тезисы 36-й Самарской областной студенческой научной конференции. // 2010 г. //
«Права человека — миф или реальность» // Тезисы 4-й Международной научной конференции молодых ученых, аспирантов и студентов. // 2010 г. //
«Хулиганство в Самаре 1920-1930-х гг.» // Тезисы 4-й Международной научной конференции молодых ученых, аспирантов и студентов. // 2010 г. //
«Обеспечение прав человека — миф или реальность» // Сборник тезисов конкурсных работ, опубликованных Государственной Думой Федерального Собрания Российской Федерации во всероссийском конкурсе молодежи, образовательных учреждений и научных организаций на лучшую работу «Моя законотворческая инициатива». // 2008 г. //
«Хулиганство в России в 20-30-е годы 20-го века на примере Самарской области» // Сборник тезисов 37-й городской научно-практической конференции. // 2008 г. //
«Генетический паспорт гражданина Российской Федерации» // Сборник тезисов конкурсных работ, опубликованных Государственной Думой Федерального Собрания Российской Федерации во всероссийском конкурсе молодежи, образовательных учреждений и научных организаций на лучшую работу «Моя законотворческая инициатива». // 2007 г. //
Ожидают публикации статья Катя икт экспорт / elibrary
ТРАНСФОРМАЦИЯ ЭКОНОМИКИ И УПРАВЛЕНИЯ: НОВЫЕ ВЫЗОВЫ И ПЕРСПЕКТИВЫ Сборник статей и тезисов докладов XII Международной научно-практической конференции Площадки How to unmount disk on Linux
Codes https://orcid.org/0000-0002-6040-3683 SCIENCE INDEX SPIN-код: 1657-2666 `,url:"https://romankurnovskii.com/ru/p/publications/"},"https://romankurnovskii.com/ru/tags/it/":{title:"it",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/it/"},"https://romankurnovskii.com/ru/posts/diploma/":{title:"IT курсы 2020",tags:["учеба","it","эмиграция"],content:`Промежуточные метрики еще в процессе расчетов
За 2020 год:
Затрачено времени на учебу/практику: ~5500 часов `,url:"https://romankurnovskii.com/ru/posts/diploma/"},"https://romankurnovskii.com/ru/tags/%D1%83%D1%87%D0%B5%D0%B1%D0%B0/":{title:"учеба",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/%D1%83%D1%87%D0%B5%D0%B1%D0%B0/"},"https://romankurnovskii.com/ru/categories/%D1%83%D1%87%D0%B5%D0%B1%D0%B0/":{title:"учеба",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/%D1%83%D1%87%D0%B5%D0%B1%D0%B0/"},"https://romankurnovskii.com/ru/tags/%D1%8D%D0%BC%D0%B8%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F/":{title:"эмиграция",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/%D1%8D%D0%BC%D0%B8%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F/"},"https://romankurnovskii.com/ru/tags/%D1%80%D0%B5%D0%BF%D0%B0%D1%82%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F/":{title:"репатриация",tags:[],content:"",url:"https://romankurnovskii.com/ru/tags/%D1%80%D0%B5%D0%BF%D0%B0%D1%82%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F/"},"https://romankurnovskii.com/ru/categories/%D1%87%D0%B5%D0%BA%D0%BB%D0%B8%D1%81%D1%82/":{title:"чеклист",tags:[],content:"",url:"https://romankurnovskii.com/ru/categories/%D1%87%D0%B5%D0%BA%D0%BB%D0%B8%D1%81%D1%82/"},"https://romankurnovskii.com/ru/p/%D1%80%D0%B5%D0%BF%D0%B0%D1%82%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F/":{title:"Чеклист репатриация в Израиль",tags:["репатриация"],content:`Инфа СОХНУТ - Еврейское агентство для Израиля, — международная сионистская организация с центром в государстве Израиль, которая занимается репатриацией в Израиль и помощью репатриантам.
https://www.jewishagency.org/ru/
Министерства алии и интеграции тел. *2994 или 03-9733333
Ссылки Чаты, где спрашивать:
https://t.me/OlimHadashim https://t.me/olehadash_com_chat https://t.me/forum_israel Няни, частные учителя, частные школы и садики в Израиле. https://www.facebook.com/groups/Nyani.Uchitelya.Shkoli.Israel
ASD в Израиле. Все о детях в спектре и их родителях https://www.facebook.com/groups/asdisraelrus
Мамочки Израиля https://www.facebook.com/groups/1524467887858435
Репатрианты в Израиле: здесь помогают и делятся опытом https://www.facebook.com/groups/1511311149184796
ОТДАМ ДАРОМ В ИЗРАИЛЕ https://www.facebook.com/groups/1601685156757272
Другие инструкции
https://olehadash.com/
Вторичка:
https://t.me/BROOTTO
Рассчет налога по зарплате
Первая неделя Сначала получить симкарту, все уведомления на нее. Брать любую, тариф в среднем около 30-40 шек мес
Банк
открыть счет (леуми или дисконт)
сразу запросить чеки (нужны для аренды квартиры и мало ли на что еще, стоят около 10шек)
самую дешевую карту, без всяких плат попросить.
Больничаная касса
взял маккаби, вроде все примерно одинаковые но у маккаби больше покрытия, может чуть подороже она
Подработки Форма для добавления в базу резюме или на поиск работы: https://forms.gle/NFB2JXs1fHrCJn5Q7
Расчет налога на зарплату: https://investomatica.com/income-tax-calculator/israel
Что спрашивают и какие документы нужны, и как с оплатой
https://t.me/joinchat/DlAMLxN_S-XCXgJ8MQxslg https://t.me/Rus_Work_Israel https://t.me/izrail_rabota https://t.me/sidejobisrael https://t.me/rabotadlyadruzei https://t.me/rabotaisraeli `,url:"https://romankurnovskii.com/ru/p/%D1%80%D0%B5%D0%BF%D0%B0%D1%82%D1%80%D0%B8%D0%B0%D1%86%D0%B8%D1%8F/"},"https://romankurnovskii.com/ru/docs/archive/":{title:"Docs",tags:[],content:" Docs EN | RU Posts EN | RU ",url:"https://romankurnovskii.com/ru/docs/archive/"},"https://romankurnovskii.com/ru/posts/archive/":{title:"Posts Archive",tags:[],content:" Docs EN | RU Posts EN | RU ",url:"https://romankurnovskii.com/ru/posts/archive/"},"https://romankurnovskii.com/ru/docs/":{title:"Roadmaps",tags:[],content:`Списком
`,url:"https://romankurnovskii.com/ru/docs/"},"https://romankurnovskii.com/ru/search/":{title:"Search page",tags:[],content:"",url:"https://romankurnovskii.com/ru/search/"},"https://romankurnovskii.com/ru/posts/":{title:"Заметки",tags:[],content:`Списком
`,url:"https://romankurnovskii.com/ru/posts/"},"https://romankurnovskii.com/ru/apps/":{title:"Приложения",tags:[],content:"",url:"https://romankurnovskii.com/ru/apps/"},"https://romankurnovskii.com/ru/authors/roman-kurnovskii/":{title:"Роман Курновский",tags:[],content:" ",url:"https://romankurnovskii.com/ru/authors/roman-kurnovskii/"}}</script><script src=https://unpkg.com/lunr/lunr.js></script>
<script src=/js/search-page.js?v3></script>
<script src=/js/base.js?5 languagemode=ru></script></div></footer><script src=https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js defer></script>
<script src=/js/zoom-image/placeholders.js?4 defer></script>
<script src=/js/zoom-image/index.js?4 defer></script>
<link rel=stylesheet href=//unpkg.com/@highlightjs/cdn-assets@11.6.0/styles/default.min.css><script src=//unpkg.com/@highlightjs/cdn-assets@11.6.0/highlight.min.js?2></script></body></html>