{
    "ru": [
        {
            "uri": "/_home/blank",
            "title": "Blank",
            "content": {
                "log": {
                    "timeTakenInMilliseconds": 3
                },
                "result": "Write anything you like here!",
                "ranges": [
                    [
                        0,
                        38
                    ],
                    [
                        67,
                        78
                    ]
                ],
                "allTagLocations": [
                    [
                        1,
                        32
                    ],
                    [
                        35,
                        38
                    ],
                    [
                        67,
                        71
                    ],
                    [
                        72,
                        78
                    ]
                ],
                "filteredTagLocations": [
                    [
                        1,
                        32
                    ],
                    [
                        35,
                        38
                    ],
                    [
                        67,
                        71
                    ],
                    [
                        72,
                        78
                    ]
                ]
            },
            "tags": []
        },
        {
            "uri": "/_home/vintage",
            "title": "Vintage",
            "content": "",
            "tags": []
        },
        {
            "uri": "/apps/_index",
            "title": "Приложения",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/apps/brewmate/",
            "title": "BrewMate",
            "description": "Homebrew GUI Apps Manager",
            "content": "\n\n\n\nBrewMate is a macOS GUI application that makes it easy to search for, install, and uninstall Homebrew casks. You can also see the top downloaded casks for the last month.\n\nInstall\n\nDownload the latest DMG file from the releases page or from sourceforge.net\nDouble-click the DMG file to open it.\nDrag the BrewMate app to your Applications folder.\nLaunch BrewMate from your Applications folder.\n\nor\nbrew install romankurnovskii/cask/brewmate --cask\n\nor\nbrew tap romankurnovskii/cask\nbrew update\nbrew install brewmate --cask\n\nFAQ\n\nIs this app free?\nYes, the app is free to download and use.\n\nWhat operating systems does this app support?\nThis app is designed for macOS, and it supports macOS 10.15 (Catalina) and newer versions.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/apps/cloud-exam-quizz/",
            "title": "Cloud exam Quizz",
            "description": "Подготовка к сдаче экзамена AWS",
            "content": "\nGoal: Check if you are ready to pass the Cloud exam\nThe application calculates progress after each answered question.\nAbility to answer at least one question and get a comment at the same time. No need to pass all questions before.\nIt is convenient to spend 20 min a day\nWorks from web/tablet/mobile\n\nLink: https://www.cloud-exam-prepare.com\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/apps/npm/cognito-token-observer/",
            "title": "cognito-token-observer",
            "description": "Отслеживает истечение срока действия access и id токенов Amazon Cognito. Обновляется по истечении срока действия.",
            "content": "",
            "tags": [
                "npm"
            ],
            "lang": "ru"
        },
        {
            "uri": "/apps/npm/hugo-lunr-ml/",
            "title": "hugo-lunr-ml",
            "description": "Создает индекс страниц Hugo для поиска lunr.js",
            "content": "",
            "tags": [
                "npm"
            ],
            "lang": "ru"
        },
        {
            "uri": "/authors/michael-cade/_index",
            "title": "Michael Cade",
            "content": "\n",
            "tags": []
        },
        {
            "uri": "/authors/roman-kurnovskii/_index",
            "title": "Роман Курновский",
            "content": "\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/homepage/about",
            "title": "Роман Курновский",
            "content": "\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/homepage/education",
            "title": "Образование",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/homepage/experience",
            "title": "Path",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/homepage/",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/homepage/pages",
            "title": "Заметки",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/p/privacy_ru",
            "title": "Политика конфиденциальности",
            "description": null,
            "content": "\nСписок приложений:\nФСФР - Базовый экзамен\n\nНастоящая Политика конфиденциальности персональных данных (далее – Политика конфиденциальности) действует в отношении всей информации, которую приложения из раздела: \"Список приложений\" могут получить о Пользователе во время использования.\n\nОбщие положения\n1.1. Целью Политики конфиденциальности является реализация требований законодательства в области обработки и защиты персональных данных.\n\n1.2. Настоящий Регламент разработан на основании Конституции Российской Федерации, Трудового кодекса Российской Федерации, Гражданского кодекса Российской Федерации, Уголовного кодекса Российской Федерации, Кодекса об административных правонарушениях Российской Федерации, Федерального закона Российской Федерации «О персональных данных» № 152-ФЗ от 27 июля 2006 года.\n\nОсновные понятия\nНа основании законодательства Российской Федерации в целях настоящего Политики конфиденциальности используются следующие понятия\n\n2.1. Администратор Приложений (далее – Администратор) – уполномоченные сотрудник, который организуют и (или) осуществляет обработку персональных данных, а также определяет цели обработки персональных данных, состав персональных данных, подлежащих обработке, действия (операции), совершаемые с персональными данными.\n\n2.2. Пользователь – лицо, являющееся субъектом персональных данных и сообщающее свои персональные данные посредством Приложений.\n\n2.3. «Персональные данные» - любая информация, относящаяся к прямо или косвенно к определяемому физическому лицу (субъекту персональных данных).\n\n2.4. «Обработка персональных данных» - любое действие (операция) или совокупность действий (операций), совершаемых с использованием средств автоматизации или без использования таких средств с персональными данными, включая сбор, запись, систематизацию, накопление, хранение, уточнение (обновление, изменение), извлечение, использование, передачу (распространение, предоставление, доступ), обезличивание, блокирование, удаление, уничтожение персональных данных.\n\nОбщие положения\n3.1. Использование Пользователем Приложений означает согласие с настоящей Политикой конфиденциальности и условиями обработки персональных данных Пользователя.\n\n3.2. В случае несогласия с условиями Политики конфиденциальности Пользователь должен прекратить использование Приложений.\n\n3.3. Настоящая Политика конфиденциальности применяется только к Приложениям.\n\nПредмет политики конфиденциальности\n4.1. Настоящая Политика конфиденциальности устанавливает обязательства по неразглашению и обеспечению режима защиты конфиденциальности персональных данных, которые Пользователь.\n\n4.2. Персональные данные, разрешённые к обработке в рамках настоящей Политики конфиденциальности, предоставляются Пользователем путём заполнения и включают в себя следующую информацию:\n\nфамилию, имя, отчество;\n\nконтактный телефон Пользователя;\n\ne-mail\n\n4.3. Любая иная персональная информация неоговоренная выше подлежит надежному хранению и нераспространению.\n\n4.4. Обработка персональных данных осуществляется с использованием интернет-сервисов сторонних организаций, в том числе с использованием интернет-сервиса Google Analitics. С порядком обработки данных с помощью интернет-сервиса Google Analitics можно ознакомиться, перейдя по ссылке https://www.google.ru/policies/privacy/partners/\n\nЦели сбора персональных данных\n5.1. Запрещено обрабатывать персональные данные Пользователя о его политических, религиозных и иных убеждениях и частной жизни.\n\n5.2. При передаче персональных данных Пользователя, Администратор предупреждает лиц, получающих персональные данные Пользователя, о том, что эти данные могут быть использованы лишь в целях, для которых они сообщены. Данная норма не распространяется на обмен персональными данными Пользователей в порядке, установленном федеральными законами.\n\n5.3. Защита персональных данных Пользователя от неправомерного их использования или утраты обеспечивается в порядке, установленном законодательством Российской Федерации.\n\n5.4. Пользователь вправе в любое время по своему усмотрению отозвать свое согласие на обработку своих персональных данных путем отправки сообщения об удалении персональных данных по следующему e-mail: r.kurnovskii@gmail.com.\n\nСпособы и сроки обработки персональных данных\n6.1. Обработка персональных данных Пользователя осуществляется без ограничения срока, любым законным способом, в том числе в информационных системах персональных данных с использованием средств автоматизации или без использования таких средств.\n\n6.2. При утрате или разглашении персональных данных Администрация сайта информирует Пользователя об утрате или разглашении персональных данных.\n\nОбязательства сторон\n7.1. Администратор обязан:\n\n7.1.1. Использовать полученную информацию исключительно для целей, указанных в п. 5 настоящей Политики конфиденциальности.\n\n7.1.2. Обеспечить хранение конфиденциальной информации в тайне.\n\n7.1.3. Принимать меры предосторожности для защиты конфиденциальности персональных данных Пользователя согласно порядку, обычно используемого для защиты такого рода информации в существующем деловом обороте.\n\n7.1.4. Осуществить блокирование, удаление персональных данных, относящихся к соответствующему Пользователю, с момента обращения или запроса Пользователя или его законного представителя либо уполномоченного органа по защите прав субъектов персональных данных на период проверки, в случае выявления недостоверных персональных данных или неправомерных действий\n\n7.2. Администратор не несет ответственности за возможное нецелевое использование персональных данных Пользователей, произошедшее из-за:\n\n7.2.1. технических неполадок в программном обеспечении, серверах или компьютерных сетях, находящихся вне контроля Администратора;\n\nДополнительные условия\n8.1. Администратор вправе вносить изменения в настоящую Политику конфиденциальности без согласия Пользователя.\n\n8.2. Новая Политика конфиденциальности вступает в силу с момента ее размещения на Сайте https://romankurnovskii.github.io/p/privacy_ru/, если иное не предусмотрено новой редакцией Политики конфиденциальности.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/p/publications",
            "title": "Печатные публикации",
            "description": "Печатные журналы / сборники",
            "content": "\nПечатные журналы / сборники\n\n\nДля цитирования:\nОсобенности региональной политики и экономические интеграции Израиля // Журнал ВАК: Наука и бизнес: пути развития // Курновский Р.М.// Номер: 1 (139) 2023 г. Страницы: 138-\n\n[Статья [pdf]](/articles/2023-01-osobennosti-regionalnoi-politiki-i-ekonomicheskie-integracii-izrailya.pdf)\nПоддержка икт-экспорта как способ покрытия внутренних потребностей̆ рынка // XII международной научно-практической конференции «Трансформация экономики и управления: новые вызовы и перспективы» 15-16 декабря 2022г. // Курновский Р.М., Великородная Е. А. //\n\n[Статья [pdf]](/articles/2022-12-podderzhka-ikt-eksporta-kak-sposob-pokrytiya-vnutrennih-potrebnostei-rynka.pdf)\nФакторы и условия, определяющие становления финансовой экосистемы в современных условиях // \tЭкономика и предпринимательство // д.э.н., проф. Коновалова М.Е., Курновский Р.М., Ширяева Д.В. // Номер: 8 (145) 2022 г. Страницы: 928-931\n\n[Статья [pdf]](/articles/2022-12-faktory-i-usloviya-opredeljajushhie-stanovlenija-finansovoi-ekosistemy-v-sovremennyh-uslovijah.pdf) / ResearchGate / academia.edu / SSRN\n«Ключевые подходы к разработке доступного, интуитивно понятного интерфейса статистического пакета» // Научный журнал // к.т.н., профессор Суханова Е. И., канд. физ.-мат., доцент Ширяева Л. К., Курновский Р. М. // 2014 г. //\n«Мобильность платформы 1С на базе приложения 1С:Монитор ERP» // Известия Института Систем Управления Самарского государственного экономического университета. // Курновский Р. М., Нечаев А. Н. // 2013 г. // Номер: 2 (8) // Страницы: 243-247 //\n«Современные инструменты моделирования архитектуры предприятия» // Известия Института Систем Управления Самарского государственного экономического университета. // 2012 г. // Номер: 3 (6) // Страницы: 256-260 //\n«Стволовая клетка — миф или реальность» // Тезисы 36-й Самарской областной студенческой научной конференции. // 2010 г. //\n«Права человека — миф или реальность» // Тезисы 4-й Международной научной конференции молодых ученых, аспирантов и студентов. // 2010 г. //\n«Хулиганство в Самаре 1920-1930-х гг.» // Тезисы 4-й Международной научной конференции молодых ученых, аспирантов и студентов. // 2010 г. //\n«Обеспечение прав человека — миф или реальность» // Сборник тезисов конкурсных работ, опубликованных Государственной Думой Федерального Собрания Российской Федерации во всероссийском конкурсе молодежи, образовательных учреждений и научных организаций на лучшую работу «Моя законотворческая инициатива». // 2008 г. //\n«Хулиганство в России в 20-30-е годы 20-го века на примере Самарской области» // Сборник тезисов 37-й городской научно-практической конференции. // 2008 г. //\n«Генетический паспорт гражданина Российской Федерации» // Сборник тезисов конкурсных работ, опубликованных Государственной Думой Федерального Собрания Российской Федерации во всероссийском конкурсе молодежи, образовательных учреждений и научных организаций на лучшую работу «Моя законотворческая инициатива». // 2007 г. //\n\nОжидают публикации\n\nCodes\n\nSCIENCE INDEX SPIN РИНЦ:** 1657-2666\nORCID:** 0000-0002-6040-3683\nWeb of Science ResearcherID:** HLQ-2418-2023\nGoogle Scholar\nSSRN\nacademia.edu\n\nLinks\n\nhttps://authors.repec.org/pro/pku734/\nhttps://ideas.repec.org/\nhttps://econpapers.repec.org/\n\nПлощадки\n\nmedium.com\ndev.to\nvc.ru\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/p/репатриация",
            "title": "Чеклист репатриация в Израиль",
            "description": "репатриация в Израиль",
            "content": "\nИнфа\n\nСОХНУТ - Еврейское агентство для Израиля,  — международная сионистская организация с центром в государстве Израиль, которая занимается репатриацией в Израиль и помощью репатриантам.\n\nhttps://www.jewishagency.org/ru/\n\nМинистерства алии и интеграции\nтел. *2994 или 03-9733333\n\nСсылки\n\nЧаты, где спрашивать:\n\nhttps://t.me/OlimHadashim\nhttps://t.me/olehadash_com_chat\nhttps://t.me/forum_israel\n\nНяни, частные учителя, частные школы и садики в Израиле.\nhttps://www.facebook.com/groups/Nyani.Uchitelya.Shkoli.Israel\n\nASD в Израиле. Все о детях в спектре и их родителях\nhttps://www.facebook.com/groups/asdisraelrus\n\nМамочки Израиля\nhttps://www.facebook.com/groups/1524467887858435\n\nРепатрианты в Израиле: здесь помогают и делятся опытом\nhttps://www.facebook.com/groups/1511311149184796\n\nОТДАМ ДАРОМ В ИЗРАИЛЕ\nhttps://www.facebook.com/groups/1601685156757272\n\nДругие инструкции\n\nhttps://olehadash.com/\n\nВторичка:\n\nhttps://t.me/BROOTTO\n\nРассчет налога по зарплате\n\nПервая неделя\n\nСначала получить симкарту, все уведомления на нее. Брать любую, тариф в среднем около 30-40 шек мес\n\nБанк\n\nоткрыть счет (леуми или дисконт)\n\nсразу запросить чеки (нужны для аренды квартиры и мало ли на что еще, стоят около 10шек)\n\nсамую дешевую карту, без всяких плат попросить.\n\nБольничаная касса\n\nвзял маккаби, вроде все примерно одинаковые но у маккаби больше покрытия, может чуть подороже она\n\nПодработки\n\nФорма для добавления в базу резюме или на поиск работы: https://forms.gle/NFB2JXs1fHrCJn5Q7\n\nРасчет налога на зарплату: https://investomatica.com/income-tax-calculator/israel\n\nЧто спрашивают и какие документы нужны, и как с оплатой\n\nhttps://t.me/joinchat/DlAMLxN_S-XCXgJ8MQxslg\nhttps://t.me/Rus_Work_Israel\nhttps://t.me/izrail_rabota\nhttps://t.me/sidejobisrael\nhttps://t.me/rabotadlyadruzei\nhttps://t.me/rabotaisraeli\n",
            "tags": [
                "репатриация"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/_index",
            "title": "Заметки",
            "content": "\nСписком",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/posts/archive/",
            "title": "Posts Archive",
            "content": "\nDocs EN | RU\nPosts EN | RU\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/posts/cheat-sheet-command-tar/",
            "title": "Шпаргалка tar архиватор",
            "description": "Необходимые команды для работы с архиватором tar",
            "content": "\nКратко\n\nСоздать:\ntar cf archive.tar directory\n\nРаспаковать:\ntar xf archive.tar\n\nСоздание\n\nmkdir my_dir # Создаем папку\ntar cf dir_archive.tar my_dir # Создаем архив с папкой\nll # Проверяем содержимое текущего каталога\n-rw-r--r--  1 r  staff   1.5K Jun  4 14:42 dir_archive.tar\ndrwxr-xr-x  2 r  staff    64B Jun  4 14:42 my_dir\n\nРаспаковка\n\ntar xf dir_archive.tar\n\nСжатие\n\ntar czf dir_archive.tar.gz dir_archive.tar\n\nРаспаковка сжатого файла\n\ntar xzf dir_archive.tar.gz\n\nСжатие с помощью bzip2\n\ntar cjf dir_archive.tar.bz2 my_dir\n\nРаспаковка с помощью bzip2\n\ntar xjf dir_archive.tar.bz2\n\nПросмотр содержимого архива\n\ntar -tvf dir_archive.tar\n",
            "tags": [
                "Linux",
                "CLI",
                "tar",
                "cheatsheet"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/diagram-support",
            "title": "Diagram Support",
            "description": null,
            "content": "\nTheme supports the rendering of diagrams by using Mermaid.\n\n\n\nPlease include the Mermaid diagram as below. Every mermaid chart/graph/diagram definition, has to have separate `` tags.\n\nIn order to render the HTML code in the Markdown file correctly, please make sure that markup.goldmark.renderer.unsafe in config.yaml is true.\n\nHere is one mermaid diagram:\n\n\n  graph TD\n  A[Client] --> B[Load Balancer]\n  B --> C[Server1]\n  B --> D[Server2]\n\n\nAnd here is another:\n\n\n  graph TD\n  A[Client] -->|tcp_123| B(Load Balancer)\n  B -->|tcp_456| C[Server1]\n  B -->|tcp_456| D[Server2]\n\n\n\n flowchart LR\n  A[Учеба]\n  B[Учеба]\n  C[Учеба]\n  A --> B --> C --> A\n\n",
            "tags": []
        },
        {
            "uri": "/posts/diploma/",
            "title": "IT курсы 2020",
            "description": "Подтвержденные знания по IT за 2020 год",
            "content": "\nПромежуточные метрики еще в процессе расчетов\n\nЗа 2020 год:\n\nЗатрачено времени на учебу/практику: ~5500 часов\n\n\n",
            "tags": [
                "учеба",
                "it",
                "эмиграция"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/docker-commands/",
            "title": "Популярные команды Docker",
            "description": "Основные команды Docker, которыми пользуюсь в процессе разработки.",
            "content": "\nFull in english\n\nТоп 10 комманд Docker\ndocker ps — смотрим список запущенных контейнеров\ndocker pull — загрузка образа\ndocker build — собирает образ\ndocker logs — смотрим логи\ndocker run — запускаем контейнер\ndocker stop — останавливает контейнер\ndocker kill — «убивает» контейнер\ndocker rm — удаляет контейнер\ndocker rmi — удаляет образ\ndocker volume ls — список томов\n\ndocker build\nДокументация\nПостроить образ из Dockerfile.\n\ndocker build [DOCKERFILE PATH]\n\nФлаги\n\n--file -f Путь, где находится Dockerfile\n--force-rm Всегда удалять временные контейнеры.\n--no-cache Не использовать кэш при построении образа.\n--rm Удалить временные контейнеры после успешного построения.\n--tag -t Название и возможный тег в формате name:tag или просто тег my_tag (опционально)\n\nПримеры\n\nПостроить образ с меткой my-org/my-image, используя Dockerfile в /tmp/Dockerfile.\n\ndocker build -t my-org:my-image -f /tmp/Dockerfile\n\ndocker run\nДокументация\n\nСоздает и запускает контейнер за один операционный шаг\n\nПримеры\n\ndocker run -it ubuntu:latest /bin/bash\nДанная команда запустит контейнер ubuntu и при старте сразу запустит /bin/bash. Если  образ ubuntu не был загружен ранее, он загрузится перед запуском.\n\nФлаги\n\n-it This will not make the container you started shut down immediately, as\nit will create a pseudo-TTY session (-t) and keep STDIN open (-i)\n--rm Automatically remove the container when it exit. Otherwise it will be\nstored and visible running docker ps -a.\n--detach -d Run container in background and print container ID\n--volume -v Bind mount a volume. Useful for accessing folders on your local\ndisk inside your docker container, like configuration files or storage that\nshould be persisted (database, logs etc.).\n\ndocker exec\nДокументация\nВыполнить команду внутри запущенного контейнера.\n\ndocker exec [CONTAINER ID]\nФлаги\n\n--detach -d Detached mode: запуск в фоновом режиме\n-it запуск в интерактивном режиме. запуск псевдотерминала pseudo-TTY  (-t) и перенаправление ввода-вывода (STDIN) (-i). Даёт доступ к выполнению команд в терминале контейнера.\n\nПримеры\n\ndocker exec [CONTAINER ID] touch /tmp/exec_works\ndocker images\nДокументация\nВывести список всех загруженных/созданных образов\n\ndocker images\n\nФлаги\n\n-q показать только ID образов\n\ndocker inspect\nДокументация\n\nПоказать всю информацию о контейнере.\n\ndocker inspect [CONTAINER ID]\n\ndocker logs\nДокументация\n\nВывести логи контейнера.\n\ndocker logs [CONTAINER ID]\n\nФлаги\n\n--details  Показывает дополнительную информацию в логе.\n--follow -f Следить за выводом журнала\n--timestamps -t Показать журналы с меткой времени\n\ndocker ps\nДокументация\n\nПоказывает информацию о всех запущенных контейнерах.\n\n\ndocker ps\n\nФлаги\n\n--all -a Show all containers (default shows just running)\n--filter -f Filter output based on conditions provided, docker ps -f=\"name=\"example\"\n--quiet -q Only display numeric IDs\n\ndocker rmi\nДокументация\n\nУдалить один или несколько образов.\n\ndocker rmi [IMAGE ID]\n\nФлаги\n\n--force -f Force removal of the image\n\nСоветы и рекомендации по докеру\n\nСборник полезных советов по Docker.\n\nУдалить все контейнеры\n\nNOTE: Удалить ВСЕ контенеры.\n\ndocker container prune\n\nили\n\ndocker rm $(docker ps -a -q)\n\nУдалить все непомеченные контейнеры\n\ndocker image prune\n\nВывести сколько памяти занимает Docker\n\ndocker system df\n\nПолучить IP-адрес работающего контейнера\n\ndocker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d '\"' -f 4\n\nСгенерировать образ на основе файла Dockerfile и добавить этому образу имя и версию\n\ndocker build -t new_image_name:v1 .\n\n. означает текущую директорию, где расположен файл Dockerfile.\n\nСгенерировать из запущенного контейнера новый образ\n\ndocker commit [CONTAINER ID] [NEW IMAGE NAME]\n\n\"Убить\" все запущенные контейнеры\n\ndocker kill $(docker ps -q)\n\nСсылки\n\ndocs.docker.com\ndocker-cheat-sheet\nhttps://sourabhbajaj.com/mac-setup/Docker/\n\n",
            "tags": [
                "docker",
                "cheatsheet"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/economics/diff-forward-contracts-futures",
            "title": "Разница Валютные форварды и Фьючерсы",
            "description": "Сравнение Валютных форвардов и Фьючерсов",
            "content": "\n\nВалютные форварды (forward contracts) - это договоры, в рамках которых две стороны соглашаются на обмен активами (обычно это валюты, сырьевые товары или финансовые инструменты) на определенную дату в будущем по заранее оговоренной цене. Это финансовый инструмент, который используется компаниями и инвесторами для защиты от рисков колебаний курсов валют. Например, если компания знает, что ей нужно купить определенную сумму валюты на будущее время для оплаты поставок, она может заключить валютный форвард, чтобы зафиксировать курс обмена на определенный период времени. Таким образом, она может избежать потерь от колебаний курса валют и защитить свою операционную прибыль.\n\nФорварды являются одним из инструментов хеджирования рисков и используются для защиты от потенциальных потерь, связанных с колебаниями цен на активы. Например, компания, занимающаяся импортом товаров, может заключить форвардный контракт на покупку валюты по фиксированной цене, чтобы защитить себя от возможного увеличения стоимости валюты в будущем.\n\nФорварды и фьючерсы относятся к группе финансовых инструментов, но они имеют некоторые отличия.\n\nФьючерсы - это стандартизованные контракты на покупку или продажу определенного актива, например, валюты, акций, сырья, в определенное время в будущем по заранее оговоренной цене. Фьючерсы торгуются на биржах и обычно имеют строгие правила в отношении размера контракта, срока исполнения, стоимости и способа расчета.\n\nФьючерсы имеют ряд особенностей:\n\nСтандартизация: все фьючерсы имеют одинаковый размер, срок действия, тип и спецификацию актива, на который они ссылаются.\nКредитный риск: каждая сторона должна обеспечить свои обязательства по фьючерсному контракту, что означает, что обе стороны могут быть подвержены кредитному риску, если одна из них не исполнит свои обязательства.\nМаржинальное финансирование: для уменьшения кредитного риска стороны должны вносить маржу (внесение определенного денежного залога) в течение срока действия контракта.\nЛиквидность: фьючерсы торгуются на биржах, что обеспечивает высокую ликвидность контрактов и возможность быстрой покупки или продажи актива по рыночной цене.\nСпекулятивные операции: фьючерсы могут использоваться не только для защиты от рисков, но и для спекулятивных операций с целью получения прибыли от изменения цен активов.\n\nФорварды - это договоренности о покупке или продаже определенного актива в будущем по заранее оговоренной цене. Они не стандартизованы и заключаются напрямую между двумя контрагентами. Форварды обычно не торгуются на биржах и могут иметь различные условия, например, размер контракта, дата исполнения и расчета.\n\nФорвардные контракты могут быть настроены индивидуально для каждой стороны, в отличие от фьючерсов, которые стандартизированы и торгуются на биржах. Также, в отличие от фьючерсов, форварды не имеют стандартных условий исполнения и могут быть заключены на любой актив и на любой период времени.\n\nТаким образом, форварды и фьючерсы имеют некоторые сходства, но их различия заключаются в степени стандартизации, регулирования и доступности для обычных инвесторов.\n",
            "tags": [
                "экономика"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/economics/raznica-mezhdu-marzhinalizmom-i-nerkantilizmom",
            "title": "Разница между Маржинализмом и Меркантилизмом",
            "description": "Сравнение Меркантилизма и Маржинализма",
            "content": "\nМаржинализм и меркантилизм - это два разных термина, которые описывают различные экономические концепции.\n\nМаржинализм - (Неоклассическое направление) - середина XIX в, это экономическая теория, которая основана на идее, что цена товара или услуги определяется его маржой, то есть разницей между продажной ценой и затратами на его производство. В рамках маржинализма обычно считается, что рост маржи является показателем эффективности экономики.\n\nМаржинализм использует такие величины, как предельная полезность, предельная производительность, предельные издержки\n\nОдним из главных аспектов является «субъективизм» - подход, при котором все явления в экономике оцениваются, а также исследуются с точки зрения определенного субъекта.\n\nОсновные элементы маржинализма:\n\nИспользование предельных (т.е. приростных) величин. Само слово «маржинализм» происходит от латинского margo, что означает край, предел. Маржиналистов интересует то, насколько изменится та или иная величина при изменении другой величины на единицу. В этом смысле весьма удобным оказывается использование дифференциального исчисления, в котором все построено на соотношении приростов разных величин.\nСубъективизм, т.е. подход, при котором все экономические явления исследуются и оцениваются с точки зрения отдельного хозяйствующего субъекта. Недаром маржинализм иногда называют субъективной школой экономики.\nГедонизм хозяйствующих субъектов. Человек рассматривался маржиналистами как рациональное существо, целью которого является максимизация собственного удовлетворения.\n\nПосле Маржиналистской революции экономическая теория из науки о материальном богатстве превратилась в науку о рациональном поведении людей.\n\nМетодологический индивидуализм. Согласно этому методологическому принципу, закономерности функционирования хозяйства в целом выводятся из поведения отдельно взятого хозяйствующего субъекта. Как писал один из творцов Маржиналистской революции К. Менгер, «то наблюдение, которое мы сперва сделали над изолированным индивидом, а затем над маленьким обществом, временно отделенным от остальных людей, равным образом относится и к более сложным отношениям народа и человеческого общества вообще»1.\n\nСтатичность. Маржиналисты потеряли интерес к «законам движения» капитализма, которыми занимались классики. Акцент экономических исследований после Маржиналистской революции  сместился к изучению использования редких ресурсов для удовлетворения потребностей людей в данный момент времени.\n\nЗамена причинно-следственного анализа функциональным. Это также стимулировало применение в экономической науке математических методов.\n\nЛиквидация приоритета сферы производства, характерного для экономического анализа классиков. Вместо этого на ранней стадии своего развития маржинализма акцент был перенесен на сферу потребления.\n\nАкцент на применении дедуктивных методов исследования в противоположность историзму и индукции.\n\nВосприятие рыночной экономики как равновесной системы (хотя последнее было не характерно для австрийской школы маржинализма). Эта равновесность неразрывна связана с рациональным оптимизирующим поведением, поскольку неравновесные состояния экономики - т. е. те состояния, которые не удовлетворяют рациональных хозяйствующих субъектов - корректируются их действиями и приводятся к равновесию.\n\n\nМеркантилизм - это экономическая теория, которая основана на идее, что основной движущей силой экономики является спрос. Меркантилисты считают, что рост спроса способствует росту экономики и снижению безработицы.\n\nМеркантилизм (Англия) - первая школа экономической науки. Представители: Томас Ман, Джон Лоу, Ричард Кантильон. Главным источником богатства представители этой школы считали торговлю, а богатство отождествляли с золотом.\n\nОсновные принципы меркантилизма:\n\nзолото и другие сокровища являются главным богатством общества;\nглавный источник получения богатства – внешняя торговля и денежный оборот для обеспечения притока в страну золота и серебра;\nгосударство должно активно вмешиваться в экономику страны;\nвнутри страны производство развивается за счет импорта дешевого сырья;\nпоощряется экспорт;\nнизкий уровень поддержания заработной платы за счет роста населения.\n\nНеобходимым условием для развития экономики меркантилисты считали превышение экспорта над импортом (активный торговый баланс).\n\n«ранний» меркантилизм (до середины XVI в.);\n«поздний» меркантилизм (середина XVI – середина XVII в.); начало установления торговых связей между странами за счет предложения относительно дешевых товаров; использование золота и серебра чаще в посреднических сделках.\n\nВ общем маржинализм сосредоточен на стоимости производства и продаже товара, в то время как меркантилизм сосредоточен на спросе и потребностях потребителей. Они оба имеют разные подходы к управлению экономикой и оба имеют свои достоинства и недостатки.\n\nРесурсы\n\nЭтапы развития экономической теории: меркантилизм, физиократы, классическая теория, марксизм, маржинализм, кейнсианство, институционализм, монетаризм\nМаржинализм и неоклассика",
            "tags": [
                "экономика"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/emoji-support",
            "title": "Emoji Support",
            "description": "Guide to emoji usage in Hugo",
            "content": "\nEmoji can be enabled in a Hugo project in a number of ways.\n\nThe emojify function can be called directly in templates or Inline Shortcodes.\n\nTo enable emoji globally, set enableEmoji to true in your site's configuration and then you can type emoji shorthand codes directly in content files; e.g.\n\n🙈 :see_no_evil:  🙉 :hear_no_evil:  🙊 :speak_no_evil:\n\n\nThe Emoji cheat sheet is a useful reference for emoji shorthand codes.\n\nN.B. The above steps enable Unicode Standard emoji characters and sequences in Hugo, however the rendering of these glyphs depends on the browser and the platform. To style the emoji you can either use a third party emoji font or a font stack; e.g.\n\n{{}}\n.emoji {\n  font-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;\n}\n{{}}\n\n{{}}\n\n.emojify {\n\tfont-family: Apple Color Emoji, Segoe UI Emoji, NotoColorEmoji, Segoe UI Symbol, Android Emoji, EmojiSymbols;\n\tfont-size: 2rem;\n\tvertical-align: middle;\n}\n@media screen and (max-width:650px) {\n  .nowrap {\n    display: block;\n    margin: 25px 0;\n  }\n}\n\n{{}}\n",
            "tags": [
                "emoji"
            ]
        },
        {
            "uri": "/posts/featured-image",
            "title": "Featured Image",
            "description": "Post with featured image.",
            "content": "\nMaecenas maximus, elit in ornare porttitor, nisi eros hendrerit nisl, sed fermentum nulla urna blandit tellus.\n\nNullam tempor lectus quis\n\nAenean vehicula non elit id varius. Mauris condimentum lacinia mollis. Nullam quis cursus metus, eget mattis erat. Aliquam nec ante lacus. In tellus augue, iaculis vitae sollicitudin quis, tempor nec urna. Aenean ut fermentum erat, vel gravida ligula. Etiam sed ex aliquet, egestas nibh eu, iaculis mi. Nunc sit amet fermentum ex. Sed convallis ac arcu tristique rhoncus. Suspendisse potenti.\n\nProin justo purus, porttitor et semper ut, commodo et nibh. Nam malesuada id arcu in tempus. Ut ornare vestibulum ultrices. Nullam tempor lectus quis ornare viverra. Vestibulum fringilla turpis ac leo fermentum, et dictum nisi consectetur. Integer ullamcorper fringilla mi, non volutpat sapien ultrices vel. Phasellus at blandit neque, pulvinar rutrum ante.\n\nVestibulum et tortor eget\n\nAliquam posuere diam non ligula tristique congue. Donec dignissim eu justo sed dictum. Praesent at massa erat. Praesent mollis viverra velit. Aliquam maximus pharetra massa a efficitur. Sed tempus egestas purus sit amet tempor. Donec porttitor varius nisi, eu venenatis risus gravida id. Pellentesque blandit nunc non urna consectetur commodo. Sed at feugiat felis, sit amet malesuada nunc. Curabitur in tempor nisl. Pellentesque accumsan est orci, in commodo felis accumsan facilisis. Nulla maximus suscipit posuere. Nulla et consequat mauris, fermentum ultricies tellus.\n\n",
            "tags": [
                "image"
            ]
        },
        {
            "uri": "/posts/gallery-example/",
            "title": "Gallery example",
            "description": null,
            "content": "\n\n\nExample of gallery\n\nImages here",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/posts/google-sheets-2-json/",
            "title": "Отображение таблицы Google Sheets в JSON",
            "description": "Экспорт google sheet в JSON, с моментальным обновлением данных",
            "content": "\nЗадача\n\nЕсть таблица google. Необходимо конвертировать ее в JSON и не делать каждый раз ручной экспорт.\n\nУсловия\n\nтаблица закрыта для общего просмотра\njson отображение читать по ссылке\n\nПлан\n\nИспользовать webapps от google. Парсить google таблицу и выдавать готовый url с json.\n\nПодготовка\n\nОткрываем Таблицу Google\nExtensions → Apps Script\n\nScreen Shot 2022-03-12 at 12.26.28.png\n\nСоздаем скрипт\n\nКак работает endpoint. Документация\n\nКогда пользователь посещает приложение или программа отправляет приложению HTTP-запрос GET, Apps Script запускает функцию doGet(e).\n\nКогда отправляется приложению HTTP-запрос POST, вместо этого Apps Script запускает doPost(e).\n\nВ обоих случаях аргумент e представляет собой параметр события, который может содержать информацию о любых параметрах запроса.\n\nДополнительные условия в запрос сейчас посылать не буду.\n\nИтого функция с получением массива и функция с выдачей результата:\n\nconst sheetName = 's1' // название листа\nconst sheetRange = 'A:J' // диапазон\n\nconst sheet = SpreadsheetApp.getActive().getSheetByName(sheetName)\n\nfunction getData(){\n  const result = []\n  const values = sheet.getRange(sheetRange).getValues()\n  const lastRow = parseInt(sheet.getLastRow())\n\n  for (let i = 1; i < lastRow; i++) {\n    result.push(values[i])\n  }\n\n  return result\n}\n\nfunction doGet() {\n  const data = getData()\n  return ContentService.createTextOutput(\n    JSON.stringify(\n      {'result': data}\n    )\n  ).setMimeType(ContentService.MimeType.JSON)\n}\n\nПубликуем приложение\n\nGoogle Sheets публикаиця приложения\n\nGoogle Sheets публикаиця приложения\nGoogle Sheets публикаиця приложения\n\nРезультат\n\nGoogle Sheets api json",
            "tags": [
                "google sheets",
                "google script"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/howto-create-deepclone-js/",
            "title": "Как сделать глубокое клонирование объекта в JavaScript",
            "description": "Как сделать глубокое клонирование объекта в JavaScript",
            "content": "\nВ JavaScript объекты копируются по ссылке. Это означает, что фактически две(или более) ссылок ссылается на один объект\nДля глубокого клонирования мы можем воспользоваться рекурсией\n\nВоспользуемся методом Object.assign() и возьмем пустой объект ({}), чтобы создать клон оригинального объекта.\nИспользуем Object.keys() и Array.prototype.forEach() для определения ключей-значений, которые нужно полностью клонировать (не ссылаться на них).\n\nconst deepClone = obj => {\n  let clone = Object.assign({}, obj);\n  Object.keys(clone).forEach(\n    key => (clone[key] = typeof obj[key] === 'object' ? deepClone(obj[key]) : obj[key])\n  );\n  return Array.isArray(obj) && obj.length\n    ? (clone.length = obj.length) && Array.from(clone)\n    : Array.isArray(obj)\n      ? Array.from(obj)\n      : clone;\n};\n\nconst a = { foo: 'bar', obj: { a: 1, b: 2 } };\nconst b = deepClone(a); // a !== b, a.obj !== b.obj\n`",
            "tags": [
                "JavaScript"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/howto-install-rhel-9-free/",
            "title": "Установка Linux RHEL 9",
            "description": "Скачать и установить Linux RHEL 9 бесплатно",
            "content": "\n\nRed Hat Enterprise Linux 9 (RHEL 9) под кодовым названием Plow стал общедоступным (GA). Компания Red Hat объявила об этом 18 мая 2022 года. Она сменила бета-версию, которая существовала с 3 ноября 2021 года.\n\nRHEL 9 - это несколько первых релизов в семействе Red Hat. Это первый крупный релиз после приобретения Red Hat компанией IBM в июле 2019 года, а также первая крупная версия после отказа от проекта CentOS в пользу CentOS Stream, который теперь является предшественником RHEL.\n\nRHEL 9 является последней основной версией RHEL и поставляется с ядром 5.14, множеством новых пакетов программного обеспечения и массой усовершенствований. В ней особое внимание уделяется безопасности, стабильности, гибкости и надежности.\n\nОписание\nRHEL 9 поставляется с новыми версиями программного обеспечения, включая Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0 и многие другие.\n\nПодготовка к установке\n\nРегистрация на портале Red Hat\n\nПодписка Red Hat Developer Subscription - это бесплатное предложение программы Red Hat Developer, предназначенное для индивидуальных разработчиков, которые хотят воспользоваться всеми преимуществами Red Hat Enterprise Linux.\n\nОна дает разработчикам доступ ко всем версиям Red Hat Enterprise Linux, а также к другим продуктам Red Hat, таким как дополнения, обновления программного обеспечения и ошибки безопасности.\n\nПрежде всего, убедитесь, что у вас есть активная учетная запись Red Hat. Если у вас еще нет учетной записи, перейдите на портал Red Hat Customer Portal, нажмите на кнопку \"Регистрация\" и заполните свои данные для создания учетной записи Red Hat.\n\nЗагрузка установочного образа\nПосле создания учетной записи Red Hat вы можете приступать к загрузке RHEL 9. Чтобы загрузить Red Hat Enterprise Linux 9 абсолютно бесплатно, зайдите на Red Hat Developer Portal  и войдите в систему, используя учетные данные своей учетной записи.\n\n\nЗатем перейдите на страницу загрузки RHEL 9 и нажмите на кнопку загрузки, показанную ниже.\n\nЯ использую MacBook M1, поэтому скачиваю образ RHEL 9 для M1 процессора aarch64\n\nВиртуальная машина\nВ качестве вирутальной машины для установки RHEL 9 использую бесплатную виртуальную машину UTM. Установить можно с помощью Homebrew, выполнив команду brew install --cask utm.\n\nУстановка Red Hat Enterprise Linux 9\n\nНастройка виртуальной машины UTM\nВ UTM нажимаем Create a New Virtual Machine -> Virtualize\n\n\nВыбираем скачанный образ RHEL 9 и нажимаем Continue\n\nГлавное меню\n\n\nПомеченные поля необходимо заполнить\n\nСоздаем Root Password\n\nUser Creation. Создаем пользователя, под которым будет осуществляться вход в систему.\n\n\n\nConnect to Red Hat. Здесь используем учетную запись, созданную выше.\n\nВводим данные аккаунта, нажимаем Register\n\nНажимаем Done\n\nВ разделе Installation Destination выбираем диск по умолчанию\n\nТеперь можем продолижть установку. На главном экране появилась кнопка Begin installation\n\nПосле завершения установки перезагружаем систему.\n\nИногда после перезагрузки запускается загрузка с установочного образа опять. Неоьбходимо либо отключить диск в настройка вирутальной машины либо перезагрузить UTM.\n\nЗапуск Red Hat Enterprise Linux 9\n\n\n\nВводим пароль и видим рабочий стол RHEL 9\n\n\nДля доступа к приложениям нажимаем кнопку Activities в верхнем левом углу\n\nНастройка Red Hat Enterprise Linux 9\n\nПроверка пользователя ROOT\nВ системе Linux пользователи относятся к разным группам, у которых есть определенные права. Если в процессе установки мы не поставили галку сделать пользователя администратором, то по умолчанию он не сможет устанавливать некоторые системные программы.\n\nВыходим из системы и заходим в систему под пользователем root (тем самым, которого создавали ранее на главном экране). Нажимаем Log out\n\n\nТеперь входим под root. Пользователя может не быть в списке. Жмем Not listed и вводим данные аккаунта.\n\nОткрываем терминал и проверяем\n\nНастройка параметров системы\n\nКнопки сворачивания приложения\nПервое, что кажется непривычным при использовании GUI, отсутствие кнопок сворачивания окон\n\n\nУстанавливаем необходимый пакет\nyum install gnome-tweaks -y\n\nПосле установки появится приложение Tweaks. Найдем его через поиск.\n\nВ приложении множество и других настроек. Мы отобразим кнопки сворачивания приложений.\n\nИдем в раздел Windows titlebars и включаем параметры Maximize, Minimize\n\nДоступ пользователю на установку приложений\n\nЧтобы постоянно не переключаться на root пользователя для устновки приложений, мы можем предоставить обычному пользвоателю доступ к установке приложений.\nДействия продолжаем делать под пользователем  root.\nОткрываем файл /etc/sudoers и добавляем пользователя\n\nsudo vi /etc/sudoers\n\nДобавляем в конец файла данные пользователя. Имя моего пользователя: rhel-user\nrhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf\n\n\nУстановим Visual Studio Code под обычным пользователем\nУстановка состоит из следующих шагов:\nдобавление нужного репозитория. Права на добавление репозитория (изменение файлов в директории по прежнему только у root пользователя)\nзагрузка и Установка\n\nПервый шаг делаем под пользователем root\nИдем на сайт https://code.visualstudio.com/docs/setup/linux\n\nКопируем код и запускаем в терминале\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc\nsudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo'\n\nПереключаемся на пользователя rhel-user. Это можно сделать и в терминале.\nОбновим репозитории\nУстановим VSCode\n\nsu rhel-user\ndnf check-update\n\nsudo dnf install code\n\nСсылки\nhttps://developers.redhat.com/products/rhel/getting-started\nhttps://www.redhat.com/sysadmin/install-linux-rhel-9",
            "tags": [
                "linux",
                "rhel"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/howto-install-ubuntu-desktop-on-arm/",
            "title": "Установка Ubuntu Desktop 22.10 (Kinetic Kudu) на ARM CPU",
            "description": "Быстрая базовая установка Ubuntu Desktop 22.10 на виртуальную машину UTM с процессором ARM M1",
            "content": "\nUbuntu - одна из популярных Linux систем и достаточно много обзоров по установке Ubuntu. В этой статье мы будем устанавливать образ Ubuntu для ARM процессора на виртуальную машину UTM. Вся установка будет проходить на Mac OS.\n\nЗагрузка установочного образа\n\nНа сайте Ubuntu доступен для скачивания только образ Ubuntu Server ARM версии 22.04 - без графического интерфейса. Но можно скачать обновленный релиз Ubuntu Desktop для ARM - Daily Build по ссылке.\n\nНаходим 64-bit ARM (ARMv8/AArch64) desktop image и скачиваем\nARMv8/AArch64\n\nВиртуальная машина\n\nВ качестве виртуальной машины для установки RHEL 9 использую бесплатную виртуальную машину UTM. Установить можно с помощью Homebrew, выполнив команду brew install --cask utm.\n\nУстановка Ubuntu Desktop\n\nНастройка виртуальной машины UTM\nВ UTM нажимаем Create a New Virtual Machine -> Virtualize\nНастройка виртуальной машины UTM\n\nВыбираем скачанный образ и нажимаем Continue, далее оставляем опции по умолчанию\n\nЗапуск Live версии\nВыбираем Try or Install Ubuntu. Запустится live образ Ubuntu. Такой образ не сохраняет свое состояние после перезагрузки.\n\n\nВходим под пользователем ubuntu:\n\n\nВидим рабочий стол и можем пользоваться.\n\nУстановка\nВнизу справа есть ярлык для стандартной установки Ubuntu. Нажимаем и запускаем обычную установку на диск.\n\n\nВыбираем нужный язык\n\nЯ выбираю минимальную установку, т.к. мне не нужны будут предустановленные игры и прочие приложения. Графический интерфейс, браузер, терминал остается со всеми базовыми настройками.\n\nОставляем по умолчанию стирание виртуального диска перед установкой\n\nСоздаем пользователя, под которым будем входить в систему.\n\nКак только установка закончится, нажимаем Restart.\n\nУ меня после перезагрузки черный экран. Поэтому я просто закрываю и снова запускаю вирутальную машину.\n\nВход в систему\nПосле запуска системы выбираем *Boot from next volume. Первым по умолчанию будет запуск с вирутального образа, но у нас уже есть система на диске, поэтому выбираем запуск со следующего по очереди диска.\n\nВходим под своим пользователем\n\nСистема предлагает скачать обновления для системы. Нажимаю установить.\n\nТеперь можно пользоваться системой и все данные будут сохраняться после перезагрузки.\nKinetic Kudu 22.10\n\nСсылки\n\nKinetic Kudu Release Schedule\nDaily Builds\n",
            "tags": [
                "linux",
                "ubuntu"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/howto-redirect-to-url/",
            "title": "Как сделать редирект на другой URL в JavaScript",
            "description": "Как сделать редирект на другой URL в JavaScript",
            "content": "\nПользователя можно перенаправлять с одной веб-страницы на любую другую несколькими способами.\nс помощью обновления мета-данных HTML. Перенаправления на стороне сервера. Например, используя файл .htaccess, PHP\nс помощью перенаправления на стороне клиента через JavaScript.\n\nДля перенаправления на другой URL с помощью JavaScript используем window.location.href или window.location.replace().\nПередать второй аргумент, чтобы произвести клик по ссылке (true - по умолчанию) или перенаправление по HTTP (false).\n\nJavaScript функции\n\nЛогика\nconst newUrl = 'https://www.google.com/';\n\nwindow.location.href = newUrl; // 1\nwindow.location.replace(newUrl); // 2\nwindow.location.assign(newUrl) // 3\n\nПример функции\nconst redirect = (url, asLink = true) =>\n  asLink ? (window.location.href = url) : window.location.replace(url);\n\nJavaScript в html\n\n\n  const newUrl = 'https://www.google.com/';\n  window.location.href = newUrl;\n\n\nredirect('https://google.com');\n\nметатег HTML\n\n\nПосле того как загрузится ткущая страница, браузер перенаправит на новую страницу, ожидая при этом 0 content=\"0 секунд.\n\nЧтобы выполнялась отложенная переадресация, укажите нужное количество секунд в атрибуте content:\n\n",
            "tags": [
                "JavaScript",
                "redirect url"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/howto-rename-files-in-python/",
            "title": "Как переименовать файлы в Python",
            "description": "Различные способы переименовывания файлов в Python",
            "content": "\nos.rename\n\nЕсли имеется весь путь до пути файла:\n\nold_source = '/Users/r/Desktop/old_source.txt'\nnew_source = '/Users/r/Desktop/new_source.txt'\nos.rename(\"old_source\", \"new_source\")\n\nЕсли имеется только имя файла, воспользуемся os.path.splitext(), который возвращает кортеж из имени файла и расширения:\n\nimport os\nfor file in os.listdir():\n    name, ext = os.path.splitext(file) # return ('путь до файла без расщирения', '.txt')\n    new_name = f\"{name}_new{ext}\"\n    os.rename(file, new_name)\n\npathlib\n\nС помощью встроенного модуля pathlib\n\nPath.rename(new_name)\n\nfrom pathlib import Path\nfor file in os.listdir():\n    f = Path(file)\n    new_name = f\"{f.stem}_new{f.suffix}\"\n    f.rename(new_name)\n\nshutil.move\n\nМодуль Shutil предлагает ряд высокоуровневых операций с файлами и коллекциями файлов. В частности, предусмотрены функции, поддерживающие копирование и удаление файлов.\n\nimport shutil\n\nold_source = '/Users/r/Desktop/old_source.txt'\nnew_source = '/Users/r/Desktop/new_source.txt'\n\nnewFileName = shutil.move(old_source, new_source)\n\nprint(\"Новый файл:\", newFileName)\nНовый файл: /Users/r/Desktop/new_source.txt\n`",
            "tags": [
                "Python"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/hugo-add-image-zoomin/",
            "title": "Увеличение картинки по нажатию в Hugo",
            "description": "Добавляем скрипт, который будет увеличивать картинку в статье при нажатии",
            "content": "\nВведение\n\nВ Hugo по умолчанию используется парсинг markdown файлов. Т.е. мы получаем html код в том виде, как он написан в markdown.\n\nДля того, чтобы нам понимать какие именно изображения мы можем увеличивать, добавим к этим изображениям отдельный тег/ключ/id\n\nИнструменты\n\nДля реализации функционала нам необходимо:\nнаписать/подключить скрипт/обработчик, который будет выполнять эффект zoomin к нужным нам изображениям\nДобавить необходимые метаданные к изображениям, чтобы скрипт их смог найти\n\nСкрипт zoomin\nДля добавления возможности увеличивать картинку при нажатии воспользуемся пакетом medium-zoom.\n\nДанный покет реализовывает данную функциональность в ненагруженном удобном стиле.\n\n\nДемо сайт\n\nЛогика скрипта\n\nСкрипт находит изображения с id и так понимает, что нужно применить свойство zoomin к этим изображениям\n\nВозможные id:\n\nzoom-default\nzoom-margin\nzoom-background\nzoom-scrollOffset\nzoom-trigger\nzoom-detach\nzoom-center\n\nПодключение скриптов\n\nДля работы скрипта, нам необходимо подключить логику, а также обработчик.\n\nВ Hugo в корне проекта есть папка static, которую можно использовать для хранения статических файлов (стиле, скриптов) и использовать для подключения на сайте. Если такой папки нет, то можно создать.\n\nВ папке static создадим папку zoom-image и добавим в нее 2 скрипта\n\nstatic/js/zoom-image/index.js\n\nconst zoomDefault = mediumZoom('#zoom-default')\nconst zoomMargin = mediumZoom('#zoom-margin', { margin: 48 })\nconst zoomBackground = mediumZoom('#zoom-background', { background: '#212530' })\nconst zoomScrollOffset = mediumZoom('#zoom-scrollOffset', {\n    scrollOffset: 0,\n    background: 'rgba(25, 18, 25, .9)',\n})\n\n// Trigger the zoom when the button is clicked\nconst zoomToTrigger = mediumZoom('#zoom-trigger')\nconst button = document.querySelector('#button-trigger')\nbutton.addEventListener('click', () => zoomToTrigger.open())\n\n// Detach the zoom after having been zoomed once\nconst zoomToDetach = mediumZoom('#zoom-detach')\nzoomToDetach.on('closed', () => zoomToDetach.detach())\n\n// Observe zooms to write the history\nconst observedZooms = [\n    zoomDefault,\n    zoomMargin,\n    zoomBackground,\n    zoomScrollOffset,\n    zoomToTrigger,\n    zoomToDetach,\n]\n\n// Log all interactions in the history\nconst history = document.querySelector('#history')\n\nobservedZooms.forEach(zoom => {\n    zoom.on('open', event => {\n        const time = new Date().toLocaleTimeString()\n        history.innerHTML += `Image \"${event.target.alt\n            }\" was zoomed at ${time}`\n    })\n\n    zoom.on('detach', event => {\n        const time = new Date().toLocaleTimeString()\n        history.innerHTML += `Image \"${event.target.alt\n            }\" was detached at ${time}`\n    })\n})\n\nstatic/js/zoom-image/placeholders.js\n\n// Show placeholders for paragraphs\nconst paragraphs = [].slice.call(document.querySelectorAll('p.placeholder'))\n\nparagraphs.forEach(paragraph => {\n  // eslint-disable-next-line no-param-reassign\n  paragraph.innerHTML = paragraph.textContent\n    .split(' ')\n    .filter(text => text.length > 4)\n    .map(text => ${text})\n    .join(' ')\n})\n\nCDN скрипт\n\nСкрипт можно скачать, а можно подгружать\n\nСсылка на скрипт\n\nДобавление в шаблон\n\nДля того, чтобы данные скрипты работали в шаблоне сайта, их необходимо подключить.\n\nЯ использую для этого шаблон baseof.html. Просто добавляю ссылки на скрипта в body шаблона.\n\nbaseof.html\n\n    ...\n\n\nID изображения\n\nHugo позволяет изменить поведение при парсинге markdown файлов с помощью хуков. Подробнее о рендер-хуках можно прочитать на сайте.\n\nВ папке *layouts\n\nДобавим файл render-image.html по следующему пути layouts -> _default -> _markup\n\n\nкод файла:\n\n\nМы добавили только id=\"zoom-default\" в код по умолчанию\n\nИтоги\n\nYour browser does not support the video tag.\n\nПроцесс\n\n{{}}",
            "tags": [
                "hugo"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/integrate-hugo-react/",
            "title": "Как подключить React .jsx в проект на Hugo",
            "description": "Подключение react компонентов в hugo проект",
            "content": "\nHugo предлагает подключение различных JS библиотек в проект. Такие изменения влекут за собой полное обновление проекта.\nСегодня мы подключим компонент react без внесения больших изменений.\n\nReact - это библиотека. Чтобы она заработала на сайте, необходимо ее подклчюить, а далее воспользоваться внутренними функциями.\n\nПодключить можно двумя способоами. С помощью подгрузки скрипта с CDN или загрузки пакета в package.json, чтопозволит использовать .jsx\n\npackage.json\n\nПлан:\n\nИмпорт пакета в package.json\nСоздание .jsx скрипта\nЗагрузка/build пакета в Hugo\n\nИмпорт\n\nВ корне проекта запускаем команду\n\nnpm i react react-dom\n\nСоздание jsx скрипта\n\nВ папке с темой assets создадим файл my-react-script.jsx\n\nimport React from 'react';\nimport * as ReactDOM from 'react-dom';\nimport { createRoot } from 'react-dom/client';\n\nconst App = () => {\n  function sayHello () {\n    alert('Hello, World!')\n  }\n\n  return (\n    Click me!\n  )\n}\n\nReactDOM.render(\n  React.createElement(App, null),\n  document.getElementById('root')\n)\n\nconst container = document.getElementById('my_render_block');\nconst root = createRoot(container);\nroot.render();\n\nДобавим блок div в место в шаблоне для отрисовки react приложения\n\nПодключение в HUGO\n\nВ файле head.html или другом файте шаблона Hugo импортируем скрипт\n\n{{ with resources.Get \"my-react-script.jsx\" }}\n{{ $options := dict \"defines\" (dict \"process.env.NODE_ENV\" \"\\\"development\\\"\" \"process.env.BaseURL\" (printf \"%s\"\n$.Site.BaseURL)) }}\n{{ $script := . | js.Build $options }}\n\n{{ end }}\n\nCDN\n\nВторой способ\n\nПодключение библиотеки React\n\nВ проекте Hugo в шаблонах обновим файл head.html. В моем проекте это шаблон, который содержит основные теги html и head.\nОткрываем layouts/partials/head.html и добавляем скрипт в раздел ``:\n\n  ... -->\n\n    ... -->\n\nВыбор места для отрисовки компонента\n\nСоздадим div блок в любом шаблоне Hugo, где будем отрисоывать React компонент.\nНапример файл layouts/partials/footer.html\n\n\nReact будет искать данный блок и отрисует внутри него компонент\n\nСоздание компонента\nВынесем создание компонента в отдельный js файл. В Hugo есть директория static в корне проекта. Если нету, то можно создать. Подробнее о static folder\n\nСоздадим файл static/js/my_react_component.js и запишем код:\nВажно: сркипт должен подключиться в проекте после блока ``\n\nconst e = React.createElement;\n\nconst MyCountButton = () => {\n  const [count, setCount] = React.useState(100);\n  return e(\n    'button',\n    { onClick: () => setCount(count + 1) },\n    count\n  );\n}\n\n// Выведем на экран компонент\n// ищем блок my_react_app и отрисовываем внутри него компонент\nReactDOM.render(React.createElement(MyCountButton), document.getElementById(\"my_react_app\"));\n\nПодключение скрипта с React компонентами\n\nТак как скрипт будет искать div \"my_react_app\", данный div блок должен быть загружен до исполнения скрипта. Поэтому в файле layouts/partials/footer.html добавляем скрипт в конец раздела ``:\n\nПример\n\nНажми на счетчик: 100\n\n{{}}\n",
            "tags": [
                "react",
                "hugo"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/interactivebrokers-deposit/",
            "title": "Пополнение Interactive Brokers с Израильского счета",
            "description": "Пополнение Interactive Brokers с Израильского счета банка Дисконт",
            "content": "\nWeb\n\nСоздание заявки в IB\n\nЗаходим на сайт https://www.interactivebrokers.co.uk/portal/#/\nНажимаем Deposit\n\n\n\nНажимаем Use a new deposit method если ранее шаблон не был создан\n\n\n\nBank Wire -> Get instructions\n\n\n\nAccount Number: Номер банковского счета.\n\n\n\nПолучаем инструкции с реквизитами для пополнения Bank Wire Instructions\n\n\n\nЭти данные Вам нужны для оплаты в Discount Bank\n\nОтправить деньги из Discount Bank\n\nЗаходим в личный кабинет банка start.telebank.co.il\n\n\n\nНажимаем: ביצוע העברה\n\nЗаполняем форму\n\n\n\nНажимаем המשך и жмем далее. Приходит смс с подверждением, вводим и жмем далее\n\n\n",
            "tags": [
                "interactivebrokers",
                "invest"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/markdown-syntax/",
            "title": "Руководство по оформлению Markdown файлов",
            "description": "Руководство по оформлению Markdown файлов",
            "content": "\nЭта статья предлагает пример базового синтаксиса Markdown, который можно использовать в файлах содержимого Hugo, а также показывает, украшаются ли основные элементы HTML с помощью CSS в теме Hugo.\n\n\nРекомендации по оформления статьи\nЗаголовки\n\nЗаголовки первого и второго уровней, выполненные с помощью подчеркивания, выглядят следующим образом:\n\nЗаголовок первого уровня\n\nЗаголовок второго уровня\nЗаголовок первого уровня\n\nЗаголовок второго уровня\nЗаголовки всех шести уровней можно обозначать и с помощью символа («#»)\n\nH1\nH2\nH3\nH4\nH5\nH6\nH1\nH2\nH3\nH4\nH5\nH6\n\nПараграфы\n\nДля оформления абзацев в html используются теги ``, но в Markdown блок текста автоматически преобразуется в параграф.\n\nДля вставки пустой строки необходимо два раза поставить символ переноса строки (нажать на Enter)\n\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit!\n\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae.\n\nЦитаты\n\nЭлемент blockquote представляет содержимое, которое цитируется из другого источника, по желанию с цитатой, которая должна находиться в элементе footer или cite, и по желанию с изменениями в строке, такими как аннотации и сокращения.\n\nБлок-цитата без указания авторства\nTiam, ad mint andaepu dandae nostion secatur sequo quae.\nОбратите внимание, что вы можете использовать синтаксис Markdown внутри блочной цитаты.\n\nБлок-цитата с указанием авторства\nDon't communicate by sharing memory, share memory by communicating.\n— Rob Pike\n\n: Приведенная выше цитата взята из книги Роба Пайка talk during Gopherfest, November 18, 2015.\nЭто пример цитаты,\nв которой перед каждой строкой\nставится угловая скобка.\nЭто пример цитаты,\nв которой угловая скобка\nставится только перед началом нового параграфа.\nВторой параграф.\nЭто пример цитаты,\nв которой перед каждой строкой\nставится угловая скобка.\nЭто пример цитаты, в которой угловая скобка ставится только перед началом нового параграфа.\nВторой параграф.\nПервый уровень цитирования\n> Второй уровень цитирования\n>> Третий уровень цитирования\nПервый уровень цитирования\nПервый уровень цитирования\n> Второй уровень цитирования\n>> Третий уровень цитирования\nПервый уровень цитирования\n\nТаблицы\n\nТаблицы не являются частью основной спецификации Markdown, но Hugo поддерживает их из коробки.\n\n   | Name  | Age |\n   | ----- | --- |\n   | Bob   | 27  |\n   | Alice | 23  |\n\n   | Name  | Age |\n   | ----- | --- |\n   | Bob   | 27  |\n   | Alice | 23  |\n\nВ ячейках разделительной строки используются только символы - и :. Символ : ставится в начале, в конце или с обеих сторон содержимого ячейки разделительной строки, чтобы обозначить выравнивание текста в соответствующем столбце по левой стороне, по правой стороне или по центру.\n\nКолонка по левому краю | Колонка по правому краю | Колонка по центру\n:--- | ---: | :---:\nТекст | Текст | Текст\n\n| Колонка по левому краю | Колонка по правому краю | Колонка по центру |\n| :--------------------- | ----------------------: | :---------------: |\n| Текст                  |                   Текст |       Текст       |\n\nMarkdown внутри таблицы\n\n| Italics   | Bold     | Code   |\n| --------- | -------- | ------ |\n| italics | bold | code |\n\n| Italics   | Bold     | Code   |\n| --------- | -------- | ------ |\n| italics | bold | code |\n\nБлоки кода\n\nБлок кода с обратными кавычками\n\n  Example HTML5 Document\n\n\n  Test\n\nБлок кода с отступом в четыре пробела\n\n      Example HTML5 Document\n\n      Test\n\n\nБлок кода с внутренним шорткодом подсветки Hugo\n\n{{}}\n\n  Example HTML5 Document\n\n\n  Test\n\n\n{{}}\n\nСписки\n\nОформляйте заголовки единообразно. В конце заголовка точку не ставьте.\n\n    | Правильно                                     | Неправильно                                 |\n    | --------------------------------------------- | ------------------------------------------- |\n    | Получение сертификата  Создание кластера | Получить сертификат  Создание кластера |\n    | Получить сертификат  Создать кластер     |\n\nЕсли требуется описать последовательность действий, используйте нумерованный список. В конце строк ставьте точку.\nЕсли порядок пунктов неважен, используйте маркированный список. Оформляйте его одним из способов:\n\n    Если элементы списка — отдельные предложения, начинайте их с заглавной буквы и ставьте точку в конце.\n    Если вводная фраза и список составляют одно предложение, то элементы списка должны начинаться со строчной буквы и завершаться точкой с запятой. Последний элемент списка завершается точкой.\n    Если список состоит из названий или значений параметров (без пояснений), знаки в конце строк не ставьте.\n\nУпорядоченный список\n\nFirst item\nSecond item\nThird item\n\nЧтобы оформить упорядоченный нумерованный список, используйте цифры с символом . или ). Рекомендованный формат разметки: цифра 1 и символ ..\n\nПервый пункт\nВторой пункт\nТретий пункт\n\nбудет отображаться как:\n\nПервый пункт\nВторой пункт\nТретий пункт\n\n\nЧтобы оформить вложенный упорядоченный список, добавьте отступ для элементов дочернего списка. Допустимый размер отступа — от двух до пяти пробелов. Рекомендуемый размер отступа — четыре пробела.\n\nНапример, разметка:\n\nПервый пункт\n    Вложенный пункт\n    Вложенный пункт\nВторой пункт\n\nбудет отображаться как:\n\nПервый пункт\n    Вложенный пункт\n    Вложенный пункт\nВторой пункт\n\nНеупорядоченный список\n\nList item\nAnother item\nAnd another item\n\nВложенный список\n\nFruit\n  Apple\n  Orange\n  Banana\nDairy\n  Milk\n  Cheese\n\nДругие элементы - abbr, sub, sup, kbd, mark\n\nGIF is a bitmap image format.\n\nH2O\n\nXn + Yn = Zn\n\nPress CTRL+ALT+Delete to end the session.\n\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n\n💡 Структура данных — это контейнер, который хранит данные в определённом формате. Этот контейнер решает, каким образом внешний мир может эти данные считать или изменить.\n",
            "tags": [
                "markdown",
                "css",
                "html",
                "themes"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/math-support",
            "title": "Math Support",
            "description": null,
            "content": "\nTheme supports the rendering of mathematical formulas by using KaTeX.\n\n\n\n\nYou can type inline equation like $E=mc^2$.\n\nAnd also displayed equation like:\n\n\n\\[ \\int u \\frac{dv}{dx}\\, dx=uv-\\int \\frac{du}{dx}v\\,dx \\]\n\n\n\nMatrix:\n\n\n\\[ \\begin{pmatrix} a&b\\\\c&d \\end{pmatrix} \\quad\n\\begin{bmatrix} a&b\\\\c&d \\end{bmatrix} \\quad\n\\begin{Bmatrix} a&b\\\\c&d \\end{Bmatrix} \\quad\n\\begin{vmatrix} a&b\\\\c&d \\end{vmatrix} \\]\n\n\nAligned equation:\n\n\n\\[\\begin{aligned}\nx ={}& a+b+c+{} \\\\\n&d+e+f+g\n\\end{aligned}\\]\n\n\nAnd many other kinds of formulas.\n",
            "tags": []
        },
        {
            "uri": "/posts/nextjs-to-github-pages-ations/",
            "title": "Публикация next.js приложения на github pages",
            "description": "Сайт на next.js использует данные из Notion. Сделать публикацию на github pages с помощью github actions",
            "content": "\nПодготовка\n\nкоммит все предыдущего состояния на случай вынужденного отката\n\nДля того чтобы Actions имели доступ к репозиторию нужно подключить ключи шифрования\n\nНастройка репозитория\n\nСоздаю ключи\n\nssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\"\n\nСоздалось 2 файла с ключами:\n\ngh-pages - приватный\ngh-pages.pub - публичный\n\nв Репозитории (не профиле)\n\nhttps://github.com/romankurnovskii/notion-project/settings/keys\n\nSettings → Deploy keys →Add new\n\nиз файла gh-pages.pub вставляю текст публичного ключа\n\nScreen Shot 2022-03-05 at 19.50.08.png\n\nSettings → Secrets\n\nИмя: ACTIONS_DEPLOY_KEY\n\nВставляю приватный ключ из приватного файла gh-pages\n\nhttps://github.com/romankurnovskii/notion-project/settings/secrets/actions/new\n\nScreen Shot 2022-03-05 at 19.52.04.png\n\nУдаляю ключи файлы чтобы случайно не закоммитить\n\nна гитхабе создаю экшн\n\nhttps://github.com/romankurnovskii/notion-project/new/main?filename=.github%2Fworkflows%2Fmain.yml&workflow_template=blank\n\nActions → Create\n\nСоздание Actions\n\nВыбираю стандартный action (Deploy...)\n\nРедактирую нижнюю часть кода\n\nname: Build\n        run: |\n          npm i\n          npm run build\n          npm run export\nname: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\n          publish_dir: ./out\n\nnpm run export - для создания статических файлов (добавлю позже)\n\nACTIONS_DEPLOY_KEY - название ключа, что создал ранее\n\npeaceiris/actions-gh-pages@v3 - action из другого популярного репозитория. Ссылаюсь на него.\n\nИтого код:\n\nname: Deploy to Github Pages\n\non:\n  push:\n    branches:\n      main\n\n  workflow_dispatch:\n\njobs:\n  deployment:\n    runs-on: ubuntu-latest\n\n    steps:\n      uses: actions/checkout@v2\n\n      name: Setup Node\n        uses: actions/setup-node@v3\n        with:\n          node-version: \"lts/*\"\n          cache: \"npm\"\n\n      name: Build\n        run: |\n          npm i\n          npm run build\n          npm run export\n      name: Deploy\n        uses: peaceiris/actions-gh-pages@v3\n        with:\n          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}\n          publish_dir: ./out\n\nоткрыть package.json\n\nнайти поле scripts, если нет создать:\n\n{\n...,\n\n  \"scripts\": {\n    \"dev\": \"next dev\",\n    \"build\": \"next build\",\n    \"start\": \"next start\",\n    \"deploy\": \"vercel --prod\",\n    \"export\": \"next export\"\n},\n...\n}\n\nЕсли npm run build && npm run export отработала, то хорошо\n\nОтладка\n\nНе отработала, ошибка:\n\ninfo  - Copying \"static build\" directory\ninfo  - No \"exportPathMap\" found in \"next.config.js\". Generating map from \"./pages\"\nError: Image Optimization using Next.js' default loader is not compatible with next export.\n  Possible solutions:\n    Use next start to run a server, which includes the Image Optimization API.\n    Use any provider which supports Image Optimization (like Vercel).\n    Configure a third-party loader in next.config.js.\n    Use the loader prop for next/image.\n  Read more: https://nextjs.org/docs/messages/export-image-api\n\nhttps://nextjs.org/docs/api-reference/next.config.js/exportPathMap\n\nпример кода из документации\n\nmodule.exports = {\n  exportPathMap: async function (\n    defaultPathMap,\n    { dev, dir, outDir, distDir, buildId }\n  ) {\n    return {\n      \"/\": { page: \"/\" },\n      \"/about\": { page: \"/about\" },\n      \"/p/hello-nextjs\": { page: \"/post\", query: { title: \"hello-nextjs\" } },\n      \"/p/learn-nextjs\": { page: \"/post\", query: { title: \"learn-nextjs\" } },\n      \"/p/deploy-nextjs\": { page: \"/post\", query: { title: \"deploy-nextjs\" } },\n    };\n  },\n};\n\nмой:\n\nmodule.exports = withBundleAnalyzer({\n  images: {\n    domains: [\"pbs.twimg.com\"],\n  },\n});\n\nScreen Shot 2022-03-05 at 19.35.50.png\n\nРедактирую \\\\*next.config.js\\*\\\n\nДобавляю:\n\nconst repoName = '/notion-project'\nmodule.exports = {\n    basePath: repoName,\n    assetPrefix: repoName,\n...\n\nhttps://github.com/romankurnovskii/notion-project/blob/main/next.config.js\n\nПроблема с установкой зависимости вовремя использованя npm установщика. Буду использовать yarn потому что он пропускает минорные уведомления для меня кажется более стабильным.\n\nПока разбирался с проблемы запуска экшенов и настройками нашёл новые экшены и без использования ключа. Обновлю код\n\nПосле того как я редактирую данные нужен они не меняются на сайте. Не меняются потому что гитхаб создаёт статические файлы, то есть нужно заново сделать новый билд. Для меня моментальные изменения не критичны поэтому я поставлю задачу билда повторяться каждый день в 7:00 утра\n\nДобавляю код в yaml файл\n\non:\npush:\nbranches: [main]\nschedule:\n  cron: \"0 7 * * *\" ## every day 7 am\n\nИтоговый результат\n\nhttps://github.com/romankurnovskii/notion-project/blob/main/.github/workflows/main.yml\n\nlines (32 sloc)  867 Bytes\nname: Deploy to GitHub Pages\n\non:\n  push:\n    branches: [main]\n  schedule:\n    cron: \"0 7 * * *\" ## every day 7 am\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [14.x]\n\n    steps:\n      name: Get files\n        uses: actions/checkout@v2\n      name: Use Node.js ${{ matrix.node-version }}\n        uses: actions/setup-node@v2\n        with:\n          node-version: ${{ matrix.node-version }}\n      name: Install packages\n        run: yarn install\n      name: Build project\n        run: yarn run build\n\n      name: Export static files\n        run: yarn run export\n\n      name: Add .nojekyll file\n        run: touch ./out/.nojekyll\n      name: Deploy\n        uses: JamesIves/github-pages-deploy-action@4.1.1 #third party github actions / ok to use\n        with:\n          branch: gh-pages\n          folder: out\n\nПосле тестового комитета и билда получаю 2 проблемы:\n\nСтилей нет, картинки не подгружены\nСсылки не работают\n\nNext.js ожидает адрес вида https://username.github.io/\n\nА у меня в конце ещё добавляется репозиторий. Т.е. добавился ещё один уровень в пути\n\nname: Deploy\n        uses: JamesIves/github-pages-deploy-action@4.1.1 #third party github actions / ok to use\n        with:\n          branch: gh-pages\n          folder: out\nname: Add .nojekyll file\n        run: touch ./out/.nojekyll\n\nИсточники\n\nhttps://wallis.dev/blog/deploying-a-next-js-app-to-github-pages\nhttps://gregrickaby.blog/article/nextjs-github-pages\nhttps://medium.com/@anotherplanet/git-tips-next-js-github-pages-2dbc9a819cb8\nhttps://www.linkedin.com/pulse/deploy-nextjs-app-github-pages-federico-antu%C3%B1a\n",
            "tags": [
                "github",
                "deploy"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/photos/22-07-02-israel-haifa-bahai-gardens/",
            "title": "Израиль - Хайфа - Бахайские сады",
            "description": "Израиль - Хайфа - Бахайские сады",
            "content": "\nМаршрут\n\n\n{{}}\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/posts/photos/icons/",
            "title": "Awesome app icons",
            "description": "Awesome app icons",
            "content": "\nAll examples\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/posts/pyscript-python-embedded-in-html/",
            "title": "PyScript - Python, встроенный в HTML",
            "description": "PyScript - Python, встроенный в HTML",
            "content": "\nPyScript\n\nPyScript - средство запуска Python в браузере, встроенное в HTML, был анонсирован на мероприятии PyCon в Солт-Лейк-Сити, США.\nPyScript\nКнопка Instl здесь для шутки, так как установка не требуется\n\nPyScript зависит от существующего проекта Pyodide, который является скомпилированным в WebAssembly интерпретатором CPython 3.8, позволяющим запускать Python в браузере, а также скомпилированных научных пакетов Python.\n\nСвязывание с файлами библиотеки CSS и JavaScript PyScript позволяет разработчикам встраивать код Python с помощью тега `, а также компонент ` (Read, Evaluate, Print, Loop), который позволяет Python печатать и выполняться динамически.\n\nPyScript является открытым исходным кодом с использованием лицензии Apache 2.0.\n\nСогласно сайту проекта, цели включают в себя включение Python в браузере без настройки на стороне сервера, запуск популярных пакетов Python, двунаправленную связь между JavaScript и Python и визуальную разработку с использованием «легкодоступных контролируемых компонентов пользовательского интерфейса, таких как кнопки, контейнеры, текстовые поля и многое другое».\n\nУпрощение использования в браузере порадует не только ученых, разрабатывающих аналитические приложения, но и программистов любого профиля, ищущих альтернативу JavaScript — хотя разработчики проекта предупреждают, что это «чрезвычайно экспериментальный проект» и что он только проверен в веб-браузере Google Chrome.\nPlease be advised that PyScript is very alpha and under heavy development. There are many known issues, from usability to loading times, and you should expect things to change often. We encourage people to play and explore with PyScript, but at this time we do not recommend using it for production.\n\nТуториал PyScript\n\nПопробуем скачать, настроить и запустить приложение PyScript в браузере.\n\nРабочая среда\n\nРазработчики PyScript пишут, что для работы не требуется никакой среды разработки, кроме веб-браузера. Попробуем запустить в Chrome.\n\nУстановка\n\nМожно скачать весь пакет с сайта, но будем использовать скрипт, с сервера pyscript.net\n\nHello World\n\nСоздаем файл hello.html\n\n    print('Hello, World!')\n\nТег `` расположен внутри HTML body. Внутри этого тега будем пиcать Python код.\n\nОткроем файл в браузере\nPyScript\n\nРаботает!\n\nТег py-script\n\nТег `` позволяет писать многострочный код\n\n      print(\"Let's compute π:\")\n      def compute_pi(n):\n          pi = 2\n          for i in range(1,n):\n            pi = 4 * i * 2 / (4 * i ** 2 - 1)\n          return pi\n\n      pi = compute_pi(100000)\n      s = f\"π is approximately {pi:.3f}\"\n      print(s)\n\n\nВажно соблюдать отступы в самом блоке Python. Но Начальную строку кода можно начинать и с начала строки\n\nprint(\"Let's compute π:\")\ndef compute_pi(n):\n    pi = 2\n    for i in range(1,n):\n      pi = 4 * i * 2 / (4 * i ** 2 - 1)\n    return pi\n\npi = compute_pi(100000)\ns = f\"π is approximately {pi:.3f}\"\nprint(s)\n\n\nЗапись внутри HTML элементов\n\nВ приведенном выше примере у нас был один тег ``, выводящий одну или несколько строк на страницу по порядку.\nВнутри `` есть доступ к модулю pyscript, который предоставляет метод .write() для отправки строк в помеченные элементы на странице.\n\nНапример, мы добавим некоторые элементы стиля и предоставим заполнители для тега `` для записи.\n\n    Today is\n\nimport datetime as dt\npyscript.write('today', dt.date.today().strftime('%A %B %d, %Y'))\n\ndef compute_pi(n):\n    pi = 2\n    for i in range(1,n):\n        pi = 4 * i * 2 / (4 * i ** 2 - 1)\n    return pi\n\npi = compute_pi(100000)\npyscript.write('my-custom-pi', f'π is approximately {pi:.3f}')\n\n\nPyScript\n\nТег py-env\n\nВ дополнение к стандартной библиотеке Python и модулю pyscript, многие сторонние пакеты работают с PyScript. Чтобы их использовать, нужно объявить зависимости с помощью тега ` в заголовке HTML. Вы также можете ссылаться на файлы .whl` прямо на диске\n\n\n './static/wheels/travertino-0.1.3-py3-none-any.whl'\n './static/wheels/my-other-package-0.0.1-py3-none-any.whl'\n\n\n  #my python code ...\n\n\nПример с NumPy\n\n        numpy\n        matplotlib\n\n    Let's plot random numbers\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.random.randn(1000)\ny = np.random.randn(1000)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\nfig\n\n\nИмпорт локальный модулей\n\nМы также можем использовать собсвтенные модули, которые импортируем внутри тега ``\n\nНапример, создадим файл `data.py' и запишем в него собственную функцию\n\ndata.py\nimport numpy as np\n\ndef make_x_and_y(n):\n    x = np.random.randn(n)\n    y = np.random.randn(n)\n    return x, y\n\nВнутри тега `` добавим стандартные модули и путь до нашего локального модуля\n\n        numpy\n        matplotlib\n        paths:\n          /data.py\n\n    Let's plot random numbers\n\nimport matplotlib.pyplot as plt\nfrom data import make_x_and_y\n\nx, y = make_x_and_y(n=1000)\n\nfig, ax = plt.subplots()\nax.scatter(x, y)\nfig\n\n\nТег py-repl\n\nТег `` создает компонент REPL(Read–eval–print loop), который отображается на странице как редактор кода, что позволяет писать исполняемый код в строке.\n\nТег py-config\n\nТег `` используется для установки и настройки общих метаданных о вашем приложении PyScript в формате YAML.\n\n\n  autoclose_loader: false\n  runtimes:\n    src: \"https://cdn.jsdelivr.net/pyodide/v0.20.0/full/pyodide.js\"\n      name: pyodide-0.20\n      lang: python\n\nТег py-title\n\nТег визуального отображения. Добавляет компонент заголовка статического текста, который стилизует текст внутри тега как заголовок страницы.\n\nТег py-box\n\nСоздает объект-контейнер, который можно использовать для размещения одного или нескольких визуальных компонентов, определяющих, как элементы `` должны выравниваться и отображаться на странице.\n\nТег py-inputbox\n\nПозволяет вставить окно с текстовым полем\n\nТег py-button\n\nДобавляет кнопку, к которой авторы могут добавлять метки и обработчики событий для действий на кнопке, таких как on_focus или on_click.\n\nРесурсы\nПримеры использования PyScript\nВопросы по PyScript\n",
            "tags": [
                "pyscript",
                "Pyodide",
                "python"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/python-snippets/",
            "title": "Сниппеты Python",
            "description": "Сниппеты Python",
            "content": "\nДата\n\nТекущая\n\nimport datetime\nx = datetime.datetime.now() # 2022-08-04 21:41:24.871910\n\nФормат YYYY-MM-DD\n\nimport datetime\nx = datetime.datetime.now().strftime(\"%Y-%m-%d\") # 2022-08-04\n\nСоздать папку\nimport os\nif not os.path.exists(name):\n  os.makedirs(name)\n\nСамое часто встречаемое значение в списке\n\nimport collections\nx = [1, 2, 7, 4, 5, 6, 7, 10]\nprint(collections.Counter(x).most_common(1)0) # 7\n\ndef most_freq(list):\n    return max(set(list), key=list.count)\n\ntest = [10, 10, 20, 20, 10, 30, 30, 30, 20, 10]\nprint(most_freq(test)) # 10\n\nСлучайное целое число\n\nimport random\nx = random.randint(1, 10) # 9\n\nОдновременная проверка нескольких флагов в Python\n\nx, y, z = 0, 1, 0\n\nif x == 1 or y == 1 or z == 1:\n    print('passed')\n\nif 1 in (x, y, z):\n    print('passed')\n\nThese only test for truthiness:\nif x or y or z:\n    print('passed')\n\nif any((x, y, z)):\n    print('passed')\n\nPdf -> Audio\n\nimport PyPDF2, pyttsx3\n\nPDF file\npath = open('clcoding.pdf', 'rb')\n\ncreating a PdfFileReader object\npdfReader = PyPDF2.PdfFileReader(path)\n\nGet an engine instance for the speech synthesis\nspeak = pyttsx3.init()\nfor pages in range(pdfReader.numPages):\n    text = pdfReader.getPage(pages).extractText()\n    speak.say(text)\n    speak.runAndWait()\n\nspeak.stop()\nПолный обзор\n\nОднострочные комментарии начинаются с символа решётки.\n\n\"\"\" Многострочный текст может быть\n    записан, используя 3 знака \" и обычно используется\n    в качестве встроенной документации\n\"\"\"\n\n####################################################\n1. Примитивные типы данных и операторы\n####################################################\n\nУ вас есть числа\n3  # => 3\n\nМатематика работает вполне ожидаемо\n1 + 1   # => 2\n8 - 1   # => 7\n10 * 2  # => 20\n35 / 5  # => 7.0\n\nРезультат целочисленного деления округляется в меньшую сторону\nкак для положительных, так и для отрицательных чисел.\n5 // 3      # => 1\n-5 // 3     # => -2\n5.0 // 3.0  # => 1.0 # работает и для чисел с плавающей запятой\n-5.0 // 3.0 # => -2.0\n\n# Результат деления возвращает число с плавающей запятой\n10.0 / 3  # => 3.3333333333333335\n\nОстаток от деления\n7 % 3  # => 1\n\nВозведение в степень\n2**3  # => 8\n\nПриоритет операций указывается скобками\n1 + 3 * 2    # => 7\n(1 + 3) * 2  # => 8\n\nБулевы значения - примитивы (Обратите внимание на заглавную букву)\nTrue   # => True\nFalse  # => False\n\nДля отрицания используется ключевое слово not\nnot True   # => False\nnot False  # => True\n\nБулевы операторы\nОбратите внимание: ключевые слова \"and\" и \"or\" чувствительны к регистру букв\nTrue and False  # => False\nFalse or True   # => True\n\nTrue и False на самом деле 1 и 0, но с разными ключевыми словами\nTrue + True  # => 2\nTrue * 8     # => 8\nFalse - 5    # => -5\n\nОператоры сравнения обращают внимание на числовое значение True и False\n0 == False   # => True\n1 == True    # => True\n2 == True    # => False\n-5 != False  # => True\n\nИспользование булевых логических операторов на типах int превращает их в булевы значения, но возвращаются оригинальные значения\nНе путайте с bool(ints) и bitwise and/or (&,|)\nbool(0)   # => False\nbool(4)   # => True\nbool(-6)  # => True\n0 and 2   # => 0\n-5 or 0   # => -5\n\nРавенство — это ==\n1 == 1  # => True\n2 == 1  # => False\n\nНеравенство — это !=\n1 != 1  # => False\n2 != 1  # => True\n\nЕщё немного сравнений\n1  True\n1 > 10  # => False\n2  True\n2 >= 2  # => True\n\nПроверка, находится ли значение в диапазоне\n1  True\n2  False\n\nСравнения могут быть записаны цепочкой\n1  True\n2  False\n\n(is vs. ==) ключевое слово is проверяет, относятся ли две переменные к одному и тому же объекту, но == проверяет если указанные объекты имеют одинаковые значения.\na = [1, 2, 3, 4]  # a указывает на новый список, [1, 2, 3, 4]\nb = a             # b указывает на то, что указывает a\nb is a            # => True, a и b относятся к одному и тому же объекту\nb == a            # => True, Объекты a и b равны\nb = [1, 2, 3, 4]  # b указывает на новый список, [1, 2, 3, 4]\nb is a            # => False, a и b не относятся к одному и тому же объекту\nb == a            # => True, Объекты a и b равны\n\nСтроки определяются символом \" или '\n\"Это строка.\"\n'Это тоже строка.'\n\nИ строки тоже могут складываться! Хотя лучше не злоупотребляйте этим.\n\"Привет \" + \"мир!\"  # => \"Привет мир!\"\n\nСтроки (но не переменные) могут быть объединены без использования '+'\n\"Привет \" \"мир!\"  # => \"Привет мир!\"\n\nСо строкой можно работать, как со списком символов\n\"Привет мир!\"[0]  # => 'П'\n\nВы можете найти длину строки\nlen(\"Это строка\")  # => 10\n\nВы также можете форматировать, используя f-строки (в Python 3.6+)\nname = \"Рейко\"\nf\"Она сказала, что ее зовут {name}.\" # => \"Она сказала, что ее зовут Рейко\"\nВы можете поместить любой оператор Python в фигурные скобки, и он будет выведен в строке.\nf\"{name} состоит из {len(name)} символов.\" # => \"Рэйко состоит из 5 символов.\"\n\nNone является объектом\nNone  # => None\n\nНе используйте оператор равенства \"==\" для сравнения\nобъектов с None. Используйте для этого \"is\"\n\"etc\" is None  # => False\nNone is None   # => True\n\nNone, 0 и пустые строки/списки/словари/кортежи приводятся к False.\nВсе остальные значения равны True\nbool(0)   # => False\nbool(\"\")  # => False\nbool([])  # => False\nbool({})  # => False\nbool(())  # => False\n\n\n####################################################\n2. Переменные и Коллекции\n####################################################\n\nВ Python есть функция Print\nprint(\"Я Python. Приятно познакомиться!\")  # => Я Python. Приятно познакомиться!\n\nПо умолчанию функция, print() также выводит новую строку в конце.\nИспользуйте необязательный аргумент end, чтобы изменить последнюю строку.\nprint(\"Привет мир\", end=\"!\")  # => Привет мир!\n\nПростой способ получить входные данные из консоли\ninput_string_var = input(\"Введите данные: \")  # Возвращает данные в виде строки\nПримечание: в более ранних версиях Python метод input() назывался raw_input()\n\nОбъявлять переменные перед инициализацией не нужно.\nПо соглашению используется нижнийрегистрс_подчёркиваниями\nsome_var = 5\nsome_var  # => 5\n\nПри попытке доступа к неинициализированной переменной выбрасывается исключение.\nОб исключениях см. раздел \"Поток управления и итерируемые объекты\".\nsome_unknown_var  # Выбрасывает ошибку NameError\n\nif можно использовать как выражение\nЭквивалент тернарного оператора '?:' в C\n\"да!\" if 0 > 1 else \"нет!\"  # => \"нет!\"\n\nСписки хранят последовательности\nli = []\nМожно сразу начать с заполненного списка\nother_li = [4, 5, 6]\n\nОбъекты добавляются в конец списка методом append()\nli.append(1)  # [1]\nli.append(2)  # [1, 2]\nli.append(4)  # [1, 2, 4]\nli.append(3)  # [1, 2, 4, 3]\nИ удаляются с конца методом pop()\nli.pop()      # => возвращает 3 и li становится равен [1, 2, 4]\nПоложим элемент обратно\nli.append(3)  # [1, 2, 4, 3].\n\nОбращайтесь со списком, как с обычным массивом\nli[0]  # => 1\n\nОбратимся к последнему элементу\nli[-1]  # => 3\n\nПопытка выйти за границы массива приведёт к ошибке индекса\nli[4]  # Выбрасывает ошибку IndexError\n\nМожно обращаться к диапазону, используя так называемые срезы\n(Для тех, кто любит математику, это называется замкнуто-открытый интервал).\nli[1:3]   # Вернуть список из индекса с 1 по 3 => [2, 4]\nli[2:]    # Вернуть список, начиная с индекса 2 => [4, 3]\nli[:3]    # Вернуть список с начала до индекса 3  => [1, 2, 4]\nli[::2]   # Вернуть список, выбирая каждую вторую запись => [1, 4]\nli[::-1]  # Вернуть список в обратном порядке => [3, 4, 2, 1]\nИспользуйте сочетания всего вышеназванного для выделения более сложных срезов\nli[начало:конец:шаг]\n\nСделать однослойную глубокую копию, используя срезы\nli2 = li[:]  # => li2 = [1, 2, 4, 3], но (li2 is li) вернет False.\n\nУдаляем произвольные элементы из списка оператором del\ndel li[2]  # [1, 2, 3]\n\nУдалить первое вхождение значения\nli.remove(2)  # [1, 3]\nli.remove(2)  # Выбрасывает ошибку ValueError поскольку 2 нет в списке\n\nВставить элемент по определенному индексу\nli.insert(1, 2)  # [1, 2, 3]\n\nПолучить индекс первого найденного элемента, соответствующего аргументу\nli.index(2)  # => 1\nli.index(4)  # Выбрасывает ошибку ValueError поскольку 4 нет в списке\n\nВы можете складывать, или, как ещё говорят, конкатенировать списки\nОбратите внимание: значения li и other_li при этом не изменились.\nli + other_li  # => [1, 2, 3, 4, 5, 6]\n\nОбъединять списки можно методом extend()\nli.extend(other_li) # Теперь li содержит [1, 2, 3, 4, 5, 6]\n\nПроверить элемент на наличие в списке можно оператором in\n1 in li  # => True\n\nДлина списка вычисляется функцией len\nlen(li)  # => 6\n\nКортежи похожи на списки, только неизменяемые\ntup = (1, 2, 3)\ntup[0]  # => 1\ntup[0] = 3  # Выбрасывает ошибку TypeError\n\nОбратите внимание, что кортеж длины 1 должен иметь запятую после последнего элемента, но кортежи другой длины, даже 0, не должны.\ntype((1))   # =>\ntype((1,))  # =>\ntype(())    # =>\n\nВсё то же самое можно делать и с кортежами\nlen(tup)         # => 3\ntup + (4, 5, 6)  # => (1, 2, 3, 4, 5, 6)\ntup[:2]          # => (1, 2)\n2 in tup         # => True\n\nВы можете распаковывать кортежи (или списки) в переменные\na, b, c = (1, 2, 3)  # a == 1, b == 2 и c == 3\nВы также можете сделать расширенную распаковку\na, *b, c = (1, 2, 3, 4) # a теперь 1, b теперь [2, 3] и c теперь 4\nКортежи создаются по умолчанию, если опущены скобки\nd, e, f = 4, 5, 6  # кортеж 4, 5, 6 распаковывается в переменные d, e и f\nсоответственно, d = 4, e = 5 и f = 6\nОбратите внимание, как легко поменять местами значения двух переменных\ne, d = d, e  # теперь d == 5, а e == 4\n\nСловари содержат ассоциативные массивы\nempty_dict = {}\nВот так описывается предзаполненный словарь\nfilled_dict = {\"one\": 1, \"two\": 2, \"three\": 3}\n\nОбратите внимание, что ключи для словарей должны быть неизменяемыми типами. Это\nсделано для того, чтобы ключ может быть преобразован в хеш для быстрого поиска.\nНеизменяемые типы включают целые числа, числа с плавающей запятой, строки, кортежи.\ninvalid_dict = {[1,2,3]: \"123\"}  # => Выбрасывает ошибку TypeError: unhashable type: 'list'\nvalid_dict = {(1,2,3):[1,2,3]}   # Однако значения могут быть любого типа.\n\nПоиск значений с помощью []\nfilled_dict[\"one\"]  # => 1\n\nВсе ключи в виде списка получаются с помощью метода keys().\nЕго вызов нужно обернуть в list(), так как обратно мы получаем\nитерируемый объект, о которых поговорим позднее. Примечание - для Python\nверсии  [\"three\", \"two\", \"one\"] в Python  [\"one\", \"two\", \"three\"] в Python 3.7+\n\nВсе значения в виде списка можно получить с помощью values().\nИ снова нам нужно обернуть вызов в list(), чтобы превратить\nитерируемый объект в список.\nТо же самое замечание насчёт порядка ключей справедливо и здесь\nlist(filled_dict.values())  # => [3, 2, 1] в Python  [1, 2, 3] в Python 3.7+\n\nПри помощи ключевого слова in можно проверять наличие ключей в словаре\n\"one\" in filled_dict  # => True\n1 in filled_dict      # => False\n\nПопытка получить значение по несуществующему ключу выбросит ошибку KeyError\nfilled_dict[\"four\"]  # Выбрасывает ошибку KeyError\n\nЧтобы избежать этого, используйте метод get()\nfilled_dict.get(\"one\")      # => 1\nfilled_dict.get(\"four\")     # => None\nМетод get поддерживает аргумент по умолчанию, когда значение отсутствует\nfilled_dict.get(\"one\", 4)   # => 1\nfilled_dict.get(\"four\", 4)  # => 4\n\nМетод setdefault() вставляет пару ключ-значение, только если такого ключа нет\nfilled_dict.setdefault(\"five\", 5)  # filled_dict[\"five\"] возвращает 5\nfilled_dict.setdefault(\"five\", 6)  # filled_dict[\"five\"] по-прежнему возвращает 5\n\nДобавление элементов в словарь\nfilled_dict.update({\"four\":4})  # => {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4}\nfilled_dict[\"four\"] = 4         # Другой способ добавления элементов\n\nУдаляйте ключи из словаря с помощью ключевого слова del\ndel filled_dict[\"one\"]  # Удаляет ключ \"one\" из словаря\n\nПосле Python 3.5 вы также можете использовать дополнительные параметры распаковки\n{'a': 1, **{'b': 2}}  # => {'a': 1, 'b': 2}\n{'a': 1, **{'a': 2}}  # => {'a': 2}\n\nМножества содержат... ну, в общем, множества\nempty_set = set()\nИнициализация множества набором значений.\nДа, оно выглядит примерно как словарь. Ну извините, так уж вышло.\nfilled_set = {1, 2, 2, 3, 4}  # => {1, 2, 3, 4}\n\nSimilar to keys of a dictionary, elements of a set have to be immutable.\nКак и ключи словаря, элементы множества должны быть неизменяемыми.\ninvalid_set = {[1], 1}  # => Выбрасывает ошибку TypeError: unhashable type: 'list'\nvalid_set = {(1,), 1}\n\nМножеству можно назначать новую переменную\nfilled_set = some_set\nfilled_set.add(5)  # {1, 2, 3, 4, 5}\nВ множествах нет повторяющихся элементов\nfilled_set.add(5)  # {1, 2, 3, 4, 5}\n\nПересечение множеств: &\nother_set = {3, 4, 5, 6}\nfilled_set & other_set  # => {3, 4, 5}\n\nОбъединение множеств: |\nfilled_set | other_set  # => {1, 2, 3, 4, 5, 6}\n\nРазность множеств: -\n{1, 2, 3, 4} - {2, 3, 5}  # => {1, 4}\n\nСимметричная разница: ^\n{1, 2, 3, 4} ^ {2, 3, 5}  # => {1, 4, 5}\n\nПроверить, является ли множество слева надмножеством множества справа\n{1, 2} >= {1, 2, 3}  # => False\n\nПроверить, является ли множество слева подмножеством множества справа\n{1, 2}  True\n\nПроверка на наличие в множестве: in\n2 in filled_set   # => True\n10 in filled_set  # => False\n\nСделать однослойную глубокую копию\nfilled_set = some_set.copy()  # {1, 2, 3, 4, 5}\nfilled_set is some_set        # => False\n\n\n####################################################\n3. Поток управления и итерируемые объекты\n####################################################\n\nДля начала создадим переменную\nsome_var = 5\n\nТак выглядит выражение if. Отступы в python очень важны!\nКонвенция заключается в использовании четырех пробелов, а не табуляции.\nPезультат: \"some_var меньше, чем 10\"\nif some_var > 10:\n    print(\"some_var точно больше, чем 10.\")\nelif some_var  dict_keys(['one', 'two', 'three']). Это объект, реализующий интерфейс Iterable\n\nМы можем проходить по нему циклом.\nfor i in our_iterable:\n    print(i)  # Выводит one, two, three\n\nНо мы не можем обращаться к элементу по индексу.\nour_iterable[1]  # Выбрасывает ошибку TypeError\n\nИтерируемый объект знает, как создавать итератор.\nour_iterator = iter(our_iterable)\n\nИтератор может запоминать состояние при проходе по объекту.\nМы получаем следующий объект, вызывая функцию next().\nnext(our_iterator)  # => \"one\"\n\nОн сохраняет состояние при вызове next().\nnext(our_iterator)  # => \"two\"\nnext(our_iterator)  # => \"three\"\n\nВозвратив все данные, итератор выбрасывает исключение StopIterator\nnext(our_iterator)  # Выбрасывает исключение StopIteration\n\nМы можем проходить по нему циклом.\nour_iterator = iter(our_iterable)\nfor i in our_iterator:\n    print(i)  # Выводит one, two, three\n\nВы можете получить сразу все элементы итератора, вызвав на нём функцию list().\nlist(our_iterable)  # => Возвращает [\"one\", \"two\", \"three\"]\nlist(our_iterator)  # => Возвращает [] потому что состояние сохраняется\n\n\n####################################################\n4. Функции\n####################################################\n\nИспользуйте def для создания новых функций\ndef add(x, y):\n    print(\"x равен %s, а y равен %s\" % (x, y))\n    return x + y  # Возвращайте результат с помощью ключевого слова return\n\nВызов функции с аргументами\nadd(5, 6)  # => Выводит \"x равен 5, а y равен 6\" и возвращает 11\n\nДругой способ вызова функции — вызов с именованными аргументами\nadd(y=6, x=5)  # Именованные аргументы можно указывать в любом порядке.\n\nВы можете определить функцию, принимающую переменное число аргументов\ndef varargs(*args):\n    return args\n\nvarargs(1, 2, 3)  # => (1,2,3)\n\nА также можете определить функцию, принимающую переменное число\nименованных аргументов\ndef keyword_args(**kwargs):\n    return kwargs\n\nВызовем эту функцию и посмотрим, что из этого получится\nkeyword_args(big=\"foot\", loch=\"ness\")  # => {\"big\": \"foot\", \"loch\": \"ness\"}\n\nЕсли хотите, можете использовать оба способа одновременно\ndef all_the_args(args, *kwargs):\n    print(args)\n    print(kwargs)\n\"\"\"\nall_the_args(1, 2, a=3, b=4) выводит:\n    (1, 2)\n    {\"a\": 3, \"b\": 4}\n\"\"\"\n\nВызывая функции, можете сделать наоборот!\nИспользуйте символ * для распаковки кортежей и ** для распаковки словарей\nargs = (1, 2, 3, 4)\nkwargs = {\"a\": 3, \"b\": 4}\nall_the_args(*args)            # эквивалентно all_the_args(1, 2, 3, 4)\nall_the_args(**kwargs)         # эквивалентно all_the_args(a=3, b=4)\nall_the_args(args, *kwargs)  # эквивалентно all_the_args(1, 2, 3, 4, a=3, b=4)\n\nВозврат нескольких значений (с назначением кортежей)\ndef swap(x, y):\n    return y, x  # Возвращает несколько значений в виде кортежа без скобок.\n(Примечание: скобки исключены, но могут быть включены)\n\nx = 1\ny = 2\nx, y = swap(x, y)     # => x = 2, y = 1\n(x, y) = swap(x,y)  # Снова, скобки были исключены, но могут быть включены.\n\nОбласть определения функций\nx = 5\n\ndef set_x(num):\nЛокальная переменная x — это не то же самое, что глобальная переменная x\n    x = num   # => 43\n    print(x)  # => 43\n\ndef set_global_x(num):\n    global x\n    print(x)  # => 5\n    x = num   # Глобальная переменная x теперь равна 6\n    print(x)  # => 6\n\nset_x(43)\nset_global_x(6)\n\nPython имеет функции первого класса\ndef create_adder(x):\n    def adder(y):\n        return x + y\n    return adder\n\nadd_10 = create_adder(10)\nadd_10(3)  # => 13\n\nТакже есть и анонимные функции\n(lambda x: x > 2)(3)                  # => True\n(lambda x, y: x * 2 + y * 2)(2, 1)  # => 5\n\nЕсть встроенные функции высшего порядка\nlist(map(add_10, [1, 2, 3]))          # => [11, 12, 13]\nlist(map(max, [1, 2, 3], [4, 2, 1]))  # => [4, 2, 3]\n\nlist(filter(lambda x: x > 5, [3, 4, 5, 6, 7]))  # => [6, 7]\n\nДля удобного отображения и фильтрации можно использовать списочные интерпретации\nИнтерпретация списка сохраняет вывод в виде списка, который сам может быть вложенным списком\n[add_10(i) for i in [1, 2, 3]]         # => [11, 12, 13]\n[x for x in [3, 4, 5, 6, 7] if x > 5]  # => [6, 7]\n\nВы также можете создавать интерпретации множеств и словарей.\n{x for x in 'abcddeef' if x not in 'abc'}  # => {'d', 'e', 'f'}\n{x: x**2 for x in range(5)}  # => {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n####################################################\n5. Модули\n####################################################\n\nВы можете импортировать модули\nimport math\nprint(math.sqrt(16))  # => 4.0\n\nВы можете получить определенные функции из модуля\nfrom math import ceil, floor\nprint(ceil(3.7))   # => 4.0\nprint(floor(3.7))  # => 3.0\n\nВы можете импортировать все функции из модуля.\nПредупреждение: это не рекомендуется\nfrom math import *\n\nВы можете сократить имена модулей\nimport math as m\nmath.sqrt(16) == m.sqrt(16)  # => True\n\nМодули Python - это обычные файлы Python. Вы\nможете писать свои собственные и импортировать их. Имя\nмодуля совпадает с именем файла.\n\nВы можете узнать, какие функции и атрибуты\nопределены в модуле.\nimport math\ndir(math)\n\nЕсли у вас есть скрипт Python с именем math.py в той же папке,\nчто и ваш текущий скрипт, файл math.py будет\nбудет загружен вместо встроенного модуля Python.\nЭто происходит потому, что локальная папка имеет приоритет\nнад встроенными библиотеками Python.\n\n\n####################################################\n6. Классы\n####################################################\n\nМы используем оператор class для создания класса\nclass Human:\n\nАтрибут класса. Он используется всеми экземплярами этого класса\n    species = \"Гомосапиенс\"\n\nОбычный конструктор, вызывается при инициализации экземпляра класса\nОбратите внимание, что двойное подчёркивание в начале и в конце имени\nозначает объекты и атрибуты, которые используются Python, но находятся\nв пространствах имён, управляемых пользователем.\nМетоды (или объекты или атрибуты), например:\ninit, str, repr и т. д. называются специальными методами.\nНе придумывайте им имена самостоятельно.\n    def init(self, name):\nПрисваивание значения аргумента атрибуту\n        self.name = name\n\nИнициализация свойства\n        self._age = 0\n\nМетод экземпляра. Все методы принимают self в качестве первого аргумента\n    def say(self, msg):\n        return \"{name}: {message}\".format(name=self.name, message=msg)\n\nДругой метод экземпляра\n    def sing(self):\n        return 'йо... йо... проверка микрофона... раз, два... раз, два...'\n\nМетод класса разделяется между всеми экземплярами\nОни вызываются с указыванием вызывающего класса в качестве первого аргумента\n    @classmethod\n    def get_species(cls):\n        return cls.species\n\nСтатический метод вызывается без ссылки на класс или экземпляр\n    @staticmethod\n    def grunt():\n        return \"grunt\"\n\nproperty похоже на геттер.\nОно превращает метод age() в одноименный атрибут только для чтения.\nОднако нет необходимости писать тривиальные геттеры и сеттеры в Python.\n    @property\n    def age(self):\n        return self._age\n\nЭто позволяет установить свойство\n    @age.setter\n    def age(self, age):\n        self._age = age\n\nЭто позволяет удалить свойство\n    @age.deleter\n    def age(self):\n        del self._age\n\nКогда интерпретатор Python читает исходный файл, он выполняет весь его код.\nПроверка name гарантирует, что этот блок кода выполняется только тогда, когда\nэтот модуль - это основная программа.\nif name == 'main':\nИнициализация экземпляра класса\n    i = Human(name=\"Иван\")\n    i.say(\"привет\")                 # Выводит: \"Иван: привет\"\n    j = Human(\"Пётр\")\n    j.say(\"привет\")                 # Выводит: \"Пётр: привет\"\ni и j являются экземплярами типа Human, или другими словами: они являются объектами Human\n\nВызов метода класса\n    i.say(i.get_species())          # \"Иван: Гомосапиенс\"\nИзменение разделяемого атрибута\n    Human.species = \"Неандертальец\"\n    i.say(i.get_species())          # => \"Иван: Неандертальец\"\n    j.say(j.get_species())          # => \"Пётр: Неандертальец\"\n\nВызов статического метода\n    print(Human.grunt())            # => \"grunt\"\n\nНевозможно вызвать статический метод с экземпляром объекта\nпотому что i.grunt() автоматически поместит \"self\" (объект i) в качестве аргумента\n    print(i.grunt())                # => TypeError: grunt() takes 0 positional arguments but 1 was given\n\nОбновить свойство для этого экземпляра\n    i.age = 42\nПолучить свойство\n    i.say(i.age)                    # => \"Иван: 42\"\n    j.say(j.age)                    # => \"Пётр: 0\"\nУдалить свойство\n    del i.age\ni.age                         # => это выбрасило бы ошибку AttributeError\n\n\n####################################################\n6.1 Наследование\n####################################################\n\nНаследование позволяет определять новые дочерние классы, которые наследуют методы и\nпеременные от своего родительского класса.\n\nИспользуя класс Human, определенный выше как базовый или родительский класс, мы можем\nопределить дочерний класс Superhero, который наследует переменные класса, такие как\n\"species\", \"name\" и \"age\", а также методы, такие как \"sing\" и \"grunt\" из класса Human,\nно также может иметь свои уникальные свойства.\n\nЧтобы воспользоваться преимуществами модульности по файлам, вы можете поместить\nвышеперечисленные классы в их собственные файлы, например, human.py\n\nЧтобы импортировать функции из других файлов, используйте следующий формат\nfrom \"имя-файла-без-расширения\" import \"функция-или-класс\"\n\nfrom human import Human\n\nУкажите родительский класс(ы) как параметры определения класса\nclass Superhero(Human):\n\nЕсли дочерний класс должен наследовать все определения родителя без каких-либо\nизменений, вы можете просто использовать ключевое слово pass (и ничего больше),\nно в этом случае оно закомментировано, чтобы разрешить уникальный дочерний класс:\npass\n\nДочерние классы могут переопределять атрибуты своих родителей\n    species = 'Сверхчеловек'\n\nДочерние классы автоматически наследуют конструктор родительского класса, включая\nего аргументы, но также могут определять дополнительные аргументы или определения\nи переопределять его методы, такие как конструктор класса.\nЭтот конструктор наследует аргумент \"name\" от класса \"Human\"\nи добавляет аргументы \"superpower\" и \"movie\":\n    def init(self, name, movie=False,\n                 superpowers=[\"сверхсила\", \"пуленепробиваемость\"]):\n\nдобавить дополнительные атрибуты класса:\n        self.fictional = True\n        self.movie = movie\nпомните об изменяемых значениях по умолчанию,\nпоскольку значения по умолчанию являются общими\n        self.superpowers = superpowers\n\nФункция \"super\" позволяет вам получить доступ к методам родительского класса,\nкоторые переопределяются дочерним, в данном случае, методом init.\nЭто вызывает конструктор родительского класса:\n        super().init(name)\n\nпереопределить метод sing\n    def sing(self):\n        return 'Бам, бам, БАМ!'\n\nдобавить дополнительный метод экземпляра\n    def boast(self):\n        for power in self.superpowers:\n            print(\"Я обладаю силой '{pow}'!\".format(pow=power))\n\n\nif name == 'main':\n    sup = Superhero(name=\"Тик\")\n\nПроверка типа экземпляра\n    if isinstance(sup, Human):\n        print('Я человек')\n    if type(sup) is Superhero:\n        print('Я супергерой')\n\nПолучить порядок поиска разрешения метода (MRO),\nиспользуемый как getattr(), так и super()\nЭтот атрибут является динамическим и может быть обновлен\n    print(Superhero.mro)    # => (,\n=> , )\n\nВызывает родительский метод, но использует свой собственный атрибут класса\n    print(sup.get_species())    # => Сверхчеловек\n\nВызов переопределенного метода\n    print(sup.sing())           # => Бам, бам, БАМ!\n\nВызывает метод из Human\n    sup.say('Ложка')            # => Тик: Ложка\n\nМетод вызова, существующий только в Superhero\n    sup.boast()                 # => Я обладаю силой 'сверхсила'!\n=> Я обладаю силой 'пуленепробиваемость'!\n\nАтрибут унаследованного класса\n    sup.age = 31\n    print(sup.age)              # => 31\n\nАтрибут, который существует только в Superhero\n    print('Достоин ли я Оскара? ' + str(sup.movie))\n\n\n####################################################\n6.2 Множественное наследование\n####################################################\n\nEще одно определение класса\nbat.py\nclass Bat:\n\n    species = 'Летучая мышь'\n\n    def init(self, can_fly=True):\n        self.fly = can_fly\n\nВ этом классе также есть метод say\n    def say(self, msg):\n        msg = '... ... ...'\n        return msg\n\nИ свой метод тоже\n    def sonar(self):\n        return '))) ... ((('\n\nif name == 'main':\n    b = Bat()\n    print(b.say('привет'))\n    print(b.fly)\n\nИ еще одно определение класса, унаследованное от Superhero и Bat\nsuperhero.py\nfrom superhero import Superhero\nfrom bat import Bat\n\nОпределите Batman как дочерний класс, унаследованный от Superhero и Bat\nclass Batman(Superhero, Bat):\n\n    def init(self, args, *kwargs):\nОбычно для наследования атрибутов необходимо вызывать super:\nsuper(Batman, self).init(args, *kwargs)\nОднако здесь мы имеем дело с множественным наследованием, а super()\nработает только со следующим базовым классом в списке MRO.\nПоэтому вместо этого мы вызываем init для всех родителей.\nИспользование args и *kwargs обеспечивает чистый способ передачи\nаргументов, когда каждый родитель \"очищает слой луковицы\".\n        Superhero.init(self, 'анонимный', movie=True,\n                           superpowers=['Богатый'], args, *kwargs)\n        Bat.init(self, args, can_fly=False, *kwargs)\nпереопределить значение атрибута name\n        self.name = 'Грустный Бен Аффлек'\n\n    def sing(self):\n        return 'на на на на на бэтмен!'\n\n\nif name == 'main':\n    sup = Batman()\n\nПолучить порядок поиска разрешения метода (MRO),\nиспользуемый как getattr(), так и super()\nЭтот атрибут является динамическим и может быть обновлен\n    print(Batman.mro)       # => (,\n=> ,\n=> ,\n=> , )\n\nВызывает родительский метод, но использует свой собственный атрибут класса\n    print(sup.get_species())    # => Сверхчеловек\n\nВызов переопределенного метода\n    print(sup.sing())           # => на на на на на бэтмен!\n\nВызывает метод из Human, потому что порядок наследования имеет значение\n    sup.say('Я согласен')          # => Грустный Бен Аффлек: Я согласен\n\nВызов метода, существующий только во втором родителе\n    print(sup.sonar())          # => ))) ... (((\n\nАтрибут унаследованного класса\n    sup.age = 100\n    print(sup.age)              # => 100\n\nУнаследованный атрибут от второго родителя,\nзначение по умолчанию которого было переопределено.\n    print('Могу ли я летать? ' + str(sup.fly)) # => Могу ли я летать? False\n\n\n####################################################\n7. Дополнительно\n####################################################\n\nГенераторы помогут выполнить ленивые вычисления\ndef double_numbers(iterable):\n    for i in iterable:\n        yield i + i\n\nГенераторы эффективны с точки зрения памяти, потому что они загружают только данные,\nнеобходимые для обработки следующего значения в итерации.\nЭто позволяет им выполнять операции с недопустимо большими диапазонами значений.\nПРИМЕЧАНИЕ: \"range\" заменяет \"xrange\" в Python 3.\nfor i in double_numbers(range(1, 900000000)):  # \"range\" - генератор.\n    print(i)\n    if i >= 30:\n        break\n\nТак же, как вы можете создать интерпретации списков, вы можете создать и\nинтерпретации генераторов.\nvalues = (-x for x in [1,2,3,4,5])\nfor x in values:\n    print(x)  # Выводит -1 -2 -3 -4 -5\n\nВы также можете преобразовать интерпретацию генератора непосредственно в список.\nvalues = (-x for x in [1,2,3,4,5])\ngen_to_list = list(values)\nprint(gen_to_list)  # => [-1, -2, -3, -4, -5]\n\nДекораторы\nВ этом примере \"beg\" оборачивает \"say\".\nЕсли say_please равно True, он изменит возвращаемое сообщение.\nfrom functools import wraps\n\n\ndef beg(target_function):\n    @wraps(target_function)\n    def wrapper(args, *kwargs):\n        msg, say_please = target_function(args, *kwargs)\n        if say_please:\n            return \"{} {}\".format(msg, \"Пожалуйста! Спасибо :)\")\n        return msg\n\n    return wrapper\n\n\n@beg\ndef say(say_please=False):\n    msg = \"Вы не купите мне сока?\"\n    return msg, say_please\n\n\nprint(say())                 # Вы не купите мне сока?\nprint(say(say_please=True))  # Вы не купите мне сока? Пожалуйста! Спасибо :)\n\n",
            "tags": [
                "python"
            ],
            "lang": "ru"
        },
        {
            "uri": "/posts/ruGPT-3-notes",
            "title": "ChatGPT/ruGPT-3",
            "description": "ruGPT-3",
            "content": "\nhttps://chat.openai.com/chat/\nhttps://russiannlp.github.io/rugpt-demo/\nКраткий экскурс в ruGPT-3. Инструкция и демонстрация\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/search/_index",
            "title": "Search page",
            "content": "",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/_index",
            "title": "Roadmaps",
            "content": "\nСписком",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/_index",
            "title": "90 дней DevOps",
            "description": "Цикл вводных статей по DevOps",
            "content": "\n\n -->\n\nОригинальный репозиторий на GitHub. Автор версии на английском - Michael Cade\nЦель данного цикла статей - быстрый обзор и прохождение по всему тьюториалу по DevOps.\n\nProgress\n\n[✔️] ♾️ 1 > Введение\n\nЧто такое и почему нам нужен DevOps\n\n[✔️] ♾️ 2 > Задачи DevOps-инженера\n[✔️] ♾️ 3 > DevOps Lifecycle - Ориентированность на приложения\n[✔️] ♾️ 4 > DevOps & Agile\n[✔️] ♾️ 5 > Plan > Code > Build > Testing > Release > Deploy > Operate > Monitor >\n[✔️] ♾️ 6 > DevOps - The real stories\n\nИзучение языка программирования\n\n[✔️] ⌨️ 7 > The Big Picture: DevOps & Learning a Programming Language\n[✔️] ⌨️ 8 > Setting up your DevOps environment for Go & Hello World\n[✔️] ⌨️ 9 > Let's explain the Hello World code\n[✔️] ⌨️ 10 > The Go Workspace & Compiling & running code\n[✔️] ⌨️ 11 > Variables, Constants & Data Types\n[✔️] ⌨️ 12 > Getting user input with Pointers and a finished program\n[✔️] ⌨️ 13 > Tweet your progress with our new App\n\nKnowing Linux Basics\n\n[✔️] 🐧 14 > The Big Picture: DevOps and Linux\n[✔️] 🐧 15 > Linux Commands for DevOps (Actually everyone)\n[✔️] 🐧 16 > Managing your Linux System, Filesystem & Storage\n[✔️] 🐧 17 > Text Editors - nano vs vim\n[✔️] 🐧 18 > SSH & Web Server(LAMP)\n[✔️] 🐧 19 > Automate tasks with bash scripts\n[✔️] 🐧 20 > Dev workstation setup - All the pretty things\n\nUnderstand Networking\n\n[✔️] 🌐 21 > The Big Picture: DevOps and Networking\n[✔️] 🌐 22 > The OSI Model - The 7 Layers\n[✔️] 🌐 23 > Network Protocols\n[✔️] 🌐 24 > Network Automation\n[✔️] 🌐 25 > Python for Network Automation\n[✔️] 🌐 26 > Building our Lab\n[✔️] 🌐 27 > Getting Hands-On with Python & Network\n\nStick to one Cloud Provider\n\n[✔️] ☁️ 28 > The Big Picture: DevOps & The Cloud\n[✔️] ☁️ 29 > Microsoft Azure Fundamentals\n[✔️] ☁️ 30 > Модули безопасности Microsoft Azure\n[✔️] ☁️ 31 > Microsoft Azure Compute Models\n[✔️] ☁️ 32 > Microsoft Azure Storage & Database Models\n[✔️] ☁️ 33 > Microsoft Azure Networking Models + Azure Management\n[✔️] ☁️ 34 > Microsoft Azure Hands-On Scenarios\n\nUse Git Effectively\n\n[✔️] 📚 35 > The Big Picture: Git - Version Control\n[✔️] 📚 36 > Installing & Configuring Git\n[✔️] 📚 37 > Gitting to know Git\n[✔️] 📚 38 > Staging & Changing\n[✔️] 📚 39 > Viewing, unstaging, discarding & restoring\n[✔️] 📚 40 > Social Network for code\n[✔️] 📚 41 > The Open Source Workflow\n\nContainers\n\n[✔️] 🏗️ 42 > The Big Picture: Containers\n[✔️] 🏗️ 43 > What is Docker & Getting installed\n[✔️] 🏗️ 44 > Docker Images & Hands-On with Docker Desktop\n[✔️] 🏗️ 45 > The anatomy of a Docker Image\n[✔️] 🏗️ 46 > Docker Compose\n[✔️] 🏗️ 47 > Docker Networking & Security\n[✔️] 🏗️ 48 > Alternatives to Docker\n\nKubernetes\n\n[✔️] ☸ 49 > The Big Picture: Kubernetes\n[✔️] ☸ 50 > Choosing your Kubernetes platform\n[✔️] ☸ 51 > Deploying your first Kubernetes Cluster\n[✔️] ☸ 52 > Setting up a multinode Kubernetes Cluster\n[✔️] ☸ 53 > Rancher Overview - Hands On\n[✔️] ☸ 54 > Kubernetes Application Deployment\n[✔️] ☸ 55 > State and Ingress in Kubernetes\n\nLearn Infrastructure as Code\n\n[✔️] 🤖 56 > The Big Picture: IaC\n[✔️] 🤖 57 > An intro to Terraform\n[✔️] 🤖 58 > HashiCorp Configuration Language (HCL)\n[✔️] 🤖 59 > Create a VM with Terraform & Variables\n[✔️] 🤖 60 > Docker Containers, Provisioners & Modules\n[✔️] 🤖 61 > Kubernetes & Multiple Environments\n[✔️] 🤖 62 > Testing, Tools & Alternatives\n\nAutomate Configuration Management\n\n[✔️] 📜 63 > The Big Picture: Configuration Management\n[✔️] 📜 64 > Ansible: Getting Started\n[✔️] 📜 65 > Ansible Playbooks\n[✔️] 📜 66 > Ansible Playbooks Continued...\n[✔️] 📜 67 > Using Roles & Deploying a Loadbalancer\n[✔️] 📜 68 > Tags, Variables, Inventory & Database Server config\n[✔️] 📜 69 > All other things Ansible - Automation Controller, AWX, Vault\n\nCreate CI/CD Pipelines\n\n[✔️] 🔄 70 > The Big Picture: CI/CD Pipelines\n[✔️] 🔄 71 > What is Jenkins?\n[✔️] 🔄 72 > Getting hands on with Jenkins\n[✔️] 🔄 73 > Building a Jenkins pipeline\n[✔️] 🔄 74 > Hello World - Jenkinsfile App Pipeline\n[✔️] 🔄 75 > GitHub Actions Overview\n[✔️] 🔄 76 > ArgoCD Overview\n\nMonitoring, Log Management, and Data Visualisation\n\n[✔️] 📈 77 > The Big Picture: Monitoring\n[✔️] 📈 78 > Hands-On Monitoring Tools\n[✔️] 📈 79 > The Big Picture: Log Management\n[✔️] 📈 80 > ELK Stack\n[✔️] 📈 81 > Fluentd & FluentBit\n[✔️] 📈 82 > EFK Stack\n[✔️] 📈 83 > Data Visualisation - Grafana\n\nStore & Protect Your Data\n\n[✔️] 🗃️ 84 > The Big Picture: Data Management\n[✔️] 🗃️ 85 > Data Services\n[✔️] 🗃️ 86 > Backup all the platforms\n[✔️] 🗃️ 87 > Hands-On Backup & Recovery\n[✔️] 🗃️ 88 > Application Focused Backups\n[✔️] 🗃️ 89 > Disaster Recovery\n[✔️] 🗃️ 90 > Data & Application Mobility\n\nLicense\n\nShield: CC BY-NC-SA 4.0\n\nThis work is licensed under a\nCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nCC BY-NC-SA 4.0\n\n[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/\n[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\n[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day01",
            "title": "1. DevOps - общее представление",
            "description": "DevOps - общее представление",
            "content": "\nВведение - День 1\n\nПервый день из 90, чтобы получить хорошее базовое понимание DevOps и инструментов.\n\nЭтот путь обучения начался для меня несколько лет назад, но тогда я сосредоточился на платформах виртуализации и облачных технологиях. В основном я изучал инфраструктуру как код и управление конфигурацией приложений с помощью Terraform и Chef.\n\nПеренесемся в март 2021 года. Мне представилась прекрасная возможность сосредоточить свои усилия на стратегии Cloud Native в Kasten by Veeam. Это должно было стать огромным фокусом на Kubernetes и DevOps, а также на сообществе, окружающем эти технологии. Я начал свое обучение и быстро понял, что помимо изучения основ Kubernetes и контейнеризации существует очень широкий мир, и именно тогда я начал общаться с сообществом и узнавать все больше и больше о культуре, инструментах и ​​​​процессах DevOps, поэтому я начал публично документировать некоторые области, которые я хотел изучить.\n\nНачнем наше путешествие\n\nЕсли вы прочитаете приведенный выше блог, вы увидите, что это содержание высокого уровня для моего учебного пути, и я скажу, что на данный момент я не являюсь экспертом ни в одном из этих разделов, но я хотел поделиться некоторыми БЕСПЛАТНЫМИ ресурсами. а некоторые платные, но вариант для обоих, так как у всех разные обстоятельства.\n\nВ течение следующих 90 дней я хочу задокументировать эти ресурсы и охватить эти основополагающие области. Я бы хотел, чтобы сообщество также приняло участие, поделилось своим путешествием и ресурсами, чтобы мы могли учиться публично и помогать друг другу.\n\nИз начального файла readme в репозитории проекта вы увидите, что я разделил все на разделы, и в основном это 12 недель плюс 6 дней. Первые 6 дней мы будем изучать основы DevOps в целом, прежде чем погрузиться в некоторые конкретные области, этот список ни в коем случае не является исчерпывающим, и мы снова будем рады, если сообщество поможет сделать этот ресурс полезным.\n\nЕще один ресурс, которым я поделюсь на этом этапе, который, я думаю, каждый должен внимательно изучить и, возможно, создать свою собственную карту ума для себя, своих интересов и позиции:\n\nDevOps Roadmap\n\nЯ нашел это отличным ресурсом, когда создавал свой первоначальный список и сообщение в блоге по этой теме. Вы также можете заметить, что помимо 12 тем, которые я перечислил здесь, в этом репозитории, есть и другие разделы, требующие более подробного рассмотрения.\n\nПервые шаги - или что такое DevOps?\n\nЕсть так много статей в блогах и видео на YouTube, которые можно перечислить здесь, но поскольку мы начинаем 90-дневное испытание и сосредоточиваемся на том, чтобы тратить около часа в день на изучение чего-то нового или о DevOps, я подумал, что было бы хорошо получить некоторые из высокого уровня «что такое DevOps» для начала.\n\nВо-первых, DevOps — это не инструмент. Вы не можете купить его, это не номер программного обеспечения или репозиторий GitHub с открытым исходным кодом, который вы можете скачать. Это также не язык программирования, это также не какая-то магия темного искусства.\n\nDevOps — это способ делать более разумные вещи в разработке программного обеспечения. - Подождите... Но если вы не разработчик программного обеспечения, вы должны отвернуться прямо сейчас и не погрузиться в этот проект??? Нет, совсем нет, оставайтесь... Потому что DevOps объединяет разработку программного обеспечения и эксплуатацию. Ранее я упоминал, что больше занимаюсь виртуальными машинами, и это, как правило, относится к сфере эксплуатации, но в сообществе есть люди с самым разным опытом, и DevOps на 100 % принесет пользу отдельным лицам, разработчикам, специалистам по эксплуатации и Все инженеры по контролю качества могут в равной степени изучить эти передовые методы, лучше разбираясь в DevOps.\n\nDevOps — это набор практик, которые помогают достичь цели этого движения: сократить время между фазой создания идеи продукта и его выпуском в производство для конечного пользователя или кого бы то ни было, внутренней команды или клиента.\n\nЕще одна область, в которую мы углубимся в первую неделю, касается Методологии Agile. DevOps и Agile широко применяются вместе для обеспечения непрерывной доставки вашего Приложения.\n\nГлавный вывод заключается в том, что образ мышления или культура DevOps позволяют сократить затянувшийся процесс выпуска программного обеспечения с потенциально многих лет до возможности более частого выпуска небольших выпусков. Другой ключевой принцип, который следует здесь усвоить, заключается в том, что речь идет о разрушении разрозненности между командами, о которых я упоминал ранее, разработчиками, эксплуатацией и контролем качества.\n\nС точки зрения DevOps, разработка, тестирование и развертывание выполняются командой DevOps.\n\nПоследнее, что я хотел бы сделать, чтобы сделать это максимально эффективным и действенным, мы должны использовать автоматизацию\n\nИсточники\n\nЯ всегда открыт для добавления дополнительных ресурсов в эти файлы readme, поскольку они здесь в качестве учебного пособия.\n\nМой совет — посмотрите все ссылки ниже, и, надеюсь, вы тоже что-то почерпнули из текста и объяснений выше.\n\nDevOps in 5 Minutes\nWhat is DevOps? Easy Way\nDevOps roadmap 2022 | Success Roadmap 2022\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day02",
            "title": "2. Задачи DevOps-инженера",
            "description": "Задачи DevOps-инженера",
            "content": "\nОбязанности DevOps специалиста\n\nНадеюсь, вы приступили к этому после просмотра ресурсов и публикации в День 1 из #90DaysOfDevOps\n\nВ первом посте был краткий обзор, но теперь мы должны углубиться в концепцию DevOps и понять, что при создании приложения есть две основные части. У нас есть часть Разработка, где разработчики программного обеспечения программируют приложение и тестируют его. Затем у нас есть часть Операции, где приложение развертывается и поддерживается на сервере.\n\nDevOps — это связующее звено между двумя\n\nЧтобы разобраться с DevOps или задачами, которые будет выполнять инженер DevOps, нам нужно понять инструменты или процесс, а также разобраться как они вместе они вместе взаимодейтвуют.\n\nВсе начинается с приложения! Вы увидите так много всего, что все дело в приложении, когда речь идет о DevOps.\n\nРазработчики создадуют приложение, это можно сделать с помощью множества различных технологических стеков, и давайте пока оставим это воображению, поскольку мы вернемся к этому позже. Это также может включать множество различных языков программирования, инструменты сборки, репозиторий кода и т. д.\n\nБудучи инженером DevOps, вы не будете программировать приложение, но хорошее понимание концепций работы разработчика и используемых им систем, инструментов и процессов является ключом к успеху.\n\nНа очень высоком уровне вам нужно будет знать, как приложение настроено для взаимодействия со всеми необходимыми службами или службами данных, а затем также добавить требования о том, как это можно или нужно протестировать.\n\nПриложение нужно будет где-то развернуть, давайте сделаем его в целом простым и сделаем это сервером, неважно где, но сервером. Затем ожидается, что к нему будет обращаться клиент или конечный пользователь в зависимости от созданного приложения.\n\nЭтот сервер должен работать где-то локально, в общедоступном облаке, без сервера (Хорошо, я зашел слишком далеко, мы не будем рассматривать бессерверный вариант, но это вариант, и все больше и больше предприятий идут по этому пути). Кто-то должен создать настройте эти серверы и подготовьте их к запуску приложения. Теперь этот элемент может пригодиться вам как инженеру DevOps для развертывания и настройки этих серверов.\n\nЭти серверы должны будут работать под управлением операционной системы, и, вообще говоря, это будет Linux, но у нас есть целый раздел или потратим неделю, где мы рассмотрим некоторые фундаментальные знания, которые вы должны получить.\n\nТакже вероятно, что нам нужно взаимодействовать с другими службами в нашей сети или среде, поэтому нам также необходимо иметь такой уровень знаний о сети и настройке, что в некоторой степени также может оказаться в руках инженера DevOps. Опять же, мы рассмотрим это более подробно в специальном разделе, посвященном DNS, DHCP, балансировщикам нагрузки (Load Balancing) и т. д.\n\nМастер на все руки\n\nОднако на этом этапе я скажу, что вам не нужно быть специалистом по сетям или инфраструктуре, вам нужны базовые знания о том, как наладить работу и общаться друг с другом, во многом так же, как, возможно, иметь базовые знания язык программирования, но вам не нужно быть разработчиком. Однако вы можете прийти к этому как специалист в какой-то области, и это отличная основа для адаптации к другим областям.\n\nВы также, скорее всего, не будете ежедневно управлять этими серверами или приложением.\n\nМы говорили о серверах, но есть вероятность, что ваше приложение будет разработано для работы в виде контейнеров, которые по-прежнему работают на сервере по большей части, но вам также потребуется понимание не только виртуализации, облачной инфраструктуры как услуги (IaaS). ), но также и контейнеризация. В эти 90 дней основное внимание будет уделяться контейнерам.\n\nОбщий обзор\n\nС одной стороны, наши разработчики создают новые функции и функции (а также исправления ошибок) для приложения.\n\nС другой стороны, у нас есть какая-то среда, инфраструктура или серверы, которые настроены и управляются для запуска этого приложения и связи со всеми необходимыми службами.\n\nБольшой вопрос заключается в том, как нам внедрить эти функции и исправления ошибок в нашу продукцию и сделать их доступными для этих конечных пользователей?\n\nКак мы выпускаем новую версию приложения? Это одна из основных задач для DevOps-инженера, и здесь важно не просто понять, как это сделать один раз, а нам нужно делать это непрерывно и автоматизированным, эффективным способом, который также должен включать тестирование!\n\nНа этом мы собираемся закончить этот день обучения, надеюсь, это было полезно. В течение следующих нескольких дней мы собираемся немного глубже погрузиться в некоторые другие области DevOps, а затем мы перейдем к разделам, в которых более подробно рассматриваются инструменты и процессы, а также их преимущества.\n\nРесурсы\n\nЯ всегда открыт для добавления дополнительных ресурсов в эти файлы Readme, поскольку они здесь в качестве учебного пособия.\n\nМой совет - просмотреть все ссылки из списка ниже, и, надеюсь, вы также что-то почерпнули из текста и объяснений выше.\n\nWhat is DevOps? - TechWorld with Nana\nWhat is DevOps? - GitHub YouTube\nWhat is DevOps? - IBM YouTube\nWhat is DevOps? - AWS\nWhat is DevOps? - Microsoft\n\nЕсли вы зашли так далеко, то поймете, хотите ли вы быть здесь или нет. До встречи в День 3\n",
            "tags": [
                "devops",
                "90daysofdevops",
                "learning"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day03",
            "title": "3. Ориентированность на приложения",
            "description": "Ориентированность на приложения",
            "content": "\nЖизненный цикл DevOps — ориентированность на приложения\n\nПо мере того, как мы будем продолжать в течение следующих нескольких недель, мы будем сталкиваться с этими названиями (Continuous Development, Testing, Deployment, Monitor) (непрерывная разработка, тестирование, развертывание, мониторинг) снова и снова.\nЕсли вы стремитесь статья инженером DevOps, то повторяемость будет тем, к чему вы привыкнете, но постоянное улучшение каждый раз — это еще одна вещь, которая делает вещи интересными.\n\nВ этом часе мы рассмотрим общий вид приложения от начала до конца, а затем вернемся назад, как в постоянном цикле.\n\nРазработка\nДавайте возьмем совершенно новый пример приложения, для начала у нас ничего не создано, возможно, как разработчик вы должны обсудить с вашим клиентом или конечным пользователем требования и придумать какой-то план или требования для вашего приложения. Затем нам нужно создать согласно требованиям наше новое приложение.\n\nЧто касается инструментов на данном этапе, здесь нет никаких реальных требований, кроме выбора вашей IDE и языка программирования, который вы хотите использовать для написания своего приложения.\n\nКак инженер DevOps, помните, что вы, вероятно, не тот, кто создает этот план или создает приложение для конечного пользователя, этим занимается опытный разработчик.\n\nНо вам также не помешает иметь возможность прочитать часть кода, чтобы вы могли принимать наилучшие решения по инфраструктуре для своего приложения.\n\nРанее мы упоминали, что приложение может быть написано на любом языке. Важно, чтобы это поддерживалось с помощью системы контроля версий, это то, что мы также подробно рассмотрим позже, и, в частности, мы углубимся в Git.\n\nТакже вероятно, что над этим проектом будет работать не один разработчик, хотя это может иметь место, но даже в этом случае передовой опыт потребует репозиторий кода для хранения и совместной работы над кодом, он может быть частным или общедоступным и может быть размещен или если говорить о частном развертывании, вы наверняка слышали, как GitHub или GitLab используются в качестве репозитория кода. Мы снова рассмотрим их позже в разделе Git.\n\nТестирование\nНа данном этапе у нас есть свои требования и наша задача - разработать приложение. Но нам нужно убедиться, что мы тестируем наш код во всех различных средах, которые у нас есть, или, возможно, в выбранном языке программирования.\n\nЭтот этап позволяет QA тестировать на наличие ошибок, чаще мы видим, что контейнеры используются для моделирования тестовой среды, что в целом может снизить накладные расходы на физическую или облачную инфраструктуру.\n\nЭтот этап также, вероятно, будет автоматизирован как часть следующей области — непрерывной интеграции.\n\nВозможность автоматизировать это тестирование по сравнению с 10, 100 или даже 1000 инженерами по контролю качества, которые должны делать это вручную, говорит сама за себя, эти инженеры могут сосредоточиться на чем-то другом в стеке, чтобы гарантировать, что вы двигаетесь быстрее и разрабатываете больше функций по сравнению с тестированием ошибок и программного обеспечения. что, как правило, является задержкой для большинства традиционных выпусков программного обеспечения, использующих методологию водопада (Waterfall).\n\nИнтеграция\n\nОчень важно, что интеграция находится в середине жизненного цикла DevOps. Это практика, когда разработчикам требуется чаще вносить изменения в исходный код. Это может быть ежедневно или еженедельно.\n\nС каждым коммитом ваше приложение может проходить этапы автоматизированного тестирования, что позволяет на раннем этапе обнаруживать проблемы или ошибки до следующего этапа.\n\nНа этом этапе вы можете сказать: «Но мы не создаем приложения, мы покупаем их в готовом виде у поставщика программного обеспечения». Не волнуйтесь, многие компании делают это и будут продолжать делать, и именно поставщик программного обеспечения будет концентрируется на трех вышеупомянутых этапах, но вы, возможно, захотите принять последний этап, поскольку это позволит быстрее и эффективнее развертывать готовые развертывания.\n\nЯ бы также сказал, что очень важно просто иметь эти вышеперечисленные знания, поскольку сегодня вы можете купить готовое программное обеспечение, но что насчет завтра или в будущем ... может быть, на следующей работе?\n\nРазвертывание / Deployment\nИтак, наше приложение создано и протестировано в соответствии с требованиями нашего конечного пользователя, и теперь нам нужно приступить к развертыванию этого приложения в рабочей среде для использования нашими конечными пользователями.\n\nЭто этап, когда код развертывается на рабочих серверах, теперь все становится чрезвычайно интересным, и именно здесь оставшиеся 86 дней мы глубже погружаемся в эти области. Потому что разные приложения требуют различного аппаратного обеспечения или конфигураций. Именно здесь Управление конфигурацией приложений и Инфраструктура как код могут сыграть ключевую роль в жизненном цикле DevOps. Возможно, ваше приложение контейнеризовано, но его также можно запустить на виртуальной машине. Это также приводит наше изучение к таким платформам, как Kubernetes, которые будут организовывать эти контейнеры и следить за тем, чтобы желаемое состояние было доступно вашим конечным пользователям.\n\nВсе эти смелые темы мы рассмотрим более подробно в течение следующих нескольких недель, чтобы лучше понять основы того, что они из себя представляют и когда их использовать.\n\nМониторинг / Monitoring\n\nВсе быстро меняется, и у нас есть наше приложение, которое мы постоянно обновляем новыми функциями и функциями, и у нас есть наше тестирование, чтобы убедиться, что функциональность не нарушена. У нас есть приложение, работающее в нашей среде, которое может постоянно поддерживать требуемую конфигурацию и производительность.\n\nНо теперь мы должны быть уверены, что наши конечные пользователи получают то, что им нужно. Здесь нам нужно убедиться, что производительность нашего приложения постоянно отслеживается, этот этап позволит вашим разработчикам принимать более взвешенные решения об улучшениях приложения в будущих выпусках, чтобы лучше обслуживать конечных пользователей.\n\nНадежность также является ключевым фактором здесь, в конце концов, мы хотим, чтобы наше приложение было доступно все время, когда оно требуется. Затем это дает возможность другим областям наблюдаемости, безопасности и управления данными, которые следует постоянно контролировать, а обратную связь всегда можно использовать для улучшения, обновления и непрерывного выпуска приложения.\n\nНекоторый вклад от сообщества здесь, в частности @_ediri, упоминает также часть этого непрерывного процесса, мы также должны привлечь команды FinOps. Приложения и данные работают и хранятся где-то, за чем вы должны постоянно следить, чтобы убедиться, что если что-то изменится с точки зрения ресурсов, ваши расходы не вызовут серьезных финансовых проблем с вашими облачными счетами.\n\nЯ думаю, что сейчас самое время упомянуть упомянутого выше «инженера DevOps». Я имею в виду, что из разговора с другими членами сообщества звание инженера DevOps не должно быть целью ни для кого, потому что на самом деле любая должность должна включать процессы DevOps и культуру, описанную здесь. DevOps следует использовать на самых разных должностях, таких как облачный инженер/архитектор, администратор виртуализации, облачный архитектор/инженер, администратор инфраструктуры. Это лишь некоторые из них, но причина использования DevOps Engineer, описанная выше, на самом деле заключалась в том, чтобы выделить объем или процесс, используемый любой из вышеперечисленных должностей, и многое другое.\n\nИсточники\n\nЯ всегда открыт для добавления дополнительных ресурсов в эти файлы readme, поскольку они здесь в качестве учебного пособия.\n\nМой совет — посмотрите все, что ниже, и, надеюсь, вы тоже что-то почерпнули из текста и объяснений выше.\n\nМетодология разработки CI/CD\nContinuous Development\nContinuous Testing - IBM YouTube\nContinuous Integration - IBM YouTube\nContinuous Monitoring\nThe Remote Flow\nFinOps Foundation - What is FinOps\nNOT FREE The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win\n\nДо встречи в День 4\n",
            "tags": [
                "devops",
                "cicd",
                "tests",
                "learning"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day04",
            "title": "4. DevOps и Agile",
            "description": "DevOps и Agile",
            "content": "\nDevOps и Agile\n\nВы знаете разницу между DevOps и Agile? Они формировались как самостоятельные понятия. Но теперь эти два термина сливаются.\n\nВ этом посте мы рассмотрим важные различия между Agile и DevOps и выясним, почему они так тесно связаны.\n\nЯ думаю, что хорошее место для начала — это немного больше узнать об общем подходе, который я увидел в изучении этой области, а именно о DevOps и Agile, даже несмотря на то, что у них схожие цели и процессы. В этом разделе я, надеюсь, мы разберемся с этим.\n\nНачнем с определений.\n\nРазработка Agile\n\nAgile — это подход, который фокусируется на более быстром получении небольших результатов, а не на выгрзуке (релизе) одного большого обновления продукта; программное обеспечение разрабатывается итерациями (неболшими изменениями). Команда выпускает новую версию каждую неделю или месяц с дополнительными обновлениями. Итоговая цель Agile — предоставить конечным пользователям оптимальный опыт.\n\nDevOps\n\nВ течение последних нескольких дней мы освещали это несколькими различными способами описания конечных целей DevOps. DevOps обычно описывает разработку программного обеспечения\nи методы доставки, основанные на сотрудничестве между разработчиками программного обеспечения и специалистами по эксплуатации. Основными преимуществами DevOps являются упрощение процесса разработки и минимизация недопонимания.\n\nВ чем разница между Agile и DevOps\n\nРазница в основном в заботах. У Agile и DevOps разные интересы, но они помогают друг другу. Agile требует коротких итераций, что возможно только с автоматизацией, которую обеспечивает DevOps. Agile хочет, чтобы клиент попробовал конкретную версию и быстро дал отзыв, что возможно только в том случае, если DevOps упростит создание новой среды.\n\nРазные участники\n\nAgile фокусируется на оптимизации взаимодействия между конечными пользователями и разработчиками, в то время как DevOps нацелен на разработчиков и членов операционной группы. Можно сказать, что Agile ориентирован на клиентов, тогда как DevOps — это набор внутренних практик.\n\nКоманда\n\nAgile обычно применяется к разработчикам программного обеспечения и руководителям проектов. Компетенции DevOps-инженеров лежат на стыке разработки, QA (обеспечения качества) и операций, поскольку они участвуют во всех этапах цикла продукта и являются частью Agile-команды.\n\nПрикладные фреймворки\n\nВ Agile есть много сред управления для достижения гибкости и прозрачности: Scrum > Kanban > Lean > Extreme > Crystal > Dynamic > Feature-Driven. DevOps фокусируется на подходе к разработке в сотрудничестве, но не предлагает конкретных методологий. Тем не менее, DevOps продвигает такие практики, как инфраструктура как код, архитектура как код, мониторинг, самовосстановление, сквозная автоматизация тестирования... Но сама по себе это не структура, а практика.\n\nОбратная связь\n\nВ Agile основным источником обратной связи является конечный пользователь, тогда как в DevOps более высокий приоритет имеет обратная связь от заинтересованных сторон и самой команды.\n\nЦелевые области\n\nAgile фокусируется на разработке программного обеспечения больше, чем на развертывании и обслуживании. DevOps также фокусируется на разработке программного обеспечения, но его ценности и инструменты также охватывают этапы развертывания и после выпуска, такие как мониторинг, высокая доступность, безопасность и защита данных.\n\nДокументация\n\nAgile отдает предпочтение гибкости и поставленным задачам, а не документации и мониторингу. С другой стороны, DevOps рассматривает проектную документацию как один из основных компонентов проекта.\n\nРиски\n\nРиски Agile вытекают из гибкости методологии. Гибкие проекты трудно предсказать или оценить, поскольку приоритеты и требования постоянно меняются.\n\nРиски DevOps возникают из-за неправильного понимания термина и отсутствия подходящих инструментов. Некоторые люди рассматривают DevOps как набор программного обеспечения для развертывания и непрерывной интеграции, не способного изменить базовую структуру процесса разработки.\n\nИспользуемые инструменты\n\nAgile-инструменты ориентированы на совместную управленческую коммуникацию, метрики и обработку отзывов. К наиболее популярным agile-инструментам относятся JIRA, Trello, Slack, Zoom, SurveyMonkey и другие.\n\nDevOps использует инструменты для командного общения, разработки программного обеспечения, развертывания и интеграции, такие как Jenkins, GitHub Actions, BitBucket и т. д. Несмотря на то, что Agile и DevOps имеют несколько разные фокусы и области действия, ключевые значения почти идентичны, поэтому вы можете комбинировать их.\n\nСобрать все вместе… хорошая идея или нет? Обсуждать?\n\nСочетание Agile и DevOps дает следующие преимущества:\n\nГибкое управление и мощные технологии.\nПрактики Agile помогают командам DevOps более эффективно сообщать о своих приоритетах.\nСтоимость автоматизации, которую вы должны заплатить за свои методы DevOps, оправдана вашим гибким требованием быстрого и частого развертывания.\nЭто приводит к укреплению: команда, внедряющая agile-практики, улучшит сотрудничество, повысит мотивацию команды и снизит текучесть кадров.\nВ результате вы получаете лучшее качество продукции.\n\nAgile позволяет вернуться к предыдущим этапам разработки продукта, чтобы исправить ошибки и предотвратить накопление технического долга. Принять Agile и DevOps\nодновременно просто выполните 7 шагов:\n\nОбъедините команды разработки и эксплуатации.\nСоздайте команды сборки и запуска, все проблемы, связанные с разработкой и эксплуатацией, обсуждаются всей командой DevOps.\nИзмените свой подход к спринтам и назначьте рейтинги приоритета, чтобы предлагать задачи DevOps, которые имеют такое же значение, как задачи разработки. Поощряйте команды разработчиков и эксплуатации обмениваться мнениями о рабочем процессе других команд и возможных проблемах.\nВключите контроль качества на все этапы разработки.\nВыбирайте правильные инструменты.\nАвтоматизируйте все, что можете.\nИзмеряйте и контролируйте, используя материальные числовые результаты.\n\nЧто вы думаете? У вас разные взгляды? Я хочу услышать от разработчиков, специалистов по эксплуатации, QA или кого-либо, кто лучше разбирается в Agile и DevOps, которые могут поделиться комментариями и отзывами по этому поводу?\n\nИсточники\n\nDevOps for Developers – Day in the Life: DevOps Engineer in 2021\n3 Things I wish I knew as a DevOps Engineer\nHow to become a DevOps Engineer feat. Shawn Powers\n\nДо встречи в День 5\n",
            "tags": [
                "devops",
                "agile",
                "scrum",
                "kanban"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day05/",
            "title": "5. Plan > Code > Build > Testing > Release > Deploy > Operate > Monitor",
            "description": "Plan > Code > Build > Testing > Release > Deploy > Operate > Monitor",
            "content": "\nСегодня мы сосредоточимся на отдельных шагах от начала до конца и на непрерывном цикле приложения в мире DevOps.\n\nDevOps\n\nПлан\n\nВсе начинается с процесса планирования, когда команда разработчиков собирается вместе и выясняет, какие типы функций и исправлений ошибок они собираются внедрить в следующем спринте. Это возможность для вас как инженера DevOps принять участие в этом и узнать, какие вещи будут происходить на вашем пути, с которыми вам нужно участвовать, а также повлиять на их решения или их путь и как бы помочь им работать с инфраструктура, которую вы построили, или направьте их к чему-то, что будет работать лучше для них, если они не на этом пути, и поэтому одна ключевая вещь, на которую здесь следует указать, это то, что разработчики или команда разработчиков программного обеспечения являются вашим клиентом как DevOps инженер, так что это ваша возможность поработать с вашим клиентом до того, как он пойдет по плохому пути.\n\nCode\n\nТеперь, как только эта сессия планирования будет завершена, разработчики начинают писать код, в разработку котоого вы можете быть вовлечены, предоставляя информацю об инфрастуктуре, микросеврисах, если таковые имеются, и т.д.\nКогда разработчики заканчивают писать код/часть кода, они объединяют (merge) все измененияю и выгруат в репозиторий.\n\nBuild\n\nЗдесь мы начнем первый из наших процессов автоматизации, потому что мы \"возьмем\" их код и построим (скомпилируем, \"сбилдим\") его в зависимости от того, какой язык они используют, это может быть транспиляция или компиляция, а может создать образ докера из этого кода в любом случае, мы собираемся пройти этот процесс, используя наш cicd pipeline (\"пайплайн\")\n\nTesting\n\nПосле того, как мы его скомпилировали проект, мы проведем на нем несколько тестов. Команда разработчиков обычно пишет тесты. У вас может быть некоторый вклад в то, какие тесты пишутся, но нам нужно запустить эти тесты. Тестирование — это способ провериь и свести к минимуму появление проблем в рабочей среде. И хотя это не гарантирует полной проверки, но мы хотим максимально точно быть уверенными, что одна из новых функций не создает новых ошибок, а две другие не ломают то, что раньше работало.\n\nRelease\n\nКак только эти тесты пройдены, мы собираемся выполнить процесс выпуска, и, опять же, в зависимости от того, над каким типом приложения вы работаете, это может быть поэтапным. Код может просто находиться в репозитории GitHub или репозитории git или где-то еще, а также это может быть процесс зарузки вашего скомпилированного кода или созданного образа докера и помещения его в реестр или репозиторий, где он находится.\n\nDeploy\n\nСледующее, что мы собираемся сделать - это \"деплой\" (публикация/развертывание). Развертывание похоже на конечный результат процесса. Потому что после развертывания приложения, когда мы запускаем код в производство, наш бизнес действительно осознает ценность всех временных усилий и тяжелой работы, которые вы и команда разработчиков программного обеспечения вложили в этот продукт до этого момента.\n\nOperate\n\nПосле того, как код выгружен  скомпилирован, мы собираемся эксплуатировать его, и эксплуатация может включать в себя что-то вроде того, что вы начинаете получать звонки от своих клиентов, которые все раздражены тем, что сайт работает медленно или их приложение работает медленно, поэтому вам нужно выяснить, почему это так.\nА а затем, возможно, создать автоматическое масштабирование, которое связано с увеличением количества серверов, доступных в пиковые периоды, и уменьшением количества серверов в непиковые периоды.\n\nMonitor\n\nВсе вышеперечисленные части ведут к последнему шагу - мониторингу, что важно особенно в отношении проблем, возникающих в рельном времени, автоматического масштабирования, устранения неполадок.\nВо время мониторига мы сохраняем данные об использовании памяти, использовании ЦП на диске, времени отклика, скорость отклика и т.д. Большая часть этого также является журналами. Журналы дают разработчикам возможность видеть, что происходит, без доступа к производственным системам.\n\nRince & Repeat\n\nOnce that's in place you go right back to the beginning to the planning stage and go through the whole thing again\n\nContinuous\n\nМногие инструменты помогают нам достичь вышеуказанного непрерывного процесса, весь этот код и конечная цель полной автоматизации облачной инфраструктуры или любой среды часто описывается как непрерывная интеграция/непрерывная доставка/непрерывное развертывание или сокращенно «CI/CD». Позже, в течение 90 дней, мы посвятим целую неделю CI/CD с некоторыми примерами и пошаговыми руководствами, чтобы понять основы.\n\nContinuous Delivery\n\nContinuous Delivery = Plan > Code > Build > Test\n\nContinuous Integration\n\nНепрерывная интеграция - это результат описанных выше этапов непрерывной \"доставки\" и результат этапа выпуска. Это относится как к неудаче, так и к успеху, но это возвращается в непрерывную доставку или перемещается в непрерывное развертывание.\n\nContinuous Integration = Plan > Code > Build > Test > Release\n\nContinuous Deployment\n\nЕсли у вас есть успешный релиз, перейдите к непрерывному развертыванию, которое включает следующие этапы.\n\nВыпуск CI выполнен успешно = непрерывное развертывание = развертывание > эксплуатация > мониторинг\n\nВы можете рассматривать эти три понятия выше как простой набор фаз жизненного цикла DevOps.\n\nЭтот последний фрагмент был для меня чем-то вроде подведения итогов третьего дня, но думаю, что на самом деле это проясняет для меня ситуацию.\n\nИсточники\n\nDevOps for Developers – Software or DevOps Engineer?\nTechworld with Nana -DevOps Roadmap 2022 - How to become a DevOps Engineer? What is DevOps?\nHow to become a DevOps Engineer in 2021 - DevOps Roadmap\n\nДо встречи в День 6\n",
            "tags": [
                "devops",
                "cicd",
                "tests",
                "learning"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day06",
            "title": "6. DevOps - Истории",
            "description": "DevOps - Истории",
            "content": "\nDevOps - Истории компаний\n\nDevOps с самого начала считался недосягаемым для многих из нас, поскольку у нас не было среды или требований, подобных Netflix или Fortune 500, но подумайте, что теперь это начинает становиться нормой, когда мы внедряем практику DevOps внутри. любой вид бизнеса.\n\nПо второй ссылке ниже в справочных материалах вы увидите множество различных отраслей и вертикалей, использующих DevOps и оказывающих огромное положительное влияние на свои бизнес-цели.\n\nОчевидно, что основным преимуществом здесь является DevOps, если он выполнен правильно, он должен помочь вашему бизнесу повысить скорость и качество разработки программного обеспечения.\n\nЯ хотел использовать этот день, чтобы посмотреть на успешные компании, которые внедрили практику DevOps, и поделиться некоторыми ресурсами по этому поводу. Приняли ли вы культуру DevOps в своем бизнесе? Был ли он успешным?\n\nЯ упомянул Netflix выше и коснусь их снова, поскольку это очень хорошая модель, которая даже до сих пор продвинута к тому, что мы обычно видим сегодня, но также упомяну некоторые другие известные бренды, которые, похоже, преуспевают.\n\nAmazon\nВ 2010 году Amazon переместила свои физические серверы в облако Amazon Web Services (AWS), что позволило им сэкономить ресурсы за счет увеличения и уменьшения емкости с очень небольшими приращениями. Мы также знаем, что это облако AWS продолжит свое существование и будет приносить огромный доход, продолжая управлять розничным филиалом компании Amazon.\n\nAmazon внедрила в 2011 году (согласно приведенному ниже ресурсу) непрерывный процесс развертывания, при котором их разработчики могли развертывать код в любое время и на любых серверах, которые им нужны. Это позволило Amazon добиться развертывания нового программного обеспечения на производственных серверах в среднем каждые 11,6 секунды!\n\nNetFlix\nКто не пользуется NetFlix? очевидно, это огромный качественный потоковый сервис, который, по крайней мере, лично для всех, обеспечивает отличный пользовательский опыт.\n\nПочему этот пользовательский опыт так хорош? Что ж, возможность предоставить услугу без воспоминаний, по крайней мере, о сбоях, требует скорости, гибкости и внимания к качеству.\n\nРазработчики NetFlix могут автоматически встраивать фрагменты кода в развертываемые веб-образы, не полагаясь на ИТ-операции. По мере обновления изображений они интегрируются в инфраструктуру Netflix с помощью специально созданной веб-платформы.\n\nНепрерывный мониторинг выполняется таким образом, что в случае сбоя развертывания образов новые образы откатываются, а трафик перенаправляется на предыдущую версию.\n\nНиже приводится отличная беседа, в которой подробно рассказывается о том, что нужно и чего нельзя делать, по которым Netflix живет и умирает в своих командах.\n\nEtsy\nКак и у многих из нас и многих компаний, медленные и болезненные развертывания были настоящим испытанием. В том же духе мы могли бы также работать в компаниях, которые имеют много бункеров и команд, которые не очень хорошо работают вместе.\n\nИз того, что я могу понять, по крайней мере, из чтения об Amazon и Netflix, Etsy, возможно, разрешила разработчикам развертывать свой собственный код примерно в конце 2009 года, что могло быть до двух других упомянутых. (интересный!)\n\nИнтересный вывод, который я прочитал здесь, заключался в том, что они поняли, что когда разработчики чувствуют ответственность за развертывание, они также берут на себя ответственность за производительность приложения, время безотказной работы и другие цели.\n\nКультура обучения является ключевой частью DevOps, даже неудача может стать успехом, если извлечь уроки. (не уверен, откуда на самом деле взялась эта цитата, но она имеет смысл!)\n\nЯ добавил несколько других историй о том, как DevOps изменил правила игры в некоторых из этих чрезвычайно успешных компаний.\n\nИсточники\n\nHow Netflix Thinks of DevOps\n[16 Popular DevOps Use Cases & Real Life Applications [2021]](https://www.upgrad.com/blog/devops-use-cases-applications/)\nDevOps: The Amazon Story\nHow Etsy makes DevOps work\nAdopting DevOps @ Scale Lessons learned at Hertz, Kaiser Permanente and lBM\nInterplanetary DevOps at NASA JPL\nTarget CIO explains how DevOps took root inside the retail giant\n\nПодведем итоги наших первых дней, посвященных DevOps.\n\nDevOps — это комбинация разработки и эксплуатации, которая позволяет одной команде управлять всем жизненным циклом разработки приложения, состоящим из разработки, тестирования, развертывания, эксплуатации.\n\nОсновное внимание и цель DevOps — сократить жизненный цикл разработки, часто предоставляя функции, исправления и функциональные возможности в тесном соответствии с бизнес-целями.\n\nDevOps — это подход к разработке программного обеспечения, с помощью которого программное обеспечение может поставляться и разрабатываться надежно и быстро. Вы также можете увидеть это как Непрерывная разработка, тестирование, развертывание, мониторинг\n\nДо встречи в День 7\n\nНа седьмой день мы погрузимся в язык программирования. Я не стремлюсь быть разработчиком, но хочу понимать, что делают разработчики.\n\nМожем ли мы достичь этого за неделю? Вероятно, нет, но если мы потратим 7 дней или 7 часов на изучение чего-то, мы будем знать больше, чем когда мы начинали.",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day07",
            "title": "7. DevOps - изучение языка программирования",
            "description": "DevOps - изучение языка программирования",
            "content": "\nОбщая картина: DevOps и изучение языка программирования\n\nЯ думаю, будет справедливо сказать, что для достижения успеха в качестве инженера DevOps в долгосрочной перспективе необходимо знать хотя бы один язык программирования на базовом уровне. Я хочу провести это первое занятие в этой статье, чтобы выяснить, почему это такой важный навык, и, надеюсь, к концу этой недели или раздела вы будете лучше понимать, почему, как и что делать. делайте, чтобы продвигаться в своем учебном путешествии.\n\nЯ думаю, что если бы я спросил в социальных сетях, нужны ли вам навыки программирования для ролей, связанных с DevOps, ответ, скорее всего, будет утвердительным? Дайте мне знать, если вы думаете иначе? Хорошо, но тогда более важный вопрос, и здесь вы не получите такого четкого ответа, какой язык программирования? Наиболее распространенным ответом, который я видел здесь, был Python, или все чаще мы видим, что Golang или Go должны быть языком, который вы изучаете.\n\nЧтобы быть успешным в DevOps, вы должны хорошо знать навыки программирования, по крайней мере, мой вывод из этого. Но мы должны понять, зачем нам это нужно, чтобы выбрать правильный путь.\n\nПонимание зачем вам нужно изучать язык программирования\n\nПричина, по которой Python и Go так часто рекомендуются инженерам DevOps, заключается в том, что многие инструменты DevOps написаны либо на Python, либо на Go, что имеет смысл, если вы собираетесь создавать инструменты DevOps. Теперь это важно, так как это действительно определит, что вы должны изучить, и это, вероятно, будет наиболее полезным. Если вы собираетесь создавать инструменты DevOps или присоединяетесь к команде, которая занимается этим, имеет смысл выучить тот же язык. Если вы собираетесь активно участвовать в Kubernetes или контейнерах, то, скорее всего, вы захотите выберите Go в качестве языка программирования. Для меня компания, в которой я работаю (Kasten by Veeam), находится в экосистеме Cloud-Native, ориентированной на управление данными для Kubernetes, и все написано на Go.\n\nНо тогда у вас может не быть четких рассуждений, подобных этим, чтобы выбрать, быть ли вам студентом или менять карьеру без реального решения за вас. Я думаю, что в этой ситуации вы должны выбрать тот, который, кажется, резонирует и подходит для приложений, с которыми вы хотите работать.\n\nПомните, что я не собираюсь становиться здесь разработчиком программного обеспечения, я просто хочу немного больше узнать о языке программирования, чтобы я мог читать и понимать, что делают эти инструменты, а затем это, возможно, приведет к тому, как мы можем помочь улучшить ситуацию.\n\nЯ также хотел бы знать, как вы взаимодействуете с этими инструментами DevOps, такими как Kasten K10 или Terraform и HCL. Это то, что мы будем называть конфигурационными файлами, и именно так вы взаимодействуете с этими инструментами DevOps, чтобы что-то происходило, обычно это будет YAML. (Мы можем использовать последний день этого раздела, чтобы немного погрузиться в YAML)\n\nЯ только что отговорил себя от изучения языка программирования?\n\nБольшую часть времени или в зависимости от роли вы будете помогать инженерным командам внедрять DevOps в свой рабочий процесс, много тестировать приложение и следить за тем, чтобы созданный рабочий процесс соответствовал тем принципам DevOps, которые мы упоминали в первые несколько дней. . Но на самом деле много времени уходит на устранение проблем с производительностью приложений или что-то в этом роде. Это возвращает меня к моей первоначальной точке зрения и рассуждениям: язык программирования, который мне нужно знать, — это тот, на котором написан код? Если их приложение написано на NodeJS, это не сильно поможет, если у вас есть значок Go или Python.\n\nПочему Go\n\nПочему Golang — следующий язык программирования для DevOps? В последние годы Go стал очень популярным языком программирования. Согласно опросу StackOverflow за 2021 год, Go занял четвертое место среди самых востребованных языков программирования, сценариев и разметки, а Python был на первом месте, но выслушайте меня. StackOverflow 2021 Developer Survey – Most Wanted Link\n\nКак я уже упоминал, некоторые из самых известных инструментов и платформ DevOps написаны на Go, такие как Kubernetes, Docker, Grafana и Prometheus.\n\nКакие характеристики Go делают его идеальным для DevOps?\n\nСборка и развертывание программ Go\nПреимущество использования такого языка, как Python, который интерпретируется в роли DevOps, заключается в том, что вам не нужно компилировать программу Python перед ее запуском. Особенно для небольших задач автоматизации вы не хотите, чтобы процесс сборки, требующий компиляции, замедлялся, несмотря на то, что Go — компилируемый язык программирования, Go компилируется непосредственно в машинный код. Go также известен быстрым временем компиляции.\n\nGo или Python для DevOps\n\nПрограммы Go статически связаны, это означает, что когда вы компилируете программу Go, все включается в один исполняемый двоичный файл, не требуется никаких внешних зависимостей, которые необходимо установить на удаленной машине, это упрощает развертывание программ Go, по сравнению с программой Python, которая использует внешние библиотеки, где вы должны убедиться, что все эти библиотеки установлены на удаленной машине, на которой вы хотите работать.\n\nGo — это независимый от платформы язык, что означает, что вы можете создавать двоичные исполняемые файлы для * всех операционных систем, Linux, Windows, macOS и т. д., и это очень легко сделать. С Python не так просто создавать эти двоичные исполняемые файлы для конкретных операционных систем.\n\nGo — очень производительный язык, он имеет быструю компиляцию и быстрое время выполнения с меньшим использованием ресурсов, таких как процессор и память, особенно по сравнению с python, в языке Go были реализованы многочисленные оптимизации, которые делают его таким производительным. (Ресурсы ниже)\n\nВ отличие от Python, который часто требует использования сторонних библиотек для реализации конкретной программы Python, go включает в себя стандартную библиотеку, которая имеет большую часть функций, которые вам понадобятся для DevOps, встроенных непосредственно в нее. Это включает в себя функциональную обработку файлов, веб-службы HTTP, обработку JSON, встроенную поддержку параллелизма и параллелизма, а также встроенное тестирование.\n\nЭто ни в коем случае не бросает Python под автобус, я просто излагаю свои причины выбора Go, но они не являются вышеупомянутым Go против Python, это обычно потому, что это имеет смысл, поскольку компания, в которой я работаю, разрабатывает программное обеспечение на Go, вот почему.\n\nЯ скажу, что как только как только вы выучите свой первый язык программирования, вам станет легче осваивать другие языки. Вероятно, у вас никогда не будет ни одной работы в какой-либо компании, где бы вам не приходилось иметь дело с управлением, архитектурой, оркестровкой, отладкой приложений JavaScript и Node JS.\n\nИсточники\n\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nТеперь в течение следующих 6 дней этой темы я намерен работать с некоторыми из ресурсов, перечисленных выше, и документировать свои заметки на каждый день. Вы заметите, что они, как правило, составляют около 3 часов в качестве полного курса, я хотел поделиться своим полным списком, чтобы, если у вас есть время, вы могли двигаться вперед и работать над каждым, если позволяет время, я буду придерживаться моего часа обучения каждый день.\n\nДо встречи в День 8\n",
            "tags": [
                "devops",
                "golang",
                "python"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day08/",
            "title": "8. Настройка DevOps окружения для запуска Hello World на Go",
            "description": "Настройка DevOps окружения для запуска Hello World на Go",
            "content": "\nНастройка DevOps окружения для запуска Hello World на Go\n\nПрежде чем мы приступим к некоторым основам Go, мы должны установить Go на нашу рабочую станцию и сделать то, чему нас учит каждый модуль «Изучение программирования 101», а именно создать приложение Hello World. Так как здесь будут описаны шаги по установке Go на ваш ПК, мы попытаемся задокументировать процесс в картинках, чтобы людям было легко следовать за ним.\n\nВозможные варианты установки Golang\nИсполняемый файл\nПакет из исходного кода\nMac Os Homebrew\n\n#Homebrew install command\nbrew install go\n\nБыстрый тьюториал для ознакомления с языком Go\n\nРассмотрим вараинт установки с помощью инсталляционного файла\n\n\n\nЕсли мы зашли так далеко, вы, вероятно, знаете, какая операционная система рабочей станции у вас установлена, поэтому выберите соответствующую загрузку, и тогда мы сможем приступить к установке. Я использую Windows для этого пошагового руководства. На следующем шаге мы можем оставить все значения по умолчанию. (Отмечу, что на момент написания это была последняя версия, поэтому скриншоты могут быть устаревшими)\n\n\n\nТакже обратите внимание, что если у вас установлена более старая версия Go, вам придется удалить ее перед установкой, поскольку в Windows она встроена в установщик, и она будет удалена и установлена как единое целое.\n\nПосле завершения вы должны открыть командную строку / терминал, и мы хотим проверить, установлен ли Go. Если вы не получите вывод, который мы видим ниже, значит, Go не установлен, и вам нужно будет повторить свои шаги.\n\ngo version\n\n\n\nДалее мы хотим проверить нашу среду на наличие Go. Это всегда полезно проверить, чтобы убедиться, что ваши рабочие каталоги настроены правильно, как вы можете видеть ниже, нам нужно убедиться, что в вашей системе есть следующий каталог.\n\n\n\n\n\nХорошо, давайте создадим этот каталог для простоты. Я собираюсь использовать команду mkdir в своем терминале PowerShell. Нам также нужно создать 3 папки в папке Go, как вы увидите ниже.\n\n\n\nТеперь у нас установлен Go, и у нас есть рабочий каталог Go, готовый к действию. Теперь нам нужна интегрированная среда разработки (IDE). Сейчас есть много доступных, которые вы можете использовать, но наиболее распространенным и тем, который я использую, является Visual Studio Code или Code. Вы можете узнать больше об IDE здесь.\n\nЕсли вы еще не загрузили и не установили VSCode на свою рабочую станцию, вы можете сделать это, перейдя по ссылке. Как вы можете видеть ниже, у вас есть разные варианты ОС.\n\n\n\nПочти так же, как и при установке Go, мы собираемся загрузить и установить и сохранить значения по умолчанию. После завершения вы можете открыть VSCode, выбрать «Открыть файл» и перейти в наш каталог Go, который мы создали выше.\n\n\n\nВы можете получить всплывающее окно о доверии, прочитать его, если хотите, а затем нажать «Да, доверять авторам». (Позже я не несу ответственности, если вы начнете открывать вещи, которым не доверяете!)\n\nТеперь вы должны увидеть три папки, которые мы также создали ранее, и теперь мы хотим щелкнуть правой кнопкой мыши папку src и создать новую папку с именем «Hello».\n\n\n\nДовольно простые вещи, я бы сказал до этого момента? Теперь мы собираемся создать нашу первую программу Go, не понимая, что мы вкладываем в этот следующий этап.\n\nЗатем создайте файл с именем main.go в папке Hello. Как только вы нажмете Enter на main.go, вас спросят, хотите ли вы установить расширение Go, а также пакеты, вы также можете проверить этот пустой файл pkg, который мы сделали несколько шагов назад, и обратите внимание, что у нас должны быть новые пакеты. там сейчас?\n\n\n\nТеперь давайте запустим это приложение Hello World, скопируйте следующий код в новый файл main.go и сохраните его.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello #90DaysOfDevOps\")\n}\n\nЯ понимаю, что вышеизложенное может не иметь никакого смысла, но мы подробнее расскажем о функциях, пакетах и многом другом позже. А пока давайте запустим наше приложение. Вернувшись в терминал и в нашу папку Hello, мы можем проверить, все ли работает. Используя приведенную ниже команду, мы можем проверить, работает ли наша общая программа обучения.\n\ngo run main.go\n\n\nОднако на этом это не заканчивается, что, если теперь мы захотим взять нашу программу и запустить ее на других машинах с Windows? Мы можем сделать это, создав наш двоичный файл, используя следующую команду\n\ngo build main.go\n\n\n\nПопробуем запустить\n\n#Windows\n./main.exe\n#Linux/Mac Os\n./main\n\nИсточники\n\nБыстрое погружение в Golang\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nУвидимся на 9-й день\n",
            "tags": [
                "devops",
                "golang",
                "hello-world"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day09",
            "title": "9. Как работает hello-world на Golang",
            "description": "Как работает hello-world на Golang",
            "content": "\nРазберемся как работает hello-world\n\nКак работает Go\n\nВчера мы прошли процедуру установки Go на ПК, а затем создали наше первое приложение Go.\n\nВ этом разделе мы собираемся глубже изучить код и понять еще несколько вещей о языке Go.\n\nЧто такое компиляция?\nПрежде чем мы перейдем к 6 строкам кода Hello World, которые написали вчера, нам нужно немного разобраться в компиляции.\n\nЯзыки программирования, которые мы обычно используем, такие как Python, Java, Go и C++, являются языками высокого уровня. Это означает, что они удобочитаемы для человека, но когда машина пытается выполнить программу, она должна быть в форме, понятной машине. Мы должны перевести наш человекочитаемый код в машинный код, что называется компиляцией.\n\n\n\n\nИз приведенного выше вы можете видеть, что мы сделали в День 8 - мы создали простой Hello World main.go, а затем использовали команду go build main.go для компиляции нашего исполняемого файла.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"Hello #90DaysOfDevOps\")\n}\n\nЧто такое пакеты?\nПакет — это набор исходных файлов в одном каталоге, которые скомпилированы вместе. Мы можем упростить это еще больше, пакет — это набор файлов .go в одном каталоге. Помните нашу папку Hello из Дня 8? Когда вы попадете в более сложные программы Go, вы можете обнаружить, что у вас есть папка1, папка2 и папка3, содержащие разные файлы .go, которые составляют вашу программу с несколькими пакетами.\n\nМы используем пакеты, чтобы мы могли повторно использовать код других людей, нам не нужно писать все с нуля. Возможно, нам нужен калькулятор как часть нашей программы, вы, вероятно, могли бы найти существующий пакет Go, содержащий математические функции, которые вы могли бы импортировать в свой код, что в конечном итоге сэкономит вам много времени и усилий.\n\nGo рекомендует организовывать код в пакеты, чтобы его было легко повторно использовать и поддерживать исходный код.\n\nHello #90DaysOfDevOps шаг за шагом\nТеперь давайте посмотрим на наш файл main.go Hello #90DaysOfDevOps и пройдемся по строкам.\n\n\nВ первой строке у нас есть package main, что означает, что этот файл принадлежит пакету с именем main. Все файлы .go должны принадлежать пакету, они также должны иметь «package something» в открывающей строке.\n\nПакет можно назвать как угодно. Мы должны назвать этот main, так как это начальная точка программы, которая будет в этом пакете, это правило.\n\n\nВсякий раз, когда мы хотим скомпилировать и выполнить наш код, мы должны сообщить машине, где должно начаться выполнение. Мы делаем это, написав функцию с именем main. Машина будет искать функцию с именем main, чтобы найти точку входа в программу.\n\nФункция — это блок кода, который может выполнять определенную задачу и может использоваться во всей программе.\n\nВы можете объявить функцию с любым именем, используя func, но в этом случае нам нужно назвать ее main, так как именно здесь начинается код.\n\n\n\n\nДалее мы рассмотрим строку 3 нашего кода, импорт, это в основном означает, что вы хотите добавить другой пакет в свою основную программу. fmt — это стандартный пакет, используемый здесь, предоставленный Go, этот пакет содержит функцию Println(), и, поскольку мы импортировали ее, мы можем использовать ее в строке 6. Существует ряд стандартных пакетов, которые вы можете включить в свою программу и используйте или повторно используйте их в своем коде, избавляя вас от необходимости писать с нуля.\n\n\nPrintln(), который у нас есть, — это способ записи в стандартный вывод на терминал, где когда-либо исполняемый файл был успешно выполнен. Не стесняйтесь изменять сообщение между скобками ().\n\nTLDR\n\nЧто такое TLDR\n\nСтрока 1** = Этот файл будет находиться в пакете с именем main, и его нужно назвать main, поскольку он включает точку входа программы.\nСтрока 3** = Чтобы использовать Println(), мы должны импортировать пакет fmt, чтобы использовать его в строке 6.\nСтрока 5** = фактическая начальная точка, это функция main.\nСтрока 6** = Это позволит нам напечатать «Hello #90DaysOfDevOps» в нашей системе.\n\nИсточники\n\nСтандартная библиотека Go\nGolang | Все Основы за 4 Часа Для Начинающих\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nУвидимся на 10-й день\n\n",
            "tags": [
                "devops",
                "golang",
                "hello-world"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day10",
            "title": "10. Окружение Go",
            "description": "Окружение Go",
            "content": "\nОкружение Go\n\nВ 8-м дне мы кратко рассмотрели рабочее пространство Go, чтобы запустить его и перейти к демонстрации «Hello #90DaysOfDevOps». Но мы должны немного рассказать о рабочем пространстве Go.\n\nПомните, что мы выбрали значения по умолчанию, а затем прошли и создали нашу папку Go в GOPATH, который уже был определен, но на самом деле этот GOPATH можно изменить, чтобы он находился там, где вы хотите.\n\nЕсли вы запустите\necho $GOPATH\nВывод должен быть похож на мой (может быть с другим именем пользователя), а именно:\n\n/home/michael/projects/go\nЗатем здесь мы создали 3 директории. src, pkg и bin\n\n\n\nsrc is where all of your Go programs and projects are stored. This handles namespacing package management for all your Go repositories. This is where you will see on our workstation we have our Hello folder for the Hello #90DaysOfDevOps project.\n\n\n\npkg — это место, где хранятся ваши заархивированные файлы пакетов, которые установлены или были установлены в программах. Это помогает ускорить процесс компиляции в зависимости от того, были ли изменены используемые пакеты.\n\n\nbin — это место, где хранятся все ваши скомпилированные двоичные файлы.\n\n\n\nНаш Hello #90DaysOfDevOps не является сложной программой, поэтому вот пример более сложной программы Go, взятой из другого замечательного ресурса, на который стоит обратить внимание GoChronicles\n\nКомпиляция и запуск кода\nНа 9-й день мы также рассмотрели краткое введение в компиляцию кода, но здесь мы можем пойти немного глубже.\n\nЧтобы запустить наш код, мы сначала должны его скомпилировать. В Go это можно сделать тремя способами.\ngo build\ngo install\ngo run\n\nПрежде чем мы перейдем к описанному выше этапу компиляции, нам нужно взглянуть на то, что мы получаем при установке Go.\n\nКогда мы установили Go на 8-й день, мы установили что-то, известное как инструменты Go, которые состоят из нескольких программ, которые позволяют нам создавать и обрабатывать наши исходные файлы Go. Одним из инструментов является «Go».\n\nСтоит отметить, что вы можете установить дополнительные инструменты, которых нет в стандартной установке Go.\n\nЕсли вы откроете командную строку и наберете «go», вы должны увидеть что-то вроде изображения ниже, а затем вы увидите «Дополнительные разделы справки» ниже, и пока нам не нужно беспокоиться об этом.\n\n\n\nВозможно, вы также помните, что мы уже использовали как минимум два из этих инструментов в День 8.\n\n\nМы хотим узнать больше о сборке, установке и запуске.\n\n\n\ngo run - Эта команда компилирует и запускает основной пакет, состоящий из файлов .go, указанных в командной строке. Команда компилируется во временную папку.\ngo build - чтобы скомпилировать пакеты и зависимости, скомпилируйте пакет в текущем каталоге. Если пакет «main», поместит исполняемый файл в текущий каталог, если нет, то он поместит исполняемый файл в папку «pkg». go build также позволяет вам создать исполняемый файл для любой платформы ОС, поддерживаемой Go.\ngo install - то же самое, что и go build, но помещает исполняемый файл в папку bin\n\nМы прошли через go build и go run, но не стесняйтесь запускать их снова здесь, если хотите, go install, как указано выше, помещает исполняемый файл в нашу папку bin.\n\n\nНадеюсь, что вы следите за мной и смотрите один из плейлистов или видеороликов ниже. Я беру их по кусочкам и перевожу в свои заметки, чтобы понять основы языка Голанг. Приведенные ниже ресурсы, вероятно, дадут вам гораздо лучшее понимание многих областей, которые вам нужны в целом, но я пытаюсь задокументировать 7 дней или 7 часов путешествия с интересными вещами, которые я нашел.\nИсточники\n\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nУвидимся на 11-й день\n",
            "tags": [
                "devops",
                "golang"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day11",
            "title": "11. Переменные и константы в Go",
            "description": "Переменные и константы в Go",
            "content": "\nПрежде чем мы перейдем к темам сегодняшнего дня, я хочу выразить огромную благодарность Techworld with Nana и этому фантастическому краткому путешествию по основам Go.\n\nВ 8-м дне мы настроили нашу среду, в 9-м дне мы разобрали код Hello #90DaysOfDevOps, а в 10-м дне) мы поработали с нашей рабочей средой Go и немного углубились в компиляцию и запуск кода.\n\nСегодня мы рассмотрим переменные, константы и типы данных при написании новой программы.\n\nПеременные и константы в Go\nДавайте начнем с планирования нашего приложения, я думаю, было бы неплохо поработать над программой, которая сообщает нам, сколько дней осталось в нашем испытании #90DaysOfDevOps.\n\nПервое, что нужно учитывать, это то, что, поскольку мы создаем наше приложение, мы приветствуем наших посетителей и даем пользователям отзывы о количестве дней, которые они выполнили, мы можем использовать термин #90DaysOfDevOps много раз на протяжении всей программы. Это отличный вариант использования переменной #90DaysOfDevOps в нашей программе.\n\nПеременные используются для хранения значений.\nКак маленькая коробка с нашей сохраненной информацией или ценностями.\nЗатем мы можем использовать эту переменную во всей программе, что также выгодно тем, что если эта задача или переменная изменится, нам нужно будет изменить это только в одном месте. Это означает, что мы могли бы перенести это на другие проблемы, с которыми мы сталкиваемся в сообществе, просто изменив значение этой переменной.\n\nЧтобы объявить это в нашей программе Go, мы определяем значение, используя ключевое слово для переменных. Это будет жить в нашем блоке кода func main, который вы увидите позже. Подробнее о Ключевых словах можно узнать здесь.\n\nНе забудьте убедиться, что ваши имена переменных являются понятными. Если вы объявляете переменную, вы должны использовать ее, иначе вы получите ошибку. Это делается для того, чтобы избежать возможного неиспользованного кода. То же самое для неиспользуемых пакетов.\n\nvar challenge = \"#90DaysOfDevOps\"\nС приведенным выше набором и использованием, как мы увидим в следующем фрагменте кода, вы можете видеть из вывода ниже, что мы использовали переменную.\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    fmt.Println(\"Welcome to\", challenge \"\")\n}\nЗатем вы увидите ниже, что мы построили наш код с помощью приведенного выше примера и получили вывод, показанный ниже.\n\n\nМы также знаем, что наш челендж длится как минимум 90 дней для этой задачи, но в следующей, может быть, будет 100, поэтому мы хотим определить переменную, которая поможет нам. Однако для нашей программы мы хотим определить это как константу. Константы похожи на переменные, за исключением того, что их значение не может быть изменено в коде (мы все еще можем создать новое приложение позже с этим кодом и изменить эту константу, но это 90 не изменится, пока мы запускаем наше приложение)\n\nДобавим const в наш код и добавим еще одну строку кода, чтобы напечатать результат.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    const daystotal = 90\n\n    fmt.Println(\"Welcome to\", challenge)\n    fmt.Println(\"This is a\", daystotal, \"challenge\")\n}\n\nЕсли мы затем снова пройдем этот процесс go build и запустим, вы увидите результат.\n\n\n\nНо это не будет концом нашей программы, мы вернемся к ней в 12-м дне, чтобы добавить больше функциональности. Теперь мы хотим добавить еще одну переменную для количества дней, в течение которых мы выполнили задание.\n\nНиже я добавил переменную dayscomplete с количеством завершенных дней.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n    var challenge = \"#90DaysOfDevOps\"\n    const daystotal = 90\n    var dayscomplete = 11\n\n    fmt.Println(\"Welcome to\", challenge, \"\")\n    fmt.Println(\"This is a\", daystotal, \"challenge and you have completed\", dayscomplete, \"days\")\n    fmt.Println(\"Great work\")\n}\nДавайте снова запустим go build, или вы можете просто использовать go run\n\n\n\nВот несколько других примеров, которые я использовал, чтобы упростить чтение и редактирование кода. До сих пор мы использовали Println, но мы можем упростить это, используя Printf, используя %v, что означает, что мы определяем наши переменные по порядку в конце строки кода. мы также используем \\n для разрыва строки.\n\nЯ использую %v, поскольку здесь используется значение по умолчанию, но есть и другие параметры, которые можно найти документации пакета fmt.\n\nПеременные также могут быть определены в вашем коде в более простом формате. Вместо того, чтобы определять, что это var и type, вы можете закодировать это следующим образом, чтобы получить ту же функциональность, но более чистый и простой вид вашего кода. Это будет работать только для переменных, а не для констант.\n\nfunc main() {\n    challenge := \"#90DaysOfDevOps\"\n    const daystotal = 90\n\nТипы в  Go\nВ приведенных выше примерах мы не определили тип переменных, это потому, что мы можем задать им значение, Go достаточно умен, чтобы знать, что это за тип, или, по крайней мере, может сделать вывод, что это на основе значения, которое вы сохранили. . Однако, если мы хотим, чтобы пользователь ввел данные, для этого потребуется определенный тип.\n\nДо сих пор в нашем коде использовались строки и целые числа. Целые числа для количества дней и строки для названия задачи.\n\nТакже важно отметить, что каждый тип данных может выполнять разные действия и вести себя по-разному. Например, целые числа могут умножаться там, где нет строк.\n\nЕсть четыре категории\n\nBasic type**: в эту категорию попадают числа, строки и логические значения.\nAggregate type**: к этой категории относятся массивы и структуры.\nReference type**: в эту категорию попадают указатели, срезы, карты, функции и каналы.\nInterface type**\n\nТип данных — важная концепция в программировании. Тип данных определяет размер и тип значений переменных.\n\nGo статически типизирован, а это означает, что после определения типа переменной он может хранить данные только этого типа.\n\nВ Go есть три основных типа данных:\n\nbool**: представляет логическое значение и может быть либо истинным, либо ложным.\nNumeric**: представляет целые типы, значения с плавающей запятой и сложные типы.\nstring**: представляет строковое значение.\n\nЯ нашел этот ресурс очень подробным о типах данных Golang by example\n\nЯ бы также посоветовал Techworld with Nana на этом этапе довольно подробно рассказать о типах данных в Go.\n\nЕсли нам нужно определить тип в нашей переменной, мы можем сделать это так:\n\n\nvar TwitterHandle string\nvar DaysCompleted uint\nПоскольку Go принимает переменные, которым задано значение, мы можем распечатать эти значения следующим образом:\n\nfmt.Printf(\"challenge is %T, daystotal is %T, dayscomplete is %T\\n\", conference, daystotal, dayscomplete)\nСуществует много различных типов целых чисел и типов с плавающей запятой, ссылки выше подробно описывают их.\n\nint** = целые числа\nunint** = беззнаковые целые числа\nfloating point types** = числа с плавающей запятой\n\nИсточники\n\nВведение в Golang\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nДалее мы начнем добавлять в нашу программу некоторые функции пользовательского ввода, чтобы программа спрашивала, сколько дней было завершено.\n\nУвидимся завтра.\n",
            "tags": [
                "devops",
                "golang"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day12",
            "title": "12. Golang - чтение данных и указатели",
            "description": "Получение пользовательского ввода с помощью указателей и готовой программы",
            "content": "\nПолучение данных с клавиуатуры\n\nВчера (Днем 11-м) мы создали нашу первую программу Go, и данные, которые мы хотели получить от пользователя, были созданы как переменные в нашем коде. Теперь мы хотим спросить пользователя данные для ввода, чтобы дать переменной значение для конечного сообщения.\n\nПолучение пользовательских данных\n\nПрежде чем мы это сделаем, давайте еще раз взглянем на наше приложение и пройдемся по переменным, которые нам нужны в качестве теста, прежде чем получить этот пользовательский ввод.\n\nДавайте теперь добавим новую переменную с именем TwitterName, вы можете найти этот новый код ниже, и если мы запустим этот код, это будет наш вывод.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\t\n\tchallenge := \"#90DaysOfDevOps\"\n\tconst daystotal = 90\n\n\tfmt.Printf(\"Welcome to %v\\n\", challenge)\n\tfmt.Printf(\"This is a %v challenge\\n\", daystotal)\n\n\tvar TwitterName string\n\tvar DaysComplete int\n\t// ask user for their twitter handle\n\n\tTwitterName = \"@MichaelCade1\"\n\tDaysComplete = 12\n\tfmt.Printf(\"%v has completed %v days of the challenge\\n\", TwitterName, DaysComplete)\n\tfmt.Println(\"Great work\")\n}\nПрежде чем мы это сделаем, давайте еще раз взглянем на наше приложение и пройдемся по переменным, которые нам нужны в качестве теста, прежде чем получить этот пользовательский ввод.\n\nВчера мы закончили с нашим кодом, выглядящим так:\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\tvar challenge = \"#90DaysOfDevOps\"\n\tconst daystotal = 90\n\tvar dayscomplete = 11\n\n\tfmt.Printf(\"Welcome to %v\\n\", challenge)\n\tfmt.Printf(\"This is a %v challenge and you have completed %v days\\n\", daystotal, dayscomplete)\n\tfmt.Println(\"Great work\")\n}\nМы вручную определили в коде наши переменные и константы challenge, daystotal, dayscomplete.\n\nДавайте теперь добавим новую переменную с именем TwitterName\n\n\n\nУ нас 12-й день, и нам нужно было бы менять dayscomplete каждый день и компилировать наш код каждый день, если бы он был жестко запрограммирован, что звучит не так уж здорово.\n\nПолучая пользовательский ввод, мы хотим получить значение, возможно, имя и количество завершенных дней. Для этого мы можем использовать другую функцию из пакета fmt.\n\nКратко о пакете fmt, различные функции для: форматированного ввода и вывода (I/O) (input and output)\n\nПечать сообщений\nСобирать пользовательский ввод\nЗаписать в файл\n\nЭто вместо того, чтобы присваивать значение переменной, мы хотим попросить пользователя ввести его.\n\n\nfmt.Scan(&TwitterName)\n\nОбратите внимание, что мы также используем & перед переменной. Этот символ известен как указатель, который мы рассмотрим в следующем разделе.\n\nВ нашем коде вы можете видеть, что мы просим пользователя ввести две переменные, TwitterName и DaysCompleted\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\n\tconst DaysTotal int = 90\n\tchallenge := \"#90DaysOfDevOps\"\n\n\tfmt.Printf(\"Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\", challenge, DaysTotal)\n\n\tvar TwitterName string\n\tvar DaysCompleted uint\n\n\t// asking for user input\n\tfmt.Println(\"Enter Your Twitter Handle: \")\n\tfmt.Scanln(&TwitterName)\n\n\tfmt.Println(\"How many days have you completed?: \")\n\tfmt.Scanln(&DaysCompleted)\n\n\tfmt.Printf(\"Thank you %v for taking part and completing %v days.\\n\", TwitterName, DaysCompleted)\n\tfmt.Println(\"Good luck\")\n}\nДавайте теперь запустим нашу программу, и вы увидите, что у нас есть входные данные для обоих вышеперечисленных.\n\n\n\nХорошо, мы получили некоторый пользовательский ввод и напечатали сообщение, но как насчет того, чтобы заставить нашу программу сообщать нам, сколько дней у нас осталось в нашей задаче.\n\nДля этого мы создали переменную с именем remainingDays, и мы жестко оценили ее в нашем коде как 90. Затем нам нужно изменить значение этого значения, чтобы распечатать remainingDays, когда мы получим пользовательский ввод DaysCompleted мы можем сделать это с помощью этого простого изменения переменной.\n\nremainingDays = remainingDays - DaysCompleted\n\nНаша программа теперь выглядит вот так:\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\n\tconst DaysTotal int = 90\n\tvar remainingDays uint = 90\n\tchallenge := \"#90DaysOfDevOps\"\n\n\tfmt.Printf(\"Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\", challenge, DaysTotal)\n\n\tvar TwitterName string\n\tvar DaysCompleted uint\n\n\t// asking for user input\n\tfmt.Println(\"Enter Your Twitter Handle: \")\n\tfmt.Scanln(&TwitterName)\n\n\tfmt.Println(\"How many days have you completed?: \")\n\tfmt.Scanln(&DaysCompleted)\n\n\t// calculate remaining days\n\tremainingDays = remainingDays - DaysCompleted\n\n\tfmt.Printf(\"Thank you %v for taking part and completing %v days.\\n\", TwitterName, DaysCompleted)\n\tfmt.Printf(\"You have %v days remaining for the %v challenge\\n\", remainingDays, challenge)\n\tfmt.Println(\"Good luck\")\n}\n\nЕсли мы теперь запустим эту программу, вы увидите, что простой расчет выполняется на основе пользовательского ввода и значения remainingDays\n\nЧто такое указатель? (Специальные переменные)\n\nУказатель — это (специальная) переменная, которая указывает на адрес памяти другой переменной.\n\nОтличное объяснение этого можно найти здесь geeksforgeeks\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\tvar challenge = \"#90DaysOfDevOps\"\n\n\tfmt.Println(challenge)\n\tfmt.Println(&challenge)\n\n}\nНиже выполняется этот код.\n\nРесурсы\nВведение в Golang\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\n\nУвидимся завтра.\n\n\n",
            "tags": [
                "devops",
                "golang"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day13",
            "title": "13. Go - подключение Twitter API",
            "description": "Go - подключение Twitter API",
            "content": "\nТвитните о своем прогрессе с нашим новым приложением\n\nВ последний день изучения этого языка программирования мы только коснулись его основ, но я думаю, что это начало.\n\nЗа последние несколько дней мы взяли небольшую идею для приложения и добавили функциональность, в этой статье я хочу воспользоваться преимуществами тех пакетов, которые мы упомянули, и создать функциональность для нашего приложения, чтобы не только дать вам обновление вашего прогресса на экране, но также отправьте твит с подробностями задачи и вашим статусом.\n\nДобавление возможности твитить свой прогресс\nПервое, что нам нужно сделать, это настроить доступ API разработчика к Twitter, чтобы это работало.\n\nПерейдите на Платформу разработчиков Twitter и войдите в систему, используя свой идентификатор Twitter и данные. Оказавшись внутри, вы должны увидеть что-то вроде приведенного ниже без приложения, которое я уже создал.\n\n\n\nЗдесь вы также можете запросить дополнительный доступ. Это может занять некоторое время, но для меня это было очень быстро.\n\nЗатем мы должны выбрать «Projects & Apps» и создать наше приложение. Ограничения зависят от доступа к вашей учетной записи, при этом у вас должно быть только одно приложение и один проект, а с повышенными правами у вас может быть 3 приложения.\n\n\n\nДайте вашему приложению имя\n\n\n\nЗатем вам будут предоставлены эти токены API, важно сохранить их в безопасном месте. (С тех пор я удалил это приложение) Они понадобятся нам позже с нашим приложением Go.\n\n\n\nТеперь у нас создано наше приложение (мне пришлось изменить имя моего приложения, так как то, что на скриншоте выше, уже было сделано, эти имена должны быть уникальными)\n\n\n\nКлючи, которые мы собрали ранее, известны как наши потребительские ключи, и нам также понадобятся наш токен доступа и секреты. Мы можем собрать эту информацию, используя вкладку «Ключи и токены».\n\n\n\nХорошо, на данный момент мы закончили работу с порталом для разработчиков Twitter. Убедитесь, что вы сохранили свои ключи, потому что они понадобятся нам позже.\n\nПерейти Twitter бот\n\nПомните код, который мы запускаем в нашем приложении?\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\n\tconst DaysTotal int = 90\n\tvar remainingDays uint = 90\n\tchallenge := \"#90DaysOfDevOps\"\n\n\tfmt.Printf(\"Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\", challenge, DaysTotal)\n\n\tvar TwitterName string\n\tvar DaysCompleted uint\n\n\t// asking for user input\n\tfmt.Println(\"Enter Your Twitter Handle: \")\n\tfmt.Scanln(&TwitterName)\n\n\tfmt.Println(\"How many days have you completed?: \")\n\tfmt.Scanln(&DaysCompleted)\n\n\t// calculate remaining days\n\tremainingDays = remainingDays - DaysCompleted\n\n\tfmt.Printf(\"Thank you %v for taking part and completing %v days.\\n\", TwitterName, DaysCompleted)\n\tfmt.Printf(\"You have %v days remaining for the %v challenge\\n\", remainingDays, challenge)\n\tfmt.Println(\"Good luck\")\n}\n\n\nТеперь нам нужно подумать о коде для отправки нашего вывода или сообщения в Twitter в виде твита. Мы будем использовать go-twitter. Это клиентская библиотека Go для Twitter API.\n\nЧтобы проверить это, прежде чем помещать это в наше основное приложение, я создал новый каталог в нашей папке src с именем go-twitter-bot, запустил go mod init github.com/michaelcade/go-twitter-bot в папке который затем создал файл go.mod, а затем мы можем начать писать наш новый main.go и протестировать его.\n\nТеперь нам нужны те ключи, токены и секреты, которые мы собрали на портале разработчиков Twitter. Мы собираемся установить их в наших переменных среды. Это будет зависеть от ОС, которую вы используете:\n\nWindows\nset CONSUMER_KEY\nset CONSUMER_SECRET\nset ACCESS_TOKEN\nset ACCESS_TOKEN_SECRET\n\nLinux / macOS\nexport CONSUMER_KEY\nexport CONSUMER_SECRET\nexport ACCESS_TOKEN\nexport ACCESS_TOKEN_SECRET\nAt this stage, you can take a look at day13_example2 at the code but you will see here that we are using a struct to define our keys, secrets and tokens.\n\nWe then have a func to parse those credentials and make that connection to the Twitter API\n\nThen based on the success we will then send a tweet.\n\nНа этом этапе вы можете взглянуть на следующий код\npackage main\n\nimport (\n\t// other imports\n\t\"fmt\"\n\t\"log\"\n\t\"os\"\n\n\t\"github.com/dghubble/go-twitter/twitter\"\n\t\"github.com/dghubble/oauth1\"\n)\n\n// Credentials stores all of our access/consumer tokens\n// and secret keys needed for authentication against\n// the twitter REST API.\ntype Credentials struct {\n\tConsumerKey       string\n\tConsumerSecret    string\n\tAccessToken       string\n\tAccessTokenSecret string\n}\n\n// getClient is a helper function that will return a twitter client\n// that we can subsequently use to send tweets, or to stream new tweets\n// this will take in a pointer to a Credential struct which will contain\n// everything needed to authenticate and return a pointer to a twitter Client\n// or an error\nfunc getClient(creds Credentials) (twitter.Client, error) {\n\t// Pass in your consumer key (API Key) and your Consumer Secret (API Secret)\n\tconfig := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret)\n\t// Pass in your Access Token and your Access Token Secret\n\ttoken := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret)\n\n\thttpClient := config.Client(oauth1.NoContext, token)\n\tclient := twitter.NewClient(httpClient)\n\n\t// Verify Credentials\n\tverifyParams := &twitter.AccountVerifyParams{\n\t\tSkipStatus:   twitter.Bool(true),\n\t\tIncludeEmail: twitter.Bool(true),\n\t}\n\n\t// we can retrieve the user and verify if the credentials\n\t// we have used successfully allow us to log in!\n\tuser, _, err := client.Accounts.VerifyCredentials(verifyParams)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog.Printf(\"User's ACCOUNT:\\n%+v\\n\", user)\n\treturn client, nil\n}\nfunc main() {\n\tfmt.Println(\"Go-Twitter Bot v0.01\")\n\tcreds := Credentials{\n\t\tAccessToken:       os.Getenv(\"ACCESS_TOKEN\"),\n\t\tAccessTokenSecret: os.Getenv(\"ACCESS_TOKEN_SECRET\"),\n\t\tConsumerKey:       os.Getenv(\"CONSUMER_KEY\"),\n\t\tConsumerSecret:    os.Getenv(\"CONSUMER_SECRET\"),\n\t}\n\n\tclient, err := getClient(&creds)\n\tif err != nil {\n\t\tlog.Println(\"Error getting Twitter Client\")\n\t\tlog.Println(err)\n\t}\n\n\ttweet, resp, err := client.Statuses.Update(\"A Test Tweet from the future, testing a #90DaysOfDevOps Program that tweets, tweet tweet\", nil)\n\tif err != nil {\n\t\tlog.Println(err)\n\t}\n\tlog.Printf(\"%+v\\n\", resp)\n\tlog.Printf(\"%+v\\n\", tweet)\n}\n\nЗдесь вы увидите, что мы используем структуру для определения наших ключей, секретов и токенов.\n\nЗатем у нас есть func, чтобы проанализировать эти учетные данные и установить это соединение с API Twitter.\n\nЗатем, в зависимости от успеха, мы отправим твит.\n\n\n\nКод выше либо выдаст вам ошибку в зависимости от того, что происходит, либо будет выполнен успешно, и вам будет отправлен твит с сообщением, указанным в коде.\n\nСоединение двух вместе - Go-Twitter-Bot + наше приложение\n\nТеперь нам нужно объединить эти два файла в наш main.go. Я уверен, что кто-то кричит, что есть лучший способ сделать это, и, пожалуйста, прокомментируйте это, поскольку вы можете иметь более одного файла .go в одном файле. project это может иметь смысл, но это работает.\n\nТак выглядит итоговый рзультат:\n\npackage main\n\nimport (\n    // other imports\n    \"fmt\"\n    \"log\"\n    \"os\"\n\n    \"github.com/dghubble/go-twitter/twitter\"\n    \"github.com/dghubble/oauth1\"\n)\n\n// Credentials stores all of our access/consumer tokens\n// and secret keys needed for authentication against\n// the twitter REST API.\ntype Credentials struct {\n    ConsumerKey       string\n    ConsumerSecret    string\n    AccessToken       string\n    AccessTokenSecret string\n}\n\n// getClient is a helper function that will return a twitter client\n// that we can subsequently use to send tweets, or to stream new tweets\n// this will take in a pointer to a Credential struct which will contain\n// everything needed to authenticate and return a pointer to a twitter Client\n// or an error\nfunc getClient(creds Credentials) (twitter.Client, error) {\n    // Pass in your consumer key (API Key) and your Consumer Secret (API Secret)\n    config := oauth1.NewConfig(creds.ConsumerKey, creds.ConsumerSecret)\n    // Pass in your Access Token and your Access Token Secret\n    token := oauth1.NewToken(creds.AccessToken, creds.AccessTokenSecret)\n\n    httpClient := config.Client(oauth1.NoContext, token)\n    client := twitter.NewClient(httpClient)\n\n    // Verify Credentials\n    verifyParams := &twitter.AccountVerifyParams{\n        SkipStatus:   twitter.Bool(true),\n        IncludeEmail: twitter.Bool(true),\n    }\n\n    // we can retrieve the user and verify if the credentials\n    // we have used successfully allow us to log in!\n    user, _, err := client.Accounts.VerifyCredentials(verifyParams)\n    if err != nil {\n        return nil, err\n    }\n\n    log.Printf(\"User's ACCOUNT:\\n%+v\\n\", user)\n    return client, nil\n}\nfunc main() {\n    creds := Credentials{\n        AccessToken:       os.Getenv(\"ACCESS_TOKEN\"),\n        AccessTokenSecret: os.Getenv(\"ACCESS_TOKEN_SECRET\"),\n        ConsumerKey:       os.Getenv(\"CONSUMER_KEY\"),\n        ConsumerSecret:    os.Getenv(\"CONSUMER_SECRET\"),\n    }\n    {\n        const DaysTotal int = 90\n        var remainingDays uint = 90\n        challenge := \"#90DaysOfDevOps\"\n\n        fmt.Printf(\"Welcome to the %v challenge.\\nThis challenge consists of %v days\\n\", challenge, DaysTotal)\n\n        var TwitterName string\n        var DaysCompleted uint\n\n        // asking for user input\n        fmt.Println(\"Enter Your Twitter Handle: \")\n        fmt.Scanln(&TwitterName)\n\n        fmt.Println(\"How many days have you completed?: \")\n        fmt.Scanln(&DaysCompleted)\n\n        // calculate remaining days\n        remainingDays = remainingDays - DaysCompleted\n\n        //fmt.Printf(\"Thank you %v for taking part and completing %v days.\\n\", TwitterName, DaysCompleted)\n        //fmt.Printf(\"You have %v days remaining for the %v challenge\\n\", remainingDays, challenge)\n        // fmt.Println(\"Good luck\")\n\n        client, err := getClient(&creds)\n        if err != nil {\n            log.Println(\"Error getting Twitter Client, this is expected if you did not supply your Twitter API tokens\")\n            log.Println(err)\n        }\n\n        message := fmt.Sprintf(\"Hey I am %v I have been doing the %v for %v days and I have %v Days left\", TwitterName, challenge, DaysCompleted, remainingDays)\n        tweet, resp, err := client.Statuses.Update(message, nil)\n        if err != nil {\n            log.Println(err)\n        }\n        log.Printf(\"%+v\\n\", resp)\n        log.Printf(\"%+v\\n\", tweet)\n    }\n\n}\n\nРезультатом этого должен быть твит, но если вы не указали свои переменные среды, вы должны получить сообщение об ошибке, подобное приведенному ниже.\n\n\n\nПосле того, как вы исправите это или решите не проходить аутентификацию в Twitter, вы можете использовать код, с которым мы закончили вчера. Вывод терминала в случае успеха будет выглядеть примерно так:\n\n\n\nПолученный твит должен выглядеть примерно так:\n\nКак скомпилировать для нескольких ОС\n\nДалее я хочу затронуть вопрос: «Как компилировать для нескольких операционных систем?» Отличительной особенностью Go является то, что он может легко компилироваться для многих различных операционных систем. Вы можете получить полный список, выполнив следующую команду:\n\ngo tool dist list\n\nИспользование наших команд go build до сих пор было замечательным, и оно будет использовать переменные среды GOOS и GOARCH, чтобы определить хост-компьютер и то, для чего должна быть собрана сборка. Но мы также можем создавать другие двоичные файлы, используя приведенный ниже код в качестве примера.\n\n\nGOARCH=amd64 GOOS=darwin go build -o ${BINARY_NAME}_0.1_darwin main.go\nGOARCH=amd64 GOOS=linux go build -o ${BINARY_NAME}_0.1_linux main.go\nGOARCH=amd64 GOOS=windows go build -o ${BINARY_NAME}_0.1_windows main.go\nGOARCH=arm64 GOOS=linux go build -o ${BINARY_NAME}_0.1_linux_arm64 main.go\nGOARCH=arm64 GOOS=darwin go build -o ${BINARY_NAME}_0.1_darwin_arm64 main.go\n\nЭто даст вам двоичные файлы в вашем каталоге для всех вышеперечисленных платформ. Затем вы можете взять это и создать make-файл для создания этих двоичных файлов всякий раз, когда вы добавляете новые функции и функции в свой код.\n\nФайл: makefile\nBINARY_NAME=90DaysOfDevOps\n\nbuild:\n\tGOARCH=amd64 GOOS=darwin go build -o ${BINARY_NAME}_0.2_darwin main.go\n\tGOARCH=amd64 GOOS=linux go build -o ${BINARY_NAME}_0.2_linux main.go\n\tGOARCH=amd64 GOOS=windows go build -o ${BINARY_NAME}_0.2_windows main.go\n\tGOARCH=arm64 GOOS=linux go build -o ${BINARY_NAME}_0.2_linux_arm64 main.go\n\tGOARCH=arm64 GOOS=darwin go build -o ${BINARY_NAME}_0.2_darwin_arm64 main.go\n\nrun:\n\t./${BINARY_NAME}\n\nbuild_and_run: build run\n\nclean:\n\tgo clean\n\trm ${BINARY_NAME}-darwin\n\trm ${BINARY_NAME}-linux\n\trm ${BINARY_NAME}-windows\nИсточники\n\nStackOverflow 2021 Developer Survey\nWhy we are choosing Golang to learn\nJake Wright - Learn Go in 12 minutes\nTechworld with Nana - Golang full course - 3 hours 24 mins\nNOT FREE Nigel Poulton Pluralsight - Go Fundamentals - 3 hours 26 mins\nFreeCodeCamp -  Learn Go Programming - Golang Tutorial for Beginners\nHitesh Choudhary - Complete playlist\nA great repo full of all things DevOps & exercises\nGoByExample - Example based learning\ngo.dev/tour/list\ngo.dev/learn\n\nНа этом блок \"язык программирования\". Так много всего, что можно охватить, и я надеюсь, что вы смогли продолжить изучение вышеизложенного и понять некоторые другие аспекты языка программирования Go.\n\nЗатем мы сосредоточимся на Linux и некоторых основах, которые мы все должны знать.",
            "tags": [
                "devops",
                "golang"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day14",
            "title": "14. DevOps и Linux",
            "description": "DevOps и Linux",
            "content": "\nОбщая картина: DevOps и Linux\nLinux и DevOps имеют очень схожие культуры и взгляды; оба ориентированы на настройку и масштабируемость. Оба эти аспекта Linux имеют особое значение для DevOps.\n\nМногие технологии начинаются с Linux, особенно если они связаны с разработкой программного обеспечения или управлением инфраструктурой.\n\nКроме того, многие проекты с открытым исходным кодом, особенно инструменты DevOps, с самого начала разрабатывались для работы в Linux.\n\nС точки зрения DevOps или фактически с точки зрения какой-либо операционной роли вы столкнетесь с Linux, я бы сказал, в основном. Есть место для WinOps, но большую часть времени вы будете администрировать и развертывать серверы Linux.\n\nЯ использую Linux ежедневно в течение нескольких лет, но мой настольный компьютер всегда был либо macOS, либо Windows. Однако, когда я перешел на роль Cloud Native, в которой я сейчас нахожусь, я сделал решительный шаг, чтобы убедиться, что мой ноутбук полностью основан на Linux и является моим ежедневным драйвером, в то время как мне по-прежнему нужна была Windows для рабочих приложений и многих моих аудио и видеоаппаратура не работает в Linux Я заставлял себя постоянно работать на рабочем столе Linux, чтобы лучше понять многие вещи, которые мы собираемся затронуть в течение следующих 7 дней.\n\nНачало\nЯ не предлагаю вам делать то же самое, что и я, в любом случае, поскольку есть более простые варианты и менее разрушительные, но я скажу, что этот полный рабочий день заставит вас быстрее научиться тому, как заставить все работать в Linux.\n\nВ течение большей части этих 7 дней я фактически собираюсь развернуть виртуальную машину в Virtual Box на моей машине с Windows. Я также собираюсь развернуть настольную версию дистрибутива Linux, в то время как многие серверы Linux, которыми вы будете администрировать, скорее всего, будут серверами без графического интерфейса и полностью основанными на оболочке. Однако, как я сказал в начале, многие инструменты, которые мы рассмотрели в течение всех этих 90 дней, начинались с Linux, я также настоятельно рекомендую вам погрузиться в работу этого рабочего стола Linux для этого обучения.\n\nВ оставшейся части этого поста мы сосредоточимся на настройке и запуске виртуальной машины Ubuntu Desktop в нашей среде Virtual Box. Теперь мы можем просто загрузить Virtual Box и получить последний Ubuntu ISO с сайтов, на которые даны ссылки, и продолжить сборку. нашу среду рабочего стола, но это не было бы очень DevOps с нашей стороны, не так ли?\n\nЕще одна веская причина использовать большинство дистрибутивов Linux заключается в том, что они бесплатны и имеют открытый исходный код. Мы также выбираем Ubuntu, поскольку это, вероятно, наиболее широко используемый дистрибутив, не думая о мобильных устройствах и корпоративных серверах RedHat Enterprise. Я могу ошибаться, но с CentOS и ее историей я уверен, что Ubuntu занимает первое место в списке, и это очень просто.\n\nHashiCorp Vagrant\n\nVagrant — это утилита CLI, которая управляет жизненным циклом ваших виртуальных машин. Мы можем использовать vagrant для запуска и отключения виртуальных машин на разных платформах, включая vSphere, Hyper-v, Virtual Box, а также Docker. У него есть другие провайдеры, но мы будем придерживаться того, что здесь мы используем Virtual Box, так что все готово.\n\nVagrant — свободное и открытое программное обеспечение для создания и конфигурирования виртуальной среды разработки. Является обёрткой для программного обеспечения виртуализации, например VirtualBox, и средств управления конфигурациями, таких как Chef, Salt и Puppet.\n\nПервое, что нам нужно сделать, это установить Vagrant на нашу машину, когда вы перейдете на страницу загрузок, вы увидите все операционные системы, перечисленные на ваш выбор. HashiCorp Vagrant Я использую Windows, поэтому я взял двоичный файл для своей системы и установил его в свою систему.\n\nДалее нам также нужно установить Virtual Box. Опять же, это также может быть установлено на многих разных операционных системах.\n\nФайл VAGRANTFILE\n\nVAGRANTFILE описывает тип машины, которую мы хотим развернуть. Он также определяет, как мы хотим, чтобы конфигурация и подготовка этой машины выглядели.\n\nКогда дело доходит до их сохранения и организации ваших VAGRANTFILE, я стараюсь помещать их в отдельные папки в своем рабочем пространстве. Ниже вы можете увидеть, как это выглядит в моей системе. Надеюсь, после этого вы поиграете с Vagrant и увидите легкость запуска разных систем, это также отлично подходит для этой кроличьей норы, известной как скачок дистрибутива для Linux Desktops.\n\n\n\n\nДавайте взглянем на этот VAGRANTFILE и посмотрим, что мы строим.\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"chenhan/ubuntu-desktop-20.04\"\n  config.vm.provider :virtualbox do |v|\n   v.memory  = 8096\n   v.cpus    = 4\n   v.customize [\"modifyvm\", :id, \"--vram\", \"128mb\"]\n  end\nend\nЭто очень простой VAGRANTFILE. В целом, мы говорим, что нам нужна конкретная «сборка». Сборка, возможно, является либо общедоступным образом, либо частной сборкой системы, которую вы ищете. Вы можете найти длинный список здесь, в общедоступном каталоге Vagrant\n\nДалее мы говорим, что хотим использовать определенного провайдера, в данном случае это «VirtualBox», а затем мы хотим определить память нашей машины как «8 ГБ, а количество процессоров — как «4». Мой опыт также говорит мне, что вы можете также добавить следующую строку, если у вас возникли проблемы с отображением. Это установит видеопамять на то, что вы хотите, я бы увеличил ее до 128 МБ, но зависит от вашей системы.\n\n\nv.customize [\"modifyvm\", :id, \"--vram\", \"\"]\n\nИнициализация нашего рабочего стола Linux\n\nТеперь мы готовы запустить нашу первую машину в терминале нашего ПК. В моем случае я использую PowerShell на своем компьютере с Windows, перейдите в папку своих проектов и там, где вы найдете свой VAGRANTFILE. Оказавшись там, вы можете ввести команду vagrant up, и если все правильно, вы увидите что-то вроде того, что показано ниже.\n\n\n\nЕще одна вещь, которую следует добавить, это то, что сеть будет настроена на NAT на вашей виртуальной машине, на данном этапе нам действительно не нужно знать о NAT, и я планирую провести целую сессию в следующем разделе о сети. Но знайте, что это просто кнопка, когда дело доходит до включения машины в вашу домашнюю сеть, это также сетевой режим по умолчанию в Virtual Box. Вы можете узнать больше в документации Virtual Box\n\nКак только vagrant up завершен, мы можем использовать vagrant ssh, чтобы перейти прямо в терминал нашей новой виртуальной машины.\n\n\n\nИменно здесь мы будем проводить большую часть наших исследований в течение следующих нескольких дней, но я также хочу погрузиться в некоторые настройки для вашей рабочей станции разработчика, которые я сделал, и это значительно упрощает вашу жизнь при использовании этого в качестве ежедневного драйвера, и, конечно же, а ты реально в DevOps разве что у тебя крутой нестандартный терминал?\n\nНо просто для подтверждения в Virtual Box вы должны увидеть приглашение для входа в систему при выборе виртуальной машины.\n\n\n\nО, и если вы зашли так далеко и спрашивали: «ЧТО ТАКОЕ ИМЯ ПОЛЬЗОВАТЕЛЯ И ПАРОЛЬ?»\n\nUsername = vagrant\nPassword = vagrant\n\nЗавтра мы рассмотрим некоторые команды и то, что они делают. Терминал станет местом, где все произойдет.\n\nРесурсы\n\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need be a hacker!)\nVargant tutorial\n\nКак я уже упоминал, далее мы рассмотрим команды, которые мы можем использовать ежедневно в наших средах Linux.",
            "tags": [
                "devops",
                "linux",
                "virtualbox",
                "vagrant"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day15",
            "title": "15. Команды Linux в DevOps",
            "description": "Команды Linux в DevOps",
            "content": "\nКоманды Linux для DevOps\n\nЯ упомянул вчера, что мы собираемся провести много времени в терминале с некоторыми командами, чтобы что-то сделать.\n\nЯ также упомянул, что с нашей виртуальной машиной, подготовленной  с помощью vagrant, мы можем использовать vagrant ssh и получить доступ к нашей машине. Вам нужно будет находиться в том же каталоге, из которого мы его предоставили.\n\nДля SSH нам не понадобятся имя пользователя и пароль, они понадобятся нам только в том случае, если решим войти в консоль Virtual Box.\n\nВот где мы хотим быть, как показано ниже:\n\nКоманды\n\nОчевидно, что я не могу охватить здесь все команды. Есть тонны документации, которые охватывают их, но также, если вы находитесь в своем терминале, и вам просто нужно понять параметры конкретной команды, у нас есть команда man, сокращенная от manual. Мы можем использовать это, чтобы просмотреть каждую из команд, которые мы коснемся в этом посте, чтобы узнать больше вариантов для каждой из них. Мы можем запустить man man, который поможет вам со страницами руководства. Чтобы выйти из справочных страниц, вы должны нажать q для выхода.\n\nПримеры:\nman ls\nman whoami\n...\n\n\n\n\nsudo Если вы знакомы с Windows и щелкаете правой кнопкой мыши по запустить от имени администратора, мы можем думать о sudo как об этом. Когда вы запускаете команду с помощью этой команды, вы будете запускать ее как «root», она запросит у вас пароль перед запуском команды.\n\n\n\nДля разовых работ, таких как установка приложений или служб, вам может понадобиться эта команда sudo, но что, если у вас есть несколько задач, и вы хотите какое-то время пожить как sudo? Здесь вы можете снова использовать sudo su так же, как sudo, после ввода вам будет предложено ввести пароль root. В тестовой виртуальной машине, такой как наша, это нормально, но мне было бы очень сложно работать как «root» в течение длительного времени, могут произойти плохие вещи. Чтобы выйти из этого возвышенного положения, вы просто набираете «exit».\n\n\n\nЯ ловлю себя на том, что все время использую clear. Команда clear делает именно то, о чем говорит: она очищает экран от всех предыдущих команд, помещая курсор наверх и предоставляя вам красивое чистое рабочее пространство. Windows, это «cls» в .mdprompt.\n\n\n\nДавайте теперь посмотрим на некоторые команды, с помощью которых мы можем создавать вещи в нашей системе, а затем визуализировать их в нашем терминале. Прежде всего, у нас есть mkdir, это позволит нам создать папку в нашей системе. С помощью следующей команды мы можем создать папку в нашем домашнем каталоге с именем Day15 mkdir Day15\n\n\n\nС помощью cd это позволяет нам изменить каталог, поэтому для перехода в наш вновь созданный каталог мы можем сделать это с помощью вкладки cd Day15, которая также может использоваться для автозаполнения доступного каталога. Если мы хотим вернуться к тому, с чего начали, мы можем использовать cd ..\n\n\n\nrmdir позволяет нам удалить каталог, если мы запустим rmdir Day15, тогда папка будет удалена (обратите внимание, что это будет работать, только если у вас ничего нет в папке)\n\n\n\nЯ уверен, что все мы делали это, когда мы переходили в глубины нашей файловой системы в каталог и не знали, где мы находимся. pwd дает нам распечатку рабочего каталога, pwd, насколько это похоже на пароль, означает печать рабочего каталога.\n\n\n\nМы знаем, как создавать папки и каталоги, но как мы создаем файлы? Мы можем создавать файлы с помощью команды «touch», если бы мы запускали «touch Day15», это создало бы файл. Игнорируйте mkdir, мы еще увидим это позже.\n\n\n\nls Я могу поставить на это свой дом, вы будете использовать эту команду так много раз, что она выведет список всех файлов и папок в текущем каталоге. Давайте посмотрим, сможем ли мы увидеть тот файл, который мы только что создали.\n\n\n\nКак мы можем найти файлы в нашей системе Linux? locate позволит нам искать в нашей файловой системе. Если мы используем locate Day15, он сообщит о местонахождении файла. Бонусом является то, что если вы знаете, что файл существует, но вы получаете пустой результат, запустите sudo updatedb, который проиндексирует все файлы в файловой системе, а затем снова запустите locate. Если у вас нет locate, вы можете установить его с помощью этой команды sudo apt install mlocate\n\n\n\nКак насчет перемещения файлов из одного места в другое? mv позволит вам перемещать ваши файлы. Пример mv Day15 90DaysOfDevOps переместит ваш файл в папку 90DaysOfDevOps.\n\n\n\nМы переместили наш файл, но что, если мы хотим переименовать его сейчас во что-то другое? Мы можем сделать это снова с помощью команды mv. Мы можем просто использовать mv Day15 day15, чтобы перейти к верхнему регистру, или мы могли бы использовать mv day15 AnotherDay, чтобы полностью изменить его, теперь используйте ls для проверки файла.\n\n\n\nХватит, теперь давайте избавимся (удалим) от нашего файла и, возможно, даже от нашего каталога, если он у нас есть. rm просто rm AnotherDay удалит наш файл. Мы также будем использовать rm -R, который будет рекурсивно работать через папку или местоположение. Мы также можем использовать rm -R -f, чтобы принудительно удалить все эти файлы. Спойлер, если вы запустите rm -R -f /, добавьте к нему sudo, и вы можете попрощаться со своей системой ....!\n\n\n\nМы рассмотрели перемещение файлов, но что, если я просто хочу скопировать файлы из одной папки в другую, просто скажу, что это очень похоже на команду mv, но мы используем cp, чтобы теперь мы могли сказать cp Day15 Desktop\n\n\n\nМы создали папки и файлы, но на самом деле мы не поместили никакого содержимого в нашу папку, мы можем добавить содержимое несколькими способами, но самый простой способ - это echo, мы также можем использовать echo, чтобы распечатать много вещей в нашей папке. терминал, я лично часто использую эхо для вывода системных переменных, чтобы узнать, установлены они или нет. мы можем использовать echo \"Hello #90DaysOfDevOps\" > Day15, и это добавит это в наш файл. Мы также можем добавить к нашему файлу, используя echo \"Commands are fun!\" >> День15\n\n\n\nЕще одна из тех команд, которые вы будете часто использовать! кошка сокращение от конкатенации. Мы можем использовать cat Day15, чтобы увидеть содержимое внутри файла. Отлично подходит для быстрого чтения этих файлов конфигурации.\n\n\n\nЕсли у вас есть длинный сложный файл конфигурации, и вы хотите или вам нужно найти что-то быстрое в этом файле, а не читать каждую строку, тогда grep вам в помощь, это позволит нам искать в вашем файле определенное слово, используя cat Day15 | grep \"#90DaysOfDevOps\"\n\n\n\nЕсли вы похожи на меня и часто используете эту команду clear, то вы можете пропустить некоторые из ранее запущенных команд, мы можем использовать «историю», чтобы узнать все те команды, которые мы запускали ранее. history -c удалит историю.\n\nКогда вы запускаете history и хотите выбрать конкретную команду, вы можете использовать !3, чтобы выбрать 3-ю команду в списке.\n\nВы также можете использовать history | grep \"Команда\" для поиска чего-то определенного.\n\nНа серверах для отслеживания времени выполнения команды может быть полезно добавлять дату и время к каждой команде в файле истории.\n\nСледующая системная переменная управляет этим поведением:\n\nHISTTIMEFORMAT=\"%d-%m-%Y %T \"\nВы можете легко добавить ее в свой bash_profile:\necho 'export HISTTIMEFORMAT=\"%d-%m-%Y %T \"' >> ~/.bash_profile\nМожем увеличить размер файла для хранения истории:\necho 'export HISTSIZE=100000' >> ~/.bash_profile\necho 'export HISTFILESIZE=10000000' >> ~/.bash_profile\n\n\n\nНужно сменить пароль? passwd позволит нам изменить наш пароль. Обратите внимание, что когда вы добавляете свой пароль таким образом, когда он скрыт, он не будет отображаться в history, однако, если ваша команда имеет -p ПАРОЛЬ, тогда он будет виден в вашей history.\n\n\n\nМы также можем добавить новых пользователей в нашу систему, мы можем сделать это с помощью useradd, мы должны добавить пользователя с помощью нашей команды sudo, мы можем добавить нового пользователя с помощью sudo useradd NewUser\n\n\n\nДля повторного создания группы требуется sudo, и мы можем использовать sudo groupadd DevOps, тогда, если мы хотим добавить нашего нового пользователя в эту группу, мы можем сделать это, запустив sudo usermod -a -G DevOps -a is add а -G это имя группы.\n\n\n\nКак добавить пользователей в группу sudo? Это было бы очень редким случаем но для того, чтобы сделать это, выполним: usermod -a -G sudo NewUser\n\nПрава / Permissions\n\nread, write and execute - — это права доступа ко всем нашим файлам и папкам в нашей системе Linux.\n\nПолный список:\n\n0 = None ---\n1 = Execute only --X\n2 = Write only -W-\n3 = Write & Exectute -WX\n4 = Read Only R--\n5 = Read & Execute R-X\n6 = Read & Write RW-\n7 = Read, Write & Execute RWX\n\nВы также увидите «777» или «775», и они представляют те же числа, что и в приведенном выше списке, но каждый из них представляет User - Group - Everyone*\n\nДавайте посмотрим на наш файл. ls -al Day15 вы можете увидеть 3 группы, упомянутые выше, пользователь и группа могут читать и изменять (write), но все остальыне только читать (read).\n\n\n\nМы можем изменить это с помощью chmod, вы можете сделать это, если вы также создаете двоичные файлы в своих системах, и вам нужно дать возможность запускать эти двоичные файлы. chmod 750 Day15 теперь запустите ls -la Day15, если вы хотите запустить это для всей папки, вы можете использовать -R, чтобы сделать это рекурсивно.\n\n\n\nКак насчет смены владельца файла? Мы можем использовать «chown» для этой операции, если мы хотим изменить владельца нашего «Day15» с пользователя «vagrant» на «NewUser», мы можем запустить «sudo chown NewUser Day15» снова, можно использовать «-R».\n\n\n\nКоманда, с которой вы столкнетесь, это awk, где она реально используется, когда у вас есть выходные данные, из которых вам нужны только определенные данные. например, запуская who, мы получаем строки с информацией, но, возможно, нам нужны только имена. Мы можем запустить кто | awk '{print $1}', чтобы получить только список этого первого столбца.\n\n\n\nЕсли вы хотите читать потоки данных из стандартного ввода, то генерирует и выполняет командные строки; это означает, что он может принимать вывод команды и передавать его в качестве аргумента другой команды. xargs — полезный инструмент для этого случая использования. Если, например, мне нужен список всех учетных записей пользователей Linux в системе, которую я могу запустить. cut -d: -f1 < /etc/passwd и получите длинный список, который мы видим ниже.\n\n\n\nЕсли я хочу заархивировать этот список, я могу сделать это, используя xargs в команде вроде этой cut -d: -f1 < /etc/passwd | sort | xargs\n\n\n\nЯ также не упомянул команду cut, которая позволяет нам удалять разделы из каждой строки файла. Его можно использовать для вырезания частей строки по положению байта, символу и полю. Команда cut -d \" \" -f 2 list.txt позволяет нам удалить первую букву, которая у нас есть, и просто отобразить наши числа. Есть так много комбинаций, которые можно использовать здесь с этой командой, я уверен, что потратил слишком много времени, пытаясь использовать эту команду, когда я мог бы быстрее извлечь данные вручную.\n\n\n\nТакже обратите внимание, если вы вводите команду, и вы больше не довольны ею, и вы хотите начать снова, просто нажмите Ctrl + c, и это отменит эту строку и начнет все заново.\n\nРесурсы\n\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need be a hacker!)\n\nЭто уже довольно большой список, но я могу с уверенностью сказать, что я использую все эти команды в своей повседневной жизни, будь то администрирование серверов Linux или мой рабочий стол Linux, это очень легко, когда вы находитесь в Windows или macOS для навигации по пользовательскому интерфейсу, но в Linux Servers их нет, все делается через терминал.\n",
            "tags": [
                "devops",
                "linux"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day16",
            "title": "16. Управление системой, файловой системой и хранилищем в Linux",
            "description": "Управление системой, файловой системой и хранилищем в Linux",
            "content": "Управление системой, файловой системой и хранилищем в Linux\n\nК этому времени мы кратко рассмотрели Linux и DevOps, а затем мы настроили нашу лабораторную среду с помощью vagant 14-й день), а затем коснулись небольшой части команд, которые будут в вашем ежедневном набор инструментов во время использования терминала - (День 15).\n\nСегодня мы рассмотрим три ключевые области обслуживания систем Linux с помощью обновлений, установки программного обеспечения. Поймем для чего используются системные папки, а также рассмотрим хранилище.\nУправление Ubuntu и программным обеспечением\nПервое, что мы собираемся рассмотреть, это то, как мы обновляем нашу операционную систему. Большинству из вас этот процесс знаком в ОС Windows и macOS, он немного отличается на рабочем столе и сервере Linux.\n\nМы рассмотрим диспетчер пакетов apt - утилита, которую мы собираемся использовать на нашей виртуальной машине Ubuntu для обновлений и установки программного обеспечения.\n\nКак правило, по крайней мере на рабочих станциях разработчиков, мы запускаем эту команду, чтобы убедиться, что у нас есть последние доступные обновления из центральных репозиториев перед установкой любого программного обеспечения.\n\nsudo apt-get update\n\n\n\nТеперь у нас есть обновленная виртуальная машина Ubuntu с установленными последними обновлениями ОС. Теперь мы хотим установить здесь некоторое программное обеспечение.\nДавайте выберем  figlet — программу, генерирующую текстовые баннеры.\nЕсли мы введем «figlet» в наш терминал, вы увидите, что приложение не установлен в нашей системе.\n\n\nОднако из вышеизложенного вы увидите, что утилита apt предлагает нам некотоыре опции установки apt install ... , которые мы можем попробовать. Это потому, что в репозиториях по умолчанию есть программа figlet. Давайте попробуем sudo apt install figlet\n\nТеперь мы можем использовать наше приложение figlet\n\n\nЕсли мы хотим удалить эту или любую из наших установок программного обеспечения, мы также можем сделать это с помощью менеджера пакетов «apt».\nsudo apt remove figlet\n\n\n\nСуществуют сторонние репозитории, которые мы также можем добавить в нашу систему, те, к которым у нас есть доступ из коробки, являются репозиториями Ubuntu по умолчанию.\n\nЕсли бы, например, мы хотели установить vagrant на нашу виртуальную машину Ubuntu, мы не смогли бы сделать это прямо сейчас, и вы можете увидеть это ниже в первой введенной команде. Затем мы добавляем ключ к репозиторию HashiCorp, а затем добавляем репозиторий в нашу систему.\ncurl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -\nsudo apt-add-repository \"deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\"\n\nКак только мы добавим репозиторий HashiCorp, мы можем запустить sudo apt install vagrant и установить vagrant в нашей системе.\n\n\n\nСуществует много вариантов, когда дело доходит до установки программного обеспечения, различных вариантов менеджеров пакетов, встроенных в Ubuntu, мы также могли бы использовать сохраненные темплейты (snapshots) для установки нашего программного обеспечения.\n\nНадеюсь, это даст вам представление о том, как управлять установками ОС и программного обеспечения в Linux.\nФайловая система\nLinux состоит из файлов конфигурации, и если вы хотите что-то изменить, вы меняете эти файлы конфигурации.\n\nВ Windows у вас есть диск C:, и это то, что мы считаем корнем. В Linux у нас есть /, где мы собираемся найти важные папки в нашей системе Linux.\n\n\n\n/bin - Сокращенно от binary, папка bin — это место, где в основном находятся наши двоичные файлы, которые нужны вашей системе, исполняемые файлы и инструменты.\n\n\n\n/boot - Все файлы, необходимые вашей системе для загрузки. Как загрузиться и с какого диска загрузиться.\n\n\n\n/dev - Вы можете найти информацию об устройстве здесь, здесь вы найдете указатели на ваши диски sda, которые будут вашим основным диском ОС.\n\n\n\n/etc - Вероятно, это самая важная папка в вашей системе Linux, где находится большинство ваших файлов конфигурации.\n\n\n\n/home - здесь вы найдете свои пользовательские папки и файлы. У нас есть пользовательская папка vagrant. В ней вы найдете папки \"Documents\" и «Desktop», с которыми мы работали для раздела команд.\n\n/lib - Мы упомянули, что /bin — это место, где находятся наши бинарные и исполняемые файлы, а /lib — это место, где вы найдете разделяемые библиотеки для них.\n\n\n\n/media - Съемные носители. Флешки, диски и тд).\n\n\n\n/mnt - Mount. Это временная точка монтирования. Подробнее мы расскажем в следующем разделе о хранении данных.\n\n\n\n/opt - Дополнительные пакеты программного обеспечения. Вы заметите, что здесь хранится некоторое программное обеспечение для vagrant и virtual box.\n\n\n\n/proc - Информация о ядре (kernel) и процессе (process), аналогичная /dev\n\n\n\n/root - Домашняя папка для root. Чтобы получить доступ, вам нужно войти в эту папку с помощью sudo.\n\n\n\n/run - Каталог, содержащий PID файлы процессов, похожий на /var/run, но в отличие от него, он размещен в TMPFS, а поэтому после перезагрузки все файлы теряются. Сохраняет состояния текущих процессов\n\n\n\n/sbin - System binaries. Так же как и /bin, содержит двоичные исполняемые файлы, которые доступны на ранних этапах загрузки, когда не примонтирован каталог /usr. Но здесь находятся программы, которые можно выполнять только с правами суперпользователя. Это разные утилиты для обслуживания системы. Например, iptables, reboot, fdisk, ifconfig,swapon и т д.\n\n\n\n/tmp - Содержит временные файлы, созданные системой, любыми программами или пользователями\n\n\n\n/usr - User Aplications. Если бы мы, как обычный пользователь, установили пакеты программного обеспечения, они обычно устанавливались бы в папку /usr/bin. Здесь находятся исполняемые файлы, исходники программ, различные ресурсы приложений, картинки, музыка и документация\n\n/usr/bin - Содержит исполняемые файлы различных программ, которые не нужны на первых этапах загрузки системы, например, музыкальные плееры, графические редакторы, браузеры и т.д.\n\n\n\n/var - Variable. Переменные файлы. Наши приложения устанавливаются в папку bin. Нам нужно где-то хранить все файлы журналов, это /var. Здесь содержатся файлы системных журналов, различные кеши, базы данных и так далее\n\n\n\n/var/log - Logs. Здесь содержатся большинство файлов логов всех программ, установленных в операционной системе. У многих программ есть свои подкаталоги в этой папке, например, /var/log/apache - логи веб-сервера, /var/log/squid - файлы журналов кеширующего сервера squid. Если в системе что-либо сломалось, скорее всего, ответы вы найдете здесь.\n\n/var/run - Содержит файлы с PID процессов, которые могут быть использованы, для взаимодействия между программами. В отличие от каталога /run данные сохраняются после перезагрузки.\n/sys - System. Информация о системе. Назначение каталогов Linux из этой папки - получение информации о системе непосредственно от ядра. Это еще одна файловая система организуемая ядром и позволяющая просматривать и изменить многие параметры работы системы, например, работу swap, контролировать вентиляторы и многое другое.\nХранение\n\nКогда мы подходим к системе Linux или любой другой системе, мы можем захотеть узнать о доступных дисках и о том, сколько свободного места у нас есть на этих дисках. Следующие несколько команд помогут нам идентифицировать, использовать и управлять хранилищем.\n\nlsblk Список заблокированных устройств. «sda» — это наш физический диск, а затем «sda1, sda2, sda3» — наши разделы на этом диске.\n\n\n\ndf дает нам немного больше информации об этих разделах, сколько всего, используется и доступно. Здесь вы можете использовать и другие флаги. Я обычно использую df -h, чтобы дать нам \"человеческий (понятный\" (human) вывод данных.\n\n\nЕсли вы добавляли новый диск в свою систему, и это то же самое в Windows, вам нужно было бы отформатировать диск в управлении дисками, в терминале Linux вы можете сделать это с помощью sudo mkfs -t ext4 /dev/sdb с sdb, относящимся к нашему недавно добавленному диску.\n\nЗатем нам нужно будет смонтировать наш недавно отформатированный диск, чтобы его можно было использовать. Мы сделали бы это в нашей ранее упомянутой папке /mnt и создали бы там каталог с sudo mkdir NewDisk, а затем использовали бы sudo mount /dev/sdb newdisk для монтирования диска в это место.\n\nТакже возможно, что вам нужно будет безопасно отключить хранилище из вашей системы, а не просто вытащить его из конфигурации. Мы можем сделать это с помощью sudo umount /dev/sdb.\n\nЕсли вы не хотите размонтировать этот диск и собираетесь использовать этот диск для базы данных или какого-либо другого варианта постоянного использования, тогда вы хотите, чтобы он был там при перезагрузке системы. Чтобы это произошло, нам нужно добавить этот диск в наш файл конфигурации /etc/fstab, чтобы он сохранялся, если вы этого не сделаете, его нельзя будет использовать при перезагрузке машины, и вам придется вручную выполнить описанное выше. процесс. Данные по-прежнему будут на диске, но они не будут автоматически монтироваться, пока вы не добавите конфигурацию в этот файл.\n\nПосле того, как вы отредактировали файл конфигурации fstab, вы можете проверить свою работу с помощью sudo mount -a, если ошибок нет, тогда ваши изменения теперь будут сохраняться при перезапусках.\n\nМы расскажем, как вы будете редактировать файл с помощью текстового редактора в будущем сеансе.\n\nРесурсы\n\nСтруктура файловой системы Linux\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need to be a hacker!)\n\n",
            "tags": [
                "devops",
                "linux",
                "файловая ситема linux"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day17",
            "title": "17. Текстовые редакторы Nano/Vim",
            "description": "Текстовые редакторы nano и vim",
            "content": "\nТекстовые редакторы nano и vim\n\nБольшинство систем Linux - сервера, и у них не будет графического интерфейса. Я также упомянул в прошлой статье, что Linux в основном состоит из файлов конфигурации, и для внесения изменений вам потребуется иметь возможность редактировать эти файлы конфигурации, чтобы изменить что-либо в системе.\n\nСуществует множество вариантов, но я думаю, что мы должны рассмотреть, вероятно, два наиболее распространенных текстовых редактора терминала. Я использовал оба этих редактора, и для меня «nano» — это удобная кнопка, когда дело доходит до быстрых изменений, но у «vim» такой широкий набор возможностей.\nnano\n\nДоступна не во всех системах.\nОтлично для начала.\n\nЕсли вы запустите nano 90DaysOfDevOps.txt, мы создадим новый файл, в котором ничего не будет, здесь мы можем добавить наш текст, и в окне внизу есть инструкции о том, что мы хотим сделать с этим файлом.\n\n\n\nМы можем нажать control x + enter, а затем запустить ls, теперь вы можете увидеть наш новый текстовый файл.\n\n\n\nМожно запустить cat для этого файла, чтобы прочитать наш файл. Затем мы можем использовать тот же nano 90DaysOfDevOps.txt, чтобы добавить дополнительный текст или изменить ваш файл.\n\nДля меня nano очень удобен, когда дело доходит до внесения небольших изменений в файлы конфигурации.\nvim\nВозможно, самый распространенный текстовый редактор.\n\nВ значительной степени поддерживается в каждом дистрибутиве Linux.\nНевероятно мощный! Скорее всего, вы найдете полный 7-часовой курс, посвященный только vim.\n\nМы можем перейти в vim с помощью команды vim или, если мы хотим отредактировать наш новый текстовый файл, мы могли бы запустить vim 90DaysOfDevOps.txt, но сначала вы увидите отсутствие меню справки внизу.\n\nПервый вопрос может быть «Как мне выйти из vim?» это будет escape, и если мы не внесли никаких изменений, то это будет :q\n\n\n\nВы начинаете в обычном «normal» режиме, есть и другие режимы «command, normal, visual, insert», если мы хотим добавить текст, нам нужно будет переключиться с «normal» на «insert», нам нужно нажать «i», если вы добавили какой-то текст и хотели бы сохранить эти изменения, тогда вы нажмете escape, а затем :wq\n\n\n\n\n\n\nВы можете подтвердить это с помощью команды cat, чтобы убедиться, что вы сохранили эти изменения.\n\nВ vim есть несколько крутых быстрых функций, которые позволяют очень быстро выполнять простые задачи, если вы знаете ярлыки, что само по себе является лекцией. Допустим, мы добавили список повторяющихся слов, и теперь нам нужно его изменить, может быть, это файл конфигурации, и мы повторяем сетевое имя, и теперь это изменилось, и мы хотим быстро изменить это. Я использую слово \"Day\" в этом примере.\n\n\n\nТеперь мы хотим заменить это слово на 90DaysOfDevOps, мы можем сделать это, нажав «esc» и набрав «:%s/Day/90DaysOfDevOps».\n\nВ результате, когда вы нажимаете Enter, слово day заменяется на 90DaysOfDevOps.\n\n\n\nКопировать и вставить стало для меня большим открытием. Копия не копия, а дерьмо. мы можем скопировать, используя yy на клавиатуре в обычном режиме. p вставьте в ту же строку, P вставьте в новую строку.\n\nВы также можете удалить эти строки, выбрав количество строк, которые вы хотите удалить, а затем dd\n\nТакже, вероятно, вам понадобится время для поиска файла, теперь мы можем использовать grep, как упоминалось в предыдущем сеансе, но мы также можем использовать vim. мы можем использовать /word, и это найдет первое совпадение, для перехода к следующему вы будете использовать клавишу n и так далее.\n\nДля vim это даже не касается поверхности, самый большой совет, который я могу дать, — взяться за руки и использовать vim везде, где это возможно.\n\nОбычный вопрос на собеседовании: какой ваш любимый текстовый редактор в Linux, и я хотел бы убедиться, что у вас есть хотя бы эти знания об обоих, чтобы вы могли ответить: «Нано» — это нормально, потому что это просто. По крайней мере, вы показываете компетентность в понимании того, что такое текстовый редактор. Но потренируйтесь с ними, чтобы стать более опытным.\n\nЕще один указатель для навигации в vim, мы можем использовать «H, J, K, L», а также наши клавиши со стрелками.\n\nРесурсы\nVim Cheat Sheet\nVim in 100 Seconds\nVim tutorial\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need to be a hacker!)\n\n",
            "tags": [
                "devops",
                "vim",
                "nano"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day18",
            "title": "18. Web Сервер и SSH",
            "description": "Web Сервер и SSH",
            "content": "\nSSH\nКак мы уже упоминали, вы, скорее всего, будете управлять множеством удаленных серверов Linux, поэтому вам необходимо убедиться, что ваше подключение к этим удаленным серверам безопасно. В этом разделе мы хотим рассказать о некоторых основах SSH (Secure Shell), которые должен знать каждый, и которые помогут вам с этим безопасным туннелем к вашим удаленным системам.\n\nНастройка соединения по SSH\nПередача файлов\nСоздайте свой закрытый ключ\n\nВведение в SSH\n\nБезопасная оболочка (Secure Shell)\nСетевой протокол (Networking Protocol)\nОбеспечивает безопасную связь\nМожет защитить любой сетевой сервис\nОбычно используется для удаленного доступа из командной строки\n\nВ нашей среде, если вы следили за нами, мы уже использовали SSH, но все это было настроено и автоматизировано с помощью нашей конфигурации vagrant, поэтому нам нужно было только запустить vagrant ssh, и мы получили доступ к нашей удаленной виртуальной машине.\n\nЕсли бы наша удаленная машина не находилась в той же системе, что и наша рабочая станция, и находилась бы в удаленном месте, возможно, в облачной системе или в центре обработки данных, к которому мы могли бы получить доступ только через Интернет, нам потребовался бы безопасный способ, чтобы получить доступ к системе для управления ею.\n\nSSH обеспечивает безопасный туннель между клиентом и сервером, поэтому злоумышленники ничего не могут перехватить.\n\n\n\nНа сервере есть служба SSH на стороне сервера, которая всегда работает и прослушивает определенный TCP-порт (22).\n\nЕсли мы используем наш клиент для подключения с правильными учетными данными или ключом SSH, мы получаем доступ к этому серверу.\n\nДобавление bridged network adapter в нашу систему\n\nЧтобы мы могли использовать SSH с нашей виртуальной машиной, нам нужно добавить сетевой адаптер на нашу машину.\n\nВыключите виртуальную машину, щелкните ее правой кнопкой мыши в Virtual Box и выберите настройки. В новом окне выберите сеть.\n\n\n\nТеперь снова включите вашу машину, и теперь у вас будет IP-адрес на вашей локальной машине. Вы можете подтвердить это с помощью команды ip addr.\n\nПроверка работы SSH-сервера\n\nМы знаем, что SSH уже настроен на нашей машине, поскольку мы использовали его с vagrant, но мы можем удостовериться, что сервер бежит, запустив\n\nsudo systemctl status ssh\n\n\n\nЕсли в вашей системе нет SSH-сервера, вы можете установить его, введя эту команду sudo apt install openssh-server\n\nЗатем вы хотите убедиться, что наш SSH разрешен и брандмауэр работает. Мы можем сделать это с помощью sudo ufw allow ssh. Это не требуется в нашей конфигурации, поскольку мы автоматизировали это с помощью нашего vagrant.\n\nУдаленный доступ — пароль SSH\n\nТеперь, когда наш SSH-сервер прослушивает порт 22 для любых входящих запросов на подключение, и мы добавили \"мост\" (bridged networking), мы можем использовать putty или SSH-клиент на нашей локальной машине для подключения к нашей системе с помощью SSH.\n\n\n\nЗатем нажмите «Открыть», если вы впервые подключаетесь к этой системе через этот IP-адрес, вы получите это предупреждение. Мы знаем, что это наша система, поэтому вы можете выбрать «yes».\n\n\n\nЗатем нам будет предложено ввести имя пользователя (vagrant) и пароль (пароль по умолчанию — vagrant). Ниже вы увидите, что теперь мы используем наш SSH-клиент (Putty) для подключения к нашей машине с использованием имени пользователя и пароля.\n\n\n\nНа этом этапе мы подключаемся к нашей виртуальной машине с нашего удаленного клиента и можем выполнять наши команды в нашей системе.\n\nУдаленный доступ — ключ SSH\n\nВышеупомянутый простой способ получить доступ к вашим системам, однако, по-прежнему зависит от имени пользователя и пароля, и если какой-либо злоумышленник получит доступ к этой информации, а также к общедоступному адресу или IP-адресу вашей системы, это может быть легко скомпрометировано. Здесь предпочтительны SSH-ключи.\n\nКлючи SSH означают, что мы предоставляем пару ключей, чтобы и клиент, и сервер знали, что это доверенное устройство.\n\nСоздать ключ несложно. На нашем локальном компьютере (Windows) мы можем выполнить следующую команду: если у вас установлен ssh-клиент в любой системе, я полагаю, что эта же команда будет работать?\n\nssh-keygen -t ed25519\n\nЯ не буду вдаваться в подробности того, что такое ed25519 и что означает здесь, но вы можете воспользоваться поиском, если хотите узнать больше о криптографии\n\n\n\nНа данный момент у нас есть созданный ключ SSH, хранящийся в C:\\Users\\micha/.ssh/\n\nНо чтобы связать это с нашей виртуальной машиной Linux, нам нужно скопировать ключ. Мы можем сделать это, используя ssh-copy-id vagrant@192.168.169.135.\n\nЯ использовал PowerShell для создания своих ключей на моем клиенте Windows, но здесь нет доступного ssh-copy-id. Есть способы, которыми вы можете сделать это в Windows, и небольшой поиск в Интернете найдет вам альтернативу, но я просто использую git bash на своем компьютере с Windows, чтобы сделать копию.\n\n\n\nТеперь мы можем вернуться к Powershell, чтобы проверить, что наше соединение теперь работает с нашими ключами SSH, и пароль не требуется.\n\nssh vagrant@192.168.169.135\n\n\n\nПри необходимости мы могли бы защититься, используя кодовую фразу. Мы также могли бы сделать еще один шаг, заявив, что пароли вообще не нужны, что означает, что будут разрешены только пары ключей через SSH. Вы можете сделать это в следующем файле конфигурации.\n\nsudo nano /etc/ssh/sshd_config\n\nздесь есть строка с PasswordAuthentication yes, она будет закомментирована #, вы должны раскомментировать и изменить yes на no. Затем вам нужно будет перезагрузить службу SSH с помощью «sudo systemctl reload sshd».\n\nНастройка веб-сервера\n\nНе имеет прямого отношения к тому, что мы только что сделали с SSH выше, но я хотел рассмотрть, поскольку это снова еще одна задача, которая может показаться вам немного сложной, но на самом деле этого не должно быть.\n\nУ нас есть виртуальная машина с Linux, и на данном этапе мы хотим добавить веб-сервер apache к нашей виртуальной машине, чтобы мы могли разместить на нем простой веб-сайт, который обслуживает мою домашнюю сеть. Обратите внимание, что эта веб-страница не будет доступна из Интернета, это можно сделать, но здесь это не рассматривается.\n\nВы также можете увидеть, что это называется стеком LAMP.\n\nL**inux Operating System\nA**pache Web Server\nm**ySQL database\nP**HP\n\nApache2\n\nApache2 — это HTTP-сервер с открытым исходным кодом. Мы можем установить apache2 с помощью следующей команды.\n\nsudo apt-get install apache2\n\nЧтобы убедиться, что apache2 установлен правильно, мы можем запустить sudo service apache2 restart.\n\nЗатем, используя сетевой адрес моста из пошагового руководства по SSH, откройте браузер и перейдите по этому адресу. Мой http://192.168.169.135/\n\nmySQL\n\nMySQL — это база данных, в которой мы будем хранить данные для нашего простого веб-сайта. Чтобы установить MySQL, мы должны использовать следующую команду sudo apt-get install mysql-server\n\nPHP\nPHP — это серверный язык (server-side scripting language), мы будем использовать его для взаимодействия с базой данных MySQL. Окончательная установка заключается в установке PHP и зависимостей с помощью sudo apt-get install php libapache2-mod-php php-mysql.\n\nПервое изменение конфигурации, которое мы хотим внести в apache из коробки, — это использование index.html, и вместо этого мы хотим использовать index.php.\n\nМы будем использовать sudo nano /etc/apache2/mods-enabled/dir.conf и переместим index.php в первый элемент списка.\n\n\n\nПерезапустите службу apache2 sudo systemctl restart apache2\n\nТеперь давайте подтвердим, что наша система правильно настроена для PHP. Создайте следующий файл с помощью этой команды, это откроет пустой файл в nano.\n\nsudo nano /var/www/html/90Days.php\n\nзатем скопируйте следующее и используйте Ctrl + x, чтобы выйти и сохранить файл.\n\n\n\nТеперь снова перейдите к IP-адресу виртуальной машины Linux с дополнительным 90Days.php в конце URL-адреса. http://192.168.169.135/90Days.php вы должны увидеть что-то похожее на показанное ниже, если PHP настроен правильно.\n\nУстановка WordPress\n\nЯ просмотрел тьюториал, чтобы установить WordPress в наш стек LAMP, некоторые команды показаны ниже, если они не показаны правильно в пошаговом руководстве How to install wordpress on Ubuntu with LAMP\n\nsudo mysql -u root -p\n\nCREATE DATABASE wordpressdb;\n\nCREATE USER 'admin-user'@'localhost' IDENTIFIED BY 'password';\n\nGRANT ALL PRIVILEGES ON wordpressdb.* TO 'admin-user'@'localhost';\n\nFLUSH PRIVILEGES;\n\nEXIT;\n\nsudo apt install php-curl php-gd php-mbstring php-xml php-xmlrpc php-soap php-intl php-zip\n\nsudo systemctl restart apache2\n\ncd /var/www\n\nsudo curl -O https://wordpress.org/latest.tar.gz\n\nsudo tar -xvf latest.tar.gz\n\nsudo rm latest.tar.gz\n\nНа данный момент вы находитесь на шаге 4 в связанной статье, вам нужно будет выполнить шаги, чтобы убедиться, что для каталога WordPress установлены все правильные разрешения.\n\nПоскольку это только внутреннее действие, вам не нужно «генерировать ключи безопасности» на этом шаге. Перейдите к шагу 5, который меняет конфигурацию Apache на WordPress.\n\nЗатем, если все настроено правильно, вы сможете получить доступ через свой внутренний сетевой адрес и запустить установку WordPress.\n\nРесурсы\n\nClient SSH GUI - Remmina\nThe Beginner's guide to SSH\nVim in 100 Seconds\nVim tutorial\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need to be a hacker!)\n\n",
            "tags": [
                "devops",
                "linux",
                "ssh"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day19/",
            "title": "19. Автоматизация задачи с помощью bash-скриптов",
            "description": "Автоматизация задачи с помощью bash-скриптов",
            "content": "Автоматизация задачи с помощью bash-скриптов\n\nОболочка, которую мы собираемся использовать сегодня, — это bash, но мы рассмотрим другую оболочку завтра, когда будем углубляться в ZSH.\n\nBASH - Bourne Again Shell («возрождённый» shell)\n\nМы могли бы почти посвятить целую секцию из 7 дней написанию сценариев оболочки, как и языкам программирования. Bash дает нам возможность работать вместе с другими инструментами автоматизации для достижения цели.\n\nЯ до сих пор разговариваю со многими людьми, которые настроили несколько сложных сценариев оболочки, чтобы что-то произошло, и они полагаются на этот сценарий для некоторых из наиболее важных вещей в бизнесе, я не говорю, что нам нужно понимать сценарии оболочки/bash. для этой цели это не путь. Но мы должны изучить сценарии оболочки/bash, чтобы работать вместе с нашими инструментами автоматизации и для специальных задач.\n\nОдним из примеров, который мы использовали, может быть VAGRANTFILE, который мы использовали для создания нашей виртуальной машины, мы могли бы обернуть его в простой сценарий bash, который удалял и обновлял его каждый понедельник утром, чтобы у нас была свежая копия нашей виртуальной машины Linux. каждую неделю мы могли бы также добавлять весь программный стек, который нам нужен, на указанную машину с Linux и так далее с помощью одного сценария bash.\n\nЯ думаю, что еще одна вещь, которую я, по крайней мере, слышу, это то, что практические вопросы по скриптам становятся все более и более очевидными во всех интервью.\n\nНачало\n\nКак и в случае со многим, что мы рассмотрим за все эти 90 дней, единственный реальный способ научиться — это делать. Практический опыт поможет впитать все это в вашу мышечную память.\n\nПрежде всего, нам понадобится текстовый редактор. В День 17 мы рассказали, наверное, о двух самых распространенных текстовых редакторах и немного о том, как их использовать.\n\nДавайте приступим прямо к делу и создадим наш первый сценарий оболочки.\n\ntouch 90DaysOfDevOps.sh - создает файл 90DaysOfDevOps.sh\n\nЗа ним следует nano 90DaysOfDevOps.sh, это откроет наш новый пустой сценарий оболочки в nano. Опять же, вы можете выбрать другой текстовый редактор.\n\nПервая строка всех скриптов bash должна выглядеть примерно так: #!/usr/bin/bash, это путь к вашему двоичному файлу bash.\n\nОднако вы должны проверить это в терминале, запустив which bash, если вы не используете Ubuntu, вы также можете попробовать whereis bash из терминала.\n\nОднако вы можете увидеть другие пути, перечисленные в уже созданных сценариях оболочки, которые могут включать:\n\n#!/bin/bash\n#!/usr/bin/env bash\n\nВ следующей строке нашего скрипта я хотел бы добавить комментарий и добавить цель скрипта или хотя бы какую-то информацию обо мне. Вы можете сделать это, используя #. Это позволяет нам комментировать определенные строки в нашем коде и предоставлять описания того, что будут делать следующие команды. Я считаю, что чем больше заметок, тем лучше для пользователя, особенно если вы делитесь этим.\n\nИногда я использую figlet, программу, которую мы установили ранее в разделе Linux, для создания аски-арта, чтобы начать что-то в наших скриптах.\n\n\nВсе команды, которые мы использовали ранее в этом разделе Linux (День 15) можно использовать здесь как простую команду для тестирования нашего скрипта.\n\nДавайте добавим в наш скрипт простой блок кода.\n\nmkdir 90DaysOfDevOps\ncd 90DaysOfDevOps\ntouch Day19\nls\nЗатем вы можете сохранить это и выйти из текстового редактора. Если мы запустим наш скрипт с ./90DaysOfDevOps.sh, вы должны получить сообщение об отказе в разрешении. Вы можете проверить права доступа к этому файлу с помощью команды ls -la, и вы увидите, что у нас нет прав на выполнение этого файла.\n\n\n\nМы можем изменить это, используя chmod +x 90DaysOfDevOps.sh, и тогда вы увидите x, означающий, что теперь мы можем запустить (execute) наш скрипт.\n\n\n\nТеперь мы можем снова запустить наш скрипт, используя ./90DaysOfDevOps.sh после того, как запуск скрипта создал новый каталог, перешел в этот каталог, а затем создал новый файл.\n\n\n\nДовольно простые вещи, но вы можете начать понимать, как это можно использовать для вызова других инструментов, как часть способов сделать вашу жизнь проще и автоматизировать вещи.\n\nПеременные, условные операторы\nБольшая часть этого раздела на самом деле является повторением того, что мы рассмотрели, когда изучали Golang, но я думаю, что нам стоит углубиться в это снова.\n\nПеременные\n\nПеременные позволяют нам один раз определить конкретный повторяющийся термин, который используется в потенциально сложном сценарии.\n\nЧтобы добавить переменную, вы просто добавляете ее вот так на чистую строку в вашем скрипте.\n\nchallenge=\"90DaysOfDevOps\"\n\nТаким образом, когда и где мы используем $challenge в нашем коде, если мы изменим переменную, это будет отражено повсюду.\n\n\n\nЕсли мы сейчас запустим наш скрипт sh, вы увидите распечатку, которая была добавлена к нашему скрипту.\n\n\n\nМы также можем запросить пользовательский ввод, который может установить наши переменные, используя следующее:\n\necho \"Enter your name\"\nread name\nЗатем это определило бы ввод как переменную $name. Затем мы могли бы использовать это позже.\n\nУсловные операторы\n\nМожет быть, мы хотим узнать, кто участвует в нашем марафоне \"90 дней\" и сколько дней они прошли, мы можем определить это, используя условные выражения if if-else else-if, это то, что мы определили ниже в нашем скрипте. .\n\n#!/bin/bash\n_   _  _                   ___   __ _              _\n/ _ \\ / _ \\|  _ \\  _ _ _   _ __ / _ \\ / |  _ \\  ____   / _ \\ _ __  _\n#| () | | | | | | |/ _` | | | / _| | | | || | | |/ _ \\ \\ / / | | | ' \\/ __|\n\\, | |_| | |_| | (_| | |_| \\ \\ || |  _| || |  /\\ V /| |_| | |_) \\ \\\n// \\__/|/ \\__,_|\\__, |___/\\___/|_| |/ \\| \\_/  \\/| ./|_/\n|_/                                     |_|\nThis script is to demonstrate bash scripting!\n\nVariables to be defined\n\nChallengeName=#90DaysOfDevOps\nTotalDays=90\n\nUser Input\n\necho \"Enter Your Name\"\nread name\necho \"Welcome $name to $ChallengeName\"\necho \"How Many Days of the $ChallengeName challenge have you completed?\"\nread DaysCompleted\n\nif [ $DaysCompleted -eq 90 ]\nthen\n  echo \"You have finished, well done\"\nelif [ $DaysCompleted -lt 90 ]\nthen\n  echo \"Keep going you are doing great\"\nelse\n  echo \"You have entered the wrong amount of days\"\nfi\nВы также можете видеть из вышеприведенного, что мы проводим некоторые сравнения или сверяем значения друг с другом, чтобы перейти к следующему этапу. У нас есть разные варианты, которые стоит отметить.\n\neq - if the two values are equal will return TRUE\nne - if the two values are not equal will return TRUE\ngt - if the first value is greater than the second value will return TRUE\nge - if the first value is greater than or equal to the second value will return TRUE\nlt - if the first value is less than the second value will return TRUE\nle - if the first value is less than or equal to the second value will return TRUE\n\nМы также можем использовать сценарии bash для получения информации о файлах и папках, это называется условиями файлов.\n\n-d file True if the file is a directory\n-e file True if the file exists\n-f file True if the provided string is a file\ng file True if the group id is set on a file\n-r file True if the file is readable\n-s file True if the file has a non-zero size\n\nFILE=\"90DaysOfDevOps.txt\"\nif [ -f \"$FILE\" ]\nthen\n  echo \"$FILE is a file\"\nelse\n  echo \"$FILE is not a file\"\nfi\n\n\nПри условии, что этот файл все еще находится в нашем каталоге, мы должны вернуть первую команду echo. Но если мы удалим этот файл, мы должны получить вторую команду echo.\n\n\nНадеюсь, вы увидите, как это можно использовать для экономии времени при поиске в системе определенных элементов.\n\nЯ нашел этот удивительный репозиторий на GitHub, в котором, кажется, бесконечное количество скриптов DevOps Bash Tools\nПример\n\nScenario: У нас есть наша компания под названием «90DaysOfDevOps», и мы работаем некоторое время, и теперь пришло время расширить команду с 1 человека до гораздо большего в ближайшие недели. Я пока единственный, кто знает процесс адаптации, поэтому мы хотим чтобы уменьшить это узкое место, автоматизировав некоторые из этих задач.\n\nRequirements:\nПользователь может быть передан в качестве аргумента командной строки.\nПользователь создается с именем аргумента командной строки.\nПароль может быть проанализирован как аргумент командной строки.\nПароль установлен для пользователя\nОтображается сообщение об успешном создании учетной записи.\n\nДавайте начнем с создания нашего сценария оболочки с помощью touch create_user.sh.\n\nПрежде чем мы двинемся дальше, давайте также создадим этот исполняемый файл, используя chmod +x create_user.sh\n\nзатем мы можем использовать nano create_user.sh, чтобы начать редактирование нашего скрипта для сценария, который мы установили.\n\nМы можем взглянуть на первое требование «Пользователь может быть передан в качестве аргумента командной строки», мы можем использовать следующее\n\n#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1\"\n\n\n\nИдем далее и запускаем ./create_user.sh Michael, замените Michael своим именем при запуске скрипта.\n\n\nДалее мы можем выполнить второе требование: «Пользователь создается с именем аргумента командной строки», это можно сделать с помощью команды useradd. Опция -m предназначена для создания домашнего каталога пользователя как /home/username.\n#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1 user account being created.\"\n\n#A user is created with the name of command line argument\nsudo useradd -m \"$1\"\n\nПредупреждение: если вы не укажете имя учетной записи пользователя, произойдет ошибка, поскольку мы не заполнили переменную $1\n\nЗатем мы можем проверить, была ли создана эта учетная запись с помощью команды awk -F: '{print $1}' /etc/passwd.\n\nMore about awk linux command\n\n\n\nНаше следующее требование: «Пароль может быть проанализирован как аргумент командной строки». Во-первых, мы никогда не собираемся делать это в продакшене, нам нужно проработать список требований в лаборатории, чтобы понять.\n#! /usr/bin/bash\n\n#A user can be passed in as a command line argument\necho \"$1 user account being created.\"\n\n#A user is created with the name of command line argument\nsudo useradd -m \"$1\"\n\n#A password can be parsed in as a command line argument.\nsudo chpasswd <<< \"$1\":\"$2\"\nЕсли мы затем запустим этот скрипт с двумя параметрами ./create_user.sh пароль 90DaysOfDevOps\n\nНа изображении ниже вы можете видеть, что мы выполнили наш скрипт, он создал нашего пользователя и пароль, а затем мы вручную перешли к этому пользователю и подтвердили это с помощью команды whoami.\n\n\n\nПоследнее требование: «Отображается сообщение об успешном создании учетной записи». На самом деле у нас уже есть это в верхней строке нашего кода, и мы можем видеть на снимке экрана выше, что у нас есть «созданная учетная запись пользователя 90DaysOfDevOps». Это осталось от нашего тестирования с параметром $1.\n\nТеперь этот сценарий можно использовать для быстрого подключения и настройки новых пользователей в наших системах Linux. Но, может быть, вместо того, чтобы некоторым историческим людям приходилось работать с этим, а затем получать новые имена пользователей или пароли для других людей, мы могли бы добавить некоторый пользовательский ввод, который мы ранее рассмотрели ранее, для захвата наших переменных.\n\n#! /usr/bin/bash\n\necho \"What is your intended username?\"\nread  username\necho \"What is your password\"\nread  password\n\n#A user can be passed in as a command line argument\necho \"$username user account being created.\"\n\n#A user is created with the name of command line argument\nsudo useradd -m $username\n\n#A password can be parsed in as a command line argument.\nsudo chpasswd <<< $username:$password\n\nШаги стали более интерактивными,\n\n\n\nПросто чтобы закончить это, возможно, мы хотим вывести успешный вывод, чтобы сказать, что наша новая учетная запись пользователя завершена.\n\n\n\nОдна вещь, которую я заметил, это то, что мы отображаем пароль на нашем входе, мы можем скрыть это, используя флаг -s в строке кода read -s password\n\n\n\nЕсли вы хотите удалить пользователя, которого вы создали для лабораторных целей, вы можете сделать это с помощью sudo userdel test_user\n\nЕще раз, я не говорю, что это будет то, что вы будете создавать в своей повседневной жизни, но я думал, что это то, что подчеркнет гибкость того, для чего вы можете использовать сценарии оболочки.\n\nПодумайте о любых повторяющихся задачах, которые вы выполняете каждый день, неделю или месяц, и о том, как вы могли бы лучше автоматизировать это. Первым вариантом, вероятно, будет использование сценария bash, прежде чем переходить к более сложной территории.\n\nЯ создал очень простой bash-файл, который помогает мне развернуть кластер Kubernetes с помощью minikube на моем локальном компьютере вместе со службами данных и Kasten K10, чтобы продемонстрировать требования и нужды, связанные с управлением данными. Project Pace. Но я не счел уместным поднимать вопрос здесь, поскольку мы еще не рассмотрели Kubernetes.\n\nРесурсы\n\nBash in 100 seconds\nBash script with practical examples - Full Course\nClient SSH GUI - Remmina\nThe Beginner's guide to SSH\nVim in 100 Seconds\nVim tutorial\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need to be a hacker!)\n",
            "tags": [
                "devops",
                "bash"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day20",
            "title": "20. Настройка рабочей среды DevOps",
            "description": "Настройка рабочей среды",
            "content": "Настройка рабочей среды\n\nНе путать с тем, как мы настраиваем серверы Linux таким образом. Я хочу продемонстрировать возможности выбора и гибкость, которые у нас есть при настройке настольного компьютера Linux.\n\nЯ использую рабочий стол Linux уже почти год, и я настроил его именно так, как я хочу с точки зрения внешнего вида. Используя нашу виртуальную машину Ubuntu в Virtual Box, мы можем выполнить некоторые настройки, которые я сделал для своего ежедневного драйвера.\n\nЯ собрал видео на YouTube, показывающее остальные, так как некоторые люди могли бы лучше следовать за ним:\n\nНажмите для доступа к видео YouTube\n\nИз коробки наша система будет выглядеть примерно так:\n\n\n\nМы также можем увидеть нашу оболочку bash по умолчанию:\n\n\nМногое из этого сводится к точечным файлам, которые мы рассмотрим в этой заключительной статье Linux из этой серии.\ndotfiles\nСначала я хочу покопаться в dotfiles, я сказал в предыдущий день, что Linux состоит из файлов конфигурации. Эти файлы представляют собой файлы конфигурации для вашей системы Linux и приложений.\n\nЯ также добавлю, что dotfiles используются не только для настройки и придания красивого вида вашему рабочему столу, но и для изменения и конфигурации dotfile, которые помогут вам повысить производительность.\n\nКак я уже упоминал, многие программы хранят свои конфигурации в этих точечных файлах. Эти файлы помогают управлять функциональностью.\n\nКаждый файл начинается с . Вы, наверное, догадались, откуда взялось название?\n\nДо сих пор мы использовали bash в качестве нашей оболочки, что означает, что у вас будут .bashrc и .bash_profile в нашей домашней папке. Ниже вы можете увидеть несколько точечных файлов, которые есть в нашей системе.\n\n\nМы собираемся изменить нашу оболочку, поэтому позже мы увидим новый точечный файл конфигурации .zshrc.\n\nНо теперь вы знаете, если мы ссылаемся на точечные файлы, вы знаете, что это файлы конфигурации. Мы можем использовать их для добавления псевдонимов в нашу командную строку, а также путей к различным местоположениям. Некоторые люди публикуют свои точечные файлы, чтобы они были общедоступными. Вы найдете мой здесь, на моем GitHub MichaelCade/dotfiles, здесь вы найдете мой пользовательский файл .zshrc, мой предпочтительный терминал - терминатор, который также имеет некоторые файлы конфигурации в папке, а затем также некоторые параметры фона.\n\nZSH\nКак я упоминал во время наших взаимодействий, до сих пор мы использовали оболочку bash по умолчанию с Ubuntu. ZSH очень похож, но имеет некоторые преимущества перед bash.\n\nZsh имеет такие функции, как интерактивное завершение с помощью табуляции, автоматический поиск файлов, интеграция с регулярными выражениями, расширенное сокращение для определения области действия команды и богатый движок тем.\n\nМы можем использовать наш менеджер пакетов «apt», чтобы установить zsh в нашей системе. Давайте продолжим и запустим sudo apt install zsh с нашего терминала bash. Я собираюсь сделать это из консоли виртуальной машины, а не через SSH.\n\nКогда команда установки завершена, вы можете запустить zsh внутри вашего терминала, это запустит сценарий настройки оболочки.\n\n\n\nЯ выбрал «1» на вопрос выше, и теперь у нас есть еще несколько вариантов.\n\n\nИз этого меню видно, что мы можем внести некоторые готовые изменения, чтобы настроить ZSH в соответствии с нашими потребностями.\n\nЕсли вы выходите из мастера с 0, а затем используете ls -la | grep .zshrc вы должны увидеть, что у нас есть новый файл конфигурации.\n\nТеперь мы хотим сделать zsh нашей оболочкой по умолчанию каждый раз, когда мы открываем наш терминал, мы можем сделать это, выполнив следующую команду, чтобы изменить нашу оболочку chsh -s $(which zsh), нам затем нужно выйти из системы и снова войти в нее для грядут изменения.\n\nКогда вы снова войдете в систему и откроете терминал, он должен выглядеть примерно так. Мы также можем подтвердить, что наша оболочка теперь изменена, запустив which $SHELL\n\n\n\nОбычно я выполняю этот шаг на каждом рабочем столе Ubuntu, который я запускаю, и в целом, не заходя дальше, обнаруживаю, что оболочка zsh немного быстрее, чем bash.\nOhMyZSH\n\nДалее мы хотим немного улучшить внешний вид, а также добавить некоторые функции, которые помогут нам перемещаться по терминалу.\n\nOhMyZSH — это бесплатная платформа с открытым исходным кодом для управления вашей конфигурацией zsh. Существует множество плагинов, тем и других вещей, которые просто делают взаимодействие с оболочкой zsh намного приятнее.\n\nВы можете узнать больше о ohmyzsh\n\nДавайте установим Oh My ZSH, у нас есть несколько вариантов с curl, wget или fetch, у нас есть первые два, доступные в нашей системе, но я начну с curl.\n\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\nКогда вы запустите приведенную выше команду, вы должны увидеть вывод, как показано ниже.\n\n\nТеперь мы можем перейти к добавлению темы для нашего опыта, в комплекте с Oh My ZSH более 100, но я выбираю для всех своих приложений, и все это тема Дракулы.\n\nЯ также хочу добавить, что эти два плагина являются обязательными при использовании Oh My ZSH.\n\ngit clone https://github.com/zsh-users/zsh-autosuggestions.git $ZSH_CUSTOM/plugins/zsh-autosuggestions\n\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM/plugins/zsh-syntax-highlighting\n\nnano ~/.zshrc\n\nотредактируйте plugins, чтобы включить plugins=(git zsh-autosuggestions zsh-syntax-highlighting)\n\nРасширения Gnome\n\nЯ также использую расширения Gnome, и в частности список ниже\n\nGnome extensions\n\n    Caffeine\n    CPU Power Manager\n    Dash to Dock\n    Desktop Icons\n    User Themes\n\nУстановка программ\n\nКраткий список программ, которые я устанавливаю на машину с помощью apt\n\n    VSCode\n    azure-cli\n    containerd.io\n    docker\n    docker-ce\n    google-cloud-sdk\n    insomnia\n    packer\n    terminator\n    terraform\n    vagrant\n\nтема Dracula\nЭтот сайт - единственная тема, которую я использую в данный момент. Выглядит четким, чистым и все выглядит отлично. Dracula Theme\n\nПо ссылке выше можем поискать zsh на сайте и найдем как минимум два варианта.\n\nСледуйте приведенным инструкциям, чтобы выполнить установку вручную или с помощью git. Затем вам нужно будет, наконец, отредактировать файл конфигурации .zshrc, как показано ниже.\n\n\nДалее нам понадобится тема Gnome Terminal Dracula со всеми инструкциями\n\nНа самом деле мне потребовалось бы много времени, чтобы задокументировать каждый шаг, поэтому я создал пошаговое видео процесса. (Нажмите на изображение ниже)\n\n\n\nЕсли вы дочитали до этого момента, значит, мы закончили наш раздел Linux в #90DaysOfDevOps. Я снова открыт для отзывов и дополнений к ресурсам здесь.\n\nЯ также подумал, что было проще показать вам многие шаги с помощью видео, чем записывать их здесь, что вы думаете об этом? У меня есть цель вернуться к этим дням и, где это возможно, создать видео-пошаговые руководства, чтобы добавить и, возможно, лучше объяснить и показать некоторые вещи, которые мы рассмотрели. Что вы думаете?\n\nРесурсы\n\nBash in 100 seconds\nBash script with practical examples - Full Course\nClient SSH GUI - Remmina\nThe Beginner's guide to SSH\nVim in 100 Seconds\nVim tutorial\nLearn the Linux Fundamentals - Part 1\nLinux for hackers (don't worry you don't need to be a hacker!)\n\nЗавтра мы начинаем наши 7 дней погружения в сетевое взаимодействие, мы будем стараться получить базовые знания и понимание сетевого взаимодействия (Networking) в DevOps.\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day21",
            "title": "21. DevOps настройка сети",
            "description": "Общая картина - DevOps и Сеть",
            "content": "Общая картина: DevOps и Сеть\n\nДобро пожаловать в День 21! Мы собираемся заняться сетевыми технологиями в течение следующих 7 дней. Сеть и DevOps являются всеобъемлющей темой, но нам также необходимо изучить некоторые основы сетевых технологий.\n\nВ конечном счете, как мы уже говорили ранее, DevOps — это культура и изменение процессов в ваших организациях. Как мы уже говорили, это могут быть виртуальные машины, контейнеры, Kubernetes, но это также может быть и сеть. Если мы используем эти принципы DevOps для нашей инфраструктуры, которая чтобы включить сеть более точно с точки зрения DevOps, вам также необходимо знать о сети, а также о различных топологиях, сетевых инструментах и ​​стеках, которые у нас есть.\n\nЯ бы сказал, что наши сетевые устройства должны быть настроены с использованием инфраструктуры как кода, и все должно быть автоматизировано, как и наши виртуальные машины, но для этого мы должны хорошо понимать, что мы автоматизируем.\nЧто такое NetDevOps | Сетевой DevOps?\n\nВы также можете услышать термины Network DevOps или NetDevOps. Возможно, вы уже являетесь сетевым инженером (network engineer) и хорошо разбираетесь в сетевых компонентах в инфраструктуре, вы понимаете элементы, используемые в сети, такие как DHCP, DNS, NAT и т. д. и т. д. У вас также будет хорошее понимание аппаратных или программно-определяемых сетей. опции, коммутаторы, маршрутизаторы и т.д. и т.п.\n\nНо если вы не сетевой инженер, то нам, вероятно, необходимо получить базовые знания по всем направлениям в некоторых из этих областей, чтобы мы могли понять конечную цель Network DevOps.\n\nНо в отношении этих терминов мы можем думать о NetDevOps или Network DevOps как о применении принципов и практик DevOps к сети, применении инструментов управления версиями и автоматизации к созданию, тестированию, мониторингу и развертыванию сети.\n\nЕсли мы думаем о сетевой DevOps как о необходимости автоматизации, мы упоминали ранее о том, что DevOps разрушает разрозненность между командами. Если сетевые команды не перейдут на аналогичную модель и процесс, они станут узким местом или даже полным провалом.\n\nИспользование принципов автоматизации подготовки, настройки, тестирования, контроля версий и развертывания — отличное начало. Автоматизация в целом обеспечит скорость развертывания, стабильность сетевой инфраструктуры и последовательное улучшение, а также процесс, который будет совместно использоваться в нескольких средах после их тестирования. Например, полностью протестированная сетевая политика, которая была полностью протестирована в одной среде, может быть быстро использована в другом месте из-за характера этого в коде, а не в процессе, созданном вручную, как это могло быть раньше.\nДействительно хорошую точку зрения и схему этого мышления можно найти здесь. Сетевой DevOps\nNetworking - основы\n\nДавайте для начала забудем о стороне DevOps, и теперь нам нужно очень кратко взглянуть на некоторые основы работы в сети.\nСетевые устройства\n\nHost — это любые устройства, которые отправляют или получают трафик.\n\n\nIP Address \"определение\" каждого хоста. (адрес)\n\n\n\nNetwork  — (Сеть) это то, что транспортирует трафик между хостами. Если бы у нас не было сетей, было бы много ручного перемещения данных!\nЛогическая группа хостов, для которых требуется аналогичное подключение.\n\n\nSwitches (Коммутаторы) облегчают связь внутри сети. Коммутатор пересылает пакеты данных между хостами. Коммутатор отправляет пакеты напрямую хостам.\nСеть: группа хостов, которым требуется одинаковое подключение.\nХосты в сети используют одно и то же пространство IP-адресов.\n\nМаршрутизатор (Router) облегчает связь между сетями. Если мы сказали ранее, что коммутатор следит за связью внутри сети, маршрутизатор позволяет нам объединить эти сети или, по крайней мере, предоставить им доступ друг к другу, если это разрешено.\n\nМаршрутизатор может обеспечить точку контроля трафика (безопасность, фильтрация, перенаправление). Все больше и больше коммутаторов теперь также предоставляют некоторые из этих функций.\n\nМаршрутизаторы узнают, к каким сетям они подключены. Это известно как маршруты, таблица маршрутизации — это все сети, о которых знает маршрутизатор.\n\nМаршрутизатор имеет IP-адрес в сетях, к которым он подключен. Этот IP-адрес также будет использоваться каждым хостом за пределами их локальной сети, также известной как шлюз.\n\nМаршрутизаторы также создают иерархию в сетях, о которой я упоминал ранее.\n\nКоммутаторы и маршрутизаторы (Switches vs Routers )\n\nМаршрутизация – это процесс перемещения данных между сетями.\n\nМаршрутизатор — это устройство, основной задачей которого является маршрутизация.\n\nКоммутация — это процесс перемещения данных в сети.\n\nКоммутатор — это устройство, основное назначение которого — коммутация.\n\nЭто во многом базовый обзор устройств, поскольку мы знаем, что существует множество различных сетевых устройств, таких как:\n\nAccess Points\nFirewalls\nLoad Balancers\nLayer 3 Switches\nIDS / IPS\nProxies\nVirtual Switches\nVirtual Routers\n\nХотя все эти устройства будут выполнять маршрутизацию и/или коммутацию.\n\nВ течение следующих нескольких дней мы собираемся узнать немного больше об этом списке.\n\nOSI Model\nNetwork Protocols\nDNS (Domain Name System)\nNAT\nDHCP\nSubnets\n\nРесурсы\n\nComputer Networking full course\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day22",
            "title": "22. Открытая сетевая модель OSI",
            "description": "7 уровней модели OSI",
            "content": "Модель OSI — 7 уровней\n\nОбщая цель сети как отрасли состоит в том, чтобы позволить двум хостам обмениваться данными. Если я хочу передать данные от одного хоста к другому хосту, мне нужно будет что-то подключить к этому хосту, перейти к другому хосту, подключить его к первому хосту.\n\nСеть позволяет нам автоматизировать это, позволяя хосту автоматически обмениваться данными по сети, и для этого эти хосты должны следовать набору правил.\n\nЭто ничем не отличается от любого другого языка. У английского есть набор правил, которым должны следовать два носителя английского языка. У испанского есть свой собственный набор правил.\n\nПравила организации сети разделены на семь разных уровней, и эти уровни известны как модель OSI.\nВведение в модель OSI\n\nМодель OSI (модель взаимодействия открытых систем)/(Open Systems Interconnection Model) — это структура, используемая для описания функций сетевой системы. Модель OSI характеризует вычислительные функции в виде универсального набора правил и требований для обеспечения функциональной совместимости между различными продуктами и программным обеспечением. В эталонной модели OSI обмен данными между вычислительной системой разделен на семь различных уровней абстракции: физический, канальный, сетевой, транспортный, сеансовый, презентационный и прикладной (Physical, Data Link, Network, Transport, Session, Presentation, Application).\n\nФизический\nУровень 1 в модели OSI, известный как физический, предполагает возможность передачи данных с одного хоста на другой с помощью средств, будь то физический кабель или мы также можем рассмотреть Wi-Fi на этом уровне. Мы также можем увидеть здесь более устаревшее оборудование вокруг концентраторов и повторителей для передачи данных с одного хоста на другой.\n\nКанал передачи данных\nУровень 2, канал передачи данных обеспечивает передачу данных от узла к узлу, где данные упакованы в кадры. Существует также уровень исправления ошибок, которые могли возникнуть на физическом уровне. Здесь мы также вводим или впервые видим MAC-адреса.\n\nЗдесь мы видим первое упоминание о коммутаторах, о которых мы рассказали в первый день нашей работы с сетью День 21\n\nСеть\nВы, вероятно, слышали термин «коммутаторы уровня 3» или «коммутаторы уровня 2». В нашей модели OSI уровень 3. Цель сети — прямая(end to end) доставка, именно здесь мы видим наши IP-адреса, также упомянутые в обзоре первого дня.\n\nМаршрутизаторы и хосты существуют на уровне 3, помните, что маршрутизатор — это возможность маршрутизации между несколькими сетями. Все, что имеет IP, может считаться уровнем 3.\n\nТак зачем же нам нужны схемы адресации как на уровне 2, так и на уровне 3? (MAC-адреса и IP-адреса)\n\nЕсли мы подумаем о передаче данных с одного хоста на другой, каждый хост имеет IP-адрес, но между ними есть несколько коммутаторов и маршрутизаторов. Каждое из устройств имеет этот MAC-адрес уровня 2.\n\nMAC-адрес уровня 2 будет передаваться только от хоста к коммутатору/маршрутизатору, он ориентирован на переходы, где IP-адреса уровня 3 будут оставаться с этим пакетом данных, пока он не достигнет своего конечного хоста. (Концы с концами)\n\nIP-адреса — уровень 3 = сквозная доставка\n\nMAC-адреса — уровень 2 = доставка между переходами\n\nТеперь есть сетевой протокол, который мы рассмотрим, но не сегодня, называемый ARP (протокол разрешения адресов), который связывает наши адреса Layer3 и Layer2.\nТранспорт\nПредоставление услуг между услугами, уровень 4 предназначен для различения потоков данных. Точно так же, как уровни 3 и 2 имели свои схемы адресации, на уровне 4 у нас есть порты.\n\nСессия, Презентация, Приложение\nРазличие между слоями 5, 6, 7 немного расплывчато\n\nСтоит взглянуть на IP-модель TCP, чтобы получить более свежее представление.\n\nДавайте теперь попробуем объяснить, что на самом деле происходит, когда хосты общаются друг с другом, используя этот сетевой стек. На одном хосте есть приложение, которое будет генерировать данные, предназначенные для отправки на другой хост.\n\nИсходный хост будет проходить так называемый процесс инкапсуляции. Эти данные будут сначала отправлены на уровень 4.\n\nУровень 4 добавит заголовок к этим данным, что может облегчить задачу уровня 4, которая заключается в доставке услуг. Это будет порт, использующий либо TCP, либо UDP. Он также будет включать исходный порт и порт назначения.\n\nЭто также может быть известно как сегмент (данные и порт).\n\nЭтот сегмент будет передан по стеку osi на уровень 3, сетевой уровень, сетевой уровень добавит к этим данным еще один заголовок.\nЭтот заголовок будет способствовать цели уровня 3, который является сквозной доставкой, что означает, что в этом заголовке у вас будет IP-адрес источника и IP-адрес назначения, заголовок плюс данные также могут называться пакетом.\n\nЗатем уровень 3 возьмет этот пакет и передаст его уровню 2, уровень 2 еще раз добавит еще один заголовок к этим данным для достижения цели уровня 2 по доставке переходов, что означает, что этот заголовок будет включать в себя MAC-адреса источника и получателя.\nЭто называется кадром, когда у вас есть заголовок и данные уровня 2.\n\nЗатем этот кадр преобразуется в единицы и нули и отправляется по физическому кабелю уровня 1 или Wi-Fi.\n\n\nВыше я упомянул названия для каждого уровня заголовка и данных, но решил нарисовать и это.\n\n\n\nОчевидно, что приложение, отправляющее данные, отправляется куда-то, поэтому получение происходит в обратном порядке, чтобы получить эту резервную копию в стеке и на принимающем хосте.\n\nРесурсы\n\nComputer Networking full course\nPractical Networking\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day23",
            "title": "23. Протоколы сети",
            "description": "Протоколы сети",
            "content": "Протоколы сети\nНабор правил и сообщений, образующих стандарт.\n\nARP - Address Resolution Protocol - протокол разрешения адресов\n\nЕсли вы хотите по-настоящему разобраться в ARP, вы можете прочитать Internet Standard здесь RFC 826\n\nARP соединяет IP-адреса с фиксированными физическими адресами машин, также известными как MAC-адреса, в сети уровня 2.\n\n\n\n\nFTP - File Transfer Protocol - протокол передачи файлов\n\nПозволяет передавать файлы из источника в место назначения. Как правило, этот процесс аутентифицируется, но при настройке можно использовать анонимный доступ. Теперь вы будете чаще видеть FTPS, который обеспечивает подключение SSL/TLS к FTP-серверам от клиента для повышения безопасности. Этот протокол можно найти на прикладном уровне модели OSI.\n\n\n\nSMTP - Simple Mail Transfer Protocol -  протокол передачи почты\n\nПочтовые серверы, используемые для передачи электронной почты, используют SMTP для отправки и получения почтовых сообщений. Вы по-прежнему обнаружите, что даже с Microsoft 365 протокол SMTP используется для той же цели.\n\n\n\nHTTP - Hyper Text Transfer Protocol - Протокол передачи гипертекста\n\nHTTP является основой Интернета и просмотра контента. Дает нам возможность легко получить доступ к нашим любимым веб-сайтам. HTTP по-прежнему широко используется, но HTTPS используется или должен использоваться на большинстве ваших любимых сайтов.\n\n\n\nSSL - Secure Sockets Layer | TLS - Transport Layer Security - Уровень защищенных сокетов | TLS — безопасность транспортного уровня\n\nTLS заменил SSL, TLS — это криптографический протокол, который обеспечивает безопасность связи по сети. Его можно найти в почте, мессенджерах и других приложениях, но чаще всего он используется для защиты HTTPS.\n\n\n\nHTTPS - HTTP secured with SSL/TLS - HTTP, защищенный с помощью SSL/TLS\n\nРасширение HTTP, используемое для безопасной связи по сети, HTTPS шифруется с помощью TLS, как упоминалось выше. Основное внимание здесь уделялось обеспечению аутентификации, конфиденциальности и целостности при обмене данными между хостами.\n\n\n\nDNS - Domain Name System - система доменных имен\n\nDNS используется для сопоставления удобных для человека доменных имен, например, все мы знаем google.com, но если вы откроете браузер и введете 8.8.8.8 вы получите Google в том виде, в каком мы его знаем. Однако удачи вам в попытках запомнить все IP-адреса всех ваших веб-сайтов, на некоторых из них мы даже используем Google для поиска информации.\n\nИменно здесь в дело вступает DNS, он гарантирует доступность хостов, служб и других ресурсов.\n\nНа всех хостах, если им требуется подключение к Интернету, они должны иметь DNS, чтобы иметь возможность разрешать эти доменные имена. DNS — это область, на изучение которой вы можете потратить дни и годы. Я бы также сказал по опыту, что DNS в основном является распространенной причиной всех ошибок, когда речь идет о сети. Однако не уверен, что сетевой инженер согласится с этим.\n\n\n\nDHCP - Dynamic Host Configuration Protocol - Протокол динамического конфигурирования сервера\n\nМы много обсуждали протоколы, необходимые для работы наших хостов, будь то доступ в Интернет или передача файлов между собой.\n\nНа каждом хосте нам нужны 4 вещи, чтобы он мог выполнять обе эти задачи.\n\nIP Address\nSubnet Mask\nDefault Gateway\nDNS\n\nМы рассмотрели IP-адрес, являющийся уникальным адресом для вашего хоста в сети, в которой он находится, мы можем думать об этом как о нашем домашнем номере.\n\nМаску подсети мы скоро рассмотрим, но вы можете думать об этом как о почтовом индексе или почтовом индексе.\n\nШлюз по умолчанию — это IP-адрес нашего маршрутизатора, как правило, в нашей сети, предоставляющий нам возможность подключения уровня 3. Вы могли бы думать об этом как о единственной дороге, которая позволяет нам покинуть нашу улицу.\n\nЗатем у нас есть DNS, как мы только что рассмотрели, чтобы помочь нам преобразовать сложные общедоступные IP-адреса в более подходящие и запоминающиеся доменные имена. Может быть, мы можем думать об этом как о гигантском сортировочном офисе, чтобы убедиться, что мы получаем правильный пост.\n\nКак я уже сказал, каждому хосту требуются эти 4 вещи, если у вас 1000 или 10 000 хостов, вам потребуется очень много времени, чтобы определить каждый из них по отдельности. Здесь в дело вступает DHCP, который позволяет вам определить область действия вашей сети, а затем этот протокол будет распространяться на все доступные хосты в вашей сети.\n\nДругой пример: вы идете в кафе, берете кофе и садитесь за свой ноутбук, или ваш телефон позволяет назвать это вашим хостом. Вы подключаете свой хост к Wi-Fi в кофейне, и вы получаете доступ к Интернету, сообщения и почта начинают пинговаться, и вы можете просматривать веб-страницы и социальные сети. Когда вы подключались к Wi-Fi в кофейне, ваша машина получала DHCP-адрес либо от выделенного DHCP-сервера, либо, скорее всего, от маршрутизатора, который также обрабатывает DHCP.\n\nSubnetting - Подсети\n\nПодсеть — это логическое подразделение IP-сети.\n\nПодсети разбивают большие сети на более мелкие, более управляемые сети, которые работают более эффективно.\n\nКаждая подсеть является логическим подразделением большей сети. Подключенные устройства с достаточным количеством подсетей имеют общий идентификатор IP-адреса, что позволяет им взаимодействовать друг с другом.\n\nМаршрутизаторы управляют связью между подсетями.\n\nРазмер подсети зависит от требований к подключению и используемой сетевой технологии.\n\nОрганизация несет ответственность за определение своего количества и размера подсетей в пределах адресного пространства.\nдоступны, и детали остаются локальными для этой организации. Подсети также могут быть сегментированы на еще более мелкие подсети для таких вещей, как соединения «точка-точка», или для подсетей, поддерживающих несколько устройств.\n\nСреди прочих преимуществ сегментация крупных\nсети в подсети включает IP-адрес\nперераспределение и уменьшает перегрузку сети, оптимизацию, сетевую связь и эффективность.\n\nПодсети также могут повысить безопасность сети.\nЕсли часть сети скомпрометирована, ее можно поместить в карантин, что затруднит перемещение злоумышленников по более крупной сети.\n\nРесурсы\n\nComputer Networking full course\nPractical Networking\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day24",
            "title": "24. Автоматизация сети",
            "description": "Автоматизация сети",
            "content": "\nАвтоматизация сети\n\nОсновы сетевой автоматизации\n\nОсновные задачи для сетевой автоматизации\n\nТестирование устройств и проверка конфигурации;\nИнициализация развернутых физических устройств и сервисов, а также развертывание и инициализация виртуальных устройств;\nСбор сетевых данных, относящихся к устройствам, системам, программному обеспечению, топологии сети, трафику и сервисам в реальном времени;\nАнализ данных, в том числе упреждающая аналитика ИИ и машинного обучения, для обеспечения информации о текущем и будущем поведении сети;\nПроверка соответствия конфигурации требованиям для обеспечения правильной работы всех сетевых устройств и сервисов;\nОбновление программного обеспечения, включая откат программного обеспечения при необходимости;\nЗамкнутая коррекция проблем с сетью, включая поиск и устранение неисправностей, а также исправление сложных и трудновыявляемых сбоев;\nПодробный анализ отчетов, панелей наблюдения, оповещений и предупреждений;\nРеализация требований безопасности;\nМониторинг сети и ее сервисов для поддержания уровня обслуживания и удовлетворенности клиентов\n\nПроцесс внедрения автоматизации специфичен для каждого бизнеса. Когда дело доходит до развертывания автоматизации, не существует универсального решения. Способность определить и использовать подход, который лучше всего подходит для вашей организации, имеет решающее значение для продвижения к поддержке и созданию более гибкой среды для пользователей. (Мы обсуждали что-то подобное в самом начале в отношении всего DevOps, изменения культуры и автоматизированного процесса, который это приносит)\n\nЧтобы разобраться во всем, вам нужно будет определить, как задача или процесс, которые вы пытаетесь автоматизировать, будут улучшать опыт конечного пользователя или ценность для бизнеса, следуя пошаговому систематическому подходу.\n\n«Если не знаешь, куда идешь, любая дорога приведет тебя туда».\n\nИмея структуру проекта, которую вы пытаетесь достичь, зная, какова ваша конечная цель, а затем шаг за шагом работая над достижением этой цели, измеряйте успех автоматизации на различных этапах на основе бизнес-результатов.\n\nСоздавайте концепции, моделируя существующие приложения. Нет необходимости разрабатывать концепции автоматизации в пузыре, потому что их нужно применять к вашему приложению, вашему сервису, вашей инфраструктуре, поэтому начните создавать концепции и моделировать их вокруг вашей существующей инфраструктуры, вы повторно существующие приложения.\n\nПодход к автоматизации сети\n\nМы должны определить задачи и выполнить обнаружение запросов на изменение сети, чтобы у вас были наиболее распространенные проблемы и проблемы, решение которых нужно автоматизировать.\n\nСоставьте список всех запросов на изменение и рабочих процессов, которые в настоящее время обрабатываются вручную.\nОпределить наиболее распространенные, трудоемкие и подверженные ошибкам действия.\nПриоритизируйте запросы, используя бизнес-ориентированный подход.\nЭто основа построения процесса автоматизации, что нужно автоматизировать, а что нет.\n\nЗатем мы должны разделить задачи и проанализировать, как разные сетевые функции работают и взаимодействуют друг с другом.\n\nКоманда инфраструктуры/сети получает заявки на изменения на нескольких уровнях для развертывания приложений.\nНа основе сетевых сервисов разделить их на разные области и понять, как они взаимодействуют друг с другом.\n  Оптимизация приложений\n  ADC (контроллер доставки приложений) (Application Delivery Controller)\n  Межсетевой экран\n  DDI (DNS, DHCP, IPAM и т. д.)\n  Маршрутизация\n  Другие\nОпределите различные зависимости, чтобы устранить деловые и культурные различия и обеспечить сотрудничество между командами.\n\nПовторно используемые политики, определение и упрощение повторно используемых сервисных задач, процессов и ввода/вывода.\n\nОпределить предложения для различных услуг, процессов и ввода/вывода.\nУпрощение процесса развертывания сократит время выхода на рынок как новых, так и существующих рабочих нагрузок.\nКогда у вас есть стандартный процесс, его можно упорядочить и согласовать с отдельными запросами для многопоточного подхода и доставки.\n\nОбъедините политики со специфическими для бизнеса действиями. Как внедрение этой политики помогает бизнесу? Экономит время? Экономит деньги? Обеспечивает лучший бизнес-результат?\n\nУбедитесь, что сервисные задачи совместимы.\nСвяжите добавочные сервисные задачи, чтобы они соответствовали созданию бизнес-сервисов.\nОбеспечьте гибкость связывания и повторного связывания сервисных задач по запросу.\nРазверните возможности самообслуживания и проложите путь к повышению операционной эффективности.\nРазрешить несколько наборов технологических навыков продолжать вносить свой вклад в надзор и соответствие.\n\nУправляйте политиками и процессами, добавляя и улучшая их, сохраняя при этом доступность и обслуживание.\n\nНачните с малого, автоматизировав существующие задачи.\nОзнакомьтесь с процессом автоматизации, чтобы вы могли определить другие области, которые могут выиграть от автоматизации.\nповторяйте свои инициативы по автоматизации, постепенно добавляя гибкость при сохранении требуемой доступности.\nИспользование поэтапного подхода прокладывает путь к успеху!\n\nОркестрируйте сетевой сервис!\n\nДля быстрой доставки приложений требуется автоматизация процесса развертывания.\nСоздание гибкой сервисной среды требует управления различными элементами в рамках набора технологических навыков.\nПодготовьтесь к комплексной оркестровке, обеспечивающей контроль над автоматизацией и порядком развертывания.\n\nИнструменты автоматизации сети\n\nХорошей новостью здесь является то, что по большей части инструменты, которые мы используем для автоматизации сети, как правило, те же, что мы будем использовать для других областей автоматизации.\n\nОпреационная система. Большую часть своего обучения я сосредоточился на использовании инструментов под Linux. Но почти все инструменты, которых мы коснемся, кросплатформенные.\n\nИнтегрированная среда разработки (IDE). Опять же, здесь особо нечего сказать, кроме всего прочего, я бы предложил Visual Studio Code в качестве вашей IDE, основываясь на обширных подключаемых модулях, доступных для стольких разных языков.\n\nУправление конфигурацией. Мы еще не добрались до раздела «Управление конфигурацией», но совершенно очевидно, что Ansible является фаворитом в этой области для управления и автоматизации конфигураций. Ansible написан на Python, но вам не нужно знать Python.\nLink to Ansible Network Modules\n\nМы также коснемся Ansible Tower в разделе управления конфигурацией, рассматривая его как внешний интерфейс с графическим интерфейсом (GUI) для Ansible.\n\nCI/CD. Мы рассмотрим больше концепций и инструментов, связанных с этим, но важно хотя бы упомянуть здесь, поскольку это охватывает не только сеть, но и все предоставление услуг и платформ.\n\nВ частности, Jenkins предоставляет или кажется популярным инструментом для сетевой автоматизации.\n\nОтслеживает репозиторий git на наличие изменений, а затем инициирует их.\n\nКонтроль версий (Version Control). Углубимся в это позже.\n\nGit обеспечивает контроль версий вашего кода на локальном устройстве - Кроссплатформенность\nGitHub, GitLab, BitBucket и т. д. — это онлайн-сайты, на которых вы определяете свои репозитории и загружаете свой код.\n\nLanguage | Scripting. Что-то, что мы здесь не рассмотрели, это Python как язык, я решил вместо этого погрузиться в Go как язык программирования, исходя из моих обстоятельств, я бы сказал, что это был тесный контакт между Golang и Python и Python, кажется, Победитель в категории «Сетевая автоматизация».\n\nЗдесь стоит упомянуть Nornir, фреймворк автоматизации, написанный на Python. Кажется, что это берет на себя роль Ansible, но особенно в отношении сетевой автоматизации. Документация Nornir\n\nАнализ API. Postman — отличный инструмент для анализа RESTful API. Помогает создавать, тестировать и изменять API.\n\nPOST >>> Для создания объектов ресурсов.\nGET >>> Для получения ресурсов.\nPUT >>> Для создания или замены ресурсов.\nPATCH >>> Для создания или обновления объекта ресурсов.\nDelete >>> Чтобы удалить ресурс\n\nPostman tool Download\n\nЕще инструменты\n\nCisco NSO (Network Services Orchestrator)\n\nNetYCE - Simplify Network Automation\n\nNetwork Test Automation\n\nВ течение следующих 3 дней я планирую более подробно изучить некоторые вещи, которые мы рассмотрели, и поработать над Python и сетевой автоматизацией.\n\nДо сих пор мы далеко не охватили все сетевые темы, но хотели сделать это достаточно широким, чтобы следовать за ним и продолжать учиться на ресурсах, которые я добавляю ниже.\n\nРесурсы\n\n3 Necessary Skills for Network Automation\nComputer Networking full course\nPractical Networking\nPython Network Automation\n",
            "tags": [
                "devops",
                "Ansible"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day25",
            "title": "25. Автоматизация сети с помощью Python",
            "description": "Автоматизация сети с помощью Python",
            "content": "\nPython для автоматизации сети\n\nPython — это стандартный язык, используемый для автоматизированных сетевых операций.\n\nХотя это не только автоматизация сети, кажется, что оно везде, когда вы ищете ресурсы, и, как упоминалось ранее, если это не Python, то обычно это Ansible, который также написан на Python.\n\nЯ думаю, что уже упоминал об этом, но в разделе «Изучение языка программирования» я выбрал Golang, а не Python, по причинам, связанным с тем, что моя компания разрабатывает Go, так что это было хорошей причиной для меня, чтобы учиться, но если это не так, тогда Python взял бы это время.\n\nУдобочитаемость и простота использования. Кажется, что Python просто имеет смысл. Похоже, что в коде нет требований к {} для начального и конечного блоков. Соедините это с сильной IDE, такой как VS Code, у вас будет довольно легкий старт, если вы хотите запустить какой-либо код Python.\n\nPycharm может быть еще одной IDE, о которой стоит упомянуть.\n\nБиблиотеки. Расширяемость Python - это настоящая золотая жила, я упоминал ранее, что это не только для сетевой автоматизации, но на самом деле существует множество библиотек для всех видов устройств и конфигураций. Вы можете увидеть огромное количество здесь PyPi\n\nЕсли вы хотите загрузить библиотеку на свою рабочую станцию, вы используете инструмент под названием «pip», чтобы подключиться к PyPI и загрузить его локально. Сетевые поставщики, такие как Cisco, Juniper и Arista, разработали библиотеки для облегчения доступа к своим устройствам.\n\nМощный и эффективный - Помните, во времена Go я прошел сценарий \"Hello World\", и мы прошли, кажется, 6 строк кода? В Питоне это\n\nprint('hello world')\n\nСложите все вышеперечисленные пункты вместе, и должно быть легко понять, почему Python обычно упоминается как инструмент де-факто при работе над автоматизацией.\n\nЯ думаю, важно отметить, что, возможно, несколько лет назад существовали сценарии, которые могли взаимодействовать с вашими сетевыми устройствами, чтобы, возможно, автоматизировать резервное копирование конфигурации или собирать журналы и другую информацию о ваших устройствах. Автоматизация, о которой мы здесь говорим, немного отличается, потому что общий сетевой ландшафт также изменился, чтобы лучше соответствовать этому образу мышления и обеспечить большую автоматизацию.\n\nПрограммно-определяемая сеть (Software-Defined Network). Контроллеры SDN несут ответственность за доставку конфигурации уровня управления на все устройства в сети, что означает только единую точку контакта для любых изменений в сети, больше не требуется telnet или SSH для доступа к каждому устройству, а также полагаются на люди, чтобы сделать это, что имеет повторяющийся шанс отказа или неправильной конфигурации.\n\nОркестрация высокого уровня (High-Level Orchestration ). Поднимитесь на уровень выше этих контроллеров SDN, и это позволит оркестровать уровни обслуживания, а затем интегрировать этот уровень оркестровки в выбранные вами платформы, VMware, Kubernetes, общедоступные облака и т. д.\n\nУправление на основе политик (Policy-based management) - Что вы хотите иметь? Какое желаемое состояние? Вы описываете это, и в системе есть все детали, как это понять, чтобы стать желаемым состоянием.\n\nНастройка рабочей среды\n\nНе у всех есть доступ к физическим маршрутизаторам, коммутаторам и другим сетевым устройствам.\n\nЯ хотел дать нам возможность ознакомиться с некоторыми из ранее упомянутых инструментов, а также получить практические навыки и научиться автоматизировать настройку наших сетей.\n\nКогда дело доходит до вариантов, есть несколько, из которых мы можем выбрать.\n\nGNS3 VM\nEve-ng\nUnimus\n\nМы построим нашу среду, используя Eve-ng, как упоминалось ранее, вы можете использовать физическое устройство, но, честно говоря, виртуальная среда означает, что у нас может быть среда-песочница. для тестирования множества различных сценариев. Кроме того, может быть интересна возможность играть с различными устройствами и топологиями.\n\nМы собираемся делать все на EVE-NG с изданием сообщества.\n\nНачало\n\nИздание сообщества поставляется в форматах ISO и OVF для загрузки.\n\nМы будем использовать загрузку в формате OVF, но в случае с ISO есть возможность сборки на «голом железе» без использования гипервизора.\n\n\n\nДля нашего пошагового руководства мы будем использовать VMware Workstation, поскольку у меня есть лицензия через мой vExpert, но вы в равной степени можете использовать VMware Player или любой другой вариант, упомянутый в документации. К сожалению, мы не можем использовать нашу ранее созданную среду в Virtual box!\n\nЗдесь также у меня возникла проблема с GNS3 с Virtual Box, хотя он и поддерживается.\n\nDownload VMware Workstation Player - FREE\n\nVMware Workstation PRO. Есть бесплатный пробный период.\n\nУстановка на VMware Workstation PRO\n\nТеперь у нас загружено и установлено программное обеспечение hypervisor, а также загружен файл EVE-NG OVF.\nТеперь мы готовы к настройке.\nОткройте VMware Workstation, а затем выберите file -> open.\n\n\n\nКогда вы загружаете образ EVE-NG OVF, он будет находиться в сжатом файле. Извлеките содержимое в свою папку, чтобы оно выглядело так.\n\n\nПерейдите в папку, в которую вы загрузили образ EVE-NG OVF, и начните импорт.\nДайте ему узнаваемое имя и сохраните виртуальную машину где-нибудь в вашей системе.\n\n\n\nКогда процесс импорта завершится, увеличьте количество процессоров до 4 и объем выделенной памяти до 8 ГБ. (Это должно быть после импорта с последней версией, если нет, то отредактируйте настройки ВМ)\n\nТакже убедитесь, что установлен флажок Virtualise Intel VT-x/EPT или AMD-V/RVI. Этот параметр указывает рабочей станции VMware передавать флаги виртуализации гостевой ОС (вложенная виртуализация). Это была проблема, с которой я столкнулся с GNS3 с Virtual Box, хотя мой процессор это позволяет.\n\nВключение и доступ\n\nПримечание и кроличья нора: помните, я упоминал, что это не будет работать с VirtualBox! Ну да, была такая же проблема с VMware Workstation и EVE-NG, но это не вина платформы виртуализации!\n\nУ меня есть WSL2, работающий на моей машине с Windows, и это, похоже, лишает возможности запускать что-либо, вложенное в вашу среду. Я смущен тем, почему виртуальная машина Ubuntu работает, поскольку она, кажется, устраняет аспект виртуализации Intel VT-d ЦП при использовании WSL2.\n\nЧтобы решить эту проблему, мы можем запустить следующую команду на нашем компьютере с Windows и перезагрузить систему, обратите внимание, что, пока она отключена, вы не сможете использовать WSL2.\n\nbcdedit /set hypervisorlaunchtype off\n\nЕсли вы хотите вернуться и использовать WSL2, вам нужно будет запустить эту команду и перезагрузиться.\n\nbcdedit /set hypervisorlaunchtype auto\n\nОбе эти команды нужно запускать от имени администратора!\n\nХорошо, вернемся к шоу. Теперь у вас должна быть включенная машина в VMware Workstation, и у вас должно появиться приглашение, похожее на это.\n\n\n\nДанные для входа:\n\nusername = root\npassword = eve\n\nЗатем вас попросят снова ввести пароль root, который позже будет использоваться для SSH-соединения с хостом.\nЗатем мы можем изменить имя хоста.\n\n\nЗатем мы определяем доменное имя DNS, я использовал имя ниже, но я не уверен, нужно ли будет его изменить позже.\n\n\nЗатем мы настраиваем сеть, я выбираю статический, чтобы указанный IP-адрес оставался постоянным после перезагрузки.\n\nНа последнем шаге укажите статический IP-адрес из сети, доступной с вашей рабочей станции.\n\nЗдесь есть несколько дополнительных шагов, где вам нужно будет указать маску подсети для вашей сети, шлюз по умолчанию и DNS.\nПосле завершения он перезагрузится, когда будет выполнено резервное копирование, вы можете взять свой статический IP-адрес и ввести его в свой браузер.\n\nИмя пользователя по умолчанию для графического интерфейса — «admin», пароль — «eve», а имя пользователя по умолчанию для SSH — «root» и пароль — «eve», но это было бы изменено, если бы вы изменили его во время установки.\n\n\nЯ выбрал HTML5 для консоли вместо нативной, так как это откроет новую вкладку в вашем браузере, когда вы будете перемещаться по разным консолям.\n\nДалее мы собираемся:\n\nУстановить клиентский пакет EVE-NG\nЗагрузить некоторые сетевые образы в EVE-NG.\nПостроить топологию сети\nДобавить \"ноды\" (машины/хосты, Nodes)\nСоединить ноды между собой\nНачнем создавать скрипты Python\nПосмотрим на telnetlib, Netmiko, Paramiko и Pexpect\n\nРесурсы\n\nFree Course: Introduction to EVE-NG\nEVE-NG - Creating your first lab\n3 Necessary Skills for Network Automation\nComputer Networking full course\nPractical Networking\nPython Network Automation\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day26",
            "title": "26. Развертывание виртуальной лаборатории EVE-NG в домашних условиях",
            "description": "Развертывание виртуальной лаборатории EVE-NG в домашних условиях",
            "content": "\nСоздание нашей лаборатории\n\nМы собираемся продолжить настройку нашей эмулируемой сети с помощью EVE-NG, а затем, надеюсь, развернуть несколько устройств и начать думать о том, как мы можем автоматизировать настройку этих устройств. В День 25 мы рассказали об установке EVE-NG на нашу машину с помощью VMware Workstation.\nУстановка клиента EVE-NG\n\nСуществует также клиентский пакет, который позволяет нам выбирать, какое приложение используется при подключении к устройствам по SSH. Он также настроит Wireshark для захвата пакетов между ссылками. Вы можете установить клиентский пакет для своей ОС (Windows, macOS, Linux).\n\nEVE-NG Client Download\n\n\n\nПодсказка: если вы используете Linux в качестве клиента, то есть этот клиентский пакет.\n\nУстановка проста: next, next и я бы посоветовал оставить значения по умолчанию.\nПолучение сетевых образов\n\nЭтот шаг непростой, я просмотрел несколько видеороликов, на которые я дам ссылки в конце, которые ссылаются на некоторые ресурсы и загрузки для нашего маршрутизатора и переключают изображения, рассказывая нам, как и куда их загрузить.\n\nВажно отметить, что я использую все в образовательных целях. Я бы предложил загрузить официальные образы от сетевых поставщиков.\n\nBlog & Links to YouTube videos\n\nHow To Add Cisco VIRL vIOS image to Eve-ng\n\nВ целом шаги здесь немного сложны и могли бы быть намного проще, но приведенные выше блоги и видео показывают процесс добавления изображений в вашу коробку EVE-NG.\n\nЯ использовал FileZilla для передачи qcow2 на виртуальную машину через SFTP.\n\nДля нашей лаборатории нам нужны Cisco vIOS L2 (коммутаторы) и Cisco vIOS (маршрутизатор).\n\nСоздаем лабораторию\n\nВнутри веб-интерфейса EVE-NG мы собираемся создать нашу новую топологию сети. У нас будет четыре коммутатора и один маршрутизатор, который будет нашим шлюзом во внешние сети.\n\n\n| Node        | IP Address  |\n| ----------- | ----------- |\n| Router      | 10.10.88.110|\n| Switch1     | 10.10.88.111|\n| Switch2     | 10.10.88.112|\n| Switch3     | 10.10.88.113|\n| Switch4     | 10.10.88.114|\n\nДобавление наших узлов в EVE-NG\n\nКогда вы впервые войдете в EVE-NG, вы увидите экран, как показано ниже, мы хотим начать с создания нашей первой лаборатории.\n\n\n\nДайте вашей лаборатории имя, а остальные поля являются необязательными.\n\n\n\nЗатем увидим пустой экран, чтобы начать создание вашей сети. Щелкните правой кнопкой мыши на своем холсте и выберите 'add node'.\n\nДалее появляется длинный список опций. Если вы следовали вышеизложенному, у вас будут два синих, показанных ниже, а остальные будут серыми и недоступными для выбора.\n\n\n\nМы хотим добавить следующее в нашу лабораторию:\n1 x Cisco vIOS Router\n4 x Cisco vIOS Switch\n\nСоединяем наши ноды\n\nТеперь нам нужно добавить возможность подключения между нашими маршрутизаторами и коммутаторами. Мы можем сделать это довольно легко, наведя курсор на устройство и увидев значок подключения, как показано ниже, а затем подключив его к устройству, к которому мы хотим подключиться.\n\n\nКогда вы закончите подключение своей среды, вы также можете добавить способ определения физических границ или местоположений с помощью прямоугольников или кругов, которые также можно найти в контекстном меню. Вы также можете добавить текст, который полезен, когда мы хотим определить наши имена или IP-адреса в наших лабораториях.\n\nЯ пошел дальше и сделал свою лабораторию такой, как показано ниже.\n\n\nYou will also notice that the lab above is all powered off, we can start our lab by selecting everything and right-clicking and selecting start selected.\n\nКак только мы запустим нашу лабораторию, вы сможете подключаться к консоли на каждом устройстве, и вы заметите, что на этом этапе они довольно тупые без настройки. Мы можем добавить некоторую конфигурацию к каждому узлу, скопировав или создав свою собственную в каждом терминале.\n\nЯ оставлю свою конфигурацию в сетевой папке репозитория для справки.\n\n| Node        | Configuration         |\n| ----------- | -----------           |\n| Router      | R1   |\n| Switch1     | SW1 |\n| Switch2     | SW2 |\n| Switch3     | SW3 |\n| Switch4     | SW4 |\n\nРесурсы\n\nFree Course: Introduction to EVE-NG\nEVE-NG - Creating your first lab\n3 Necessary Skills for Network Automation\nComputer Networking full course\nPractical Networking\nPython Network Automation\n\nБольшинство примеров, которые я использую здесь, поскольку я не сетевой инженер, взяты из этой обширной книги, которая не является бесплатной, но я использую некоторые примеры оттуда, чтобы помочь понять автоматизацию сети.\nHands-On Enterprise Automation with Python (Book)\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day27",
            "title": "27. Работа с сетью в Python",
            "description": "Работа с сетью в Python",
            "content": "Практическое знакомство с Python и сетью\n\nВ этом заключительном разделе основ работы с сетью мы рассмотрим некоторые задачи и инструменты автоматизации с помощью нашей лабораторной среды, созданной День 26\n\nМы будем использовать туннель SSH для подключения к нашим устройствам с нашего клиента по сравнению с telnet. Туннель SSH, созданный между клиентом и устройством, зашифрован. Мы также рассмотрели SSH в разделе Linux в День 18\n\nДоступ к нашей виртуальной эмулируемой среде\n\nЧтобы мы могли взаимодействовать с нашими коммутаторами, нам либо нужна рабочая станция внутри сети EVE-NG, и вы можете развернуть там Linux-систему с установленным Python для выполнения вашей автоматизации (Ресурс для настройки Linux внутри EVE-NG) или можно сделать как я и определить облако для доступа со своей рабочей станции.\n\n\n\nДля этого мы щелкнули правой кнопкой мыши на нашем холсте и выбрали сеть, а затем выбрали \"Management(Cloud0)\", чтобы подключиться к нашей домашней сети.\n\n\nОднако внутри этой сети у нас ничего нет, поэтому нам нужно добавить соединения из новой сети на каждое из наших устройств.\nЯ вошел в систему на каждом из наших устройств и выполнил следующие команды для интерфейсов, применимых к тому месту, где появляется облако.\n\nenable\nconfig t\nint gi0/0\nip add dhcp\nno sh\nexit\nexit\nsh ip int br\nПоследний шаг дает нам адрес DHCP из нашей домашней сети. Список сетей моего устройства выглядит следующим образом:\n\n| Node    | IP Address   | Home Network IP |\n| ------- | ------------ | --------------- |\n| Router  | 10.10.88.110 | 192.168.169.115 |\n| Switch1 | 10.10.88.111 | 192.168.169.178 |\n| Switch2 | 10.10.88.112 | 192.168.169.193 |\n| Switch3 | 10.10.88.113 | 192.168.169.125 |\n| Switch4 | 10.10.88.114 | 192.168.169.197 |\n\nSSH к сетевому устройству\n\nИмея все вышеперечисленное, мы теперь можем подключаться к нашим устройствам в нашей домашней сети, используя нашу рабочую станцию. Я использую Putty, но также имею доступ к другим терминалам, таким как git bash, которые дают мне возможность подключаться к нашим устройствам по SSH.\n\nНиже вы можете видеть, что у нас есть SSH-соединение с нашим маршрутизатором. (Р1)\n\nИспользование Python для сбора информации с наших устройств\n\nПервый пример того, как мы можем использовать Python, — это сбор информации со всех наших устройств, и, в частности, я хочу иметь возможность подключаться к каждому из них и запускать простую команду, чтобы предоставить мне конфигурацию и настройки интерфейса. Я сохранил этот скрипт:\n#!/usr/bin/env python\nfrom netmiko import ConnectHandler\nfrom getpass import getpass\n\n#password = getpass()\n\nR1 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.115\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW1 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.178\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW2 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.193\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW3 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.125\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW4 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.197\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\ncommand = \"show ip int brief\"\nfor device in (R1, SW1, SW2, SW3, SW4):\n    net_connect = ConnectHandler(**device)\n    print(net_connect.find_prompt())\n    print(net_connect.send_command(command))\n    net_connect.disconnect()\n\nТеперь, когда я запускаю это, я вижу каждую конфигурацию порта на всех моих устройствах.\n\n\n\nЭто может быть удобно, если у вас много разных устройств, создайте этот один скрипт, чтобы вы могли централизованно контролировать и быстро понимать все конфигурации в одном месте.\n\nИспользование Python для настройки наших устройств\n\nВышеупомянутое полезно, но как насчет использования Python для настройки наших устройств, в нашем сценарии у нас есть транковый порт между 'SW1' и 'SW2', снова представьте, если бы это нужно было сделать на многих из тех же коммутаторов, которые мы хотим автоматизировать, и не нужно вручную подключаться к каждому коммутатору, чтобы внести изменения в конфигурацию.\n\nДля этого мы можем использовать следующий скрипт. Это подключится через SSH и выполнит это изменение на нашем 'SW1', которое также изменится на 'SW2'.\nfrom netmiko import ConnectHandler\n\nSW2 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.193\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n    \"secret\": \"access123\",\n}\n\ncore_sw_config = [\"int range gig0/1 - 2\", \"switchport trunk encapsulation dot1q\",\n                  \"switchport mode trunk\", \"switchport trunk allowed vlan 1,2\"]\n\nprint(\"########## Connecting to Device {0} ############\".format(SW2))\nnet_connect = ConnectHandler(**SW2)\nnet_connect.enable()\n\nprint(\"* Sending Configuration to Device *\")\nnet_connect.send_config_set(core_sw_config)\n\n\n\nТеперь если посмотреть на код, вы увидите, что появляется сообщение «sending configuration to device», но нет подтверждения того, что это произошло. Мы могли бы добавить дополнительный код в наш скрипт, чтобы выполнить эту проверку и проверку на нашем switch или мы могли бы изменить наш сценарий, прежде чем показать нам это.\n\n#!/usr/bin/env python\nfrom netmiko import ConnectHandler\nfrom getpass import getpass\n\n#password = getpass()\n\nSW1 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.178\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW2 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.193\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW3 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.125\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\n\nSW4 = {\n    \"device_type\": \"cisco_ios\",\n    \"host\": \"192.168.169.197\",\n    \"username\": \"admin\",\n    \"password\": \"access123\",\n}\ncommand = \"show int trunk\"\nfor device in (SW1, SW2, SW3, SW4):\n    net_connect = ConnectHandler(**device)\n    print(net_connect.find_prompt())\n    print(net_connect.send_command(command))\n    net_connect.disconnect()\n\nРезервное копирование конфигураций вашего устройства\n\nДругим вариантом использования может быть захват наших сетевых конфигураций и обеспечение их резервного копирования, но опять же, мы не хотим подключаться ко всем устройствам, которые у нас есть в нашей сети, поэтому мы также можем автоматизировать это с помощью скрипта\nimport sys\nimport time\nimport paramiko\nimport os\nimport cmd\nimport datetime\n\nnow = datetime.datetime.now()\ndt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\nprint(\"Your backup has started at\", dt_string)\t\ntic = time.perf_counter()\n\n#user = input(\"Enter username:\")\n#password = input(\"Enter Paswd:\")\n#enable_password = input(\"Enter enable pswd:\")\nuser = \"admin\"\npassword = \"access123\"\nenable_password = \"access123\"\n\nport=22\nf0 = open('backup.txt')\nfor ip in f0.readlines():\n       ip = ip.strip()\n       filename_prefix ='/Users/shambhu/Documents' + ip\n       ssh = paramiko.SSHClient()\n       ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n       ssh.connect(ip,port, user, password, look_for_keys=False)\n       chan = ssh.invoke_shell()\n       time.sleep(2)\n       chan.send('enable\\n')\n       chan.send(enable_password +'\\n')\n       time.sleep(1)\n       chan.send('term len 0\\n')\n       time.sleep(1)\n       chan.send('sh run\\n')\n       time.sleep(20)\n       output = chan.recv(999999)\n       filename = \"%s_%.2i%.2i%i_%.2i%.2i%.2i\" % (ip,now.year,now.month,now.day,now.hour,now.minute,now.second)\n       f1 = open(filename, 'a')\n       f1.write(output.decode(\"utf-8\") )\n       f1.close()\n       ssh.close()\n       f0.close()\ntoc = time.perf_counter()\nprint(\"Congratulations You Have Backed Up Your 90DaysOfDevOps Lab\")\nprint(f\"Your backup duration was {toc - tic:0.4f} seconds\")\n\ndt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\nprint(\"Your backup completed at\", dt_string)\n\nВам также потребуется заполнить backup.txt IP-адресами, для которых вы хотите сделать резервную копию.\n\n192.168.169.115\n192.168.169.178\n192.168.169.193\n192.168.169.125\n192.168.169.197\n\nЗапустите свой скрипт, и вы должны увидеть что-то вроде того, что показано ниже.\n\n\n\nЭто может быть я просто пишу простой скрипт печати на питоне, поэтому я также должен показать вам файлы резервных копий.\n\nParamiko\n\nШироко используемый модуль Python для SSH. Вы можете узнать больше по официальной ссылке GitHub здесь\n\nМы можем установить этот модуль с помощью команды pip install paramiko.\n\n\n\nМы можем проверить установку, войдя в оболочку Python и импортировав модуль paramiko.\n\nNetmiko\n\nМодуль netmiko предназначен специально для сетевых устройств, тогда как paramiko — это более широкий инструмент для обработки SSH-соединений в целом.\n\nNetmiko, который мы использовали выше вместе с paramiko, можно установить с помощью pip install netmiko.\n\nNetmiko поддерживает множество сетевых поставщиков и устройств, список поддерживаемых устройств можно найти на странице GitHub.\n\nДругие модули\n\nТакже стоит упомянуть несколько других модулей, на которые у нас не было возможности взглянуть, но они дают гораздо больше функциональных возможностей, когда речь идет об автоматизации сети.\n\nnetaddr используется для работы с IP-адресами и управления ими, опять же установка проста с помощью pip install netaddr\n\nвы можете захотеть сохранить большую часть конфигурации вашего коммутатора в электронной таблице Excel, xlrd позволит вашим сценариям читать книгу Excel и преобразовывать строки и столбцы в матрицу. pip install xlrd, чтобы установить модуль.\n\nЕще несколько случаев использования сетевой автоматизации, которые я не имел возможности изучить, можно найти здесь\n\nЯ думаю, что это завершает наш раздел «Сетевые ресурсы» #90DaysOfDevOps. Networking — это одна из областей, которую я действительно не касался какое-то время, и есть так много всего, что нужно осветить, но я надеюсь, что мои заметки и ресурсы, которыми я делюсь, будут полезны для некоторый.\n\nРесурсы\n\nFree Course: Introduction to EVE-NG\nEVE-NG - Creating your first lab\n3 Necessary Skills for Network Automation\nComputer Networking full course\nPractical Networking\nPython Network Automation\nHands-On Enterprise Automation with Python (Book)\n\nУвидимся завтра, где начнем изучать облачные вычисления и получите хорошее представление и базовые знания\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day28",
            "title": "28. DevOps в облаке",
            "description": "Обзор применения инфрастуктуры DevOps в облаке",
            "content": "Общая картина: DevOps и облака\n\nКогда дело доходит до облачных вычислений и того, что они предлагают, это очень хорошо сочетается с духом и процессами DevOps. Мы можем думать об облачных вычислениях, предоставляющих технологии и услуги, в то время как DevOps, как мы уже много раз упоминали ранее, касается процессов и их улучшения.\n\nНо начать с этого путешествия по обучению в облаке сложно, и убедиться, что вы знаете и понимаете все элементы или лучший сервис для выбора по правильной цене, сбивает с толку.\n\n\nНакладывается ли на облака парадигма DevOps? Мой ответ здесь — нет, но чтобы по-настоящему воспользоваться преимуществами облачных вычислений и, возможно, избежать больших счетов за облачные вычисления, от которых пострадало так много людей, важно думать об облачных вычислениях и DevOps вместе.\n\nЕсли мы посмотрим на то, что мы подразумеваем под Public Cloud в общем смысле, речь идет о снятии некоторой ответственности с управляемой службы, чтобы вы и ваша команда могли сосредоточиться на более важных аспектах, имя которых должно быть приложением и конечными пользователями. . В конце концов, Public Cloud  — это просто чей-то компьютер.\n\n\nВ этом первом разделе я хочу немного подробнее рассказать о том, что такое Public Cloud, и о некоторых блоках, которые в целом называются Public Cloud .\nSaaS\nПервая область, которую следует рассмотреть, — это программное обеспечение как услуга (SaaS - Software as a service,). Эта услуга устраняет почти все накладные расходы на управление службой, которую вы, возможно, когда-то запускали локально. Давайте подумаем о Microsoft Exchange для нашей электронной почты. Раньше это была физическая коробка, которая находилась в вашем центре обработки данных или, может быть, в шкафу под лестницей. Вам нужно будет кормить и поить этот сервер. Под этим я подразумеваю, что вам нужно будет обновлять его, и вы будете нести ответственность за покупку серверного оборудования, скорее всего, за установку операционной системы, установку необходимых приложений, а затем за исправление, если что-то пойдет не так, вам придется устранить неполадки и получить вещи встали на свои места.\n\nО, и вам также нужно будет убедиться, что вы делаете резервную копию своих данных, хотя по большей части это не меняется и с SaaS.\n\nЧто делает SaaS и, в частности, Microsoft 365, потому что я упомянул, что Exchange устраняет эти накладные расходы на администрирование, и они предоставляют услугу, которая обеспечивает ваши функции обмена по почте, а также многие другие параметры производительности (Office 365) и варианты хранения (OneDrive), которые в целом дают большой опыт для конечного пользователя.\n\nШироко распространены и другие приложения SaaS, такие как Salesforce, SAP, Oracle, Google, Apple. Все это избавляет от необходимости управлять большим количеством стека.\n\nЯ уверен, что есть история с приложениями на основе DevOps и SaaS, но я изо всех сил пытаюсь выяснить, что они могут собой представлять. Я знаю, что у Azure DevOps есть отличная интеграция с Microsoft 365, которую я мог бы изучить и сообщить.\n\nPublic Cloud\n\nДалее у нас есть public cloud. Большинство людей думают об этом по-разному, некоторые считают, что это только гипермасштаберы, такие как Microsoft Azure, Google Cloud Platform и AWS.\n\n\nНекоторые также видят в общедоступном облаке гораздо более широкое предложение, включающее не только гиперскейлеры, но и тысячи MSP (managed service provider) по всему миру. В этом посте мы собираемся рассмотреть общедоступное облако, включая гиперскейлеры и MSP, хотя позже мы специально углубимся в один или несколько гиперскейлеров, чтобы получить базовые знания.\n\n\nтысячи других компаний могли бы присоединиться к этому, я просто выбираю из местных, региональных, телекоммуникационных и глобальных брендов, с которыми я работал и о которых знаю.\n\nВ разделе SaaS мы упомянули, что облако сняло ответственность или бремя администрирования частей системы. Если SaaS, мы видим, что многие уровни абстракции удалены, то есть физические системы, сеть, хранилище, операционная система и даже приложения в некоторой степени. Когда дело доходит до облака, существуют различные уровни абстракции, которые мы можем удалить или оставить в зависимости от ваших требований.\n\nМы уже упоминали SaaS, но есть еще по крайней мере два, которые следует упомянуть в отношении общедоступного облака.\n\nИнфраструктура как услуга. Вы можете думать об этом уровне как о виртуальной машине, но в то время как локально вам придется заботиться о физическом уровне в облаке, это не так, физический уровень является обязанностью облачных провайдеров, и вы будете управлять и управлять операционной системой, данными и приложениями, которые вы хотите запустить.\n\nПлатформа как услуга. Это по-прежнему снимает ответственность уровней, и на самом деле это означает, что вы берете под свой контроль данные и приложение, но вам не нужно беспокоиться об аппаратном обеспечении или операционной системе.\n\nЕсть много других предложений AaS, но это два основных принципа. Вы можете увидеть предложения вокруг StaaS (Storage as a service), которые предоставляют вам уровень хранения, но не нужно беспокоиться об оборудовании под ним. Или вы, возможно, слышали о CaaS для контейнеров как об услуге, к которой мы вернемся позже. Еще одна услуга как услуга, которую мы рассмотрим в течение следующих 7 дней, — это FaaS (Functions as a Service), где, возможно, вам не нужна работающая система. все время, и вы просто хотите, чтобы функция выполнялась как и когда.\n\nЕсть много способов, которыми общедоступное облако может предоставить уровни абстракции управления, от которых вы хотите отказаться и заплатить за них.\n\nPrivate Cloud\nНаличие собственного центра обработки данных не осталось в прошлом. Я думаю, что это стало возрождением среди многих компаний, которым было трудно управлять моделью OPEX, а также набором навыков только в использовании общедоступного облака.\n\nЗдесь важно отметить, что общедоступное облако, скорее всего, теперь будет вашей ответственностью и будет находиться на вашей территории.\n\nУ нас есть некоторые интересные вещи, происходящие в этой сфере не только с VMware, которая доминировала в эпоху виртуализации, и с локальными инфраструктурными средами. У нас также есть гиперскейлеры, предлагающие локальную версию своих публичных облаков.\n\nHybrid Cloud\n\nВ продолжение упоминаний о публичном и частном облаке мы также можем охватить обе эти среды, чтобы обеспечить гибкость между ними, возможно, воспользоваться услугами, доступными в общедоступном облаке, а затем также воспользоваться преимуществами функций и возможностей локальной среды. или это может быть правило, которое предписывает вам хранить данные локально.\n\n\nСобрав все это вместе, у нас есть много вариантов, где мы будем хранить и запускать наши рабочие нагрузки.\n\n\nПрежде чем мы перейдем к конкретному гипермасштабу, я спросил силу Твиттера, куда нам следует двигаться?\n\n\nLink to Twitter Poll\n\nКакой бы процент ни получил самый высокий процент, мы углубимся в предложения, я думаю, что важно упомянуть, что услуги во всех них очень похожи, поэтому я говорю начать с одного, потому что я обнаружил, что, зная основа одного из них и как создавать виртуальные машины, настраивать сеть и т. д. Я смог перейти к другим и быстро набраться опыта в этих областях.\n\nВ любом случае, я поделюсь отличными БЕСПЛАТНЫМИ ресурсами, которые охватывают все три гиперскейлера.\n\nЯ также собираюсь разработать сценарий, как я делал это в других разделах, где мы можем что-то построить по мере продвижения по дням.\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\nAWS Basics for Beginners - Full Course\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day29",
            "title": "29. Знакомство с Microsoft Azure",
            "description": "Туториал по Microsoft Azure с нуля",
            "content": "\nЗнакомство с Microsoft Azure\n\nПрежде чем мы начнем, победителем опроса в Твиттере стала Microsoft Azure, отсюда и название страницы. Это было довольно интересно увидеть результаты, полученные в течение 24 часов.\n\n\nЯ бы сказал, что с точки зрения освещения этой темы я лучше понимаю и пользуюсь услугами, доступных в Microsoft Azure. Сегодня я склоняюсь к Amazon AWS. Однако я выделил разделы для всех трех основных облачных провайдеров.\n\nЯ ценю, что их больше, и опрос включал только эти 3, и, в частности, были некоторые комментарии об Oracle Cloud. Я хотел бы услышать больше о других облачных провайдерах, которые используются в дикой природе.\n\nОсновы\n\nПредоставляет общедоступные облачные сервисы\nГеографически распределены (более 60 регионов по всему миру)\nДоступ через Интернет и/или частные соединения\nМультитенантная модель\nВыставление счетов на основе потребления - (Плати по мере использования | Плати по мере роста)\nБольшое количество типов услуг и предложений для различных требований.\n\nMicrosoft Azure Global Infrastructure\n  Сколько бы мы ни говорили о SaaS и Hybrid Cloud, мы не планируем затрагивать эти темы здесь.\n\nЛучший способ начать и продолжить работу — щелкнуть ссылку, которая позволит вам зарегистрировать Бесплатную учетную запись Microsoft Azure\n\nРегионы\n\nЯ связал интерактивную карту выше, но мы можем видеть изображение под широтой регионов, предлагаемых на платформе Microsoft Azure по всему миру.\n\nimage taken from Microsoft Docs - 01/05/2021\n\nВы также увидите несколько sovereign облаков, что означает, что они не связаны или не могут взаимодействовать с другими регионами, например, они будут связаны с правительствами, такими как «AzureUSGovernment», а также «AzureChinaCloud» и другими.\n\nКогда мы развертываем наши службы в Microsoft Azure, мы выбираем регион почти для всего. Однако важно отметить, что не все услуги доступны в каждом регионе. Вы можете увидеть Продукты, доступные по регионам на момент написания моего письма, что в западно-центральной части США мы не можем использовать Azure Databricks.\n\nЯ также упомянул «почти все» выше, есть определенные службы, связанные с регионом, такие как Azure Bot Services, Bing Speech, Azure Virtual Desktop, статические веб-приложения и некоторые другие.\n\nЗа кулисами регион может состоять из более чем одного центра обработки данных. Они будут называться зонами доступности.\n\nНа изображении ниже вы увидите, что это снова взято из официальной документации Microsoft, в которой описывается, что такое регион и как он состоит из зон доступности. Однако не во всех регионах есть несколько зон доступности.\n\n\n\nВ Microsoft хорошая документация, и вы можете прочитать больше о Регионах и зонах доступности здесь.\n\nПодписки\n\nПомните, что мы упоминали, что Microsoft Azure — это облако модели потребления, и вы обнаружите, что все основные поставщики облачных услуг следуют этой модели.\n\nЕсли вы являетесь Предприятием, вы можете захотеть или заключить соглашение Enterprise с Microsoft, чтобы ваша компания могла использовать эти службы Azure.\n\nЕсли вы похожи на меня и используете Microsoft Azure для обучения, у нас есть несколько других вариантов.\n\nУ нас есть Бесплатная учетная запись Microsoft Azure, которая обычно дает вам несколько бесплатных облачных кредитов, которые вы можете потратить в Azure в течение некоторого времени.\n\nСуществует также возможность использовать подписку Visual Studio, которая дает вам, возможно, несколько бесплатных кредитов каждый месяц вместе с вашей годовой подпиской на Visual Studio, которая много лет назад была широко известна как MSDN. Visual Studio\n\nЗатем, наконец, вручите кредитную карту и заплатите, как вы идете, модель. Оплата по мере использования\n\nПодписку можно рассматривать как границу между разными подписками, потенциально являющимися центрами затрат, но совершенно разными средами. Подписка — это место, где создаются ресурсы.\n\nManagement Groups\n\nГруппы управления дают нам возможность разделять управление в нашей Azure AD или в нашей клиентской среде. Группы управления позволяют нам контролировать политики, RBAC (Role-based access control) и бюджеты.\n\nПодписки принадлежат этим группам управления, поэтому у вас может быть много подписок в вашем клиенте Azure AD. Эти подписки также могут управлять политиками, RBAC и бюджетами.\n\nResource Manager and Resource Groups\n\nAzure Resource Manager\n\nAPI на основе JSON, основанный на поставщиках ресурсов.\nРесурсы принадлежат группе ресурсов и имеют общий жизненный цикл.\nПараллелизм\nРазвертывания на основе JSON являются декларативными, идемпотентными и понимают зависимости между ресурсами для управления созданием и порядком.\n\nResource Groups\n\nКаждый ресурс Azure Resource Manager существует в одной и только одной группе ресурсов!\nГруппы ресурсов создаются в регионе, который может содержать ресурсы из-за пределов региона.\nРесурсы можно перемещать между группами ресурсов\nГруппы ресурсов не отгорожены от других групп ресурсов, между группами ресурсов может быть связь.\nГруппы ресурсов также могут управлять политиками, RBAC и бюджетами.\n\nПрактика\n\nДавайте подключимся и убедимся, что у нас есть Подписка. Мы можем проверить нашу простую готовую Группу управления. Затем мы можем пойти и создать новую выделенную Группу ресурсов в предпочитаемом нами Регионе.\n\nПри первом входе на наш портал Azure вверху вы увидите возможность поиска ресурсов, служб и документов.\n\n\n\nСначала мы рассмотрим нашу подписку. Здесь вы увидите, что я использую подписку Visual Studio Professional, которая дает мне бесплатный \"кредит\" каждый месяц.\n\n\n\nЕсли мы углубимся в это, вы получите более широкое представление и посмотрите, что происходит или что можно сделать с подпиской, мы можем увидеть информацию о выставлении счетов с функциями управления слева, где вы можете определить контроль доступа к IAM, а ниже доступно больше ресурсов.\n\n\n\nМожет возникнуть ситуация, когда у вас есть несколько подписок, и вы хотите управлять ими всеми в рамках одной, и именно здесь можно использовать группы управления для разделения групп ответственности. В моем ниже вы можете видеть, что есть только моя корневая группа арендатора с моей подпиской.\n\nВы также увидите на предыдущем изображении, что родительская группа управления — это тот же идентификатор, который используется в корневой группе арендатора.\n\n\n\nЗатем у нас есть группы ресурсов, здесь мы объединяем наши ресурсы и можем легко управлять ими в одном месте. У меня есть несколько созданных для различных других проектов.\n\n\n\nЧто мы собираемся делать в течение следующих нескольких дней, мы хотим создать нашу группу ресурсов. Это легко сделать в этой консоли, выбрав опцию создания на предыдущем изображении.\n\n\nПроисходит этап проверки, после чего у вас есть возможность просмотреть свое творение, а затем создать его. Вы также увидите внизу «Загрузить шаблон для автоматизации», это позволяет нам получить формат JSON, чтобы мы могли выполнить это просто автоматически позже, если мы захотим, мы также рассмотрим это позже.\n\nНажмите «Create», затем в нашем списке групп ресурсов у нас теперь есть группа «90DaysOfDevOps», готовая к тому, что мы будем делать в следующем сеансе.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\nAWS Basics for Beginners - Full Course\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day30",
            "title": "30. Модули безопасности Microsoft Azure",
            "description": "Модули безопасности Microsoft Azure",
            "content": "\nMicrosoft Azure Security Models\n\nСледуя обзору Microsoft Azure, мы начнем с безопасности Azure и посмотрим, как это может помочь в наши дни. По большей части я обнаружил, что встроенных ролей было достаточно, и зная это, мы можем создавать и работать со многими различными областями аутентификации и конфигураций. Я обнаружил, что Microsoft Azure довольно продвинута с ее инструментом Active Directory по сравнению с другими общедоступными облаками.\n\nЭто одна из областей, в которой Microsoft Azure, по-видимому, работает иначе, чем другие поставщики общедоступных облаков, в Azure ВСЕГДА есть Azure AD.\n\nСлужбы каталогов (Directory Services )\n\nAzure Active Directory содержит принципы безопасности, используемые Microsoft Azure и другими облачными службами Microsoft.\nАутентификация осуществляется с помощью таких протоколов, как SAML, WS-Federation, OpenID Connect и OAuth2.\nЗапросы выполняются через REST API, который называется Microsoft Graph API.\nУ арендаторов по умолчанию есть имя tenant.onmicrosoft.com, но они также могут иметь собственные доменные имена.\nПодписки связаны с арендатором Azure Active Directory.\n\nЕсли мы сравним с AWS, эквивалентным предложением будет AWS IAM (управление идентификацией и доступом), хотя все еще очень разные\n\nAzure AD Connect предоставляет возможность репликации учетных записей из AD в Azure AD. Сюда также могут входить группы и иногда объекты. Это может быть гранулировано и отфильтровано. Поддерживает несколько лесов и доменов.\n\nВ Microsoft Azure Active Directory (AD) можно создавать облачные учетные записи, но большинство организаций уже учли своих пользователей в собственной локальной Active Directory.\n\nAzure AD Connect также позволяет вам видеть не только серверы Windows AD, но и другие Azure AD, Google и другие. Это также дает возможность сотрудничать с внешними людьми и организациями, что называется Azure B2B.\n\nВарианты аутентификации между доменными службами Active Directory и Microsoft Azure Active Directory возможны с синхронизацией удостоверений с хэшем пароля.\n\n\nПередача хэша пароля необязательна, если он не используется, требуется сквозная аутентификация.\n\nНиже приведено видео, в котором подробно рассказывается о сквозной аутентификации.\n\nUser sign-in with Azure Active Directory Pass-through Authentication\n\nФедерации (Federation)\n\nСправедливости ради стоит сказать, что если вы используете Microsoft 365, Microsoft Dynamics и локальную Active Directory, их довольно легко понять и интегрировать в Azure AD для федерации. Однако вы можете использовать другие службы за пределами экосистемы Microsoft.\n\nAzure AD может выступать в качестве посредника федерации для этих других приложений сторонних производителей и других служб каталогов.\n\nЭто будет отображаться на портале Azure как корпоративные приложения, для которых существует большое количество вариантов.\n\n\n\nЕсли вы прокрутите вниз страницу корпоративного приложения, вы увидите длинный список рекомендуемых приложений.\n\n\n\nЭта опция также позволяет «принести свою» интеграцию, приложение, которое вы разрабатываете, или приложение, не являющееся галереей.\n\nЯ не изучал это раньше, но вижу, что это вполне подходящий набор функций по сравнению с другими облачными провайдерами и возможностями.\n\nУправление доступом на основе ролей\n\nМы уже рассмотрели в День 29 области, которые мы собираемся охватить здесь, мы можем настроить управление доступом на основе ролей в соответствии с одной из этих областей.\n\nSubscriptions\nManagement Group\nResource Group\nResources\n\nРоли можно разделить на три, в Microsoft Azure много встроенных ролей. Эти три:\n\nOwner\nContributor\nReader\n\nВладелец и участник очень похожи по своим границам, однако владелец может изменять разрешения.\n\nДругие роли относятся к определенным типам ресурсов Azure, а также к пользовательским ролям.\n\nМы должны сосредоточиться на назначении разрешений группам и пользователям.\n\nРазрешения наследуются.\n\nЕсли мы вернемся назад и посмотрим на группу ресурсов «90DaysOfDevOps», которую мы создали, и проверим контроль доступа (IAM) внутри, вы увидите, что у нас есть список участников и администратор доступа пользователей клиента, и у нас есть список владельцев (но Я не могу это показать)\n\n\n\nМы также можем проверить роли, которые мы назначили здесь, являются ли они встроенными ролями и к какой категории они относятся.\n\n\n\nМы также можем использовать вкладку проверки доступа, если мы хотим проверить учетную запись по этой группе ресурсов и убедиться, что учетная запись, к которой мы хотим иметь этот доступ, имеет правильные разрешения, или, может быть, мы хотим проверить, не имеет ли пользователь слишком много доступа.\n\nMicrosoft Defender for Cloud\n\nMicrosoft Defender for Cloud (ранее известный как Azure Security Center) предоставляет информацию о безопасности всей среды Azure.\n\nЕдиная панель мониторинга для просмотра общего состояния безопасности всех ресурсов Azure и других ресурсов (через Azure Arc) и рекомендации по усилению безопасности.\n\nУровень бесплатного пользования включает постоянную оценку и рекомендации по безопасности.\n\nПлатные планы для защищенных типов ресурсов (например, серверы, AppService, SQL, хранилище, контейнеры, KeyVault).\n\nЯ перешел на другую подписку для просмотра Центра безопасности Azure, и вы можете увидеть здесь, основываясь на очень небольшом количестве ресурсов, что у меня есть некоторые рекомендации в одном месте.\n\nAzure Policy\n\nAzure Policy — это собственная служба Azure, которая помогает применять организационные стандарты и оценивать соответствие в масштабе.\n\nИнтегрирован в Microsoft Defender для облака. Azure Policy проверяет несоответствующие ресурсы и применяет исправления.\n\nОбычно используется для управления согласованностью ресурсов, соблюдением нормативных требований, безопасностью, стоимостью и стандартами управления.\n\nИспользует формат JSON для хранения логики оценки и определения того, соответствует ли ресурс требованиям или нет, а также любых действий, которые необходимо предпринять в случае несоответствия (например, аудит, аудит, если не существует, запретить, изменить, развернуть, если не существует).\n\nБесплатно для использования. Исключение составляют подключенные ресурсы Azure Arc, взимаемые за сервер в месяц за использование гостевой конфигурации политики Azure.\n\nПрактика\n\nЯ купил домен и хотел бы добавить этот на свой портал Azure Active Directory, Add your custom domain name using the Azure Active Directory Portal\n\n\n\nТеперь мы можем создать нового пользователя в нашем новом домене Active Directory.\n\n\n\nТеперь мы хотим создать группу для всех наших новых пользователей 90DaysOfDevOps в одной группе. Мы можем создать группу, как показано ниже, обратите внимание, что я использую «Динамический пользователь», это означает, что Azure AD будет запрашивать учетные записи пользователей и добавлять их динамически по сравнению с назначенными, когда вы вручную добавляете пользователя в свою группу.\n\n\n\nСуществует множество вариантов создания вашего запроса, мой план состоит в том, чтобы просто найти основное имя и убедиться, что оно содержит мой запрос.\n\n\n\nТеперь, поскольку мы уже создали нашу учетную запись пользователя, мы можем проверить, работают ли правила. Для сравнения я также добавил здесь еще одну учетную запись, связанную с другим доменом, и вы можете видеть, что из-за этого правила наш пользователь не попадет в эту группу.\n\n\n\nС тех пор я добавил нового пользователя, и если мы пойдем и проверим группу, мы увидим наших участников.\n\n\n\nЕсли у нас есть это требование x100, то мы не собираемся делать все это в консоли, мы собираемся воспользоваться либо массовыми параметрами для создания, приглашения, удаления пользователей, либо вы захотите изучить PowerShell для достичь этого автоматизированного подхода к масштабированию.\n\nТеперь мы можем перейти к нашей группе ресурсов и указать, что в группе ресурсов 90DaysOfDevOps мы хотим, чтобы владельцем была группа, которую мы только что создали.\n\n\n\nМы также можем войти сюда и запретить доступ назначений к нашей группе ресурсов.\n\nТеперь, если мы войдем на портал Azure с нашей новой учетной записью пользователя, вы увидите, что у нас есть доступ только к нашей группе ресурсов 90DaysOfDevOps, а не к другим, показанным на предыдущих рисунках, потому что у нас нет доступа.\n\n\n\nВышеприведенное замечательно, если это пользователь, имеющий доступ к ресурсам внутри вашего портала Azure, но не каждый пользователь должен знать о портале, но для проверки доступа мы можем использовать Портал приложений Это портал единого входа, который мы тестируем.\n\n\n\nВы можете настроить этот портал под своим собственным брендом, и мы, возможно, вернемся к этому позже.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\nAWS Basics for Beginners - Full Course\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day31",
            "title": "31. Microsoft Azure Среда выполнения приложений",
            "description": "Microsoft Azure Среда выполнения приложений",
            "content": "\nСреда выполнения приложений\nВслед за вчерашним обзором основ моделей безопасности в Microsoft Azure, сегодня мы собираемся изучить различные службы вычислений, доступные нам в Azure.\n\nПараметры службы доступности\n\nЭтот раздел мне близок, учитывая мою роль в управлении данными. Как и в случае с локальной средой, очень важно обеспечить доступность ваших служб.\n\nВысокая доступность (Защита в пределах региона)\nАварийное восстановление (Защита между регионами)\nРезервное копирование (Восстановление с момента времени)\n\nMicrosoft развертывает несколько регионов в пределах геополитических границ.\n\nДве концепции Azure для доступности услуг.\n\nНаборы доступности (виртуальных машин)  — обеспечивают отказоустойчивость в центре обработки данных.\n\nЗоны доступности — обеспечивают отказоустойчивость между центрами обработки данных в пределах региона.\n\nВиртуальные машины\n\nПредоставляет виртуальные машины различных серий и размеров с различными возможностями (иногда огромными) Размеры виртуальных машин в Azure\nСуществует множество различных вариантов и фокусов для виртуальных машин, от высокопроизводительных, с малой задержкой до виртуальных машин с большим объемом памяти.\nУ нас также есть расширяемый тип ВМ, который можно найти в серии B. Это отлично подходит для рабочих нагрузок, где у вас могут быть низкие требования к ЦП по большей части, но требуется, чтобы, возможно, один раз в месяц требовалась всплеск производительности.\nВиртуальные машины размещаются в виртуальной сети, которая может обеспечить подключение к любой сети.\nПоддержка гостевых ОС Windows и Linux.\nСуществуют также ядра, настроенные для Azure, если речь идет о конкретных дистрибутивах Linux. Ядра, настроенные Azure\n\nШаблоны\nВ Microsoft Azure шаблоны исполнений можно конфигурировать с помощью JSON.\n\nСуществует несколько различных порталов и консолей управления, которые мы можем использовать для создания наших ресурсов. Предпочтительнее будет через шаблоны JSON.\n\nИдемпотентные развертывания в инкрементном или полном режиме — т.е. повторяемое желаемое состояние.\n\nСуществует большой выбор шаблонов, которые могут экспортировать развернутые определения ресурсов. Мне нравится думать об этой функции шаблонов как о чем-то вроде AWS CloudFormation или, возможно, о Terraform для мультиоблачного варианта. Подробнее о Terraform мы расскажем в разделе «Инфраструктура как код».\n\nМасштабирование\n\nАвтоматическое масштабирование — это крупная функция общедоступного облака, позволяющая сократить ресурсы, которые вы не используете, или активировать, когда они вам нужны.\n\nВ Azure у нас есть так называемые масштабируемые наборы виртуальных машин (VMSS) для IaaS. Это позволяет автоматически создавать и масштабировать изображение золотого стандарта на основе расписаний и показателей.\n\nЭто идеально подходит для обновления окон, чтобы вы могли обновлять свои образы и развертывать их с наименьшими последствиями.\n\nВ другие службы, такие как службы приложений Azure, встроено автоматическое масштабирование.\n\nКонтейнеры\n\nМы не рассмотрели контейнеры как пример использования и то, что и как они могут и должны быть необходимы в нашем учебном путешествии по DevOps, но мы должны упомянуть, что у Azure есть некоторые конкретные службы, ориентированные на контейнеры, которые следует упомянуть.\n\nСлужба Azure Kubernetes (AKS) (Azure Kubernetes Service) — предоставляет управляемое решение Kubernetes.\n\nЭкземпляры контейнеров Azure — контейнеры как услуга с посекундной оплатой. Запустите образ и интегрируйте его с вашей виртуальной сетью, не нуждаясь в оркестровке контейнеров.\n\nService Fabric — имеет множество возможностей, но включает оркестрацию для экземпляров контейнеров.\n\nAzure также имеет реестр контейнеров, который предоставляет частный реестр для образов Docker, диаграмм Helm, артефактов Open Container Initiative (OCI) и образов. Подробнее об этом снова, когда мы дойдем до раздела контейнеров.\n\nМногие службы контейнеров действительно могут использовать контейнеры \"под капотом\", но это абстрагируется от наших требований к управлению.\n\nСлужбы приложений\n\nСлужбы приложений Azure предоставляют решение для размещения приложений, которое обеспечивает простой способ установки служб.\nАвтоматическое развертывание и масштабирование.\nПоддерживает решения на базе Windows и Linux.\nСлужбы выполняются в плане службы приложений, который имеет тип и размер.\nКоличество различных сервисов, включая веб-приложения, приложения API и мобильные приложения.\nПоддержка слотов развертывания для надежного тестирования и продвижения.\n\nБессерверные вычисления\n\nЦель бессерверных вычислений заключается в том, что мы платим только за время выполнения функции, и нам не нужно постоянно запускать виртуальные машины или приложения PaaS. Мы просто запускаем нашу функцию, когда она нам нужна, а затем она исчезает.\n\nФункции Azure — предоставляет бессерверный код. Если мы вернемся к нашему первому взгляду на общедоступное облако, вы вспомните уровень абстракции управления, с бессерверными функциями вы будете управлять только кодом.\n\nУ меня есть план, ориентированный на события в больших масштабах, когда я получу здесь немного практики, надеюсь, позже.\n\nОбеспечивает входную и выходную привязку ко многим Azure и сторонним службам.\n\nПоддерживает множество различных языков программирования. (C#, NodeJS, Python, PHP, bash, Golang, Rust или любой исполняемый файл)\n\nСетка событий Azure позволяет запускать логику из служб и событий.\n\nПриложение Azure Logic обеспечивает графический рабочий процесс и интеграцию.\n\nМы также можем рассмотреть пакетную службу Azure, которая может выполнять крупномасштабные задания на узлах Windows и Linux с согласованным управлением и планированием.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\nAWS Basics for Beginners - Full Course\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day32",
            "title": "32. Модели хранилища Microsoft Azure",
            "description": "Модели хранилища Microsoft Azure",
            "content": "Модели хранилища\nСлужбы хранилища\n\nСлужбы хранилища Azure предоставляются учетными записями хранения.\nДоступ к учетным записям хранения в основном осуществляется через REST API.\nУчетная запись хранения должна иметь уникальное имя, являющееся частью DNS-имени .core.windows.net.\nРазличные варианты репликации и шифрования.\nНаходится в группе ресурсов\n\nМы можем создать нашу группу хранения, просто выполнив поиск группы хранения в строке поиска в верхней части портала Azure.\n\n\nЗатем мы можем выполнить шаги по созданию нашей учетной записи хранения, помня, что это имя должно быть уникальным, а также оно должно быть написано строчными буквами, без пробелов, но может включать цифры.\n\n\nМы также можем выбрать уровень избыточности, который мы хотели бы использовать для нашей учетной записи хранения и всего, что мы здесь храним. Чем дальше по списку, тем дороже вариант, но также и распространение ваших данных.\n\nДаже опция избыточности по умолчанию дает нам 3 копии наших данных.\n\nAzure Storage Redundancy\n\nКонцепции из ссылки выше:\n\nЛокально-избыточное хранилище** — трижды реплицирует ваши данные в пределах одного центра обработки данных в основном регионе.\n\nГеоизбыточное хранилище** — трижды синхронно копирует ваши данные в одном физическом расположении в основном регионе с помощью LRS.\n\nХранилище с избыточностью в пределах зоны** — синхронно реплицирует данные службы хранилища Azure в трех зонах доступности Azure в основном регионе.\n\nХранилище с избыточностью в геозонах** — сочетает в себе высокую доступность, обеспечиваемую избыточностью в зонах доступности, с защитой от региональных сбоев, обеспечиваемой георепликацией. Данные в учетной записи хранения GZRS копируются в три зоны доступности Azure в основном регионе, а также реплицируются во второй географический регион для защиты от региональных аварий.\n\n\n\nПросто возвращаюсь к параметрам производительности. У нас есть Стандарт и Премиум на выбор. В нашем пошаговом руководстве мы выбрали «Стандартный», но «Премиум» дает вам некоторые специфические опции.\n\n\nЗатем в раскрывающемся списке вы можете увидеть, что у нас есть эти три варианта на выбор.\n\n\nДля учетной записи хранения доступно множество дополнительных параметров, но пока нам не нужно вдаваться в это. Эти параметры связаны с шифрованием и защитой данных.\n\nУправляемые диски\n\nДоступ к хранилищу можно получить несколькими способами.\n\nАутентифицированный доступ через:\nОбщий ключ для полного контроля.\nShared Access Signature для делегированного, детализированного доступа.\nAzure Active Directory (где доступно)\n\nПубличный доступ:\nОбщий доступ также может быть предоставлен для включения анонимного доступа, в том числе через HTTP.\n— Примером этого может быть размещение базового контента и файлов в блочном BLOB-объекте, чтобы браузер мог просматривать и скачивать эти данные.\n\nЕсли вы получаете доступ к своему хранилищу из другой службы Azure, трафик остается в Azure.\n\nКогда дело доходит до производительности хранилища, у нас есть два разных типа:\nStandard** - Максимальное количество операций ввода-вывода в секунду\nPremium** - Гарантированное количество операций ввода-вывода в секунду\n\nСуществует также разница между неуправляемыми и управляемыми дисками, которую следует учитывать при выборе правильного хранилища для поставленной задачи.\n\nХранилище виртуальной машины\n\nДиски ОС виртуальной машины обычно хранятся в постоянном хранилище.\nНекоторым рабочим нагрузкам без сохранения состояния не требуется постоянное хранилище, и уменьшение задержки является большим преимуществом.\nСуществуют виртуальные машины, поддерживающие эфемерные управляемые диски ОС, созданные в локальном хранилище узла.\n   Их также можно использовать с масштабируемыми наборами виртуальных машин.\n\nУправляемые диски — это надежное блочное хранилище, которое можно использовать с виртуальными машинами Azure. Вы можете иметь Ultra Disk Storage, Premium SSD, Standard SSD, Standard HDD. Они также несут некоторые характеристики.\n\nПоддержка снимков и изображений\nПростое перемещение между SKU\nЛучшая доступность в сочетании с наборами доступности\nПлата взимается в зависимости от размера диска, а не от использованного хранилища.\n\nХранилище архивов\n\nCool Tier** — доступен классный уровень хранилища для блокировки и добавления больших двоичных объектов.\n   Более низкая стоимость хранения\n   Более высокая стоимость сделки.\nArchive Tier* — Архивное хранилище доступно для блочных больших двоичных объектов.\n   Это настраивается для каждого BLOB-объекта.\n   Более низкая стоимость, более длительная задержка поиска данных.\n   Такая же надежность данных, как и в обычном хранилище Azure.\n   Пользовательские уровни данных могут быть включены по мере необходимости.\n\nОбщий доступ к файлам\n\nИз вышеописанного создания нашей учетной записи хранения теперь мы можем создавать общие файловые ресурсы.\n\n\n\nЭто обеспечит файловые ресурсы SMB2.1 и 3.0 в Azure.\n\nМожно использовать в Azure и извне через SMB3 и порт 445, открытый для Интернета.\n\nПредоставляет общее хранилище файлов в Azure.\n\nМожно сопоставить с помощью стандартных клиентов SMB в дополнение к REST API.\n\nВы также можете почитать Azure NetApp Files (SMB и NFS)\n\nСлужбы кэширования и мультимедиа\n\nСеть доставки содержимого Azure предоставляет кэш статического веб-содержимого с местоположениями по всему миру.\n\nСлужбы мультимедиа Azure предоставляют технологии транскодирования мультимедиа в дополнение к службам воспроизведения.\n\nМодели баз данных Microsoft Azure\n\nЕще в День 28 мы рассмотрели различные варианты обслуживания. Одним из них была PaaS (Platform as a Service) (платформа как услуга), где вы абстрагируете большую часть инфраструктуры и операционной системы, и вам остается контролировать приложение или, в данном случае, модели базы данных.\n\nРеляционные базы данных\n\nБаза данных SQL Azure предоставляет реляционную базу данных как службу на основе Microsoft SQL Server.\n\nЭто SQL, работающий с последней веткой SQL с доступным уровнем совместимости базы данных, где требуется конкретная версия функциональности.\n\nЕсть несколько вариантов того, как это можно настроить: мы можем предоставить единую базу данных, которая предоставляет одну базу данных в экземпляре, в то время как эластичный пул позволяет использовать несколько баз данных, которые совместно используют пул емкости и совместно масштабируются.\n\nДоступ к этим экземплярам базы данных можно получить как к обычным экземплярам SQL.\n\nДополнительные управляемые предложения для MySQL, PostgreSQL и MariaDB.\n\nРешения NoSQL\n\nAzure Cosmos DB — это реализация NoSQL, не зависящая от схемы.\n\n99,99% SLA\n\nГлобально распределенная база данных с однозначными задержками на 99-м процентиле в любой точке мира с автоматическим возвратом в исходное положение.\n\nКлюч раздела, используемый для разделения/разбиения/распределения данных.\n\nПоддерживает различные модели данных (документы, ключ-значение, график, удобный для столбцов)\n\nПоддерживает различные API (DocumentDB SQL, MongoDB, Azure Table Storage и Gremlin).\n\n\n\nДоступны различные модели согласованности, основанные на теореме CAP.\n\nКэширование\n\nНе вдаваясь в подробности о системах кэширования, таких как Redis, я хотел добавить, что у Microsoft Azure есть служба под названием Azure Cache for Redis.\n\nКэш Azure для Redis предоставляет хранилище данных в памяти на основе программного обеспечения Redis.\n\nЭто реализация Redis Cache с открытым исходным кодом.\n    Размещенный безопасный экземпляр кэша Redis.\n    Доступны разные уровни\n    Приложение должно быть обновлено, чтобы использовать кеш.\n    Предназначен для приложения, которое имеет высокие требования к чтению по сравнению с записью.\n    На основе хранилища ключей-значений.\n\n\n\nЯ ценю, что за последние несколько дней было много заметок и теории о Microsoft Azure, но я хотел охватить строительные блоки, прежде чем мы перейдем к практическим аспектам того, как эти компоненты объединяются и работают.\n\nУ нас есть еще немного теории, связанной с сетью, прежде чем мы сможем запустить и запустить некоторые основанные на сценариях развертывания сервисов. Мы также хотим взглянуть на некоторые различные способы взаимодействия с Microsoft Azure по сравнению с порталом, который мы использовали до сих пор.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day33",
            "title": "33. Сетевые модели Microsoft Azure + Управление Azure",
            "description": "Сетевые модели Microsoft Azure + Управление Azure",
            "content": "\nМы рассмотрим сетевые модели в Microsoft Azure и некоторые варианты управления для Azure. До сих пор мы использовали только платформу Azure, но упомянули и другие области, которые можно использовать для управления и создания наших ресурсов на платформе.\n\nСетевые модели Azure\n\nВиртуальные сети\n\nВиртуальная сеть — это конструкция, созданная в Azure.\nВиртуальной сети назначен один или несколько диапазонов IP-адресов.\nВиртуальные сети живут в рамках подписки внутри региона.\nВ виртуальной сети создаются виртуальные подсети для разбиения сетевого диапазона.\nВиртуальные машины размещаются в виртуальных подсетях.\nВсе виртуальные машины в виртуальной сети могут обмениваться данными.\n65 536 частных IP-адресов на виртуальную сеть.\nПлатите только за исходящий трафик из региона. (данные покидают регион)\nПоддерживаются IPv4 и IPv6.\n  IPv6 для общедоступных и внутри виртуальных сетей.\n\nМы можем сравнить виртуальные сети Azure с AWS VPC. Однако следует отметить некоторые отличия:\n\nВ AWS создается виртуальная сеть по умолчанию, чего нет в Microsoft Azure, вам необходимо создать свою первую виртуальную сеть в соответствии с вашими требованиями.\nВсе виртуальные машины в Azure по умолчанию имеют доступ к Интернету через NAT. Нет шлюзов NAT в соответствии с AWS.\nВ Microsoft Azure нет понятия частных или общедоступных подсетей.\nОбщедоступные IP-адреса — это ресурс, который может быть назначен виртуальным сетевым адаптерам или балансировщикам нагрузки.\nВиртуальная сеть и подсети имеют свои собственные списки управления доступом, позволяющие делегировать уровень подсети.\nПодсети в зонах доступности, тогда как в AWS у вас есть подсети для каждой зоны доступности.\n\nУ нас также есть виртуальный сетевой пиринг. Пиринг между виртуальными сетями позволяет эффективно соединить две Виртуальные сети Azure. После создания пиринговой связи две виртуальные сети выглядят как одна сеть в плане подключения. Точно так же трафик между виртуальными машинами в одноранговых виртуальных сетях использует магистральную инфраструктуру Майкрософт. Как и трафик между виртуальными машинами в одной сети, трафик направляется только через частную сеть корпорации Майкрософт.\n\nКонтроль доступа\n\nAzure использует группы безопасности сети, они сохраняют состояние.\nРазрешить создавать правила, а затем назначать их группе безопасности сети.\nГруппы безопасности сети применяются к подсетям или виртуальным машинам.\nПри применении к подсети он по-прежнему применяется к сетевой карте виртуальной машины и не является \"Edge\" устройством.\n\n\n\nПравила объединены в группу безопасности сети.\nВ зависимости от приоритета возможны гибкие конфигурации.\nБолее низкий номер приоритета означает высокий приоритет.\nБольшая часть логики построена на IP-адресах, но также могут использоваться некоторые теги и метки.\n\n\n| Description      | Priority | Source Address     | Source Port | Destination Address | Destination Port | Action |\n| ---------------- | -------- | ------------------ | ----------- | ------------------- | ---------------- | ------ |\n| Inbound 443      | 1005     | *                  | *           | *                   | 443              | Allow  |\n| ILB              | 1010     | Azure LoadBalancer | *           | *                   | 10000            | Allow  |\n| Deny All Inbound | 4000     | *                  | *           | *                   | *                | DENY   |\n\n\nУ нас также есть группы безопасности приложений (Application Security Groups) (ASG) .\n\nГде журналы потоков групп безопасности) (NSG) (Network Security Groups) сети сосредоточены на диапазонах IP-адресов, которые может быть сложно поддерживать для растущих сред.\nASG позволяют определять настоящие имена (моникеры) для различных ролей приложений (веб-серверы, серверы БД, WebApp1 и т. д.).\nСетевая карта виртуальной машины становится членом одной или нескольких групп ASG.\n\nЗатем группы ASG можно использовать в правилах, которые являются частью групп безопасности сети, для управления потоком связи и по-прежнему могут использовать функции NSG, такие как теги обслуживания.\n\n\n| Action | Name               | Source     | Destination | Port         |\n| ------ | ------------------ | ---------- | ----------- | ------------ |\n| Allow  | AllowInternettoWeb | Internet   | WebServers  | 443(HTTPS)   |\n| Allow  | AllowWebToApp      | WebServers | AppServers  | 443(HTTPS)   |\n| Allow  | AllowAppToDB       | AppServers | DbServers   | 1443 (MSSQL) |\n| Deny   | DenyAllinbound     | Any        | Any         | Any          |\n\nБалансировщики нагрузки\n\nLoad Balancing. В Microsoft Azure есть два отдельных решения для балансировки нагрузки. (От Microsoft Azure и сторонние на маркетплейсе) Оба могут работать с внешними или внутренними конечными ендпоинтами.\n\nБалансировщик нагрузки (Layer 4), поддерживающий распределение на основе хэшей и переадресацию портов.\nШлюз приложений (Layer 7) поддерживает такие функции, как разгрузка SSL, сопоставление сеансов на основе файлов cookie и маршрутизация контента на основе URL-адресов.\n\nКроме того, с помощью шлюза приложений вы можете дополнительно использовать компонент брандмауэра веб-приложения.\n\nСредства управления Azure\n\nМы потратили большую часть нашего теоретического времени на изучение портала Azure, я бы предположил, что когда дело доходит до следования культуре DevOps и обработки многих этих задач, особенно связанных с подготовкой, будет выполняться через API или инструмент командной строки. Я хотел коснуться некоторых из тех других инструментов управления, которые у нас есть, поскольку нам нужно знать это, когда мы автоматизируем подготовку наших сред Azure.\n\nПортал Azure\n\nПортал Microsoft Azure — это веб-консоль, которая представляет собой альтернативу инструментам командной строки. Вы можете управлять своими подписками на портале Azure. Создавайте, управляйте и контролируйте все, от простого веб-приложения до сложных облачных развертываний. Еще одна вещь, которую вы найдете на портале, — это хлебные крошки. JSON, как упоминалось ранее, является основой всех ресурсов Azure. Возможно, вы начнете с портала, чтобы понять функции, службы и функциональные возможности, а затем позже поймете JSON внизу, чтобы включить в ваши автоматизированные рабочие процессы.\n\n\n\nСуществует также портал Azure Preview, который можно использовать для просмотра и тестирования новых и предстоящих услуг и улучшений.\n\nPowerShell\n\nПрежде чем мы перейдем к Azure PowerShell, стоит сначала познакомиться с PowerShell. PowerShell — это среда автоматизации задач и управления конфигурацией, оболочка командной строки и язык сценариев. Мы могли бы и осмелились сказать это, сравнив это с тем, что мы рассмотрели в разделе Linux, посвященном сценариям оболочки. PowerShell впервые появился в ОС Windows, но теперь он кроссплатформенный.\n\nAzure PowerShell — это набор командлетов для управления ресурсами Azure непосредственно из командной строки PowerShell.\n\nПри желании мы можеем подключиться к подписке с помощью команды PowerShell «Connect-AzAccount».\n\n\nЗатем, если мы хотим найти некоторые конкретные команды, связанные с виртуальными машинами Azure, мы можем запустить следующую команду. Вы можете потратить часы на изучение и понимание этого языка программирования PowerShell.\n\n\nMicrosoft предлагает отличные краткие руководства по началу работы и подготовке служб из PowerShell здесь\n\nVisual Studio Code\n\nVisual Studio Code — это бесплатный редактор исходного кода, созданный Microsoft для Windows, Linux и macOS.\n\nВ Visual Studio Code встроено множество интеграций и инструментов, которые вы можете использовать для взаимодействия с Microsoft Azure и службами внутри.\n\nCloud Shell\nAzure Cloud Shell — это интерактивная, аутентифицированная, доступная через браузер оболочка для управления ресурсами Azure. Это обеспечивает гибкость выбора оболочки, которая лучше всего подходит для вашей работы.\n\n\n\nКак видно из рисунка ниже, когда мы впервые запускаем Cloud Shell на портале, мы можем выбирать между Bash и PowerShell.\n\n\n\nЧтобы использовать облачную оболочку, вам нужно будет предоставить немного места в своей подписке.\n\nКогда вы выбираете использование облачной оболочки, она запускает компьютер, эти компьютеры являются временными, но ваши файлы сохраняются двумя способами; через образ диска и подключенный файловый обменник.\n\n\n\nCloud Shell работает на временном хосте, предоставляемом для каждого сеанса и каждого пользователя.\nВремя ожидания Cloud Shell истекает через 20 минут без интерактивной активности.\nCloud Shell требует подключения общего файлового ресурса Azure.\n— Cloud Shell использует один и тот же файловый ресурс Azure как для Bash, так и для PowerShell.\nCloud Shell назначается по одному компьютеру для каждой учетной записи пользователя.\nCloud Shell сохраняет $HOME, используя образ размером 5 ГБ, хранящийся в вашей общей папке.\nРазрешения установлены как у обычного пользователя Linux в Bash\n\nПодробнее о Cloud Shell\n\nAzure CLI\n\nAzure CLI можно установить в Windows, Linux и macOS. После установки вы можете ввести «az», а затем другие команды для создания, обновления, удаления и просмотра ресурсов Azure.\n\nКогда я впервые приступил к изучению Azure, меня немного смутило наличие Azure PowerShell и Azure CLI.\n\nЯ также хотел бы получить отзывы от сообщества по этому поводу. Но я вижу, что Azure PowerShell — это модуль, добавленный в Windows PowerShell или PowerShell Core (также доступен в других ОС, но не во всех), тогда как Azure CLI — это кроссплатформенная программа командной строки, которая подключается к Azure и выполняет эти команды. .\n\nОбе эти опции имеют разный синтаксис, хотя, насколько я вижу и что я сделал, они могут выполнять очень похожие задачи.\n\nНапример, для создания виртуальной машины из PowerShell будет использоваться командлет New-AzVM, а в Azure CLI — az VM create.\n\nРанее вы видели, что в моей системе установлен модуль Azure PowerShell, но затем у меня также установлен Azure CLI, который можно вызывать через PowerShell на моем компьютере с Windows.\n\n\n\nВывод здесь, как мы уже упоминали, заключается в выборе правильного инструмента. Azure работает на основе автоматизации. Каждое действие, которое вы совершаете внутри портала, где-то преобразуется в код, выполняемый для чтения, создания, изменения или удаления ресурсов.\n\nСравнение\n\nAzure CLI\n\nКроссплатформенный интерфейс командной строки, устанавливаемый на Windows, macOS, Linux\nРаботает в Windows PowerShell, Cmd или Bash и других оболочках Unix.\n\nAzure PowerShell\n\nКроссплатформенный модуль PowerShell, работает на Windows, macOS, Linux\nТребуется Windows PowerShell или PowerShell\n\nЕсли по какой-то причине вы не можете использовать PowerShell в своей среде, но можете использовать .mdor bash, тогда Azure CLI будет вашим выбором.\n\nЗавтра попробуем создать несколько сценариев и приступим к работе в Azure.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\nAWS Basics for Beginners - Full Course\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day34",
            "title": "34. Практические скрипты Microsoft Azure",
            "description": "Практические скрипты Microsoft Azure",
            "content": "\nПрактические скрипты Microsoft Azure\nПоследние 6 дней были сосредоточены на Microsoft Azure и общедоступном облаке в целом, большая часть этой основы должна была содержать много теории, чтобы понять строительные блоки Azure, но также это будет хорошо перенесено на других крупных облачных провайдеров. .\n\nВ самом начале я упомянул о базовых знаний об общедоступном облаке и выборе одного провайдера, по крайней мере, для начала. Если вы танцуете между разными облаками, я считаю, что вы можете довольно легко заблудиться, тогда как выбрав одно, вы поймете основы. и когда они у вас есть, довольно легко прыгнуть в другие облака и ускорить свое обучение.\n\nНа этом заключительном занятии я буду выбирать свои практические скрипты с этой страницы, которая является справочной информацией, созданной Microsoft и используемой для подготовки к AZ-104 Администратор Microsoft Azure\n\nЗдесь есть некоторые из них, такие как контейнеры и Kubernetes, которые мы еще не рассмотрели подробно, поэтому я не хочу пока вдаваться в них.\n\nВ предыдущих постах мы создали большинство модулей 1,2 и 3.\n\nВиртуальная сеть\nМы пройдем пройти модуль 04:\n\nЯ прошел по инструкции и изменил несколько названий на #90DaysOfDevOps. Я также вместо использования Cloud Shell вошел в систему с моим новым пользователем, созданным в предыдущие дни с помощью Azure CLI на моем компьютере с Windows.\n\nВы можете сделать это, используя az login, который откроет браузер и позволит вам аутентифицировать свою учетную запись.\n\nЗатем я создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для выполнения некоторых из приведенных ниже задач. Вы можете найти связанные файлы в этой папке.\n  (Облако\\01Виртуальная сеть)\n\n\n\nMod04_90DaysOfDevOps-vms-loop-parameters.json\n\n\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"vmSize\": {\n            \"value\": \"Standard_D2s_v3\"\n        },\n        \"adminUsername\": {\n            \"value\": \"Student\"\n        },\n        \"adminPassword\": {\n            \"value\": \"Pa55w.rd1234\"\n        }\n    }\n}\n\n\n\n\nMod04_90DaysOfDevOps-vms-loop-template.json\n\n\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"vmSize\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"Standard_D2s_v3\",\n            \"metadata\": {\n                \"description\": \"VM size\"\n            }\n        },\n        \"vmName\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"90day-vm\",\n            \"metadata\": {\n                \"description\": \"VM name Prefix\"\n            }\n        },\n        \"vmCount\": {\n            \"type\": \"int\",\n            \"defaultValue\": 2,\n            \"metadata\": {\n                \"description\": \"Number of VMs\"\n            }\n        },\n        \"adminUsername\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"Admin username\"\n            }\n        },\n        \"adminPassword\": {\n            \"type\": \"securestring\",\n            \"metadata\": {\n                \"description\": \"Admin password\"\n            }\n        },\n        \"virtualNetworkName\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"90daysofdevops\",\n            \"metadata\": {\n                \"description\": \"Virtual network name\"\n            }\n        }\n    },\n    \"variables\": {\n        \"nic\": \"90daysofdevops\",\n        \"virtualNetworkName\": \"[parameters('virtualNetworkName')]\",\n        \"subnetName\": \"subnet\",\n        \"subnet0Name\": \"subnet0\",\n        \"subnet1Name\": \"subnet1\",\n        \"computeApiVersion\": \"2018-06-01\",\n        \"networkApiVersion\": \"2018-08-01\"\n    },\n    \"resources\": [\n        {\n            \"name\": \"[concat(parameters('vmName'),copyIndex())]\",\n            \"copy\": {\n                \"name\": \"VMcopy\",\n                \"count\": \"[parameters('vmCount')]\"\n            },\n            \"type\": \"Microsoft.Compute/virtualMachines\",\n            \"apiVersion\": \"[variables('computeApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Creating VMs\",\n            \"dependsOn\": [\n                \"[concat(variables('nic'),copyIndex())]\"\n            ],\n            \"properties\": {\n                \"osProfile\": {\n                    \"computerName\": \"[concat(parameters('vmName'),copyIndex())]\",\n                    \"adminUsername\": \"[parameters('adminUsername')]\",\n                    \"adminPassword\": \"[parameters('adminPassword')]\",\n                    \"windowsConfiguration\": {\n                        \"provisionVmAgent\": \"true\"\n                    }\n                },\n                \"hardwareProfile\": {\n                    \"vmSize\": \"[parameters('vmSize')]\"\n                },\n                \"storageProfile\": {\n                    \"imageReference\": {\n                        \"publisher\": \"MicrosoftWindowsServer\",\n                        \"offer\": \"WindowsServer\",\n                        \"sku\": \"2019-Datacenter\",\n                        \"version\": \"latest\"\n                    },\n                    \"osDisk\": {\n                        \"createOption\": \"fromImage\"\n                    },\n                    \"dataDisks\": []\n                },\n                \"networkProfile\": {\n                    \"networkInterfaces\": [\n                        {\n                            \"properties\": {\n                                \"primary\": true\n                            },\n                            \"id\": \"[resourceId('Microsoft.Network/networkInterfaces', concat(variables('nic'),copyIndex()))]\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"Microsoft.Network/virtualNetworks\",\n            \"name\": \"[variables('virtualNetworkName')]\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Virtual Network\",\n            \"properties\": {\n                \"addressSpace\": {\n                    \"addressPrefixes\": [\n                        \"10.40.0.0/22\"\n                    ]\n                },\n                \"subnets\": [\n                    {\n                        \"name\": \"[variables('subnet0Name')]\",\n                        \"properties\": {\n                            \"addressPrefix\": \"10.40.0.0/24\"\n                        }\n                    },\n                    {\n                        \"name\": \"[variables('subnet1Name')]\",\n                        \"properties\": {\n                            \"addressPrefix\": \"10.40.1.0/24\"\n                        }\n                    }\n                ]\n            }\n        },\n        {\n            \"name\": \"[concat(variables('nic'),copyIndex())]\",\n            \"copy\":{\n                \"name\": \"nicCopy\",\n                \"count\": \"[parameters('vmCount')]\"\n            },\n            \"type\": \"Microsoft.Network/networkInterfaces\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Primary NIC\",\n            \"dependsOn\": [\n                \"[concat('Microsoft.Network/virtualNetworks/', variables('virtualNetworkName'))]\"\n            ],\n            \"properties\": {\n                \"ipConfigurations\": [\n                    {\n                        \"name\": \"ipconfig1\",\n                        \"properties\": {\n                            \"subnet\": {\n                                \"id\": \"[resourceId('Microsoft.Network/virtualNetworks/subnets', variables('virtualNetworkName'), concat(variables('subnetName'),copyIndex()))]\"\n                            },\n                            \"privateIPAllocationMethod\": \"Dynamic\"\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n    \"outputs\": {}\n}\n\n\n\n\nModule4_90DaysOfDevOps.ps1\n\n\n$rgName = '90DaysOfDevOps'\n\nNew-AzResourceGroupDeployment `\n-ResourceGroupName $rgName `\n-TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-template.json `\n-TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-parameters.json\n\n\nУбедитесь, что вы изменили расположение файла в скрипте в соответствии с вашей средой.\n\nНа этом первом этапе у нас нет виртуальной сети или виртуальных машин, созданных в нашей среде, у меня есть только место хранения облачной оболочки, настроенное в моей группе ресурсов.\n\nСначала я запускаю свой скрипт в PowerShell\n$rgName = '90DaysOfDevOps'\n\nNew-AzResourceGroupDeployment `\n-ResourceGroupName $rgName `\n-TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-template.json `\n-TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\01VirtualNetworking\\Mod04_90DaysOfDevOps-vms-loop-parameters.json\n\nЗадача 1: Создать и настроить виртуальную сеть\n\n\nЗадача 2. Развернуть виртуальные машины в виртуальной сети.\n\n\nЗадача 3. Настройка частных и общедоступных IP-адресов виртуальных машин Azure.\n\n\nЗадача 4: Настройка групп безопасности сети\n\n\n\n\nЗадача 5. Настройка Azure DNS для внутреннего разрешения имен.\n\nУправление сетевым трафиком\nПереходим к модулю 06:\n\nДля этого практического занятия я создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для создания некоторых из приведенных ниже задач.\n\nЗадача 1: Обеспечение лабораторной среды\n\nЗапустим PowerShell скрипт\n\n$rgName = '90DaysOfDevOps'\n\nNew-AzResourceGroupDeployment `\n   -ResourceGroupName $rgName `\n   -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\02TrafficManagement\\Mod06_90DaysOfDevOps-vms-loop-template.json `\n   -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\02TrafficManagement\\Mod06_90DaysOfDevOps-vms-loop-parameters.json\n\n   $location = (Get-AzResourceGroup -ResourceGroupName $rgName).location\n   $vmNames = (Get-AzVM -ResourceGroupName $rgName).Name\n\n   foreach ($vmName in $vmNames) {\n     Set-AzVMExtension `\n     -ResourceGroupName $rgName `\n     -Location $location `\n     -VMName $vmName `\n     -Name 'networkWatcherAgent' `\n     -Publisher 'Microsoft.Azure.NetworkWatcher' `\n     -Type 'NetworkWatcherAgentWindows' `\n     -TypeHandlerVersion '1.4'\n   }\n\n\nЗадача 2. Настройка топологии узловой сети\n\n\nЗадача 3. Проверка транзитивности пиринга виртуальной сети.\n\nДля этого моя группа 90DaysOfDevOps не имела доступа к Network Watcher из-за разрешений, я ожидаю, что это связано с тем, что Network Watcher  — это один из тех ресурсов, которые не привязаны к группе ресурсов, где наш RBAC был покрыт для этого пользователя. Я добавил в группу 90DaysOfDevOps роль участника Network Watcher из восточной части США.\n\n\nЭто ожидаемо, поскольку виртуальные сети с двумя лучами не связаны друг с другом (пиринг виртуальных сетей не является транзитивным).\n\nЗадача 4. Настройка маршрутизации в топологии «концентратор-луч».\n\nУ меня была еще одна проблема: моя учетная запись не могла запустить скрипт от имени моего пользователя в группе 90DaysOfDevOps, в чем я не уверен, поэтому я вернулся в свою основную учетную запись администратора. Группа 90DaysOfDevOps является владельцем всего в группе ресурсов 90DaysOfDevOps, поэтому хотелось бы понять, почему я не могу запустить команду внутри виртуальной машины?\n\n\n\n\n\nTask 5: Подключаем Azure Load Balancer\n\n\n\n\nTask 6: Подключаем Azure Application Gateway\n\nХранищиле Azure\nПереходим к модулю 07:\n\nДля этого практического занятия я также создал сценарий PowerShell и несколько ссылок из модуля, чтобы использовать их для создания некоторых из приведенных ниже задач.\n\nЗадача 1: Обеспечение лабораторной среды\n\nСначала запускаем PowerShell script\n$rgName = '90DaysOfDevOps'\n\nNew-AzResourceGroupDeployment `\n   -ResourceGroupName $rgName `\n   -TemplateFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\03Storage\\Mod07_90DaysOfDevOps-vm-template.json `\n   -TemplateParameterFile C:\\Users\\micha\\demo\\90DaysOfDevOps\\Days\\Cloud\\03Storage\\Mod07_90DaysOfDevOps-vm-parameters.json `\n   -AsJob\n\n\n\nФайл Mod07_90DaysOfDevOps-vm-template.json\n{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n      \"vmSize\": {\n        \"type\": \"string\",\n        \"defaultValue\": \"Standard_D2s_v3\",\n        \"metadata\": {\n          \"description\": \"Virtual machine size\"\n        }\n      },\n      \"adminUsername\": {\n        \"type\": \"string\",\n        \"metadata\": {\n          \"description\": \"Admin username\"\n        }\n      },\n      \"adminPassword\": {\n        \"type\": \"securestring\",\n        \"metadata\": {\n          \"description\": \"Admin password\"\n        }\n      }\n    },\n  \"variables\": {\n    \"vmName\": \"90Days-vm0\",\n    \"nicName\": \"90Days-nic0\",\n    \"virtualNetworkName\": \"90Days-vnet0\",\n    \"publicIPAddressName\": \"90Days-pip0\",\n    \"nsgName\": \"90Days-nsg0\",\n    \"vnetIpPrefix\": \"10.70.0.0/22\",\n    \"subnetIpPrefix\": \"10.70.0.0/24\",\n    \"subnetName\": \"subnet0\",\n    \"subnetRef\": \"[resourceId('Microsoft.Network/virtualNetworks/subnets', variables('virtualNetworkName'), variables('subnetName'))]\",\n    \"computeApiVersion\": \"2018-06-01\",\n    \"networkApiVersion\": \"2018-08-01\"\n  },\n    \"resources\": [\n        {\n            \"name\": \"[variables('vmName')]\",\n            \"type\": \"Microsoft.Compute/virtualMachines\",\n            \"apiVersion\": \"[variables('computeApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"dependsOn\": [\n                \"[variables('nicName')]\"\n            ],\n            \"properties\": {\n                \"osProfile\": {\n                    \"computerName\": \"[variables('vmName')]\",\n                    \"adminUsername\": \"[parameters('adminUsername')]\",\n                    \"adminPassword\": \"[parameters('adminPassword')]\",\n                    \"windowsConfiguration\": {\n                        \"provisionVmAgent\": \"true\"\n                    }\n                },\n                \"hardwareProfile\": {\n                    \"vmSize\": \"[parameters('vmSize')]\"\n                },\n                \"storageProfile\": {\n                    \"imageReference\": {\n                        \"publisher\": \"MicrosoftWindowsServer\",\n                        \"offer\": \"WindowsServer\",\n                        \"sku\": \"2019-Datacenter\",\n                        \"version\": \"latest\"\n                    },\n                    \"osDisk\": {\n                        \"createOption\": \"fromImage\"\n                    },\n                    \"dataDisks\": []\n                },\n                \"networkProfile\": {\n                    \"networkInterfaces\": [\n                        {\n                            \"properties\": {\n                                \"primary\": true\n                            },\n                            \"id\": \"[resourceId('Microsoft.Network/networkInterfaces', variables('nicName'))]\"\n                        }\n                    ]\n                }\n            }\n        },\n        {\n            \"type\": \"Microsoft.Network/virtualNetworks\",\n            \"name\": \"[variables('virtualNetworkName')]\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Virtual Network\",\n            \"properties\": {\n                \"addressSpace\": {\n                    \"addressPrefixes\": [\n                        \"[variables('vnetIpPrefix')]\"\n                    ]\n                },\n                \"subnets\": [\n                    {\n                        \"name\": \"[variables('subnetName')]\",\n                        \"properties\": {\n                            \"addressPrefix\": \"[variables('subnetIpPrefix')]\"\n                        }\n                    }\n                ]\n            }\n        },\n        {\n            \"name\": \"[variables('nicName')]\",\n            \"type\": \"Microsoft.Network/networkInterfaces\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Primary NIC\",\n            \"dependsOn\": [\n                \"[variables('publicIpAddressName')]\",\n                \"[variables('nsgName')]\",\n                \"[variables('virtualNetworkName')]\"\n            ],\n            \"properties\": {\n                \"ipConfigurations\": [\n                    {\n                        \"name\": \"ipconfig1\",\n                        \"properties\": {\n                            \"subnet\": {\n                                \"id\": \"[variables('subnetRef')]\"\n                            },\n                            \"privateIPAllocationMethod\": \"Dynamic\",\n                            \"publicIpAddress\": {\n                                \"id\": \"[resourceId('Microsoft.Network/publicIpAddresses', variables('publicIpAddressName'))]\"\n                            }\n                        }\n                    }\n                ],\n                \"networkSecurityGroup\": {\n                    \"id\": \"[resourceId('Microsoft.Network/networkSecurityGroups', variables('nsgName'))]\"\n                }\n            }\n        },\n        {\n            \"name\": \"[variables('publicIpAddressName')]\",\n            \"type\": \"Microsoft.Network/publicIpAddresses\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Public IP for Primary NIC\",\n            \"properties\": {\n                \"publicIpAllocationMethod\": \"Dynamic\"\n            }\n        },\n        {\n            \"name\": \"[variables('nsgName')]\",\n            \"type\": \"Microsoft.Network/networkSecurityGroups\",\n            \"apiVersion\": \"[variables('networkApiVersion')]\",\n            \"location\": \"[resourceGroup().location]\",\n            \"comments\": \"Network Security Group (NSG) for Primary NIC\",\n            \"properties\": {\n                \"securityRules\": [\n                    {\n                        \"name\": \"default-allow-rdp\",\n                        \"properties\": {\n                            \"priority\": 1000,\n                            \"sourceAddressPrefix\": \"*\",\n                            \"protocol\": \"Tcp\",\n                            \"destinationPortRange\": \"3389\",\n                            \"access\": \"Allow\",\n                            \"direction\": \"Inbound\",\n                            \"sourcePortRange\": \"*\",\n                            \"destinationAddressPrefix\": \"*\"\n                        }\n                    }\n                ]\n            }\n        }\n    ],\n    \"outputs\": {}\n}\n\n\n\nФайл Mod07_90DaysOfDevOps-vm-parameters.json\n{\n\"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#\",\n\"contentVersion\": \"1.0.0.0\",\n\"parameters\": {\n\"vmSize\": {\n\"value\": \"Standard_D2s_v3\"\n},\n\"adminUsername\": {\n\"value\": \"Student\"\n},\n\"adminPassword\": {\n\"value\": \"Pa55w.rd1234\"\n}\n}\n}\n\n\n\n\n\nЗадача 2. Создание и настройка учетных записей хранения Azure.\n\n\n\nЗадача 3. Управление хранилищем BLOB-объектов\n\n\n\nЗадача 4. Управление проверкой подлинности и авторизацией для службы хранилища Azure.\n\n\n\n\nЯ был немного нетерпелив, ожидая, что это все сработает, но в конце концов это сработало.\n\n\n\nЗадача 5. Создание и настройка общих папок Azure Files.\n\nВ команде запуска это не сработает с michael.cade@90DaysOfDevOps.com, поэтому я использовал свою учетную запись с повышенными правами.\n\n\n\n\n\nЗадача 6. Управление сетевым доступом для службы хранилища Azure.\n\nServerless (внедрение веб-приложений)\nПереходим к модулю 09a:\n\nЗадача 1. Создание веб-приложения Azure.\n\n\n\nЗадача 2. Создание промежуточного слота развертывания.\n\n\n\nЗадача 3. Настройка параметров развертывания веб-приложений.\n\n\n\nЗадача 4. Развертывание кода в промежуточном слоте развертывания.\n\n\n\nЗадача 5: Поменять промежуточные слоты местами\n\n\n\nЗадача 6. Настройка и тестирование автоматического масштабирования веб-приложения Azure.\n$rgName = '90DaysOfDevOps'\n$webapp = Get-AzWebApp -ResourceGroupName $rgName\n#The following following will start an infinite loop that sends the HTTP requests to the web app\nwhile ($true) { Invoke-WebRequest -Uri $webapp.DefaultHostName }\n\n\nНа этом мы завершаем раздел о Microsoft Azure и public cloud в целом.\n\nРесурсы\n\nHybrid Cloud and MultiCloud\nMicrosoft Azure Fundamentals\nGoogle Cloud Digital Leader Certification Course\n\nДалее мы углубимся в системы контроля версий, особенно в git, а затем также рассмотрим обзоры репозиториев кода, и мы выберем GitHub, так как это мой предпочтительный вариант.\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day35",
            "title": "35. Git — контроль версий",
            "description": "Git — контроль версий",
            "content": "Общая картина: Git — контроль версий\n\nПрежде чем мы перейдем к git, нам нужно понять, что такое контроль версий? В этой статье мы рассмотрим, что такое контроль версий и основы git.\n\nЧто такое контроль версий?\n\nGit — не единственная система контроля версий, поэтому рассмотрим, какие варианты и какие методологии доступны для контроля версий.\n\nНаиболее очевидным и большим преимуществом контроля версий является возможность отслеживать историю проекта. Мы можем посмотреть на этот репозиторий с помощью git log и увидеть, что у нас есть много коммитов и много комментариев, а также то, что произошло на данный момент в проекте. Не волнуйтесь, мы перейдем к командам позже. А теперь подумайте, если бы это был настоящий программный проект, полный исходного кода, и несколько человек в разное время принимают участие в нашем программном обеспечении, разные авторы, а затем и рецензенты, все регистрируются здесь, чтобы мы знали, что произошло, когда, кем и кто рецензировал.\n\n\n\nУправление версиями, прежде чем это стало крутым, было чем-то вроде ручного создания копии вашей версии, прежде чем вы вносили изменения. Возможно, вы также закомментируете старый бесполезный код на всякий случай.\n\n\n\nТем не менее, Управление версиями не является резервной копией!\n\nЕще одним преимуществом контроля версий является возможность управления несколькими версиями проекта. Давайте создадим пример, у нас есть бесплатное приложение, доступное во всех операционных системах, а затем у нас есть платное приложение, также доступное во всех операционных системах. БОльшая часть кода используется обоими приложениями. Мы могли бы копировать и вставлять наш код при каждом коммите в каждое приложение, но это будет очень грязно, особенно если вы масштабируете свою разработку более чем на одного человека, а также будут допущены ошибки.\n\nВ премиум-приложении у нас будут дополнительные функции, назовем их премиальными коммитами, бесплатная версия будет содержать только обычные коммиты.\n\nСпособ, которым это достигается в системе управления версиями, — это ветвление (branching).\n\n\n\nВетвление позволяет использовать два потока кода для одного и того же приложения, как мы указали выше. Но мы по-прежнему хотим, чтобы новые функции, которые появляются в нашей бесплатной версии исходного кода, были в нашей премиум-версии, и для этого у нас есть то, что называется слиянием.\n\n\n\nТеперь это такое же простое, но слияние может быть сложным, потому что у вас может быть команда, работающая над бесплатной версией, и другая команда, работающая над платной премиальной версией, и что, если обе они изменят код, который влияет на аспекты общего кода. Может быть, переменная обновляется и что-то ломает. Тогда у вас есть конфликт, который нарушает одну из функций. Контроль версий не может устранить конфликты, которые зависят от вас. Но контроль версий позволяет легко управлять этим.\n\nОсновная причина, по которой вы до сих пор не взялись за управление версиями, — это возможность совместной работы. Возможность делиться кодом между разработчиками, и когда я говорю код, как я уже говорил раньше, все чаще и чаще мы видим гораздо больше вариантов использования по другим причинам для использования системы управления версиями, может быть, это совместная презентация, над которой вы работаете с коллегой, или вызов 90DaysOfDevOps. где у вас есть сообщество, предлагающее свои исправления и обновления на протяжении всего проекта.\n\nБез контроля версий, как команды разработчиков программного обеспечения вообще справлялись с этим? Когда я работаю над своими проектами, мне достаточно трудно следить за вещами. Я ожидаю, что они разделят код на каждый функциональный модуль. Возможно, небольшая часть головоломки заключалась в том, чтобы собрать воедино кусочки, а затем решить проблемы и проблемы, прежде чем что-либо было выпущено.\n\nС контролем версий у нас есть единственный источник правды. Мы все еще можем работать над разными модулями, но это позволяет нам лучше взаимодействовать.\n\n\n\nЕще одна вещь, которую следует упомянуть здесь, это то, что не только разработчики могут извлечь выгоду из контроля версий. Все члены команды должны иметь представление, но также и инструменты управления проектом и т.д.\nУ нас также может быть build машина, например Jenkins, о которой мы поговорим в другом модуле. Зада подобных инструментов - создать и упаковывать систему, автоматизируя тесты и предоставляя метрики.\nЧто такое Git?\n\nGit — это инструмент, который отслеживает изменения в исходном коде или любом файле, или мы могли бы также сказать, что Git — это распределенная система контроля версий с открытым исходным кодом.\n\nЕсть много способов, которыми git можно использовать в наших системах, чаще всего или, по крайней мере, для меня я видел его в командной строке, но у нас также есть графические пользовательские интерфейсы и инструменты, такие как Visual Studio Code, которые имеют операции с поддержкой git, которые мы может воспользоваться.\n\nТеперь мы пройдемся по общему обзору еще до того, как установим Git на нашу локальную машину.\n\nВозьмем папку, которую мы создали ранее.\n\n\n\nЧтобы использовать эту папку с контролем версий, нам сначала нужно инициировать этот каталог с помощью команды `git init. А пока представьте, что эта команда помещает наш каталог в качестве репозитория в базу данных где-то на нашем компьютере.\n\n\n\nТеперь мы можем создать несколько файлов и папок, и наш исходный код может начаться, или, может быть, он уже есть, и у нас уже есть что-то здесь. Мы можем использовать команду git add ., которая помещает все файлы и папки в нашем каталоге в снимок, но мы еще ничего не зафиксировали в этой базе данных. Мы просто говорим, что все файлы с . готовы к добавлению.\n\n\n\nЗатем мы хотим продолжить и зафиксировать наши файлы, мы делаем это с помощью команды git commit -m \"My First Commit\". Мы можем указать причину нашей фиксации, и это предлагается, чтобы мы знали, что произошло для каждой фиксации.\n\n\n\nТеперь мы можем увидеть, что произошло в истории проекта. С помощью команды git log.\n\n\n\nМы также можем проверить состояние нашего репозитория с помощью git status, это показывает, что нам нечего коммитить, и мы можем добавить новый файл с именем samplecode.ps1. Если мы затем запустим тот же статус `git, вы увидите, что мы файл для фиксации.\n\n\n\nДобавьте наш новый файл с помощью команды git add samplecode.ps1, а затем мы снова запустим git status и увидим, что наш файл готов к фиксации.\n\n\n\n\nЗатем выполните команду git commit -m \"My Second Commit\".\n\n\n\nДругой git status теперь показывает, что все снова чисто.\n\n\n\nЗатем мы можем использовать команду git log, которая показывает последние изменения и первую фиксацию.\n\n\n\nЕсли мы хотим увидеть изменения между нашими коммитами, то есть какие файлы были добавлены или изменены, мы можем использовать git diff b8f8 709a\n\n\n\nЗатем отображается то, что изменилось, в нашем случае мы добавили новый файл.\n\n\n\nМы также можем, и мы углубимся в это позже, но мы можем прыгать вокруг наших коммитов, то есть мы можем путешествовать во времени! Используя наш номер фиксации, мы можем использовать команду git checkout 709a, чтобы вернуться назад во времени, не теряя наш новый файл.\n\n\n\nНо в равной степени мы также захотим двигаться вперед, и мы можем сделать это таким же образом с номером коммита, или вы можете видеть здесь, что мы используем команду git switch -, чтобы отменить нашу операцию.\n\n\n\n\nTLDR;\n\nОтслеживание истории проектов\nУправление несколькими версиями проекта\nОбмен кодом между разработчиками и более широкий круг команд и инструментов\nКоординация работы в команде\n\nЭто могло показаться прыжком, но, надеюсь, вы можете увидеть, даже не зная, что команды использовали возможности и общую картину, лежащую в основе контроля версий.\n\nДалее мы установим и настроим git на вашем локальном компьютере и немного углубимся в некоторые другие варианты использования и команды, которые мы можем реализовать в Git.\n\nРесурсы\n\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\n\n",
            "tags": [
                "devops",
                "git"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day36",
            "title": "36. Установка и настройка Git",
            "description": "Установка и настройка Git",
            "content": "Установка и настройка Git\n\nGit — это кроссплатформенный инструмент с открытым исходным кодом для контроля версий. Если я нравлюсь вам, вы используете Ubuntu или большинство сред Linux, вы можете обнаружить, что у вас уже установлен git, но мы собираемся выполнить установку и настройку.\n\nДаже если у вас уже установлен git в вашей системе, также рекомендуется убедиться, что мы в курсе последних событий.\nУстановка Git\n\nМы будем работать с Windows и Linux, но вы также можете найти macOS в списке здесь\n\nДля Windows мы можем загрузить наши установщики с официального сайта.\n\nВы также можете использовать winget на своем компьютере с Windows, думайте об этом как о своем диспетчере пакетов приложений Windows.\n\nПрежде чем мы что-либо установим, давайте посмотрим, какая версия у нас есть на нашей машине с Windows. Откройте окно PowerShell и запустите  git --version\n\nМы также можем проверить нашу версию Git для Ubuntu.\n\n\n\nЗагружаем последнюю версию установщика. Важно отметить, что git удалит предыдущие версии перед установкой последней.\n\nЭто означает, что процесс, показанный ниже, по большей части такой же, как если бы вы устанавливали не из git.\n\nЭто очень простая установка. После загрузки дважды щелкните и начните. Прочтите лицензионное соглашение GNU. Но помните, что это бесплатное программное обеспечение с открытым исходным кодом.\n\n\n\nТеперь мы можем выбрать дополнительные компоненты, которые мы хотели бы также установить, но также связать с git. В Windows я всегда устанавливаю Git Bash, так как это позволяет нам запускать сценарии bash в Windows.\n\n\n\nЗатем мы можем выбрать, какой исполняемый файл SSH мы хотим использовать. IN оставьте это как пакетный OpenSSH, который вы могли видеть в разделе Linux.\n\n\n\nЗатем у нас есть экспериментальные функции, которые мы можем захотеть включить, мне они не нужны, поэтому я не включаю, вы всегда можете вернуться во время установки и включить их позже.\n\n\n\nУстановка завершена, теперь мы можем открыть Git Bash или последние примечания к выпуску.\n\n\n\nПоследняя проверка — посмотреть в нашем окне PowerShell, какая у нас сейчас версия git.\n\n\n\nСупер простые вещи, и теперь мы на последней версии. На нашей машине с Linux мы немного отстали, поэтому мы также можем пройти этот процесс обновления.\n\nЯ просто запускаю команду sudo apt-get install git.\n\n\n\nВы также можете запустить следующее, которое добавит репозиторий git для установки программного обеспечения.\n\nsudo add-apt-repository ppa:git-core/ppa -y\nsudo apt-get update\nsudo apt-get install git -y\ngit --version\nНастройка Git\n\nКогда мы впервые используем git, нам нужно определить некоторые настройки,\n\nИмя\nЭл. адрес\nРедактор по умолчанию\nОкончание строки\n\nЭто можно сделать на трех уровнях\n\nSystem = Все пользователи\nGlobal = все репозитории текущего пользователя\nLocal = текущий репозиторий\n\nПример:\n\ngit config --global user.name \"My Name\"\ngit config --global user.email email@example.com\"\n\nВ зависимости от вашей операционной системы будет определять текстовый редактор по умолчанию. На моей машине с Ubuntu без настройки следующая команда использует Тano. Приведенная ниже команда изменит это на код Visual Studio.\n\ngit config --global core.editor \"code --wait\"\n\nЧтобы увидеть всю конфигурацию git, мы можем использовать команду git config --global -e\n\nНа любом компьютере этот файл будет называться .gitconfig, на моем компьютере с Windows вы найдете его в каталоге своей учетной записи пользователя.\n\nТеория Git\n\nЯ упомянул во вчерашнем посте, что существуют и другие типы контроля версий, и мы можем разделить их на два разных типа. Один клиент-сервер, а другой распределенный.\nКлиент-серверный контроль версий\n\nДо появления git клиент-сервер был де-факто методом контроля версий. Примером этого может быть Apache Subversion, которая представляет собой систему управления версиями с открытым исходным кодом, основанную в 2000 году.\n\nВ этой модели управления версиями клиент-сервер на первом этапе разработчик загружает исходный код, фактические файлы с сервера. Это не устраняет конфликты, но устраняет сложность конфликтов и способы их разрешения.\n\n\n\nТеперь, например, скажем, у нас есть два разработчика, работающих над одними и теми же файлами, и один из них выигрывает гонку и первым фиксирует или загружает свой файл обратно на сервер со своими новыми изменениями. Когда второй разработчик идет на обновление, у них возникает конфликт.\n\n\n\nИтак, теперь разработчику нужно вывести первое изменение кода разработчика рядом с его проверкой, а затем зафиксировать, как только эти конфликты будут урегулированы.\n\nРаспределенный контроль версий\n\nGit — не единственная распределенная система контроля версий. Но это очень де-факто.\n\nНекоторые из основных преимуществ Git:\n\nБыстрый\nГибкий\nБезопасный и надежный\n\nВ отличие от модели управления версиями клиент-сервер, каждый разработчик загружает исходный репозиторий, то есть все. История коммитов, все ветки и т.д. и т.п.\n\nРесурсы\n\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day37",
            "title": "37. Шпаргалка по Git",
            "description": "Знакомство с Git",
            "content": "Знакомство с Git\n\nВ последних двух постах мы узнали о системах контроля версий и некоторых основных рабочих процессах git как системы контроля версий День 35. Затем мы установили git в нашу систему, обновили и настроили. Мы также немного углубились в теорию между системой контроля версий клиент-сервер и Git, которая является распределенной системой контроля версий День 36.\n\nТеперь мы пройдемся по некоторым командам и вариантам использования, которые мы все обычно видим в git.\n\nГде получить помощь по git?\n\nБудут времена, когда вы просто не сможете вспомнить или просто не знаете команду, которая вам нужна для работы с git. Вам понадобится помощь.\n\nСамо собой разумеется, что Google или любая другая поисковая система, вероятно, будет вашим первым портом захода при поиске помощи.\n\nВо-вторых, следующим местом будет официальный сайт git и документация. git-scm.com/docs Здесь вы найдете не только подробные ссылки на все доступные команды, но и множество различных ресурсов.\n\n\n\nМы также можем получить доступ к этой же документации, которая очень полезна, если у вас нет подключения к терминалу. Например, если мы выбрали команду git add, мы можем запустить git add --help, и мы увидим ниже руководство.\n\n\n\nМы также можем в оболочке использовать git add -h, который даст нам краткий обзор доступных опций.\n\nМифы Git\n\n«У Git нет контроля доступа» — вы можете уполномочить \"лидера\" поддерживать исходный код.\n\n«Git слишком тяжелый» — у Git есть возможность предоставлять неглубокие репозитории, что в основном означает меньший объем истории, если у вас большие проекты.\n\nНедостатки\n\nНе идеально подходит для двоичных файлов. Отлично подходит для исходного кода, но не подходит, например, для исполняемых файлов или видео.\n\nGit не удобен для пользователя, тот факт, что нам приходится тратить время на обсуждение команд и функций инструмента, вероятно, является ключевым признаком этого.\n\nВ целом, git сложно освоить, но легко использовать.\nЭкосистема git\n\nЯ хочу кратко рассказать об экосистеме вокруг git, но не углубляться в некоторые из этих областей, но я думаю, что важно отметить их здесь на высоком уровне.\n\nПочти все современные инструменты разработки поддерживают Git.\n\nИнструменты разработчика. Мы уже упоминали код Visual Studio, но вы найдете плагины git и интеграции в возвышенный текст и другие текстовые редакторы и IDE.\n\nКомандные инструменты. Также упоминаются такие инструменты, как Jenkins с точки зрения CI/CD, Slack из среды обмена сообщениями и Jira для управления проектами и отслеживания проблем.\n\nОблачные провайдеры. Все крупные облачные провайдеры поддерживают git, Microsoft Azure, Amazon AWS, Google Cloud Platform.\n\nСервисы на основе Git. Затем у нас есть GitHub, GitLab и BitBucket, о которых мы поговорим более подробно позже. Я слышал об этих сервисах как о социальной сети для кода!\n\nШпаргалка по Git\n\nМы не рассмотрели большинство этих команд, но просмотрев некоторые шпаргалки, доступные в Интернете, я хотел задокументировать некоторые из команд git и их назначение. Нам не нужно запоминать все это, и с большей практикой и использованием вы выберете, по крайней мере, основы git.\n\nОсновы Git\n\n| Command       | Example                     | Description                                                                                                                                                           |\n| ------------- | --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git init      | git init       | создает пустой репозиторий git в указанном каталоге.                                                                                                                  |\n| git clone     | git clone           | клонирует репозиторий, расположенный в , на локальный компьютер.                                                                                                |\n| git config    | git config user.name      | определяет имя автора, которое будет использоваться для всех коммитов в текущем репозитории,  system, global, local флаг для установки параметров конфигурации. |\n| git add       | git add        | он подготовит все изменения в  для следующего коммита. Мы также можем добавить  и  для добавления всех изменененных файлов всего.                |\n| git commit -m | git commit -m \"\" | фиксирует промежуточный коммит, запишет , чтобы подробно описать, что точно сохраняем.                                                                       |\n| git status    | git status                | выведит список файлов, которые помещены в архив, не помещены в архив и не отслеживаются.                                                                              |\n| git log       | git log                   | Отображение всей истории коммитов в формате по умолчанию. У этой команды есть дополнительные параметры.                                                               |\n| git diff      | git diff                  | Показать неустановленные изменения между вашим индексом и рабочим каталогом.                                                                                          |\n\nGit Отмена изменений\n\n| Command    | Example               | Description                                                                                                         |\n| ---------- | --------------------- | ------------------------------------------------------------------------------------------------------------------- |\n| git revert | git revert  | создает новую фиксацию, которая отменяет все изменения, сделанные в , а затем примените ее к текущей ветке. |\n| git reset  | git reset     | убрать  из индекса коммита (изменения не теряются).                                                           |\n| git clean  | git clean -n        | увидеть, какие файлы являются лишними, перед их непосредственным удалением                                          |\n| git clean  | git clean -f        | удалить неотслеживаемые файлы и папки из рабочей копии                                                              |\n| git clean  | git clean -fd       | удалить их                                                                                                          |\n\nGit переписать историю\n\n| Command    | Example              | Description                                                                                                                                                     |\n| ---------- | -------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git commit | git commit --amend | Заменяет последний коммит поэтапными изменениями и последним коммитом. Используйте без статуса stage, чтобы отредактировать сообщение последнего коммита.     |\n| git rebase | git rebase   | Перебазировать текущую ветку на .  может быть идентификатором фиксации, именем ветки, тегом или относительной ссылкой на HEAD.                      |\n| git reflog | git reflog         | Показать журнал изменений в HEAD локального репозитория. Добавьте флаг --relative-date для отображения информации о дате или --all для отображения всех ссылок. |\n\nGit Branches\n\n| Command      | Example                    | Description                                                                                                   |\n| ------------ | -------------------------- | ------------------------------------------------------------------------------------------------------------- |\n| git branch | git branch   | Перечислите все ветки в вашем репо. Добавьте аргумент , чтобы создать новую ветку с именем . |\n| git checkout  | git checkout -b  | Создайте и извлеките новую ветку с именем . Отбросьте флаг -b, чтобы проверить существующую ветку. |\n| git merge | git merge    | Объединить ветку  с текущей веткой. |\nGit Remote Repositories\n\n| Command        | Example                       | Description                                                                                                                                               |\n| -------------- | ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git remote add | git remote add   | Создайте новое подключение к удаленному репозиторию. После добавления пульта вы можете использовать  в качестве ярлыка для  в других командах. |\n| git fetch      | git fetch   | Выбирает конкретную  из репозитория. Оставьте , чтобы получить все удаленные ссылки.                                                       |\n| git pull       | git pull            | Получить указанную удаленную копию текущей ветки и немедленно объединить ее с локальной копией.                                                           |\n| git push       | git push    | Отправьте ветку на  вместе с необходимыми коммитами и объектами. Создает именованную ветку в удаленном репо, если она не существует.              |\nGit Diff\n\n| Command           | Example             | Description                                                         |\n| ----------------- | ------------------- | ------------------------------------------------------------------- |\n| git diff HEAD     | git diff HEAD     | Показать разницу между рабочим каталогом и последним коммитом.      |\n| git diff --cached | git diff --cached | Показать разницу между поэтапными изменениями и последней фиксацией |\nGit Config\n\n| Command                                             | Example                                                | Description                                                                                                                                                                  |\n| --------------------------------------------------- | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git config --global user.name                  | git config --global user.name                   | Определите имя автора, которое будет использоваться для всех коммитов текущим пользователем.                                                                                 |\n| git config --global user.email               | git config --global user.email                | Определите адрес электронной почты автора, который будет использоваться для всех коммитов текущего пользователя.                                                             |\n| git config --global alias   | git config --global alias   | Создать ярлык для команды git.                                                                                                                                               |\n| git config --system core.editor           | git config --system core.editor            | Установите текстовый редактор, который будет использоваться командами для всех пользователей на машине. Аргумент  должен быть командой, запускающей нужный редактор. |\n| git config --global --edit                          | git config --global --edit                           | Откройте файл глобальной конфигурации в текстовом редакторе для редактирования вручную.                                                                                      |\nGit Rebase\n\n| Command              | Example                | Description                                                                                                                                    |\n| -------------------- | ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| git rebase -i  | git rebase -i  | Интерактивно перебазировать текущую ветку на . Запускает редактор для ввода команд того, как каждый коммит будет перенесен в новую базу. |\n\nGit Pull\n\n| Command                    | Example                      | Description                                                                                                                              |\n| -------------------------- | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n| git pull --rebase  | git pull --rebase  | Получить удаленную копию текущей ветки и перебазировать ее в локальную копию. Использует git rebase вместо слияния для интеграции веток. |\n\nGit Reset\n\n| Command                   | Example                     | Description                                                                                                                                                                |\n| ------------------------- | --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git reset                 | git reset                 | Сбросьте промежуточную область, чтобы она соответствовала самой последней фиксации, но оставьте рабочий каталог без изменений.                                             |\n| git reset --hard          | git reset --hard          | Сбросить промежуточную область и рабочий каталог, чтобы они соответствовали самой последней фиксации, и перезаписать все изменения в рабочем каталоге                      |\n| git reset         | git reset         | Переместите конец текущей ветки назад к , сбросьте промежуточную область, чтобы она соответствовала, но оставьте рабочий каталог в покое                           |\n| git reset --hard  | git reset --hard  | То же, что и предыдущее, но сбрасывает и промежуточную область, и рабочий каталог, чтобы они совпадали. Удаляет незафиксированные изменения и все фиксации после . |\n\nGit Push\n\n| Command                   | Example                     | Description                                                                                                                                                         |\n| ------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| git push  --force | git push  --force | Делает git push, даже если это приводит к слиянию без быстрой перемотки вперед. Не используйте флаг --force, если вы абсолютно не уверены, что знаете, что делаете. |\n| git push  --all   | git push  --all   | Переместите все свои локальные ветки на указанный удаленный сервер.                                                                                                 |\n| git push  --tags  | git push  --tags  | Теги не добавляются автоматически при отправке ветки или использовании флага --all. Флаг --tags отправляет все ваши локальные теги в удаленное репо.                |\n\nРесурсы\n\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\nGit cheatsheet\n\n",
            "tags": [
                "devops",
                "git"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day38",
            "title": "38. Staging и Изменения",
            "description": "Git Staging и Изменение файлов",
            "content": "\nWorking directory\n\nGit - это система трёх основных стадий: working directory, staging area и repository.\n\n\nПройдем поэтапно каждую стадию.\n\nСоздадим пустую папку.\nmkdir my_fodler\ncd my_folder\nСделаем инициализацию git проекта.\ngit init\n\nПосле инициализации git репозитория создается скрытая папка .git\n\nЗдесь хранятся сведения о репозитории git, а также информация о наших ветках и коммитах.\nStaging/Stage\n\nСейчас у нас пустая папка. Создадим пустой файл README.md и выполним команду\ngit status\nGit знает о новом файле, но этот файл еще не зафиксирован в staging. Текущее расположение файла - Working directory, директория, где проиниализирован .git проект.\n\nstaging - это хранилище для файлов с изменениями, информация о которых попадет в единый коммит\n\n\n\nЧтобы файл перешел в staging, необходимо его добавить. Для этого выполним команду\ngit add README.md\nПосле добавления файла в staging area, цвет поменялся на зеленый\n\n\n\nМожно добавить все измененные файлы с помощью команды\ngit add .\nЗнак . означает, что мы хотим добавить все обновленные файлы и папки.\n\nДалее необходимо зафиксировать изменения в репозитории. Для этого выполним команду\ngit commit -m \"Add README.md (или другой значимый комментарий)\"\n\nКоммит изменений\nВ процессе работы мы добавляем много различных файлов. Если мы захотим добавить более длинный и осмысленный коммит, то можно запусть команду без комментария\n\ngit commit\nОткроется стандартный редактор текста. Записываем комментарий и сохраняем.\n\n\nПроверим результат\ngit status\n\nТребования к именам коммитов\n\nУ каждой компании/проекта есть свои требования к именам коммитов. В компании может быть несколько проектов, каждый из которых должен иметь свои требования к именам коммитов. В проекте может быть несколько веток, каждая из которых должна иметь свои требования к именам коммитов.\n\nСуществует гайдлайн, на который можно ориентироваться. Такой подход точно будет понятен для всех новых проектов. Некоторые проекты, соблюдабщие данную конвенцию: angular, electron\n\nКоммит:\nДолжен использоваться present tense (\"add feature\" not \"added feature\")\nДолжен использоваться imperative mood (\"move cursor to...\" not \"moves cursor to...\")\n\nПримеры имен коммитов\ninit: - используется для начала проекта/таска. Примеры:\ninit: start youtube-task\ninit: start mentor-dashboard task\nfeat: - это реализованная новая функциональность из технического задания (добавил поддержку зумирования, добавил footer, добавил карточку продукта). Примеры:\nfeat: add basic page layout\nfeat: implement search box\nfeat: implement request to youtube API\nfeat: implement swipe for horizontal list\nfeat: add additional navigation button\nfeat: add banner\nfeat: add social links\nfeat: add physical security section\nfeat: add real social icons\nfix: - исправил ошибку в ранее реализованной функциональности. Примеры:\nfix: implement correct loading data from youtube\nfix: change layout for video items to fix bugs\nfix: relayout header for firefox\nfix: adjust social links for mobile\nrefactor: - новой функциональности не добавлял / поведения не менял. Файлы в другие места положил, удалил, добавил. Изменил форматирование кода (white-space, formatting, missing semi-colons, etc). Улучшил алгоритм, без изменения функциональности. Примеры:\nrefactor: изменение структуры проекта\nrefactor: переименование переменных для лучшей читабельности\nrefactor: применить eslint\nrefactor: применить prettier\ndocs: - используется при работе с документацией/readme проекта. Примеры:\ndocs: обновить readme с дополнительной информацией\ndocs: обновить описание метода run()\n\nПропуск Staging Area\n\nМожно сразу добавить коммит, добавим параметр -a в git commit:\n\nУдаление файлов\nФиксация удаления как и добавления файлов происхоит через комит\n\nСоздадим файл -> Добавим в stage -> Удалим файл\n\ntouch old_file.txt\ngit add old_file.txt\ngit commit -m \"add old_file to be removed\"\n\nУдаляем файл\n\ngit rm old_file.txt\ngit status\n\nПереименование/Перемещение файлов\n\nМы можем переименовывать или перемещать файлы в проекте средствами операционной системы. Таке это можно делать командами git.\n\nПример:\n\ngit mv old_file.txt new_file.txt\n\nПропуск/игнорирование файлов\n\nВ Git это можно сделать рзличными способами:\nИгнорировать изменения в неотслеченных файлах с помощью .gitignore файла\nИгнорировать изменения в неотслеченных файлах с помощью exclude файла\nОстановка отслеживания файла и пропуск изменений с помощью git update-index\nОстановка отслеживания файла и пропуск изменений с помощью git rm\n\n.gitignore\n\nДостаточно в файл .gitignore добавить путь до файлов или папок, которые необходимо игнорировать\n\n\n\nПосле обновления файл переходит в категорию Untracked files\n\n\n\nЕсли файлы уже добавлены в stage, но нужно убрать файл, то можно использовать команду git rm --cached\n\nStatus сокращенно\n\ngit status -s\n\nРесурсы\n\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day39",
            "title": "39. Просмотр, удаление, отмена и восстановление",
            "description": "Просмотр, удаление, отмена и восстановление версий в Git",
            "content": "GIT - Просмотр, удаление, отмена и восстановление\n\nПросмотр файлов в Stagig area и Working area\n\nЕсли некоторые файлы/папки уже добавлены в staging area, то можно просмотреть их рахницу по отношению в главной ветке комадой:  git diff --staged\n\n\n\nЭто покажет нам все внесенные изменения и все новые файлы, которые мы добавили или удалили.\n\nИзменения в измененных файлах обозначаются символами --- или +++ Вы можете видеть ниже, что мы только что добавили +add some text, что означает, что это новые строки.\n\nМы также можем запустить git diff, чтобы сравнить наш staging area с нашим рабочим каталогом. Если мы внесем некоторые изменения в наш только что добавленный файл code.txt и добавим несколько строк текста.\n\nИнструменты для визуального отображения\n\nВот несколько инструментов для визуального сравнения коммитов и веток:\n\nKDiff3\nP4Merge\nWinMerge (только для Windows)\nVSCode\n\nЕсли запустим git difftool то запустится визуальный инструмент сравнения по умолчанию.\n\nПроверить текущие настройки git config --global -e\n\nЧтобы установить инструмент в git, выполним следующую команду git config --global diff.tool vscode.\n\nТеперь при запуске git difftool откроется vscode\nПосле этого открывается редактор VScode на странице diff и сравнивает их, мы изменили только один файл, добавив строку кода с правой стороны.\n\nМожем использовать git difftool --staged для сравнения файлов в staging area с \"прокомиченными\" файлами.\n\nVScode, как и большинство IDE, имеют встроенную функциональность, поэтому очень редко вам понадобится запускать эти команды из терминала, хотя это полезно, если у вас по какой-то причине не установлена IDE.\n\nПросмотр истории изменений\n\nПросмотреть историю изменений в Git можно командой git log\n\n\nКаждый коммит имеет свою шестнадцатеричную строку, уникальную для репозитория. Здесь вы можете увидеть, над какой веткой мы работаем, а также автора, дату и комментарий коммита.\n\nУ нас также есть git log --oneline, и это даёт нам гораздо меньшую версию шестнадцатеричной строки, которую мы можем использовать в других командах diff.\n\nЧтобы просмотреть коммиты с самого первого, а не послденего, как по умолчанию, запустим   git log --oneline --reverse, и теперь мы видим наш первый коммит в верхней части страницы.\n\nПросмотр коммита\n\nМожно просмотреть данные ко конкретном коммите более детально: git show или git show \n\n\n\n\n\nМы также можем использовать git show HEAD~1, где 1 - это количество шагов назад от текущей версии, к которой мы хотим вернуться.\n\nЭто отличный вариант, если вам нужна подробная информация о файлах, но если мы хотим получить список всех файлов в дереве для всего каталога снимков. Мы можем добиться этого, используя команду git ls-tree HEAD~1, снова вернувшись на один снимок назад от последнего коммита. Ниже мы видим два пятна, которые обозначают файлы, в то время как дерево обозначает каталог. В этой информации вы также можете увидеть коммиты и теги.\n\nПроверим коммит\n\nUnstaging\n\nБывают случаи, когда вы, возможно, использовали git add ., но на самом деле есть файлы, которые вы пока не хотите фиксировать в этом снапшоте. В этом примере ниже я добавил newfile.txt в область staging, но я не готов зафиксировать этот файл, поэтому я собираюсь использовать git restore --staged newfile.txt, чтобы отменить шаг git add.\n\nМы также можем сделать то же самое с изменёнными файлами, такими как main.js, и снять фиксацию, см. выше у нас есть greem M для modified, а ниже мы снимаем фиксацию этих изменений.\n\nЯ нашел эту команду весьма полезной во время 90DaysOfDevOps, поскольку иногда я работаю заранее, когда чувствую, что хочу сделать заметки для следующего дня, но не хочу фиксировать и выкладывать в публичный репозиторий GitHub.\n\nОтмена локальных изменений\n\nИногда мы можем вносить изменения, но эти изменения нас не устраивают, и мы хотим их отбросить. Мы снова воспользуемся командой git restore и сможем восстановить файлы из наших снимков или предыдущих версий. Мы можем запустить команду git restore . для нашего каталога, и мы восстановим все из нашего снимка, но обратите внимание, что наш неотслеживаемый файл все еще присутствует. Нет предыдущего отслеживаемого файла под названием newfile.txt.\n\nТеперь, чтобы удалить newfile.txt или любой другой неотслеживаемый файл. Мы можем использовать git clean, но получим только предупреждение.\n\nИли, если мы знаем о последствиях, мы можем запустить git clean -fd, чтобы принудительно удалить все каталоги.\n\nВосстановление файла до более ранней версии\n\nКак мы уже упоминали, большая часть того, чем может помочь Git, - это возможность восстановления копий файлов из снимков (это не резервное копирование, но это очень быстрая точка восстановления). Я советую вам также сохранять копии вашего кода в других местах, используя для этого решение для резервного копирования.\n\nВ качестве примера давайте удалим наш самый важный файл в каталоге, обратите внимание, что мы используем команды на базе unix для удаления этого файла из каталога, а не команды git.\n\nТеперь у нас нет readme.mdin в нашей рабочей директории. Мы могли бы использовать git rm readme.md и тогда это было бы отражено в нашей базе данных git. Давайте также удалим его отсюда, чтобы имитировать его полное удаление.\n\nТеперь зафиксируем это с сообщением и докажем, что у нас больше нет ничего в рабочем каталоге или в области постановки.\n\nБыла допущена ошибка, и теперь нам нужно вернуть этот файл!\n\nМы можем использовать команду git undo, которая отменит последний коммит, но что если это было давно? Мы можем использовать команду git log, чтобы найти наши коммиты, и тогда мы обнаружим, что наш файл находится в последнем коммите, но мы не хотим, чтобы все эти коммиты были отменены, поэтому мы можем использовать эту команду git restore --source=HEAD~1 README.md, чтобы найти файл и восстановить его из нашего снимка.\n\nВы можете видеть, что с помощью этого процесса мы вернули файл в наш рабочий каталог.\n\nТеперь у нас есть новый неотслеживаемый файл, и мы можем использовать наши команды, упомянутые ранее, для отслеживания, этапа и фиксации наших файлов и изменений.\n\nRebase / Merge\n\nЭто, кажется, самая большая головная боль, когда речь заходит о Git и о том, когда использовать rebase, а когда использовать merge в ваших git-репозиториях.\n\nПрежде всего, нужно знать, что и git rebase, и git merge решают одну и ту же задачу. Оба они интегрируют изменения из одной ветки в другую. Однако они делают это по-разному.\n\nДавайте начнем с новой функции в новой выделенной ветке. Основная ветку продолжает работу с новыми коммитами.\n\nПростой вариант здесь - использовать git merge feature main, который объединит основную ветку с веткую feature.\n\nСлияние простое, потому что оно неразрушающее. Существующие ветви никак не изменяются. Однако это также означает, что функциональная ветку будет иметь неактуальный коммит слияния каждый раз, когда вам нужно будет включить изменения, внесённые выше по течению. Если main очень занят или активен, это может привести к загрязнению истории функциональной ветви.\n\nВ качестве альтернативного варианта мы можем перебазировать функциональную ветку на основную ветку с помощью команды\n\ngit checkout feature\ngit rebase main\nЭто перемещает ветку feature (всю ветку feature), эффективно включая все новые коммиты в main. Но вместо использования коммита слияния, rebasing переписывает историю проекта, создавая совершенно новые коммиты для каждого коммита в исходной ветке.\n\n\n\nСамым большим преимуществом ребасинга является гораздо более чистая история проекта. Это также устраняет ненужные коммиты слияния. и если сравнить последние два изображения, то можно увидеть, что история проекта намного чище.\n\nХотя это еще не окончательный вывод, потому что выбор более чистой истории также связан с компромиссами. Если вы не будете следовать The Golden rule of rebasing, переписывание истории проекта может стать потенциально катастрофой для вашего рабочего процесса совместной работы. И, что менее важно, при пересборке теряется контекст, предоставляемый коммитом слияния - вы не можете увидеть, когда изменения, внесенные выше по течению, были включены в функцию.\n\nСсылки\n\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\nGit cheatsheet\nExploring the Git command line – A getting started guide\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day40",
            "title": "40. GitHub | GitLab | BitBucket",
            "description": "Обзор онлайн репозиториев для Git",
            "content": "Социальная сеть для кода\nИзучение GitHub | GitLab | BitBucket\n\nСегодня я хочу рассказать о некоторых сервисах на основе git, о которых мы, вероятно, все слышали и ожидаем, что будем использовать их ежедневно.\nGitHub\n\nНаиболее распространенным, по крайней мере для меня, является GitHub, GitHub — это веб-хостинг для git. Чаще всего он используется разработчиками программного обеспечения для хранения своего кода. Управление исходным кодом с функциями контроля версий git, а также множеством дополнительных функций. Это позволяет командам или открытым участникам легко общаться и обеспечивает социальный аспект кодирования. (отсюда и название социальной сети) С 2018 года GitHub является частью Microsoft.\n\nGitHub существует уже довольно давно и был основан в 2007-2008 годах. Сегодня на платформе более 40 миллионов пользователей.\n\nОсновные возможности GitHub\n\nCode Repository\nPull Requests\nProject Management toolset - Issues\nCI / CD Pipeline - GitHub Actions\n\nС точки зрения ценообразования GitHub предлагает различные уровни ценообразования для своих пользователей. Дополнительную информацию можно найти на странице Цены.\n\nДля этого мы рассмотрим бесплатный уровень.\n\nЯ собираюсь использовать свою уже созданную учетную запись GitHub во время этого пошагового руководства, если у вас нет учетной записи, то на открывающейся странице GitHub есть вариант регистрации и несколько простых шагов для настройки.\n\nGitHub opening page\n\nКогда вы впервые входите в свою учетную запись GitHub, вы получаете страницу, содержащую множество виджетов, дающих вам варианты того, где и что вы хотели бы увидеть или сделать. Во-первых, у нас есть «All Activity», это даст вам представление о том, что происходит с вашими репозиториями или действиями в целом, связанными с вашей организацией или учетной записью.\n\n\n\nЗатем у нас есть наши репозитории кода, либо наши собственные, либо репозитории, с которыми мы недавно взаимодействовали. Мы также можем быстро создавать новые репозитории или репозитории поиска.\n\n\n\nЗатем у нас есть наша недавняя активность, для меня это проблемы и  pull requests, которые я недавно создал или в которых участвовал.\n\n\n\nВ правой части страницы есть несколько ссылок на репозитории, которые могут нас заинтересовать, скорее всего, на основе вашей недавней активности или собственных проектов.\n\nЧестно говоря, я очень редко бываю на своей домашней странице, которую мы только что видели и описали, хотя теперь я вижу, что лента может быть действительно полезной, чтобы помочь взаимодействовать с сообществом немного лучше в определенных проектах.\n\nДалее, если мы хотим зайти в наш профиль на GitHub, мы можем перейти в правый верхний угол, и на вашем изображении будет выпадающий список, который позволит вам перемещаться по вашему аккаунту. Отсюда для доступа к своему профилю выберите \"Ваш профиль\"\n\nДалее появится страница вашего профиля, по умолчанию, если вы не измените свою конфигурацию, вы не увидите того, что есть у меня, я добавил некоторые функции, которые показывают мои последние записи в блоге на vZilla, а также мои последние видео на моем канале YouTube.\n\nЛично вы не собираетесь тратить много времени на просмотр своего профиля, но это хорошая страница профиля, которой можно поделиться со своей сетью, чтобы они могли увидеть крутые проекты, над которыми вы работаете.\n\nЗатем мы можем перейти к основному элементу GitHub - репозиториям. Здесь вы увидите свои собственные репозитории, а если у вас есть частные репозитории, они также будут показаны в этом длинном списке.\n\nПоскольку этот репозиторий так важен для GitHub, позвольте мне выбрать довольно загруженный в последнее время и просмотреть некоторые основные функции, которые мы можем использовать здесь, в дополнение ко всему, что я уже использую, когда дело доходит до редактирования нашего кода в git. моя локальная система.\n\nПрежде всего, в предыдущем окне я выбрал репозиторий 90DaysOfDevOps, и мы видим это представление. Вы можете видеть из этого представления, что у нас есть много информации, у нас есть наша основная структура кода в середине, показывающая наши файлы и папки, которые хранятся в нашем репозитории. Наш файл readme.md отображается внизу. Справа от страницы у нас есть раздел о репозитории, где у репозитория есть описание и назначение. Затем у нас есть много информации под этим, показывающей, сколько людей отметили проект, разветвились и смотрят.\n\n\n\nЕсли мы прокрутим вниз немного дальше, вы также увидите, что у нас есть Releases, они относятся к части задачи golang. У нас нет никаких пакетов в нашем проекте, здесь перечислены наши соавторы. Затем у нас есть используемые языки, опять же из разных разделов задачи.\n\nВ верхней части страницы вы увидите список вкладок. Они могут различаться, и их можно изменить, чтобы отображались только те, которые вам нужны. Вы увидите здесь, что я не использую все это, и я должен удалить их, чтобы убедиться, что весь мой репозиторий в порядке.\n\nВо-первых, у нас была вкладка кода, которую мы только что обсуждали, но эти вкладки всегда доступны при навигации по репозиторию, что очень полезно, так что мы можем быстро и легко переходить между разделами. Далее у нас есть вкладка вопросов.\n\nПроблемы позволяют отслеживать вашу работу на GitHub, где происходит разработка. В этом конкретном репозитории вы можете увидеть, что у меня есть некоторые проблемы, связанные с добавлением диаграмм или опечаток, но также у нас есть проблема, указывающая на необходимость или требование для китайской версии репозитория.\n\nЕсли это был репозиторий кода, то это отличное место, чтобы сообщить о проблемах или проблемах с сопровождающими, но помните, будьте внимательны и подробны в отношении того, о чем вы сообщаете, давайте как можно больше подробностей.\n\n\n\nСледующая вкладка — Pull Requests. Pull Requests позволяют вам сообщать другим об изменениях, которые вы отправили в ветку в репозитории. Здесь кто-то мог разветвить ваш репозиторий, внести изменения, такие как исправления ошибок или улучшения функций, или просто опечататься во многих случаях в этом репозитории.\n\nМы рассмотрим разветвление позже.\n\n\n\nЯ считаю, что следующая вкладка совершенно новая? Но я подумал, что для такого проекта, как #90DaysOfDevOps, это может действительно помочь направить контент, а также помочь сообществу, когда они проходят свой собственный путь обучения. Я создал несколько дискуссионных групп для каждого раздела задачи, чтобы люди могли присоединиться и обсудить.\n\n\n\nВкладка \"Actions\" позволит вам создавать, тестировать и развертывать код и многое другое прямо из GitHub. GitHub Actions будет чем-то, что мы рассмотрим в разделе задачи, посвященном CI/CD, но именно здесь мы можем установить некоторую конфигурацию, чтобы автоматизировать шаги для нас.\n\nВ моем основном профиле GitHub я использую GitHub Actions для получения последних сообщений в блогах и видео на YouTube, чтобы обновлять информацию на этом домашнем экране.\n\n\n\nЯ уже говорил о том, что GitHub - это не только хранилище исходного кода, но и инструмент управления проектами. Вкладка \"Проект\" позволяет нам создавать проектные таблицы типа канбан, чтобы мы могли связывать проблемы и PR для лучшего сотрудничества над проектом и иметь видимость этих задач.\n\nЯ знаю, что проблемы, как мне кажется, являются хорошим местом для регистрации запросов о возможностях, и это так, но страница вики позволяет составить полную дорожную карту проекта с указанием текущего состояния и в целом лучше документировать ваш проект, будь то устранение неполадок или контент типа how-to.\n\nНе совсем применимо к этому проекту, но вкладка Security действительно существует для того, чтобы убедиться, что участники проекта знают, как обращаться с определенными задачами, здесь мы можем определить политику, а также дополнения для сканирования кода, чтобы убедиться, что ваш код, например, не содержит секретных переменных окружения.\n\nДля меня вкладка insights очень важна, она предоставляет так много информации о репозитории, начиная от того, сколько активности происходило и заканчивая коммитами и проблемами, а также сообщает о посещаемости репозитория. В левой части вы можете увидеть список, который позволяет вам подробно ознакомиться с метриками репозитория.\n\nНаконец, у нас есть вкладка Settings, где мы можем подробно описать, как мы управляем нашим репозиторием, в настоящее время я единственный сопровождающий репозитория, но мы можем разделить эту ответственность. Здесь мы можем определить интеграции и другие подобные задачи.\n\nЭто был очень быстрый обзор GitHub, я думаю, что есть еще несколько областей, которые я, возможно, упомянул и которые нуждаются в более подробном объяснении. Как уже упоминалось, GitHub содержит миллионы репозиториев, в которых в основном хранится исходный код, и они могут быть общедоступными или частными.\n\nForking\n\nЯ собираюсь больше рассказать об Open-Source на завтрашней сессии, но большая часть любого репозитория кода — это возможность сотрудничать с сообществом. Давайте подумаем о сценарии: мне нужна копия репозитория, потому что я хочу внести в него некоторые изменения, может быть, я хочу исправить ошибку или, может быть, я хочу что-то изменить, чтобы использовать его для моего варианта использования, который, возможно, не был предполагаемый вариант использования для первоначального сопровождающего кода. Это то, что мы бы назвали разветвлением репозитория. Форк — это копия репозитория. Разветвление репозитория позволяет вам свободно экспериментировать с изменениями, не затрагивая исходный проект.\n\nПозвольте мне вернуться на начальную страницу после входа в систему и увидеть один из предложенных репозиториев.\n\n\n\nЕсли мы нажмем на этот репозиторий, мы получим тот же вид, что и репозиторий 90DaysOfDevOps.\n\n\n\nЕсли мы обратим внимание, ниже у нас есть 3 варианта: watch, fork и star.\n\nWatch - обновление, когда что-то происходит с хранилищем.\nFork - копия репозитория.\nStar - \"Я думаю, что ваш проект крутой\".\n\n\n\nУчитывая наш сценарий, когда нам нужна копия репозитория для работы, мы воспользуемся опцией fork. Если вы являетесь членом нескольких организаций, то вам придётся выбрать, где будет происходить форк, я выберу свой профиль.\n\nТеперь у нас есть собственная копия репозитория, над которой мы можем свободно работать и изменять по своему усмотрению. Это начало процесса подачи запросов на исправление, о котором мы уже вкратце упоминали, но более подробно рассмотрим завтра.\n\nХорошо, я слышу, как вы говорите, но как мне внести изменения в этот репозиторий и код, если он находится на веб-сайте, ну, вы можете просматривать и редактировать на веб-сайте, но это не будет таким же, как использование вашей любимой IDE в вашей локальной системе. с вашей любимой цветовой темой. Чтобы получить копию этого репозитория на нашем локальном компьютере, мы выполним клонирование репозитория. Это позволит нам работать над вещами локально, а затем отправлять наши изменения обратно в нашу разветвленную копию репозитория.\n\nУ нас есть несколько вариантов получения копии этого кода, как вы можете видеть ниже.\n\nДоступна локальная версия GitHub Desktop, которая дает вам визуальное настольное приложение для отслеживания изменений и отправки и получения изменений между локальным и github.\n\nДля этой небольшой демонстрации я буду использовать URL-адрес HTTPS, который мы видим там.\n\n\n\nТеперь на нашей локальной машине я перейду в каталог, в который я хочу загрузить этот репозиторий, а затем выполню команду git clone url.\n\nТеперь мы можем обратиться к VScode, чтобы действительно внести некоторые изменения.\n\nТеперь давайте сделаем некоторые изменения, я хочу изменить все эти ссылки и заменить их на что-то другое.\n\nТеперь, если мы вернемся на GitHub и найдем наш readme.mdin в этом репозитории, вы сможете увидеть несколько изменений, которые я внес в файл.\n\nНа данном этапе это может быть завершено, и мы можем быть довольны нашим изменением, поскольку мы единственные люди, которые будут использовать наше новое изменение, но, возможно, это было изменение ошибки, и если это так, то мы захотим внести свой вклад через Pull Request  чтобы уведомить сопровождающих исходного репозитория о наших изменениях и посмотреть, примут ли они наши изменения.\n\nМы можем сделать это, используя кнопку вклада, выделенную ниже. Я расскажу об этом подробнее завтра, когда мы рассмотрим рабочие процессы с открытым исходным кодом.\n\n\n\nЯ долго просматривал GitHub и слышал, как некоторые из вас плачут, но как насчет других вариантов!\n\nНу, есть, и я собираюсь найти некоторые ресурсы, которые охватывают основы для некоторых из них. В своих путешествиях вы столкнетесь с GitLab и BitBucket, и хотя они основаны на git, у них есть свои отличия.\n\nВы также столкнетесь с размещенными вариантами. Чаще всего здесь я видел GitLab как размещенную версию по сравнению с GitHub Enterprise (не верите, что есть бесплатный размещенный GitHub?)\n\nРесурсы\n\nLearn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners\nBitBucket Tutorials Playlist\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\nGit cheatsheet\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day41",
            "title": "41. Рабочий процесс с открытым исходным кодом",
            "description": null,
            "content": "Рабочий процесс с открытым исходным кодом\n\nКогда мы изучали основы GitHub, мы проходили процесс форка произвольного проекта и внесения изменений в наш локальный репозиторий. Здесь мы хотим сделать еще один шаг вперед и внести свой вклад в проект с открытым исходным кодом. Помните, что вклад не обязательно должен заключаться в исправлении ошибок, кодировании функций, это может быть и документация. Каждая мелочь помогает, и это также позволит вам поработать с некоторыми функциями git, которые мы рассмотрели.\nФорк проекта\n\nПервое, что нам нужно сделать, это найти проект, в который мы можем внести свой вклад. Я недавно выступал с презентациями в Kanister Project и хотел бы поделиться своими презентациями, которые теперь есть на YouTube, с основным readme.mdf-файлом проекта.\n\nПрежде всего, нам нужно форкнуть проект. Давайте проделаем этот процесс. Я собираюсь перейти по ссылке, указанной выше, и форкнуть репозиторий.\n\nТеперь у нас есть наша копия всего репозитория.\n\nДля справки в файле Readme.mdfile в списке оригинальных Presenations указаны только эти два, поэтому нам нужно исправить это в нашем процессе.\n\nКлонирование на локальную машину\n\nТеперь у нас есть собственный форк, который мы можем перенести на локальную машину и начать вносить правки в файлы. Используя кнопку code на нашем репозитории, мы можем получить URL, а затем использовать git clone url в каталоге, куда мы хотим поместить репозиторий.\n\nВносим изменения\n\nУ нас есть локальный проект, поэтому мы можем открыть VSCode или IDE или текстовый редактор по вашему выбору, чтобы добавить свои изменения.\n\nФайл readme.mdfile написан на языке markdown, и поскольку я изменяю чужой проект, я собираюсь следовать существующему форматированию проекта для добавления нашего содержимого.\n\nТестируем свои изменения\n\nВ качестве лучшей практики мы должны тестировать наши изменения, это совершенно логично, если бы это было изменение кода приложения, вы бы хотели убедиться, что приложение продолжает функционировать после изменения кода, но мы также должны убедиться, что документация отформатирована и выглядит правильно.\n\nВ VScode у нас есть возможность добавить множество плагинов, одним из которых является возможность предварительного просмотра страниц в формате markdown.\n\nВерните изменения в наш форкнутый репозиторий\n\nУ нас нет аутентификации, чтобы отправить наши изменения непосредственно в репозиторий Kanister, поэтому мы должны пойти этим путем. Теперь, когда я доволен нашими изменениями, мы можем выполнить некоторые из этих хорошо известных команд git.\n\n\n\nТеперь мы возвращаемся в GitHub, чтобы еще раз проверить изменения и затем внести вклад в мастер-проект.\n\nТеперь мы можем вернуться в верхнюю часть нашего форкнутого репозитория для Kanister и увидеть, что мы на 1 коммит опережаем ветку kanisterio:master.\n\nДалее мы нажимаем на кнопку \"Внести вклад\", выделенную выше. Мы видим опцию \"Open Pull Request\".\n\nOpen a pull request\n\nНа следующем изображении происходит довольно много всего: слева вверху вы видите, что мы находимся в оригинальном или основном репозитории. Затем вы можете увидеть, что мы сравниваем, а это оригинальный основной и наш форкнутый репозиторий. Затем у нас есть кнопка создания запроса на притяжение, к которой мы скоро вернёмся. У нас есть единственный коммит, но если бы изменений было больше, то здесь могло бы быть несколько коммитов. Затем у нас есть изменения, которые мы внесли в readme.mdfile.\n\nМы просмотрели вышеуказанные изменения и готовы создать pull request, нажав на зеленую кнопку.\n\nЗатем, в зависимости от того, как мейнтейнер проекта настроил функциональность Pull Request в своём репозитории, у вас может быть или не быть шаблона, который даст вам указания на то, что хочет видеть мейнтейнер.\n\nЗдесь вам снова нужно составить содержательное описание того, что вы сделали, четкое и краткое, но достаточно подробное. Вы можете видеть, что я сделал простой обзор изменений и отметил документацию.\n\nСоздайте запрос на исправление\n\nТеперь мы готовы к созданию запроса на исправление. После нажатия кнопки \"Create Pull Request\" в верхней части страницы вы получите краткое описание вашего запроса.\n\nПрокручивая страницу вниз, вы, вероятно, увидите, что происходит автоматизация, в данном случае нам требуется рецензия, и происходят некоторые проверки. Мы видим, что Travis CI находится в процессе и началась сборка, которая проверит наше обновление и убедится, что перед тем, как что-то будет слито, мы не сломаем что-то своими добавлениями.\n\nЕще одна вещь, которую следует отметить, это то, что красный цвет на снимке экрана выше, может выглядеть немного пугающе и выглядеть так, как будто вы совершили ошибки! Не волнуйтесь, вы ничего не нарушили, мой главный совет - этот процесс поможет вам и сопровождающим проекта. Если вы допустили ошибку, по крайней мере, по моему опыту, сопровождающий свяжется с вами и посоветует, что делать дальше.\n\nЭтот запрос на исправление теперь общедоступен для всех added Kanister presentation/resource #1237.\n\nЯ собираюсь опубликовать это до того, как слияние и запрос на исправление будут приняты, так что, возможно, мы сможем получить небольшой приз для тех, кто всё ещё следит за развитием событий и сможет добавить картинку к успешному PR?\n\nФоркните этот репозиторий на свой собственный аккаунт GitHub\nДобавьте свою картинку и, возможно, текст\nВнесите изменения в свой форкнутый репозиторий.\nСоздайте PR, который я увижу и одобрю.\nЯ придумаю какой-нибудь приз.\n\nНа этом мы завершаем знакомство с Git и GitHub, далее мы погружаемся в контейнеры, что начинается с рассмотрения общей картины того, как, почему контейнеры, а также с рассмотрения виртуализации и того, как мы к ней пришли.\n\nРесурсы\n\nLearn GitLab in 3 Hours | GitLab Complete Tutorial For Beginners\nBitBucket Tutorials Playlist\nWhat is Version Control?\nTypes of Version Control System\nGit Tutorial for Beginners\nGit for Professionals Tutorial\nGit and GitHub for Beginners - Crash Course\nComplete Git and GitHub Tutorial\nGit cheatsheet\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day42",
            "title": "42. Контейнеры",
            "description": "Контенеры Docker для запуска приложений",
            "content": "Контейнеры\n\nЭтот раздел будет посвящен контейнерам. Будем рассматривать Docker, вникая в некоторые ключевые области, чтобы понять больше о контейнерах.\n\nЯ также попытаюсь провести практические занятия по созданию контейнера, который мы сможем использовать не только в этом разделе, но и в последующих.\n\nПочему другой способ запуска приложений?\n\nПервое, на что мы должны обратить внимание, - зачем нам нужен другой способ запуска программ или приложений? Просто выбор велик, мы можем запускать наши приложения в разных формах, мы можем видеть приложения, развернутые на физическом оборудовании с операционной системой и одним приложением, мы можем видеть виртуальную машину или облачные IaaS экземпляры, запускающие наше приложение, которое затем интегрируется в базу данных снова в виртуальной машине или как PaaS предложение в публичном облаке. Или мы можем увидеть наши приложения, работающие в контейнерах.\n\nНи один из перечисленных вариантов не является неправильным или правильным, но у каждого из них есть свои причины для существования, и я также твердо уверен, что ни один из них не исчезнет. Я видел много материалов, в которых обсуждаются контейнеры и виртуальные машины, и на самом деле здесь не должно быть спора, поскольку это больше похоже на спор между яблоками и грушами, где они оба являются фруктами (способы запуска наших приложений), но это не одно и то же.\n\nЯ бы также сказал, что если вы начинаете и разрабатываете приложение, вам следует склониться к контейнерам просто потому, что мы рассмотрим некоторые из этих областей позже, но речь идет об эффективности, скорости и размере. Но за это тоже приходится платить, если вы не имеете представления о контейнерах, то вам придется учиться, чтобы понять, зачем это нужно, и вжиться в этот образ мышления. Если вы разрабатывали свои приложения особым образом или вы не работаете в новой среде, то у вас может быть больше болевых точек, с которыми нужно справиться, прежде чем рассматривать контейнеры.\n\nУ нас есть много различных вариантов, когда нужно загрузить ту или иную часть программного обеспечения, есть множество различных операционных систем, которые мы можем использовать. И конкретные инструкции о том, что нам нужно сделать, чтобы установить наши приложения.\n\nВ последнее время я все чаще замечаю, что приложения, для которых раньше требовалась полноценная серверная ОС, виртуальная машина, физический или облачный экземпляр, теперь выпускают версии своего программного обеспечения на основе контейнеров. Я нахожу это интересным, поскольку это открывает мир контейнеров и Kubernetes для всех, а не только для разработчиков приложений.\n\nКак вы уже, наверное, поняли, я не собираюсь утверждать, что ответ - это контейнеры, в чем вопрос! Но я хотел бы обсудить, что это еще один вариант, о котором мы должны знать при развертывании наших приложений.\n\nУ нас уже давно существует контейнерная технология, так почему же именно сейчас, за последние 10 лет, она стала популярной, я бы сказал, даже более популярной в последние 5 лет. У нас были контейнеры в течение десятилетий. Все сводится к вызову контейнеров или, лучше сказать, образов, тому, как мы распространяем наше программное обеспечение, потому что если у нас будет только контейнерная технология, то у нас останется много тех же проблем, которые были с управлением программным обеспечением.\n\nЕсли мы подумаем о Docker как об инструменте, то причина его взлета заключается в экосистеме образов, которые легко найти и использовать. Их легко установить на свои системы и запустить в работу. Важной частью этого является согласованность во всем пространстве, во всех этих различных проблемах, с которыми мы сталкиваемся при работе с программным обеспечением. Неважно, MongoDB это или nodeJS, процесс запуска любого из них будет одинаковым. Процесс остановки любого из них одинаков. Все эти проблемы будут существовать, но самое приятное, что когда мы объединяем хорошие технологии контейнеров и образов, у нас появляется единый набор инструментов для решения всех этих различных проблем. Некоторые из этих проблем перечислены ниже:\n\nСначала нам нужно найти программное обеспечение в Интернете.\nЗатем мы должны загрузить это программное обеспечение.\nДоверяем ли мы источнику?\nНужна ли нам лицензия? Какая лицензия?\nСовместима ли она с различными платформами?\nЧто представляет собой пакет? Бинарный? Исполняемый? Менеджер пакетов?\nКак сконфигурировать программу?\nЗависимости? Были ли они учтены при загрузке или они нам тоже нужны?\nЗависимости зависимостей?\nКак нам запустить приложение?\nКак мы остановим приложение?\nБудет ли оно автозапускаться?\nЗапускаться при загрузке?\nКонфликты ресурсов?\nКонфликтующие библиотеки?\nКонфликты портов\nБезопасность программного обеспечения?\nОбновления программного обеспечения?\nКак удалить программное обеспечение?\n\nМы можем разделить вышеперечисленное на 3 области сложности программного обеспечения, с которыми помогают справиться контейнеры и образы.\n\n| Распространение | Установка     | Эксплуатация          |\n| --------------- | ------------- | --------------------- |\n| Найти           | Установить    | Запустить             |\n| Скачать         | Конфигурация  | Безопасность          |\n| Лицензия        | Деинсталляция | Порты                 |\n| Пакет           | Зависимости   | Конфликты с ресурсами |\n| Доверие         | Платформа     | Автоперезагрузка      |\n| Поиск           | Библиотеки    | Обновления            |\n\nКонтейнеры и образы помогут нам устранить некоторые из этих проблем, с которыми мы сталкиваемся при работе с другими программами и приложениями.\n\nНа высоком уровне мы можем перенести установку и эксплуатацию в один список: образы помогут нам с точки зрения распространения, а контейнеры помогут с установкой и эксплуатацией.\n\nХорошо, возможно, звучит здорово и захватывающе, но нам все еще нужно понять, что такое контейнер, и теперь я упомянул образы, поэтому давайте рассмотрим эти области далее.\n\nЕще одна вещь, которую вы могли часто видеть, когда мы говорили о контейнерах для разработки программного обеспечения, - это аналогия с морскими контейнерами: морские контейнеры используются для перевозки различных товаров по морю с помощью больших судов.\n\nКакое отношение это имеет к нашей теме о контейнерах? Подумайте о коде, который пишут разработчики программного обеспечения, как мы можем перенести этот код с одной машины на другую?\n\nЕсли мы подумаем о том, что мы уже говорили о распространении программного обеспечения, установке и операциях, то теперь мы начнем выстраивать это в визуальную среду. У нас есть аппаратное обеспечение и операционная система, на которой вы будете запускать несколько приложений. Например, nodejs имеет определенные зависимости и нуждается в определенных библиотеках. Если вы хотите установить MySQL, то ему нужны необходимые библиотеки и зависимости. Каждое программное приложение будет иметь свою библиотеку и зависимость. Нам может крупно повезти, и у нас не будет конфликтов между приложениями, где определенные библиотеки и зависимости сталкиваются, вызывая проблемы, но чем больше приложений, тем больше вероятность или риск конфликтов. Однако речь не идет об одном развертывании, когда все исправления ваших программных приложений будут обновлены, и тогда мы также можем столкнуться с этими конфликтами.\n\nКонтейнеры могут помочь решить эту проблему. Контейнеры помогают создать ваше приложение, отправить приложение, развернуть и масштабировать эти приложения с легкостью самостоятельно. Давайте рассмотрим архитектуру, у вас есть аппаратное обеспечение и операционная система, а поверх них - контейнерный движок, такой как docker, который мы рассмотрим позже. Программное обеспечение контейнерного движка помогает создавать контейнеры, которые упаковывают библиотеки и зависимости вместе с ними, так что вы можете легко перемещать этот контейнер с одной машины на другую, не беспокоясь о библиотеках и зависимостях, поскольку они поставляются как часть пакета, который является ничем иным, как контейнером, так что вы можете иметь различные контейнеры, которые можно перемещать между системами, не беспокоясь о базовых зависимостях, которые необходимы приложению.\nпотому что все, что нужно приложению для работы, упаковано как\nконтейнер, который можно перемещать.\n\nПреимущества контейнеров\n\nКонтейнеры помогают упаковать все зависимости внутри контейнера и\nизолировать его.\n\nКонтейнерами легко управлять\n\nВозможность перехода от одной системы к другой.\n\nКонтейнеры помогают упаковать программное обеспечение, и вы можете легко отправить его без каких-либо дублирующих усилий.\n\nКонтейнеры легко масштабируются.\n\nИспользуя контейнеры, вы можете масштабировать независимые контейнеры и использовать балансировщик нагрузки\nили сервис, который поможет разделить трафик, и вы сможете масштабировать приложения горизонтально. Контейнеры обеспечивают большую гибкость и облегчают управление приложениями.\n\nЧто такое контейнер?\n\nКогда мы запускаем приложения на нашем компьютере, это может быть веб-браузер или VScode, который вы используете для чтения этого сообщения. Это приложение работает как процесс или то, что известно как процесс. На наших ноутбуках или системах мы обычно запускаем несколько приложений или, как мы сказали, процессов. Когда мы открываем новое приложение или нажимаем на значок приложения, это приложение, которое мы хотим запустить, иногда это приложение может быть службой, которую мы просто хотим запустить в фоновом режиме, наша операционная система полна служб, которые работают в фоновом режиме, предоставляя вам возможность пользоваться системой.\n\nЗначок приложения представляет собой ссылку на исполняемый файл в файловой системе, после чего операционная система загружает этот файл в память. Интересно, что этот исполняемый файл иногда называют образом, когда речь идет о процессе.\n\nКонтейнеры - это процессы, а контейнер - это стандартная единица программного обеспечения, которая упаковывает код и все его зависимости, чтобы приложение быстро и надежно работало в разных вычислительных средах.\n\nКонтейнерное программное обеспечение всегда будет работать одинаково, независимо от инфраструктуры. Контейнеры изолируют программное обеспечение от его окружения и обеспечивают его единообразную работу, несмотря на различия, например, между разработкой и постановкой на хранение.\n\nЯ упоминал образы в последнем разделе, когда речь шла о том, как и почему контейнеры и образы вместе сделали контейнеры популярными в нашей экосистеме.\n\nЧто такое образ?\n\nОбраз контейнера - это легкий, автономный, исполняемый пакет программного обеспечения, который включает все необходимое для запуска приложения: код, время выполнения, системные инструменты, системные библиотеки и настройки. Образы контейнеров становятся контейнерами во время выполнения.\n\nЧто такое контейнер?\n\nКогда мы запускаем приложения на нашем компьютере, это может быть веб-браузер или VScode, который вы используете для чтения этого сообщения. Это приложение работает как процесс или то, что известно как процесс. На наших ноутбуках или системах мы склонны запускать несколько приложений или, как мы сказали, процессов. Когда мы открываем новое приложение или нажимаем на значок приложения, это приложение, которое мы хотели бы запустить, иногда это приложение может быть службой, которую мы просто хотим запустить в фоновом режиме, наша операционная система полна служб, которые работают в фон, предоставляющий вам пользовательский опыт, который вы получаете с вашей системой.\n\nЭтот значок приложения представляет собой ссылку на исполняемый файл где-то в вашей файловой системе, затем операционная система загружает этот исполняемый файл в память. Интересно, что этот исполняемый файл иногда называют образом, когда мы говорим о процессе.\n\nКонтейнеры — это процессы. Контейнер — это стандартная единица программного обеспечения, которая упаковывает код и все его зависимости, чтобы приложение быстро и надежно запускалось из одной вычислительной среды в другую.\n\nКонтейнерное программное обеспечение всегда будет работать одинаково, независимо от инфраструктуры. Контейнеры изолируют программное обеспечение от его среды и обеспечивают его единую работу, несмотря на различия, например, между разработкой и промежуточной стадией.\n\nЯ упомянул изображения в предыдущем разделе, когда речь шла о том, как и почему сочетание контейнеров и изображений сделало контейнеры популярными в нашей экосистеме.\n\nСсылки\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day43",
            "title": "43. Установка Docker",
            "description": null,
            "content": "Что такое Docker и его установка\n\nВ предыдущей статье я хотя бы раз упомянул Docker, и это потому, что Docker действительно является новатором в создании популярности контейнеров, несмотря на то, что они существуют уже очень давно.\n\nЗдесь мы будем использовать и объяснять docker, но мы также должны упомянуть [Open Container Initiative (OCI)] (https://www.opencontainers.org/), которая является организацией по отраслевым стандартам, поощряющей инновации и избегающей опасности блокировки поставщиков. Благодаря OCI у нас есть выбор при выборе инструментария для контейнеров, включая Docker, CRI-O, Podman, LXC и другие.\n\nDocker - это программная среда для создания, запуска и управления контейнерами. Термин \"docker\" может относиться как к инструментам (командам и демону), так и к формату файлов Dockerfile.\n\nМы будем использовать Docker Personal, который является бесплатным (для образования и обучения). Он включает в себя все самое необходимое, что нам нужно для получения хорошего фундамента знаний о контейнерах и инструментах.\n\nВозможно, стоит разделить некоторые инструменты \"docker\", которые мы будем использовать и для чего они нужны. Термин docker может относиться к проекту docker в целом, который является платформой для разработчиков и администраторов для разработки, доставки и запуска приложений. Также это может быть ссылка на процесс docker daeemon, запущенный на хосте, который управляет образами и контейнерами и называется Docker Engine.\n\nDocker Engine\n\nDocker Engine - это технология контейнеризации с открытым исходным кодом для создания и контейнеризации приложений. Docker Engine действует как клиент-серверное приложение:\n\nСервер с долго работающим процессом-демоном dockerd.\nAPI, определяющие интерфейсы, которые программы могут использовать для общения и обучения демона Docker.\nКлиент docker с интерфейсом командной строки (CLI).\n\nВышеизложенное было взято из официальной документации Docker и конкретного Docker Engine Overview\n\nDocker Desktop\nУ нас есть рабочий стол docker для систем Windows и macOS. Простая в установке, легковесная среда разработки docker. Нативное приложение для ОС, использующее возможности виртуализации на хостовой операционной системе.\n\nЭто лучшее решение, если вы хотите создавать, отлаживать, тестировать, упаковывать и отправлять Docker-приложения на Windows или macOS.\n\nНа Windows мы также можем воспользоваться преимуществами WSL2 и Microsoft Hyper-V. Мы рассмотрим некоторые преимущества WSL2 по ходу дела.\n\nБлагодаря интеграции с возможностями гипервизора на хостовой операционной системе docker предоставляет возможность запускать ваши контейнеры с операционными системами Linux.\n\nDocker Compose\nDocker compose - это инструмент, позволяющий запускать более сложные приложения в нескольких контейнерах. Преимуществом является возможность использования одного файла и команды для запуска приложения.\n\nDocker Hub\nЦентрализованный ресурс для работы с Docker и его компонентами. Чаще всего он известен как реестр для размещения образов Docker. Но здесь есть множество дополнительных сервисов, которые можно использовать для автоматизации или интеграции в GitHub, а также для сканирования безопасности.\n\nDockerfile\n\nDockerfile - это текстовый файл, содержащий команды, которые обычно выполняются вручную для создания образа docker. Docker может собирать образы автоматически, читая инструкции, которые содержатся в нашем dockerfile.\n\nУстановка Docker Desktop\n\nДокументация docker documenation просто потрясающая, и если вы только начинаете в нее погружаться, то вам стоит ее просмотреть и прочитать. Мы будем использовать Docker Desktop на Windows с WSL2. Я уже выполнил установку на своей машине, которую мы используем здесь.\n\nОбратите внимание перед установкой на системные требования, Install Docker Desktop on Windows, если вы используете macOS, включая архитектуру процессора на базе M1, вы также можете взглянуть на Install Docker Desktop on macOS.\n\nЯ проведу установку Docker Desktop для Windows на другой машине Windows и запишу процесс ниже.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day44",
            "title": "44. Установка образов Docker в Docker Desktop",
            "description": null,
            "content": "\nОбразы Docker и практическая работа с Docker Desktop\n\nТеперь у нас в системе установлен Docker Desktop. (Если вы используете Linux, у вас все еще есть опции, но нет графического интерфейса, но docker, очевидно, работает на Linux)Install Docker Engine on Ubuntu (Другие дистрибутивы также доступны).\n\nВ этом посте мы собираемся начать с развертывания некоторых образов в нашей среде. Напомним, что такое образ Docker - образ Docker - это файл, используемый для выполнения кода в контейнере Docker. Образы Docker действуют как набор инструкций для создания контейнера Docker, как шаблон. Образы Docker также служат отправной точкой при использовании Docker.\n\nСейчас самое время пойти и создать свой аккаунт на DockerHub\n\nDockerHub - это централизованный ресурс для работы с Docker и его компонентами. Наиболее известен как реестр для размещения образов докеров. Но здесь есть множество дополнительных сервисов, которые можно использовать для автоматизации или интеграции в GitHub, а также для сканирования безопасности.\n\nЕсли вы прокрутите вниз после входа в систему, вы увидите список образов контейнеров, вы можете увидеть образы баз данных для mySQL, hello-world и т.д. и т.п. Рассматривайте их как отличные базовые образы, или вам может понадобиться просто образ базы данных, и вам лучше всего использовать официальный образ, что означает, что вам не нужно создавать свой собственный.\n\nМы можем углубиться в просмотр доступных изображений и осуществлять поиск по категориям, операционным системам и архитектурам. Единственное, что я выделил ниже, это Office Image, это должно дать вам уверенность в происхождении этого образа контейнера.\n\nМы также можем искать конкретное изображение, например, wordpress может быть хорошим базовым изображением, которое нам нужно, мы можем сделать это в верхней части и найти все изображения контейнеров, связанные с wordpress. Ниже обратите внимание, что у нас также есть проверенный издатель.\n\nОфициальные образы - Официальные образы Docker - это курируемый набор открытых исходных кодов Docker и репозиториев решений \"drop-in\".\n\nПроверенный издатель - высококачественный контент Docker от проверенных издателей. Эти продукты публикуются и поддерживаются непосредственно коммерческой организацией.\n\nИзучение Docker Desktop\n\nУ нас в системе установлен Docker Desktop, и если открыть его, то, если он у вас еще не установлен, вы увидите нечто похожее на изображение ниже. Как вы можете видеть, у нас нет запущенных контейнеров, но наш движок docker запущен.\n\nПоскольку это была не свежая установка для меня, у меня есть некоторые изображения, которые уже загружены и доступны в моей системе. Скорее всего, здесь вы ничего не увидите.\n\nВ разделе удаленных репозиториев вы найдете все образы контейнеров, которые хранятся в вашем хабе docker. Ниже показано, что у меня нет никаких образов.\n\nМы также можем уточнить это на нашем сайте dockerhub и подтвердить, что у нас там нет репозиториев.\n\n\n\nДалее у нас есть вкладка Volumes, если у вас есть контейнеры, которым требуется постоянство, то здесь мы можем добавить эти тома в вашу локальную файловую систему или общую файловую систему.\n\nНа момент написания статьи также существует вкладка Dev Environments, которая поможет вам сотрудничать с вашей командой вместо того, чтобы перемещаться между различными ветками git. Мы не будем ее рассматривать.\n\nВернувшись на первую вкладку, вы увидите, что там есть команда, которую мы можем запустить - это контейнер для запуска. Давайте запустим docker run -d -p 80:80 docker/getting-started в нашем терминале.\n\nЕсли мы снова проверим окно рабочего стола docker, то увидим, что у нас есть запущенный контейнер.\n\nВы могли заметить, что я использую WSL2, и для того, чтобы вы могли использовать его, вам нужно убедиться, что он включен в настройках.\n\nЕсли теперь мы снова перейдем на вкладку Images, вы должны увидеть используемый образ под названием docker/getting-started.\n\nВернитесь на вкладку Containers/Apps, нажмите на ваш запущенный контейнер. По умолчанию вы увидите журналы, а в верхней части есть несколько опций на выбор, в нашем случае я уверен, что это будет веб-страница, запущенная в этом контейнере, поэтому мы выберем опцию \"Открыть в браузере\".\n\nКогда мы нажмем на кнопку выше, конечно же, откроется веб-страница на вашем локальном хосте и отобразится что-то похожее на то, что показано ниже.\n\nЭтот контейнер также содержит более подробную информацию о том, что такое контейнеры и изображения.\n\nТеперь мы запустили наш первый контейнер. Пока ничего страшного. А что если мы захотим вытащить один из образов контейнера из DockerHub? Может быть, там есть докер-контейнер hello world, который мы могли бы использовать.\n\nЯ остановил начальный контейнер, не то чтобы он занимал много ресурсов, но для аккуратности, пока мы проходим еще несколько шагов.\n\nВернемся в терминал и выполним команду docker run hello-world и посмотрим, что произойдет.\n\nВы можете видеть, что у нас не было локального образа, поэтому мы стянули его, а затем получили сообщение, записанное в образ контейнера, с информацией о том, что он сделал, чтобы запуститься, и некоторые ссылки на точки отсчета.\n\nОднако, если мы посмотрим в Docker Desktop, у нас нет запущенных контейнеров, но есть вышедший контейнер, который использовал сообщение hello-world, то есть он появился, передал сообщение и затем завершился.\n\nИ в последний раз, давайте просто проверим вкладку images и увидим, что у нас есть новый образ hello-world локально в нашей системе, что означает, что если мы снова выполним команду docker run hello-world в нашем терминале, нам не придется ничего вытаскивать, если только версия не изменится.\n\nВ сообщении от контейнера hello-world была поставлена задача запустить что-то более амбициозное.\n\nВызов принят!\n\n\n\nЗапустив docker run -it ubuntu bash в нашем терминале, мы собираемся запустить контейнерную версию Ubuntu, а не полную копию операционной системы. Вы можете узнать больше об этом конкретном образе на DockerHub.\n\nВы можете видеть ниже, когда мы выполним команду, у нас появится интерактивная подсказка (-it) и мы запустим оболочку bash в нашем контейнере.\n\nУ нас есть оболочка bash, но у нас не так много больше, поэтому образ этого контейнера занимает менее 30 мб.\n\nНо мы все еще можем использовать этот образ, и мы все еще можем установить программное обеспечение, используя наш менеджер пакетов apt, мы можем обновить наш образ контейнера и обновить также.\n\nИли, может быть, мы хотим установить какое-то программное обеспечение в наш контейнер, я выбрал очень плохой пример, поскольку pinta - это редактор изображений, и его размер превышает 200мб, но, надеюсь, вы поняли, к чему я веду. Это значительно увеличит размер нашего контейнера, но все же мы будем находиться в мб, а не в гб.\n\nЯ хотел, чтобы вы получили общее представление о Docker Desktop и не таком уж страшном мире контейнеров, когда вы разбиваете его на простые сценарии использования, но нам нужно рассказать о некоторых сетевых возможностях, безопасности и других вариантах, которые у нас есть по сравнению с просто загрузкой образов контейнеров и их использованием таким образом. К концу раздела мы хотим создать что-то, загрузить в наш репозиторий DockerHub и иметь возможность развернуть это.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day45",
            "title": "45. Что из себя представляет оьбраз Docker",
            "description": null,
            "content": "Анатомия образа Docker\n\nНа прошлом занятии мы рассмотрели некоторые основы использования Docker Desktop в сочетании с DockerHub для развертывания и запуска некоторых проверенных образов. Вкратце о том, что такое образ, вы не забудете, если я продолжу упоминать.\n\nОбраз Docker - это шаблон, доступный только для чтения, содержащий набор инструкций для создания контейнера, который может работать на платформе Docker. Это удобный способ упаковки приложений и предварительно сконфигурированных серверных сред, которые вы можете использовать для личного пользования или публично делиться ими с другими пользователями Docker. Образы Docker также являются отправной точкой для тех, кто впервые использует Docker.\n\nЧто произойдет, если мы захотим создать свой собственный образ Docker? Для этого мы создадим Dockerfile. Вы видели, как мы могли взять образ контейнера Ubuntu и добавить наше программное обеспечение, и у нас получился образ контейнера с программным обеспечением, которое мы хотели, и все хорошо, но если этот контейнер выключить или выбросить, то все эти обновления и установки программного обеспечения пропадут, и не будет повторяющейся версии того, что мы сделали. Это отлично подходит для демонстрации возможностей, но не помогает при транспортировке образов в несколько сред с одним и тем же набором программного обеспечения, устанавливаемого при каждом запуске контейнера.\nЧто такое Dockerfile\n\nDockerfile - это текстовый файл, содержащий команды, которые обычно выполняются вручную для создания образа docker. Docker может собирать образы автоматически, читая инструкции, содержащиеся в нашем dockerfile.\n\nКаждый из файлов, составляющих образ docker, называется слоем. Эти слои образуют серию образов, поэтапно создаваемых друг над другом. Каждый слой зависит от слоя, расположенного непосредственно под ним. Порядок расположения слоев является ключевым фактором эффективности управления жизненным циклом образов docker.\n\nМы должны расположить слои, которые меняются чаще всего, как можно выше в стеке, потому что при внесении изменений в слой образа Docker перестраивает не только этот слой, но и все слои, созданные на его основе. Поэтому изменение слоя на самом верху требует наименьшего объема работы по пересборке всего образа.\n\nКаждый раз, когда docker запускает контейнер из образа (как мы делали вчера), он добавляет слой, доступный для записи, известный как слой контейнера. В нем хранятся все изменения, вносимые в контейнер в течение всего времени его работы. Этот слой - единственное различие между работающим контейнером и исходным образом. Любое количество подобных контейнеров может иметь общий доступ к одному и тому же базовому образу, сохраняя при этом свое индивидуальное состояние.\n\nВернемся к примеру, который мы использовали вчера с образом Ubuntu. Мы можем выполнить одну и ту же команду несколько раз и на первый контейнер установить pinta, а на второй - figlet. Это два разных приложения, разного назначения, разного размера и т.д. и т.п.. Каждый контейнер, который мы установили, имеет один и тот же образ, но не одно и то же состояние, и это состояние исчезает, когда мы удаляем контейнер.\n\nВ приведенном выше примере используется образ Ubuntu, но также существует множество других готовых образов контейнеров, доступных на DockerHub и в других сторонних репозиториях. Эти образы обычно называют родительским образом. Это фундамент, на котором строятся все остальные слои, и базовые строительные блоки для наших контейнерных сред.\n\nНаряду с набором отдельных файлов слоев, образ Docker также включает дополнительный файл, известный как манифест. Это, по сути, описание образа в формате JSON, содержащее такую информацию, как теги образа, цифровая подпись и подробные сведения о том, как настроить контейнер для различных типов хост-платформ.\n\nКак создать образ docker\n\n Есть два способа создания образа docker. Мы можем сделать это на лету, используя процесс, который мы начали вчера, мы выбираем наш базовый образ, раскручиваем контейнер, устанавливаем все программное обеспечение и депенансы, которые мы хотим иметь на нашем контейнере.\n\n Затем мы можем использовать команду docker commit container name, после чего у нас будет локальная копия этого образа в разделе docker images и на вкладке docker desktop images.\n\n Супер просто, я бы не рекомендовал этот метод, если вы не хотите понять процесс, будет очень сложно управлять жизненным циклом таким образом и много ручной настройки/переконфигурации. Но это самый быстрый и простой способ создания образа docker. Отлично подходит для тестирования, устранения неполадок, проверки зависимостей и т.д.\n\nМы собираемся создать наш образ с помощью dockerfile. Это дает нам чистый, компактный и повторяемый способ создания образов. Намного проще управлять жизненным циклом и легко интегрировать в процессы непрерывной интеграции и непрерывной доставки. Но, как вы уже поняли, это немного сложнее, чем первый упомянутый процесс.\n\nИспользование метода dockerfile гораздо больше соответствует реальным развертываниям контейнеров корпоративного уровня.\n\nСоздание dockerfile - это трехэтапный процесс, в ходе которого вы создаете dockerfile и добавляете команды, необходимые для сборки образа.\n\nВ следующей таблице приведены некоторые из утверждений dockerfile, которые мы будем использовать или которые вы, скорее всего, будете использовать.\n\n| Команда    | Задача                                                                                                                                                                                                                                                                  |\n| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| FROM       | Чтобы указать родительский образ                                                                                                                                                                                                                                        |\n| WORKDIR    | Чтобы задать рабочий каталог для всех последующих команд в Dockerfile.                                                                                                                                                                                                  |\n| RUN        | Для установки любых приложений и пакетов, необходимых для контейнера.                                                                                                                                                                                                   |\n| COPY       | Для копирования файлов или каталогов из определенного места.                                                                                                                                                                                                            |\n| ADD        | Как COPY, но также может работать с удаленными URL и распаковывать сжатые файлы.                                                                                                                                                                                        |\n| ENTRYPOINT | Команда, которая всегда будет выполняться при запуске контейнера. Если она не указана, по умолчанию используется /bin/sh -c. Аргументы, передаваемые точке входа. Если ENTRYPOINT не задан (по умолчанию /bin/sh -c), .md будут командами, которые выполняет контейнер. |\n| EXPOSE     | Для определения порта, через который будет осуществляться доступ к вашему контейнерному приложению.                                                                                                                                                                     |\n| LABEL      | Чтобы добавить метаданные к образу.                                                                                                                                                                                                                                     |\n\n\nТеперь у нас есть подробная информация о том, как создать наш первый dockerfile, мы можем создать рабочий каталог и создать наш dockerfile. Я создал рабочий каталог в этом репозитории, где вы можете увидеть файлы и папки, которые мне предстоит пройти. Containers\n\nВ этом каталоге я собираюсь создать файл .dockerignore, аналогичный .gitignore, который мы использовали в предыдущем разделе. В этом файле будут перечислены все файлы, которые могут быть созданы в процессе сборки Docker и которые вы хотите исключить из окончательной сборки.\n\nПомните, что все, что связано с контейнерами, - это компактность, максимальная скорость и отсутствие лишнего объема.\n\nСоздадим простой Dockerfile с приведенной ниже схемой, которую также можно найти в папке по ссылке выше.\n\nUse the official Ubuntu 18.04 as base\nFROM ubuntu:18.04\nInstall nginx and curl\nRUN apt-get update && apt-get upgrade -y\nRUN apt-get install -y nginx curl\nRUN rm -rf /var/lib/apt/lists/*\n\nПерейдите в этот каталог в терминале, а затем выполните команду docker build -t 90daysofdevops:0.1 . мы используем -t, а затем задаем имя и тег изображения.\n\nТеперь, когда мы создали наш образ, мы можем запустить его с помощью Docker Desktop или командной строки docker. Я использовал Docker Desktop Я запустил контейнер, и вы можете видеть, что у нас есть curl, доступный нам в cli контейнера.\n\nВ Docker Desktop также есть возможность использовать пользовательский интерфейс для выполнения некоторых других задач с этим новым образом.\n\nМы можем проинспектировать наш образ, при этом очень хорошо виден dockerfile и строки кода, которые мы хотели запустить в нашем контейнере.\n\nУ нас есть опция pull, теперь она не работает, потому что это изображение нигде не размещено, поэтому мы получим ошибку. Однако у нас есть Push to hub, который позволит нам отправить наш образ на DockerHub.\n\nЕсли вы используете ту же docker build, которую мы запустили ранее, то это тоже не сработает, вам понадобится команда сборки docker build -t {{username}}/{{imagename}}:{{version}}.\n\n\n\nЕсли мы посмотрим на наш репозиторий DockerHub, то увидим, что мы только что выложили новый образ. Теперь в Docker Desktop мы сможем использовать эту вкладку pull.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\nBlog on gettng started building a docker image\nDocker documentation for building an image\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day46",
            "title": "46. Docker Compose",
            "description": null,
            "content": "Docker Compose\n\nВозможность запуска одного контейнера может быть отличной, если у вас есть самодостаточный образ, в котором есть все, что вам нужно для одного случая использования, но все становится интересным, когда вы ищете возможность создания нескольких приложений между различными образами контейнеров. Например, если у меня есть фронт-энд сайта, но есть потребность в базе данных бэкенда, я могу поместить все в один контейнер, но лучше и эффективнее было бы иметь собственный контейнер для базы данных.\n\nИменно здесь на помощь приходит Docker compose - инструмент, позволяющий запускать более сложные приложения в нескольких контейнерах. Преимущество заключается в том, что для запуска приложения можно использовать один файл и команду. Пример, который я собираюсь рассмотреть в этой заметке, взят из [Docker QuickStart sample apps (Quickstart: Compose and WordPress)] (https://docs.docker.com/samples/wordpress/).\n\nВ этом первом примере мы собираемся:\n\nИспользовать Docker compose для создания WordPress и отдельного экземпляра MySQL.\nИспользовать YAML файл, который будет называться docker-compose.yml.\nСоберите проект\nНастроить WordPress через браузер\nВыключение и очистка\n\nУстановка Docker Compose\nКак уже упоминалось, Docker Compose - это инструмент, если вы работаете на macOS или Windows, то compose включен в вашу установку Docker Desktop. Однако вы можете захотеть запустить свои контейнеры на сервере Windows или Linux, и в этом случае вы можете установить их, используя эти инструкции Install Docker Compose.\n\nЧтобы убедиться, что docker-compose установлен в нашей системе, мы можем открыть терминал и просто ввести приведенную выше команду.\n\nDocker-Compose.yml (YAML)\n\nСледующее, о чем нужно поговорить, это docker-compose.yml, который вы можете найти в папке container репозитория. Но что более важно, нам нужно немного обсудить YAML в целом.\n\nYAML можно было бы посвятить отдельную сессию, поскольку вы можете встретить его в самых разных местах. Но по большей части\n\n\"YAML - это удобный для человека язык сериализации данных для всех языков программирования\".\n\nОн обычно используется для файлов конфигурации и в некоторых приложениях, где данные хранятся или передаются. Вы, несомненно, сталкивались с XML-файлами, которые обычно предлагают тот самый файл конфигурации. YAML предоставляет минимальный синтаксис, но нацелен на те же случаи использования.\n\nYAML Ain't Markup Language (YAML) - это язык сериализации, популярность которого неуклонно растет в течение последних нескольких лет. Возможности сериализации объектов делают его реальной заменой таким языкам, как JSON.\n\nАббревиатура YAML была сокращением от Yet Another Markup Language. Но сопровождающие переименовали его в YAML Ain't Markup Language, чтобы сделать больший акцент на его функциях, ориентированных на данные.\n\nВ любом случае, вернемся к файлу docker-compose.yml. Это файл конфигурации того, что мы хотим сделать, когда речь идет о развертывании нескольких контейнеров на нашей единой системе.\n\nПрямо из приведенного выше руководства вы можете увидеть, что содержимое файла выглядит следующим образом:\n\nversion: \"3.9\"\n\nservices:\n  db:\n    image: mysql:5.7\n    volumes:\n      db_data:/var/lib/mysql\n    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: somewordpress\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n  wordpress:\n    depends_on:\n      db\n    image: wordpress:latest\n    volumes:\n      wordpress_data:/var/www/html\n    ports:\n      \"8000:80\"\n    restart: always\n    environment:\n      WORDPRESS_DB_HOST: db\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n      WORDPRESS_DB_NAME: wordpress\nvolumes:\n  db_data: {}\n  wordpress_data: {}\n\nМы объявляем версию, а затем большая часть этого файла docker-compose.yml состоит из наших служб, у нас есть служба db и служба wordpress. Вы можете видеть, что для каждого из них определено изображение, с которым связан тег версии. В отличие от наших первых прохождений, сейчас мы также вводим состояние в нашу конфигурацию, но теперь мы собираемся создать тома, чтобы мы могли хранить там наши базы данных.\n\nЗатем у нас есть некоторые переменные окружения, такие как пароли и имена пользователей. Очевидно, что эти файлы могут стать очень сложными, но конфигурационный файл YAML упрощает то, как они выглядят в целом.\n\nСборка проекта\n\nДалее мы можем вернуться в терминал и использовать некоторые команды с помощью нашего инструмента docker-compose. Перейдите в каталог, где находится ваш файл docker-compose.yml.\n\nВ терминале мы можем просто выполнить команду docker-compose up -d, которая запустит процесс извлечения образов и создания вашего многоконтейнерного приложения.\n\nСимвол -d в этой команде означает отделенный режим, что означает, что команда Run выполняется или будет выполняться в фоновом режиме.\n\n\n\nЕсли теперь мы выполним команду docker ps, вы увидите, что у нас запущено 2 контейнера, один из которых - wordpress, а другой - mySQL.\n\nДалее мы можем проверить, что у нас запущен WordPress, открыв браузер и перейдя по адресу http://localhost:8000, вы должны увидеть страницу установки wordpress.\n\nМы можем выполнить настройку WordPress, а затем начать создавать наш сайт по своему усмотрению в консоли ниже.\n\nЕсли мы откроем новую вкладку и перейдем по тому же адресу, что и раньше http://localhost:8000, то увидим простую тему по умолчанию с названием нашего сайта \"90DaysOfDevOps\", а затем образец поста.\n\nПрежде чем мы сделаем какие-либо изменения, откройте Docker Desktop и перейдите на вкладку volumes, здесь вы увидите два тома, связанных с нашими контейнерами, один для wordpress и один для db.\n\nМоя текущая тема для wordpress - \"Twenty Twenty-Two\", и я хочу изменить ее на \"Twenty Twenty\" Вернувшись в панель управления, мы можем внести эти изменения.\n\nЯ также собираюсь добавить новый пост на свой сайт, и здесь ниже вы видите последнюю версию нашего нового сайта.\n\n\nОчищать или нет\n\nЕсли мы сейчас используем команду docker-compose down, это приведет к остановке наших контейнеров. Но наши тома останутся на месте.\n\nМы можем просто подтвердить в Docker Desktop, что наши тома все еще там.\n\nЕсли мы захотим вернуть все обратно, мы можем выполнить команду docker up -d из той же директории, и наше приложение снова будет запущено.\n\nЗатем мы переходим в браузере по тому же адресу http://localhost:8000 и замечаем, что наш новый пост и смена темы все еще на месте.\n\nЕсли мы хотим избавиться от контейнеров и этих томов, то выполнение команды docker-compose down --volumes также уничтожит тома.\n\nТеперь, когда мы снова используем docker-compose up -d, мы начнем все сначала, однако образы все еще будут локальными в нашей системе, поэтому вам не нужно будет повторно брать их из репозитория DockerHub.\n\nЯ знаю, что когда я начал погружаться в docker-compose и его возможности, я был в замешательстве относительно того, где он находится рядом с инструментами оркестровки контейнеров, такими как Kubernetes, ну, все, что мы сделали здесь в этой короткой демонстрации, сосредоточено на одном хосте, у нас есть wordpress и db, запущенные на локальной настольной машине. У нас нет нескольких виртуальных машин или нескольких физических машин, у нас также нет возможности легко увеличивать и уменьшать требования нашего приложения.\n\nВ следующем разделе мы рассмотрим Kubernetes, но сначала у нас есть еще несколько дней, посвященных контейнерам в целом.\n\nЭто также отличный ресурс для примеров приложений docker compose с множеством интеграций. Awesome-Compose.\n\nВ вышеупомянутом репозитории есть отличный пример, который развернет Elasticsearch, Logstash и Kibana (ELK) на одном узле.\n\nЯ загрузил файлы в папку Containers Когда у вас есть эта папка локально, перейдите туда и вы можете просто использовать docker-compose up -d.\n\nЗатем мы можем проверить наличие запущенных контейнеров с помощью docker ps.\n\nТеперь мы можем открыть браузер для каждого из контейнеров:\n\nЧтобы удалить все, мы можем использовать команду docker-compose down.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\nBlog on gettng started building a docker image\nDocker documentation for building an image\nYAML Tutorial: Everything You Need to Get Started in Minute\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day47",
            "title": "47. Сетевое взаимодействие Docker и безопасность",
            "description": null,
            "content": "Docker Networking & Security\n\nВо время этой сессии по контейнерам мы уже кое-что сделали, но не рассмотрели, как все работает за кулисами с точки зрения сетевых технологий, а также не затронули безопасность, поэтому мы планируем эту сессию.\n\nОсновы сетевого взаимодействия Docker\n\nОткройте терминал и введите команду docker network - это основная команда для настройки и управления сетями контейнеров.\n\nНиже показано, как мы можем использовать эту команду и все доступные подкоманды. Мы можем создавать новые сети, составлять список существующих, проверять и удалять сети.\n\nДавайте посмотрим на существующие сети, которые у нас есть с момента установки, поэтому из коробки Docker networking выглядит как использование команды docker network list.\n\nКаждая сеть получает уникальный ID и NAME. Каждая сеть также связана с одним драйвером. Обратите внимание, что сеть \"bridge\" и сеть \"host\" имеют те же имена, что и их соответствующие драйверы.\n\n\n\nДалее мы можем более детально рассмотреть наши сети с помощью команды docker network inspect.\n\nЗапустив команду docker network inspect bridge, я могу получить все детали конфигурации конкретного имени сети. Сюда входят имя, ID, драйверы, подключенные контейнеры и, как вы можете видеть, многое другое.\n\nDocker: Bridge Networking\n\nКак вы видели выше, стандартная установка Docker Desktop дает нам предварительно созданную сеть под названием bridge Если вы обратитесь к команде docker network list, то увидите, что сеть под названием bridge связана с драйвером bridge. То, что у них одинаковое имя, не означает, что это одно и то же. Связаны, но не одно и то же.\n\nВывод выше также показывает, что сеть bridge имеет локальную привязку. Это означает, что сеть существует только на этом хосте Docker. Это справедливо для всех сетей, использующих драйвер моста - драйвер моста обеспечивает работу сети на одном хосте.\n\nВсе сети, созданные с помощью драйвера моста, основаны на мосте Linux (он же виртуальный коммутатор).\n\nПодключение контейнера\n\nПо умолчанию новым контейнерам назначается сеть bridge, то есть, если вы не укажете сеть, все контейнеры будут подключены к сети bridge.\n\nДавайте создадим новый контейнер командой docker run -dt ubuntu sleep infinity.\n\nКоманда sleep выше просто будет поддерживать работу контейнера в фоновом режиме, чтобы мы могли возиться с ним.\n\nЕсли мы затем проверим нашу сеть моста с помощью docker network inspect bridge, вы увидите, что у нас есть контейнер, соответствующий тому, что мы только что развернули, потому что мы не указали сеть.\n\nМы также можем погрузиться в контейнер, используя docker exec -it 3a99af449ca2 bash, вам придется использовать docker ps, чтобы получить идентификатор контейнера.\n\nОтсюда наш образ не имеет ничего для пинга, поэтому нам нужно выполнить следующую команду.apt-get update && apt-get install -y iputils-ping затем пингуем внешний адрес интерфеса. ping -c5 www.90daysofdevops.com\n\n\n\nЧтобы устранить эту проблему, мы можем запустить docker stop 3a99af449ca2 и снова использовать docker ps для поиска ID вашего контейнера, но это приведет к удалению нашего контейнера.\nНастройте NAT для внешнего подключения\n\nНа этом шаге мы запустим новый контейнер NGINX и назначим порт 8080 на хосте Docker на порт 80 внутри контейнера. Это означает, что трафик, поступающий на хост Docker по порту 8080, будет передаваться на порт 80 внутри контейнера.\n\nЗапустите новый контейнер на основе официального образа NGINX, выполнив команду docker run --name web1 -d -p 8080:80 nginx.\n\n\n\n\nПросмотрите состояние контейнера и сопоставление портов, выполнив команду docker ps.\n\n\n\nВерхняя строка показывает новый контейнер web1, запущенный NGINX. Обратите внимание на команду, которую запускает контейнер, а также на сопоставление портов - 0.0.0.0:8080->80/tcp сопоставляет порт 8080 на всех интерфейсах хоста с портом 80 внутри контейнера web1. Это сопоставление портов делает веб-сервис контейнера доступным из внешних источников (через IP-адрес хоста Docker на порту 8080).\n\nТеперь нам нужен IP-адрес нашего реального хоста, мы можем сделать это, зайдя в терминал WSL и используя команду ip addr.\n\nЗатем мы можем взять этот IP, открыть браузер и перейти по адресу http://172.25.218.154:8080/ Ваш IP может быть другим. Это подтверждает, что NGINX доступен.\n\nЯ взял эти инструкции с этого сайта с далекого 2017 DockerCon, но они актуальны и сегодня. Однако остальная часть руководства посвящена Docker Swarm, и я не собираюсь рассматривать его здесь. Docker Networking - DockerCon 2017\n\nОбеспечение безопасности контейнеров\n\nКонтейнеры обеспечивают безопасную среду для рабочих нагрузок по сравнению с полной конфигурацией сервера. Они позволяют разбить ваши приложения на более мелкие, слабо связанные компоненты, изолированные друг от друга, что помогает уменьшить поверхность атаки в целом.\n\nНо они не застрахованы от хакеров, которые хотят использовать системы в своих целях. Нам по-прежнему необходимо понимать подводные камни безопасности этой технологии и придерживаться лучших практик.\n\nОткажитесь от прав root\n\nВсе контейнеры, которые мы развернули, использовали права root для процессов внутри контейнеров. Это означает, что они имеют полный административный доступ к вашим контейнерам и хост-средам. Теперь для целей прохождения мы знали, что эти системы не будут работать долго. Но вы видели, как легко их запустить.\n\nМы можем добавить несколько шагов к нашему процессу, чтобы дать возможность не root-пользователям быть предпочтительной лучшей практикой. При создании нашего dockerfile мы можем создать учетные записи пользователей. Вы можете найти этот пример также в папке containers в репозитории.\n\nИспользуем официальную версию Ubuntu 18.04 в качестве базовой\nFROM ubuntu:18.04\nRUN apt-get update && apt-get upgrade -y\nRUN groupadd -g 1000 basicuser && useradd -r -u 1000 -g basicuser basicuser\nпользователь basicuser\n\nМы также можем использовать docker run --user 1009 ubuntu Команда Docker run переопределяет любого пользователя, указанного в вашем Dockerfile. Поэтому в следующем примере ваш контейнер всегда будет запускаться с наименьшими привилегиями при условии, что идентификатор пользователя 1009 также имеет самый низкий уровень прав.\n\nОднако этот метод не устраняет основной недостаток безопасности самого образа. Поэтому лучше указать в Dockerfile пользователя, не являющегося root, чтобы ваши контейнеры всегда запускались безопасно.\n\nЧастный репозитории\n\nЕще одна область, которую мы активно используем, - это публичные реестры в DockerHub, а частный реестр образов контейнеров, созданный вашей организацией, означает, что вы можете размещать их там, где пожелаете, или же для этого существуют управляемые сервисы, но в целом это дает вам полный контроль над образами, доступными для вас и вашей команды.\n\nDockerHub отлично подходит для создания базового уровня, но он предоставляет только базовый сервис, где вам придется во многом доверять издателю образа.\n\nLean & Clean\n\nМы уже упоминали об этом, хотя это и не связано с безопасностью. Но размер вашего контейнера также может влиять на безопасность с точки зрения поверхности атаки, если у вас есть ресурсы, которые вы не используете в своем приложении, то они не нужны в вашем контейнере.\n\nЭто также является моей основной проблемой при использовании последних образов, потому что это может принести много лишнего в ваши образы. DockerHub показывает сжатый размер для каждого образа в хранилище.\n\ndocker image - отличная команда для просмотра размера ваших образов.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\nBlog on gettng started building a docker image\nDocker documentation for building an image\nYAML Tutorial: Everything You Need to Get Started in Minute\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day48",
            "title": "48. Альтернативы Docker",
            "description": null,
            "content": "Альтернативы Docker\n\nВ самом начале этого раздела я говорил, что мы будем использовать Docker, просто потому, что ресурсов очень много, а сообщество очень большое, но также именно с него начался толчок к популярности контейнеров. Я бы посоветовал вам пойти и посмотреть немного истории о Docker и о том, как он появился, я нашел это очень полезным.\n\nНо, как я уже упоминал, существуют и другие альтернативы Docker. Если мы подумаем о том, что такое Docker и что мы уже рассмотрели. Это платформа для разработки, тестирования, развертывания и управления приложениями.\n\nЯ хочу выделить несколько альтернатив Docker, которые вы можете увидеть или увидите в будущем.\n\nPodman\n\nЧто такое Podman? Podman - это контейнерный движок без демонов для разработки, управления и запуска OCI-контейнеров в вашей системе Linux. Контейнеры могут быть запущены от имени root или в режиме rootless.\n\nЯ буду рассматривать это с точки зрения Windows, но знаю, что, как и в случае с Docker, здесь не требуется виртуализация, поскольку он будет использовать базовую ОС, чего нельзя сделать в мире Windows.\n\nPodman может быть запущен под WSL2, хотя и не так гладко, как в случае с Docker Desktop. Существует также удаленный клиент Windows, с помощью которого можно подключиться к виртуальной машине Linux, где будут запущены ваши контейнеры.\n\nМой Ubuntu на WSL2 - это версия 20.04. Следуя следующим шагам, вы сможете установить Podman на свой экземпляр WSL.\n\necho \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_20.04/ /\" |\nsudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\n\nДобавим ключ GPG\n\ncurl -L \"https://download.opensuse.org/repositories/devel:/kubic:\\\n/libcontainers:/stable/xUbuntu_20.04/Release.key\" | sudo apt-key add -\n\nЗапустите обновление системы с помощью команды sudo apt-get update && sudo apt-get upgrade. Наконец, мы можем установить podman с помощью команды sudo apt install podman.\n\nТеперь мы можем использовать многие из тех же команд, которые мы использовали для docker, однако обратите внимание, что у нас нет красивого пользовательского интерфейса рабочего стола docker. Вы можете видеть ниже, я использовал podman images и у меня ничего не появилось после установки, затем я использовал podman pull ubuntu для извлечения образа контейнера ubuntu.\n\nЗатем мы можем запустить наш образ Ubuntu с помощью podman run -dit ubuntu и podman ps, чтобы увидеть наш запущенный образ.\n\nЧтобы попасть в этот контейнер, мы можем выполнить команду podman attach dazzling_darwin, имя вашего контейнера, скорее всего, будет другим.\n\nЕсли вы переходите от docker к podman, то обычно также необходимо изменить ваш конфигурационный файл на alias docker=podman, тогда любая команда, запущенная с помощью docker, будет использовать podman.\n\nLXC\n\nLXC - это механизм контейнеризации, который позволяет пользователям снова создавать несколько изолированных контейнерных сред Linux. В отличие от Docker LXC действует как гипервизор для создания нескольких Linux-машин с отдельными системными файлами, сетевыми функциями. Появился еще до Docker, а затем сделал короткое возвращение из-за недостатков Docker.\n\nLXC такой же легкий, как и docker, и легко развертывается.\n\nContainerd\n\nАвтономная среда выполнения контейнеров. Containerd обеспечивает простоту и надежность, а также, конечно, переносимость. Ранее Containerd был инструментом, работающим как часть контейнерных сервисов Docker, пока Docker не решил вывести свои компоненты в самостоятельные.\n\nПроект в Cloud Native Computing Foundation, что ставит его в один ряд с такими популярными контейнерными инструментами, как Kubernetes, Prometheus и CoreDNS.\n\nДругие инструменты Docker\n\nМы могли бы также упомянуть инструменты и опции вокруг Rancher, VirtualBox, но мы можем рассказать о них более подробно в другой раз.\n\nGradle\n\nСканирование сборки позволяет командам совместно отлаживать свои скрипты и отслеживать историю всех сборок.\nОпции выполнения дают командам возможность непрерывной сборки так, чтобы при каждом вводе изменений задание выполнялось автоматически.\nНастраиваемый макет репозитория дает командам возможность рассматривать любую структуру файловых каталогов как хранилище артефактов.\n\nPacker\n\nВозможность параллельного создания нескольких машинных образов для экономии времени разработчиков и повышения эффективности.\nКоманды могут легко отлаживать сборки с помощью отладчика Packer, который проверяет сбои и позволяет командам опробовать решения перед перезапуском сборки.\nПоддержка многих платформ с помощью плагинов, что позволяет командам настраивать свои сборки.\n\nLogspout\n\nИнструмент для ведения логов - настраиваемость инструмента позволяет командам отправлять одни и те же логи в несколько мест назначения.\nКоманды могут легко управлять своими файлами, поскольку инструмент требует только доступа к сокету Docker.\nПолностью с открытым исходным кодом и прост в развертывании.\n\nLogstash\n\nНастройте свой конвейер с помощью подключаемой структуры Logstash.\nЛегко анализируйте и преобразуйте данные для анализа и повышения ценности бизнеса.\nРазнообразие выходов Logstash позволяет направлять данные туда, куда вам нужно.\n\nPortainer\n\nИспользуйте готовые шаблоны или создавайте свои собственные для развертывания приложений.\nСоздавайте команды и назначайте роли и разрешения для членов команды.\nУзнайте, что запущено в каждой среде, используя приборную панель инструмента.\n\nРесурсы\n\nTechWorld with Nana - Docker Tutorial for Beginners\nProgramming with Mosh - Docker Tutorial for Beginners\nDocker Tutorial for Beginners - What is Docker? Introduction to Containers\nWSL 2 with Docker getting started\nBlog on gettng started building a docker image\nDocker documentation for building an image\nYAML Tutorial: Everything You Need to Get Started in Minute\nPodman | Daemonless Docker | Getting Started with Podman\nLXC - Guide to building a LXC Lab\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day49",
            "title": "49. Основы Kubernetes",
            "description": null,
            "content": "Общая картина: Kubernetes\n\nВ предыдущем разделе мы рассмотрели контейнеры. Контейнеры не справляются с задачей масштабирования и оркестровки. Лучшее, что мы можем сделать, это использовать docker-compose для объединения нескольких контейнеров. Когда речь заходит о Kubernetes, который является оркестратором контейнеров, это дает нам возможность масштабирования в автоматическом режиме или в зависимости от нагрузки ваших приложений и сервисов.\n\nКак платформа Kubernetes предлагает возможность оркестровки контейнеров в соответствии с вашими требованиями и желаемым состоянием. Мы рассмотрим Kubernetes в этом разделе, поскольку она быстро развивается как следующая волна инфраструктуры. С точки зрения DevOps, Kubernetes - это лишь одна из платформ, базовое понимание которой вам понадобится. Вам также потребуется понимание \"голого металла\", виртуализации и, скорее всего, облачных сервисов. Kubernetes - это просто еще один вариант запуска наших приложений.\n\nЧто такое оркестровка контейнеров?\n\nЯ упомянул Kubernetes и упомянул оркестровку контейнеров, Kubernetes - это технология, а оркестровка контейнеров - это концепция или процесс, стоящий за технологией. Kubernetes - не единственная платформа для оркестровки контейнеров, у нас также есть Docker Swarm, HashiCorp Nomad и другие. Но Kubernetes набирает силу, поэтому я хочу рассказать о Kubernetes, но хочу сказать, что она не единственная.\n\nЧто такое Kubernetes?\n\nПервое, что вам следует прочитать, если вы новичок в Kubernetes, - это официальная документация. Мой опыт глубокого погружения в Kubernetes чуть больше года назад показал, что это будет крутая кривая обучения. Будучи выходцем из сферы виртуализации и хранения данных, я думал о том, насколько пугающим это кажется.\n\nНо на самом деле сообщество, бесплатные учебные ресурсы и документация просто потрясающие. Kubernetes.io\n\nKubernetes - это портативная, расширяемая платформа с открытым исходным кодом для управления контейнерными рабочими нагрузками и сервисами, которая облегчает как декларативную конфигурацию, так и автоматизацию. Она имеет большую, быстро развивающуюся экосистему. Услуги, поддержка и инструменты Kubernetes широко доступны.\n\nВажные моменты, которые следует отметить из вышеприведенного цитаты: Kubernetes является открытым исходным кодом с богатой историей, восходящей к Google, который передал проект в фонд Cloud Native computing Foundation (CNCF), и в настоящее время он развивается сообществом открытого исходного кода, а также крупными корпоративными поставщиками, которые внесли свой вклад, чтобы сделать Kubernetes тем, чем он является сегодня.\n\nЯ уже упоминал, что контейнеры - это здорово, и в предыдущем разделе мы говорили о том, как контейнеры и образы контейнеров изменили и ускорили внедрение облачных нативных систем. Но сами по себе контейнеры не дадут вам готового к производству опыта, который необходим вашему приложению. Kubernetes дает нам следующее:\n\nОбнаружение сервисов и балансировка нагрузки Kubernetes может открыть контейнер, используя DNS-имя или собственный IP-адрес. Если трафик на контейнер высок, Kubernetes может сбалансировать нагрузку и распределить сетевой трафик так, чтобы развертывание было стабильным.\n\nОркестровка хранилищ Kubernetes позволяет автоматически монтировать системы хранения по вашему выбору, например, локальные хранилища, общедоступные облачные провайдеры и многое другое.\n\nАвтоматизированное развертывание и откат Вы можете описать желаемое состояние для развернутых контейнеров с помощью Kubernetes, и он может изменить фактическое состояние на желаемое с контролируемой скоростью. Например, вы можете автоматизировать Kubernetes для создания новых контейнеров для развертывания, удаления существующих контейнеров и переноса всех их ресурсов в новый контейнер.\n\nАвтоматическая упаковка контейнеров Вы предоставляете Kubernetes кластер узлов, которые он может использовать для выполнения контейнерных задач. Вы сообщаете Kubernetes, сколько процессора и памяти (RAM) требуется каждому контейнеру. Kubernetes может разместить контейнеры на ваших узлах, чтобы наилучшим образом использовать ваши ресурсы.\n\nСамовосстановление Kubernetes перезапускает вышедшие из строя контейнеры, заменяет контейнеры, уничтожает контейнеры, которые не отвечают на заданную пользователем проверку работоспособности, и не рекламирует их клиентам, пока они не будут готовы к обслуживанию.\n\nУправление секретами и конфигурациями Kubernetes позволяет хранить и управлять конфиденциальной информацией, такой как пароли, токены OAuth и ключи SSH. Вы можете развертывать и обновлять секреты и конфигурацию приложений, не перестраивая образы контейнеров и не раскрывая секреты в конфигурации стека.\n\nKubernetes предоставляет вам основу для отказоустойчивого запуска распределенных систем.\n\nContainer Orchestration управляет развертыванием, размещением и жизненным циклом контейнеров.\n\nНа нее также возложено множество других обязанностей:\n\nУправление кластером объединяет узлы в одну цель.\n\nУправление расписанием распределяет контейнеры по узлам с помощью планировщика.\n\nОбнаружение сервисов знает, где находятся контейнеры, и распределяет между ними запросы клиентов.\n\nРепликация обеспечивает наличие необходимого количества узлов и контейнеров для требуемой рабочей нагрузки.\n\nУправление здоровьем обнаруживает и заменяет нездоровые контейнеры и узлы.\n\nОсновные компоненты Kubernetes\n\nKubernetes - это контейнерный оркестратор для обеспечения, управления и масштабирования приложений. Вы можете использовать его для управления жизненным циклом контейнерных приложений в кластере узлов, который представляет собой набор рабочих машин, таких как виртуальные машины или физические машины.\n\nДля работы вашим приложениям может понадобиться множество других ресурсов, таких как тома, сети и секреты, которые помогут вам подключаться к базам данных, общаться с бэкграундом и защищать ключи. С помощью Kubernetes вы можете добавить эти ресурсы в свое приложение. Инфраструктурные ресурсы, необходимые вашим приложениям, управляются декларативно.\n\nКлючевой парадигмой Kubernetes является ее декларативная модель. Вы предоставляете нужное вам состояние, а Kubernetes его реализует. Если вам нужно пять экземпляров, вы не запускаете пять отдельных экземпляров самостоятельно. Вместо этого вы сообщаете Kubernetes, что вам нужно пять экземпляров, и Kubernetes автоматически согласовывает состояние. Если с одним из ваших экземпляров что-то пойдет не так и он выйдет из строя, Kubernetes все равно будет знать нужное вам состояние и создаст экземпляры на доступном узле.\nУзел\n\nПлан управления\n\nКаждый кластер Kubernetes требует наличия узла Control Plane, компоненты которого принимают глобальные решения относительно кластера (например, планирование), а также обнаруживают и реагируют на события кластера.\n\nРабочий узел\n Рабочая машина, на которой выполняются рабочие нагрузки Kubernetes. Это может быть физическая (bare metal) машина или виртуальная машина (VM). На каждом узле может размещаться один или несколько стручков. Узлы Kubernetes управляются плоскостью управления\n\n\n\nСуществуют и другие типы узлов, но я не буду их здесь рассматривать.\n\nkubelet\n\nАгент, который запускается на каждом узле кластера. Он следит за тем, чтобы контейнеры запускались в Pod.\n\nКуплет принимает набор PodSpecs, которые предоставляются через различные механизмы, и гарантирует, что контейнеры, описанные в этих PodSpecs, запущены и здоровы. Куплет не управляет контейнерами, которые не были созданы Kubernetes.\n\n\n\nkube-proxy\n\nkube-proxy - это сетевой прокси, который работает на каждом узле вашего кластера, реализуя часть концепции Kubernetes Service.\n\nkube-proxy поддерживает сетевые правила на узлах. Эти сетевые правила позволяют сетевое взаимодействие с вашими Pods из сетевых сессий внутри или вне вашего кластера.\n\nkube-proxy использует уровень фильтрации пакетов операционной системы, если он есть и доступен. В противном случае kube-proxy сам перенаправляет трафик.\n\n\n\nВремя выполнения контейнера\n\nВремя выполнения контейнеров - это программное обеспечение, которое отвечает за запуск контейнеров.\n\nKubernetes поддерживает несколько сред выполнения контейнеров: Docker, containerd, CRI-O и любую реализацию Kubernetes CRI (Container Runtime Interface).\n\n\n​\nКластер\n\nКластер - это группа узлов, где узлом может быть физическая машина или виртуальные машины. На каждом из узлов будет установлена среда выполнения контейнеров (Docker), а также будет запущен сервис kubelet, который является агентом, принимающим команды от главного контроллера (подробнее об этом позже), и прокси, который используется для прокси-соединений с Pods от другого компонента (сервисы, которые мы рассмотрим позже).\n\nНа нашей плоскости управления, которую можно сделать высокодоступной, будет несколько уникальных ролей по сравнению с рабочими узлами, самой важной из них будет сервер kube API, именно с ним будет происходить любое взаимодействие для получения информации или отправки информации в наш кластер Kubernetes.\n\nKube API-Server\n\nСервер API Kubernetes проверяет и настраивает данные для объектов api, которые включают стручки, сервисы, контроллеры репликации и другие. API-сервер обслуживает REST-операции и предоставляет фронтенд к общему состоянию кластера, через который взаимодействуют все остальные компоненты.\n\nПланировщик\n\nПланировщик Kubernetes - это процесс в плоскости управления, который назначает Pods узлам. Планировщик определяет, какие узлы являются допустимыми для размещения каждого Pod в очереди планирования в соответствии с ограничениями и доступными ресурсами. Затем планировщик ранжирует каждый допустимый узел и привязывает Pod к подходящему узлу.\n\nМенеджер контроллера\n\nМенеджер контроллеров Kubernetes - это демон, который встраивает основные контуры управления, поставляемые с Kubernetes. В приложениях робототехники и автоматизации контур управления - это не завершающийся цикл, который регулирует состояние системы. В Kubernetes контроллер - это контур управления, который следит за общим состоянием кластера через apiserver и вносит изменения, пытаясь переместить текущее состояние в желаемое.\n\netcd.\n\nПоследовательное и высокодоступное хранилище значений ключей, используемое в качестве резервного хранилища Kubernetes для всех данных кластера.\n\n\n\nkubectl\n\nДля управления этим с точки зрения CLI у нас есть kubectl, kubectl взаимодействует с сервером API.\n\nИнструмент командной строки Kubernetes, kubectl, позволяет выполнять команды для кластеров Kubernetes. Вы можете использовать kubectl для развертывания приложений, проверки и управления ресурсами кластера, а также для просмотра журналов.\n\nPods\n\nPod - это группа контейнеров, которые образуют логическое приложение. Например, если у вас есть веб-приложение, в котором запущен контейнер NodeJS, а также контейнер MySQL, то оба этих контейнера будут находиться в одном Pod. Pod также может иметь общие тома данных, а также разделять одно и то же сетевое пространство имен. Помните, что Pods являются эфемерными и могут быть подняты и опущены главным контроллером. Kubernetes использует простое, но эффективное средство идентификации Pods с помощью концепции Labels (имя - значения).\n\nПодсистемы управляют томами, секретами и конфигурацией контейнеров.\n\nПодсистемы являются эфемерными. Они предназначены для автоматического перезапуска после смерти.\n\nPods реплицируются при горизонтальном масштабировании приложения с помощью ReplicationSet. Каждый Pod будет выполнять один и тот же код контейнера.\n\nPods живут на рабочих узлах (Worker Nodes).\n\nРазвертывания\n\nВы можете просто решить запустить Pods, но когда они умирают, они умирают.\n\nРазвертывание позволит вашему стручку работать непрерывно.\n\nРазвертывания позволяют вам обновлять работающее приложение без простоя.\n\nРазвертывания также определяют стратегию перезапуска стручков, когда они умирают\n\nReplicaSets\n\nРазвертывание также может создать набор реплик.\n\nReplicaSet гарантирует, что ваше приложение имеет необходимое количество Pods.\n\nReplicaSets будет создавать и масштабировать Pods на основе развертывания\n\nРазвертывание, наборы реплик, подсистемы не являются исключительными, но могут быть\n\nStatefulSets\n\nТребуется ли вашему приложению хранить информацию о его состоянии?\n\nБаза данных нуждается в состоянии\n\nПодсистемы StatefulSet не являются взаимозаменяемыми.\n\nКаждый Pod имеет уникальный постоянный идентификатор, который контроллер сохраняет при любом перепланировании.\n\nКаждый Pod имеет уникальный, постоянный идентификатор, который контроллер сохраняет при любом перепланировании.\n\nDaemonSets\n\nDaemonSets предназначены для непрерывного процесса.\n\nОни запускают по одному Pod на узел.\n\nКаждый новый узел, добавленный в кластер, получает запущенный pod.\n\nПолезны для фоновых задач, таких как мониторинг и сбор логов.\n\nКаждый Pod имеет уникальный, постоянный идентификатор, который контроллер сохраняет при любом перепланировании.\n\nСервисы\n\nединая конечная точка для доступа к Pods\n\nунифицированный способ маршрутизации трафика к кластеру и, в конечном итоге, к списку Pods.\n\nИспользуя сервис, Pods можно поднимать и опускать, не затрагивая ничего.\n\nЭто лишь краткий обзор и заметки о фундаментальных строительных блоках Kubernetes, мы можем использовать эти знания и добавить некоторые другие области, такие как Storage и Ingress, чтобы улучшить наши приложения, но у нас также есть большой выбор, где будет работать наш кластер Kubernetes. Следующая сессия будет посвящена этим вариантам, где я могу запустить кластер Kubernetes, а также изучению некоторых особенностей хранения данных.\n\nРесурсы\n\nKubernetes Documentation\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day50",
            "title": "50. Выбор платформы Kubernetes для проекта",
            "description": null,
            "content": "Выбор платформы Kubernetes\n\nЯ хотел бы использовать эту сессию для разбора некоторых платформ или, может быть, дистрибутивов - более подходящий термин для этого, одна вещь, которая была проблемой в мире Kubernetes - это устранение сложности.\n\nKubernetes the hard way рассказывает о том, как построить из ничего полноценный функциональный кластер Kubernetes, очевидно, что это крайность, но все больше и больше людей, по крайней мере, тех, с кем я общаюсь, хотят устранить эту сложность и запустить управляемый кластер Kubernetes. Проблема в том, что это стоит больше денег, но преимущества могут быть следующими: если вы используете управляемый сервис, действительно ли вам нужно знать архитектуру узлов и то, что происходит с точки зрения плоскости управления узлов, когда обычно у вас нет к этому доступа.\n\nЗатем у нас есть локальные дистрибутивы для разработки, которые позволяют нам использовать наши собственные системы и запускать локальную версию Kubernetes, чтобы разработчики могли иметь полную рабочую среду для запуска своих приложений на платформе, для которой они предназначены.\n\nОбщая основа всех этих концепций заключается в том, что все они являются разновидностью Kubernetes, что означает, что мы должны иметь возможность свободно мигрировать и перемещать наши рабочие нагрузки туда, куда нам нужно, в соответствии с нашими требованиями.\n\nВо многом наш выбор будет зависеть от того, какие инвестиции были сделаны. Я уже упоминал об опыте разработчиков, но некоторые из локальных сред Kubernetes, в которых работают наши ноутбуки, отлично подходят для ознакомления с технологией без затрат денег.\n\nBare-Metal Clusters\n\nВариантом для многих может быть запуск ОС Linux прямо на нескольких физических серверах для создания кластера, это также может быть Windows, но я не слышал о темпах внедрения Windows, контейнеров и Kubernetes. Очевидно, что если вы - компания, и вы приняли решение о покупке физических серверов, то это может быть способом создания кластера Kubernetes, но управление и администрирование здесь означает, что вам придется создавать и управлять всем с нуля.\n\nВиртуализация\n\nНезависимо от тестовых и учебных сред или готовых корпоративных кластеров Kubernetes виртуализация является отличным способом продвижения, обычно это возможность запускать виртуальные машины в качестве узлов и затем объединять их в кластер. Вы получаете базовую архитектуру, эффективность и скорость виртуализации, а также возможность эффективно использовать существующие затраты. Например, VMware предлагает отличное решение для виртуальных машин и Kubernetes в различных вариантах.\n\nМой первый кластер Kubernetes был создан на основе виртуализации с использованием Microsoft Hyper-V на старом сервере, который был способен запускать несколько виртуальных машин в качестве узлов.\n\nВарианты локального рабочего стола\n\nСуществует несколько вариантов запуска локального кластера Kubernetes на вашем настольном компьютере или ноутбуке. Как уже говорилось ранее, это дает разработчикам возможность увидеть, как будет выглядеть их приложение, без необходимости создавать несколько дорогостоящих или сложных кластеров. Лично я часто использую этот кластер, в частности, я использую minikube. Он обладает отличной функциональностью и дополнениями, которые меняют способ создания и запуска приложений.\n\nKubernetes Managed Services\nЯ уже упоминал о виртуализации, и это может быть достигнуто с помощью гипервизоров локально, но мы знаем из предыдущих разделов, что мы также можем использовать виртуальные машины в публичном облаке в качестве узлов. Я говорю об управляемых сервисах Kubernetes - это предложения, которые мы видим у крупных гипермасштабирующих компаний, а также у MSP, которые убирают уровни управления и контроля от конечного пользователя; это может быть удаление плоскости управления от конечного пользователя, что происходит с Amazon EKS, Microsoft AKS и Google Kubernetes Engine. (GKE)\n\nНепреодолимый выбор\n\nВыбор - это здорово, но есть момент, когда он становится чрезмерным, и это не глубокий обзор всех вариантов в каждой из перечисленных выше категорий. В дополнение к вышеперечисленному у нас есть OpenShift от Red Hat, и этот вариант действительно может быть использован во всех вышеперечисленных вариантах у всех основных облачных провайдеров и, вероятно, сегодня обеспечивает наилучшее общее удобство для администраторов независимо от того, где развернуты кластеры.\n\nИтак, с чего вы начнете свое обучение, как я уже сказал, я начал с пути виртуализации, но это было потому, что у меня был доступ к физическому серверу, который я мог использовать для этой цели, я ценю и фактически с тех пор у меня больше нет такой возможности.\n\nСейчас я бы посоветовал использовать Minikube в качестве первого варианта или Kind (Kubernetes в Docker), но Minikube дает нам некоторые дополнительные преимущества, которые почти абстрагируют сложность, так как мы можем просто использовать дополнительные модули и быстро создавать вещи, а затем разрушать их, когда мы закончим, мы можем запускать несколько кластеров, мы можем запускать их почти везде, кросс-платформенные и аппаратно-агностические.\n\nЯ проделал небольшой путь в изучении Kubernetes, поэтому я собираюсь оставить выбор платформы и конкретику здесь, чтобы перечислить варианты, которые я пробовал, чтобы дать мне лучшее понимание платформы Kubernetes и того, где она может работать. Что я мог бы сделать с нижеприведенными записями в блоге, так это еще раз взглянуть на них, обновить их и перенести сюда, вместо того, чтобы они были ссылками на записи в блоге.\n\nРесурсы\n\nKubernetes playground – How to choose your platform\nKubernetes playground – Setting up your cluster\nGetting started with Amazon Elastic Kubernetes Service (Amazon EKS)\nGetting started with Microsoft Azure Kubernetes Service (AKS)\nGetting Started with Microsoft AKS – Azure PowerShell Edition\nGetting started with Google Kubernetes Service (GKE)\nKubernetes, How to – AWS Bottlerocket + Amazon EKS\nGetting started with CIVO Cloud\nMinikube - Kubernetes Demo Environment For Everyone\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day51",
            "title": "51. Установка minikube",
            "description": null,
            "content": "Развертывание первого кластера Kubernetes\n\nВ этом посте мы собираемся запустить кластер Kubernetes на нашей локальной машине с помощью minikube, это даст нам базовый кластер Kubernetes для остальной части раздела Kubernetes, хотя позже мы рассмотрим развертывание кластера Kubernetes и в VirtualBox. Причина, по которой мы выбрали этот метод, а не развертывание управляемого кластера Kubernetes в публичном облаке, заключается в том, что это будет стоить денег даже при бесплатном уровне, однако я поделился некоторыми блогами, если вы захотите развернуть такую среду в предыдущем разделе День 50.\n\nЧто такое Minikube?\n\nMinikube быстро создает локальный кластер Kubernetes на macOS, Linux и Windows.\n\nДля начала, независимо от ОС вашей рабочей станции, вы можете запустить minikube. Сначала перейдите на страницу проекта. Первая опция, которая у вас есть, это выбор метода установки. Я не использовал этот метод, но вы можете выбрать мой способ (о моем способе речь впереди).\n\nНиже упоминается, что вам необходимо иметь \"Менеджер контейнеров или виртуальных машин, такой как: Docker, Hyperkit, Hyper-V, KVM, Parallels, Podman, VirtualBox или VMware\" - это то, где будет работать MiniKube, и это простой вариант, и если не указано в репозитории, я использую Docker. Вы можете установить Docker на свою систему, используя следующую ссылку.\n\nПонятное руководство по установке minikube\n\nМой способ установки minikube\n\nЯ уже некоторое время использую arkade, чтобы получить все эти инструменты Kubernetes и CLI, вы можете посмотреть шаги установки на этом github репозитории для начала работы с Arkade. Я также упоминал об этом в других записях блога, когда мне нужно было что-то установить. Простота установки: достаточно нажать arkade get и посмотреть, доступен ли ваш инструмент или cli, очень удобна. В разделе Linux мы говорили о менеджере пакетов и процессе получения нашего программного обеспечения, вы можете думать об Arkade как о рынке для всех ваших приложений и clis для Kubernetes. Очень удобный инструмент, который нужно иметь в своих системах, написанный на Golang и кроссплатформенный.\n\nВ длинном списке доступных приложений в arkade minikube является одним из них, поэтому с помощью простой команды arkade get minikube мы загружаем бинарник и можем приступать.\n\n\n\nНам также понадобится kubectl как часть нашего инструментария, поэтому вы можете получить его через arkade или, как я полагаю, в документации по minikube он представлен как часть команд curl, упомянутых выше. Подробнее о kubectl мы расскажем позже в этом посте.\n\nПолучение и запуск кластера Kubernetes\n\nВ этом конкретном разделе я хочу рассказать о доступных нам вариантах запуска кластера Kubernetes на вашей локальной машине. Мы можем просто выполнить следующую команду, и она запустит кластер для использования.\n\nminikube используется в командной строке, и, проще говоря, после того как вы все установили, вы можете выполнить команду minikube start для развертывания вашего первого кластера Kubernetes. Ниже вы увидите, что драйвер Docker по умолчанию является местом, где мы будем запускать наш вложенный узел виртуализации. В начале статьи я упомянул о других доступных опциях, которые помогут вам расширить вид локального кластера Kubernetes.\n\nОдин кластер Minikube будет состоять из одного контейнера docker, в котором будут находиться узел плоскости управления и рабочий узел в одном экземпляре. Обычно вы разделяете эти узлы по отдельности. Об этом мы расскажем в следующем разделе, где мы рассмотрим домашние лабораторные среды Kubernetes, но немного ближе к производственной архитектуре.\n\nЯ уже несколько раз говорил об этом, мне очень нравится minikube из-за доступных дополнений, возможность развернуть кластер с помощью простой команды, включающей все необходимые дополнения с самого начала, действительно помогает мне каждый раз развертывать одну и ту же необходимую установку.\n\nНиже представлен список этих аддонов, я обычно использую аддоны csi-hostpath-driver и volumesnapshots, но вы можете увидеть длинный список ниже. Конечно, эти аддоны могут быть развернуты с помощью Helm, о чем мы расскажем позже в разделе Kubernetes, но это значительно упрощает работу.\n\nЯ также определяю в нашем проекте некоторые дополнительные конфигурации, apiserver установлен на 6433 вместо случайного порта API, я определяю время выполнения контейнера также на containerd, однако docker используется по умолчанию, и CRI-O также доступен. Я также устанавливаю определенную версию Kubernetes.\n\nТеперь мы готовы развернуть наш первый кластер Kubernetes с помощью minikube. Я уже упоминал, что вам также понадобится kubectl для взаимодействия с вашим кластером. Вы можете установить kubectl с помощью arkade, выполнив команду arkade get kubectl.\n\nили вы можете загрузить кросс-платформенную версию со следующих сайтов\n\nLinux\nmacOS\nWindows\n\nПосле установки kubectl мы можем взаимодействовать с нашим кластером с помощью простой команды kubectl get nodes.\n\n\nЧто такое kubectl?\n\nТеперь у нас есть наш кластер minikube | Kubernetes, и я попросил вас установить Minikube, где я объяснил, что он делает, но я не объяснил, что такое kubectl и что он делает.\n\nkubectl - это программа, которая используется или позволяет вам взаимодействовать с кластерами Kubernetes, мы используем ее здесь для взаимодействия с нашим кластером minikube, но мы также используем kubectl для взаимодействия с нашими корпоративными кластерами в публичном облаке.\n\nМы используем kubectl для развертывания приложений, проверки и управления ресурсами кластера. Гораздо лучший Обзор kubectl можно найти здесь, в официальной документации Kubernetes.\n\nkubectl взаимодействует с сервером API, расположенным на узле Control Plane, о котором мы вкратце рассказывали в одном из предыдущих постов.\n\nkubectl шпаргалка\n\nНаряду с официальной документацией я также обнаружил, что при поиске команд kubectl у меня постоянно открыта эта страница. Unofficial Kubernetes\n\n|Listing Resources               |                                           |\n| ------------------------------ | ----------------------------------------- |\n|kubectl get nodes               |List all nodes in cluster                  |\n|kubectl get namespaces          |List all namespaces in cluster             |\n|kubectl get pods                |List all pods in default namespace cluster |\n|kubectl get pods -n name        |List all pods in \"name\" namespace          |\n|kubectl get pods -n name        |List all pods in \"name\" namespace          |\n\n|Creating Resources              |                                           |\n| ------------------------------ | ----------------------------------------- |\n|kubectl create namespace name   |Create a namespace called \"name\"           |\n|kubectl create -f [filename]    |Create a resource from a JSON or YAML file:|\n\n|Editing Resources               |                                           |\n| ------------------------------ | ----------------------------------------- |\n|kubectl edit svc/servicename    |To edit a service                          |\n\n|More detail on Resources        |                                                        |\n| ------------------------------ | ------------------------------------------------------ |\n|kubectl describe nodes          | display the state of any number of resources in detail,|\n\n|Delete Resources                |                                                        |\n| ------------------------------ | ------------------------------------------------------ |\n|kubectl delete pod              | Remove resources, this can be from stdin or file       |\n\nВы захотите узнать краткие названия некоторых команд kubectl, например, -n - это краткое название для namespace, что облегчает ввод команды, а также, если вы пишете скрипты, вы можете получить гораздо более аккуратный код.\n\n| Short name           | Full name                    |\n| -------------------- | ---------------------------- |\n|  csr                 |  certificatesigningrequests  |\n|  cs                  |  componentstatuses           |\n|  cm                  |  configmaps                  |\n|  ds                  |  daemonsets                  |\n|  deploy              |  deployments                 |\n|  ep                  |  endpoints                   |\n|  ev                  |  events                      |\n|  hpa                 |  horizontalpodautoscalers    |\n|  ing                 |  ingresses                   |\n|  limits              |  limitranges                 |\n|  ns                  |  namespaces                  |\n|  no                  |  nodes                       |\n|  pvc                 |  persistentvolumeclaims      |\n|  pv                  |  persistentvolumes           |\n|  po                  |  pods                        |\n|  pdb                 |  poddisruptionbudgets        |\n|  psp                 |  podsecuritypolicies         |\n|  rs                  |  replicasets                 |\n|  rc                  |  replicationcontrollers      |\n|  quota               |  resourcequotas              |\n|  sa                  |  serviceaccounts             |\n|  svc                 |  services                    |\n\nВ заключение хочу добавить, что я создал еще один проект на основе minikube, чтобы помочь мне быстро развернуть демонстрационные среды для демонстрации сервисов данных и защиты этих рабочих нагрузок с помощью Kasten K10, Project Pace можно найти там и буду рад вашим отзывам или взаимодействию, он также показывает или включает некоторые автоматизированные способы развертывания кластеров minikube и создания различных приложений сервисов данных.\n\nДалее мы перейдем к развертыванию нескольких узлов в виртуальные машины с помощью VirtualBox, но здесь мы будем действовать проще, как мы делали в разделе Linux, где мы использовали vagrant для быстрого запуска машин и развертывания нашего программного обеспечения, как мы хотим.\n\nЯ добавил этот список к вчерашнему посту, который представляет собой блоги с описанием развертывания различных кластеров Kubernetes.\n\nKubernetes playground – How to choose your platform\nKubernetes playground – Setting up your cluster\nGetting started with Amazon Elastic Kubernetes Service (Amazon EKS)\nGetting started with Microsoft Azure Kubernetes Service (AKS)\nGetting Started with Microsoft AKS – Azure PowerShell Edition\nGetting started with Google Kubernetes Service (GKE)\nKubernetes, How to – AWS Bottlerocket + Amazon EKS\nGetting started with CIVO Cloud\nMinikube - Kubernetes Demo Environment For Everyone\n\nРесурсы\n\nKubernetes Documentation\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\n\n",
            "tags": [
                "devops",
                "minikube"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day52",
            "title": "52. Настройка многоузлового кластера Kubernetes",
            "description": null,
            "content": "Настройка многоузлового кластера Kubernetes\n\nЯ хотел назвать эту статью \"Настройка многоузлового кластера Kubernetes с помощью Vagrant\", но подумал, что это будет слишком длинно!\n\nНа вчерашней сессии мы использовали классный проект для развертывания нашего первого кластера Kubernetes и немного поработали с самым важным инструментом CLI, с которым вы столкнетесь при использовании Kubernetes (kubectl).\n\nЗдесь мы будем использовать VirtualBox в качестве основы, но, как мы уже говорили о Vagrant в разделе Linux, мы можем использовать любой гипервизор или инструмент виртуализации. Это был День 14, когда мы прошли и развернули машину Ubuntu для раздела Linux.\n\nКраткая информация о Vagrant\n\nVagrant - это утилита CLI, которая управляет жизненным циклом ваших виртуальных машин. Мы можем использовать vagrant для запуска и разворачивания виртуальных машин на различных платформах, включая vSphere, Hyper-v, Virtual Box и Docker. У него есть и другие поставщики, но мы будем придерживаться этого, мы используем Virtual Box, так что все готово.\n\nЯ собираюсь использовать базовый уровень этого блога и репозитория, чтобы пройтись по конфигурации. Однако я бы посоветовал, если вы впервые развертываете кластер Kubernetes, посмотреть, как это делается вручную, и тогда вы хотя бы будете знать, как это выглядит. Хотя я должен сказать, что эти операции и усилия дня 0 становятся все более эффективными с каждым выпуском Kubernetes. Я сравниваю это с временами VMware и ESX, когда для развертывания 3 серверов ESX требовался по меньшей мере день, а теперь мы можем сделать это за час. Мы движемся в этом направлении, когда речь идет о Kubernetes\".\n\nЛабораторная среда Kubernetes\n\nЯ загрузил в папку Kubernetes vagrantfile, который мы будем использовать для создания нашей среды. Возьмите его и перейдите в этот каталог в терминале. Я снова использую Windows, поэтому я буду использовать PowerShell для выполнения команд рабочей станции с vagrant. Если у вас нет vagrant, вы можете использовать arkade, о котором мы говорили вчера при установке minikube и других инструментов. Простая команда arkade get vagrant должна заставить вас загрузить и установить последнюю версию vagrant.\n\nКогда вы окажетесь в своей директории, вы можете просто запустить vagrant up, и если все настроено правильно, вы должны увидеть в терминале следующее.\n\n В терминале вы увидите ряд шагов, но тем временем давайте посмотрим, что мы на самом деле создаем.\n\nИз приведенного выше изображения видно, что мы собираемся создать 3 виртуальные машины, у нас будет узел плоскости управления и два рабочих узла. Если вы вернетесь к День 49, вы увидите более подробное описание этих областей, которые мы видим на изображении.\n\nТакже на изображении мы указываем, что наш доступ к kubectl будет происходить извне кластера и попадать в kube apiserver, в то время как на самом деле в рамках инициализации vagrant мы развертываем kubectl на каждом из этих узлов, чтобы мы могли получить доступ к кластеру изнутри каждого из наших узлов.\n\nПроцесс создания этой лаборатории может занять от 5 до 30 минут в зависимости от вашей установки.\n\nЯ собираюсь в ближайшее время рассказать о скриптах, но если вы посмотрите в файл vagrant, то заметите, что мы вызываем 3 скрипта как часть развертывания, и именно здесь создается кластер. Мы видели, как легко использовать vagrant для развертывания наших виртуальных машин и установки ОС с помощью боксов vagrant, но возможность запуска скрипта оболочки как часть процесса развертывания - это то, что становится довольно интересным в автоматизации этих лабораторных сборок.\n\nПосле завершения мы можем подключиться по ssh к одному из наших узлов vagrant ssh master из терминала должен получить доступ, имя пользователя и пароль по умолчанию - vagrant/vagrant.\n\nВы также можете использовать vagrant ssh node01 и vagrant ssh node02 для получения доступа к рабочим узлам, если хотите.\n\nТеперь мы находимся на одном из вышеуказанных узлов нашего нового кластера, мы можем выдать команду kubectl get nodes, чтобы показать наш 3-узловой кластер и его статус.\n\nНа данный момент у нас есть запущенный 3-узловой кластер, с 1 узлом плоскости управления и 2 рабочими узлами.\n\nVagrantfile и Shell Script walkthrough\n\nЕсли мы посмотрим на наш vagrantfile, вы увидите, что мы определяем количество рабочих узлов, сетевые IP-адреса для мостовой сети в VirtualBox, а также некоторые именования. Еще вы заметите, что мы также вызываем некоторые скрипты, которые мы хотим запустить на определенных хостах.\n\nNUM_WORKER_NODES=2\nIP_NW=\"10.0.0.\"\nIP_START=10\n\nVagrant.configure(\"2\") do |config|\n    config.vm.provision \"shell\", inline: > /etc/hosts\n        echo \"$IP_NW$((IP_START+1))  worker-node01\" >> /etc/hosts\n        echo \"$IP_NW$((IP_START+2))  worker-node02\" >> /etc/hosts\n    SHELL\n    config.vm.box = \"bento/ubuntu-21.10\"\n    config.vm.box_check_update = true\n\n    config.vm.define \"master\" do |master|\n      master.vm.hostname = \"master-node\"\n      master.vm.network \"private_network\", ip: IP_NW + \"#{IP_START}\"\n      master.vm.provider \"virtualbox\" do |vb|\n          vb.memory = 4048\n          vb.cpus = 2\n          vb.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n      end\n      master.vm.provision \"shell\", path: \"scripts/common.sh\"\n      master.vm.provision \"shell\", path: \"scripts/master.sh\"\n    end\n\n    (1..NUM_WORKER_NODES).each do |i|\n      config.vm.define \"node0#{i}\" do |node|\n        node.vm.hostname = \"worker-node0#{i}\"\n        node.vm.network \"private_network\", ip: IP_NW + \"#{IP_START + i}\"\n        node.vm.provider \"virtualbox\" do |vb|\n            vb.memory = 2048\n            vb.cpus = 1\n            vb.customize [\"modifyvm\", :id, \"--natdnshostresolver1\", \"on\"]\n        end\n        node.vm.provision \"shell\", path: \"scripts/common.sh\"\n        node.vm.provision \"shell\", path: \"scripts/node.sh\"\n      end\n    end\n  end\n  Давайте разберем эти выполняемые скрипты. У нас есть три скрипта, перечисленные в вышеуказанном VAGRANTFILE для запуска на определенных узлах.\n\nmaster.vm.provision \"shell\", path: \"scripts/common.sh\"\n\nПриведенный выше скрипт будет направлен на подготовку узлов, он будет запущен на всех трех наших узлах и удалит все существующие компоненты Docker и переустановит Docker и ContainerD, а также kubeadm, kubelet и kubectl. Этот скрипт также обновит существующие пакеты программного обеспечения в системе.\n\nmaster.vm.provision \"shell\", path: \"scripts/master.sh\"\n\nСкрипт master.sh будет выполняться только на узле плоскости управления, этот скрипт создаст кластер Kubernetes с помощью команд kubeadm. Он также подготовит контекст конфигурации для доступа к этому кластеру, о чем мы расскажем далее.\n\nnode.vm.provision \"shell\", path: \"scripts/node.sh\"\n\nЭто просто возьмет конфиг, созданный мастером, и присоединит наши узлы к кластеру Kubernetes, этот процесс присоединения снова использует kubeadm и другой скрипт, который можно найти в папке config.\n\nДоступ к кластеру Kubernetes\n\n Теперь у нас есть два развернутых кластера: кластер minikube, который мы развернули в предыдущем разделе, и новый 3-узловой кластер, который мы только что развернули на VirtualBox.\n\n Также в этом конфигурационном файле, к которому у вас будет доступ на машине, с которой вы запускали vagrant, описано, как мы можем получить доступ к нашему кластеру с нашей рабочей станции.\n\n Прежде чем мы покажем это, позвольте мне коснуться контекста.\n\nКонтекст важен, необходима возможность доступа к кластеру Kubernetes с рабочего стола или ноутбука. Существует множество различных вариантов, и люди используют различные операционные системы в качестве повседневных драйверов.\n\nПо умолчанию клиент Kubernetes CLI (kubectl) использует папку C:\\Users\\username\\.kube\\config для хранения информации о кластере Kubernetes, такой как конечная точка и учетные данные. Если вы развернули кластер, вы сможете увидеть этот файл в этом месте. Но если вы до сих пор использовали главный узел для выполнения всех команд kubectl через SSH или другими способами, то эта статья, надеюсь, поможет вам освоить возможность подключения к рабочей станции.\n\nЗатем нам нужно получить файл kubeconfig из кластера или мы также можем получить его из нашего файла конфигурации после развертывания, получить содержимое этого файла либо через SCP, либо просто открыть консольный сеанс на главном узле и скопировать на локальную машину windows.\n\nЗатем мы хотим взять копию этого файла конфигурации и переместить в место $HOME/.kube/config.\n\nТеперь с локальной рабочей станции вы сможете запустить kubectl cluster-info и kubectl get nodes, чтобы убедиться, что у вас есть доступ к вашему кластеру.\n\nЭто не только обеспечивает подключение и управление с вашей windows-машины, но и позволяет нам выполнить проброс портов для доступа к определенным сервисам с нашей windows-машины.\n\nЕсли вам интересно, как управлять несколькими кластерами на рабочей станции, у меня есть более подробное описание здесь.\n\nЯ добавил этот список, в котором представлены блоги, посвященные различным развертываемым кластерам Kubernetes.\n\nKubernetes playground – How to choose your platform\nKubernetes playground – Setting up your cluster\nGetting started with Amazon Elastic Kubernetes Service (Amazon EKS)\nGetting started with Microsoft Azure Kubernetes Service (AKS)\nGetting Started with Microsoft AKS – Azure PowerShell Edition\nGetting started with Google Kubernetes Service (GKE)\nKubernetes, How to – AWS Bottlerocket + Amazon EKS\nGetting started with CIVO Cloud\nMinikube - Kubernetes Demo Environment For Everyone\n\nРесурсы\n\nKubernetes Documentation\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day53",
            "title": "53. Обзор Rancher",
            "description": null,
            "content": "Обзор Rancher - практическое применение\r\n\r\nВ этом разделе мы рассмотрим Rancher, до сих пор все, что мы делали, было в cli и с использованием kubectl, но у нас есть несколько действительно хороших пользовательских интерфейсов и инструментов управления несколькими кластерами, чтобы дать нашим операционным командам хорошую видимость управления кластером.\nRancher, согласно их сайту\r\n\r\nRancher - это полный программный стек для команд, внедряющих контейнеры. Он решает операционные проблемы и проблемы безопасности при управлении несколькими кластерами Kubernetes в любой инфраструктуре, обеспечивая команды DevOps интегрированными инструментами для запуска контейнерных рабочих нагрузок.\r\n\r\nRancher позволяет нам развертывать кластеры Kubernetes производственного уровня практически из любого места, а затем обеспечивает централизованную аутентификацию, контроль доступа и наблюдаемость. Я упоминал в предыдущем разделе, что существует почти непреодолимый выбор, когда речь идет о Kubernetes и о том, где вы должны или можете их запустить, но с Rancher действительно не имеет значения, где они находятся.\nРазвертывание Rancher\r\n\r\nПервое, что нам нужно сделать, это развернуть Rancher на нашей локальной рабочей станции, есть несколько способов и мест, которые вы можете выбрать для выполнения этого шага, я хочу использовать свою локальную рабочую станцию и запустить Rancher как контейнер docker. Выполнив приведенную ниже команду, мы получим образ контейнера и доступ к пользовательскому интерфейсу rancher.\nДоступны и другие методы развертывания rancher Rancher Quick-Start-Guide\r\nsudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher.\r\n\r\nКак вы можете видеть на нашем рабочем столе Docker, у нас есть запущенный контейнер rancher.\nДоступ к пользовательскому интерфейсу Rancher\r\n\r\nЗапустив вышеуказанный контейнер, мы должны иметь возможность перейти к нему через веб-страницу. По адресу https://localhost откроется страница входа в систему, как показано ниже.\nСледуйте инструкциям ниже, чтобы получить требуемый пароль. Поскольку я использую Windows, я решил использовать bash для Windows, так как для этого требуется команда grep.\nЗатем мы можем взять указанный выше пароль и войти в систему, на следующей странице мы можем задать новый пароль.\nПосле выполнения вышеуказанных действий мы войдем в систему и увидим наш начальный экран. В рамках развертывания Rancher мы также увидим локальный кластер K3s.\nКраткий экскурс по rancher\r\n\r\nПервое, на что мы посмотрим, это наш локально развернутый кластер K3S. Вы можете видеть ниже, что мы получаем хорошее представление о том, что происходит внутри нашего кластера. Это развертывание по умолчанию, и мы еще ничего не развертывали в этом кластере. Видно, что он состоит из 1 узла и имеет 5 развертываний. Также вы можете видеть, что есть некоторые статистические данные по стручкам, ядрам и памяти.\nВ меню слева есть вкладка Apps & Marketplace, которая позволяет нам выбрать приложения, которые мы хотели бы запустить на наших кластерах. Как уже упоминалось ранее, Rancher дает нам возможность запускать и управлять несколькими различными кластерами. С помощью рынка мы можем очень легко развернуть наши приложения.\nЕще одна вещь, о которой стоит упомянуть, это то, что если вам понадобится получить доступ к любому кластеру, управляемому Rancher, в правом верхнем углу есть возможность открыть оболочку kubectl для выбранного кластера.\nСоздание нового кластера\r\n\r\nНа последних двух занятиях мы создали кластер minikube локально и использовали Vagrant с VirtualBox для создания 3-узлового кластера Kubernetes, с помощью Rancher мы также можем создавать кластеры. В папке Rancher Folder вы найдете дополнительные файлы vagrant, которые создадут те же 3 узла, но без шагов по созданию нашего кластера Kubernetes (мы хотим, чтобы Rancher сделал это за нас).\r\n\r\nТем не менее, мы хотим установить docker и обновить ОС, поэтому вы увидите скрипт common.sh, запускаемый на каждом из наших узлов. Это также установит Kubeadm, Kubectl и т.д. Но он не запустит команды Kubeadm для создания и объединения наших узлов в кластер.\nМы можем перейти в папку vagrant и просто запустить vagrant up, и это начнет процесс создания наших 3 виртуальных машин в virtualbox.\nТеперь, когда у нас есть наши узлы или ВМ на месте и готовы, мы можем использовать Rancher для создания нашего нового кластера Kubernetes. Первый экран для создания кластера дает вам несколько вариантов того, где находится ваш кластер, то есть используете ли вы службы Kubernetes, управляемые публичным облаком, vSphere или что-то еще.\nМы выберем \"custom\", так как не используем ни одну из интегрированных платформ. На открывшейся странице вы определяете имя вашего кластера (ниже написано local, но вы не можете использовать local, наш кластер называется vagrant). Здесь вы можете определить версии Kubernetes, сетевых провайдеров и некоторые другие параметры конфигурации, чтобы запустить ваш кластер Kubernetes.\nНа следующей странице вы найдете регистрационный код, который необходимо запустить на каждом из узлов и включить соответствующие службы: etcd, controlplane и worker. Для нашего главного узла нам нужны etcd и controlplane, поэтому команду можно увидеть ниже.\n`bash\r\nsudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.6.3 --server https://10. 0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --etcd --controlplane\r\n`\r\n\r\nЕсли сетевое взаимодействие настроено правильно, то вы должны довольно быстро увидеть следующее на приборной панели rancher, указывающее на то, что первый мастер-узел сейчас регистрируется и кластер создается.\nЗатем мы можем повторить процесс регистрации для каждого из рабочих узлов с помощью следующей команды, и через некоторое время вы получите свой кластер, способный использовать рынок для развертывания приложений.\n`bash\r\nsudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run rancher/rancher-agent:v2.6.3 --server https://10. 0.0.1 --token mpq8cbjjwrj88z4xmf7blqxcfmwdsmq92bmwjpphdkklfckk5hfwc2 --ca-checksum a81944423cbfeeb92be0784edebba1af799735ebc30ba8cbe5cc5f996094f30b --worker\r\n`\r\n\r\n\r\n\r\nЗа последние 3 занятия мы использовали несколько различных способов запуска кластера Kubernetes, в оставшиеся дни мы рассмотрим прикладную сторону платформы, вероятно, самую важную. Мы рассмотрим сервисы и возможность предоставления и использования наших сервисов в Kubernetes.\nМне сказали, что требования к загрузке узлов rancher требуют, чтобы эти виртуальные машины имели 4 ГБ оперативной памяти, иначе они будут работать с ошибками, с тех пор я обновил информацию, так как наши рабочие узлы имели 2 ГБ.\r\nРесурсы\nKubernetes Documentation\r\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\r\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\r\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\r\n\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day54",
            "title": "54. Развертывание приложений Kubernetes",
            "description": null,
            "content": "Развертывание приложений Kubernetes\nТеперь мы, наконец, переходим к реальному развертыванию некоторых приложений в наших кластерах, некоторые говорят, что именно для этого существует Kubernetes - для доставки приложений.\nИдея заключается в том, что мы можем взять наши образы контейнеров и развернуть их в виде стручков в нашем кластере Kubernetes, чтобы воспользоваться преимуществами Kubernetes как контейнерного оркестратора.\r\nРазвертывание приложений в Kubernetes\r\n\r\nСуществует несколько способов развертывания наших приложений в кластере Kubernetes, мы рассмотрим два наиболее распространенных подхода - YAML-файлы и диаграммы Helm.\nДля развертывания приложений мы будем использовать кластер minikube. Мы рассмотрим некоторые из ранее упомянутых компонентов или строительных блоков Kubernetes.\nНа протяжении всего этого раздела и раздела о контейнерах мы говорили об образах и преимуществах Kubernetes, а также о том, как мы можем легко справляться с масштабированием на этой платформе.\nВ этом первом шаге мы просто создадим приложение без статических данных в нашем кластере minikube. Мы будем использовать дефакто стандартное приложение без статики в нашей первой демонстрации nginx. Мы настроим Deployment, который предоставит нам наши стручки, а затем мы также создадим службу, которая позволит нам перейти к простому веб-серверу, размещенному в стручке nginx. Все это будет содержаться в пространстве имен.\nСоздание YAML\r\n\r\nВ первом демо мы хотим определить все, что мы делаем с YAML, мы могли бы создать целый раздел о YAML, но я собираюсь пропустить это и оставить некоторые ресурсы в конце, которые расскажут о YAML более подробно.\nМы можем создать следующее как один YAML-файл или разбить его на части для каждого аспекта нашего приложения, то есть это могут быть отдельные файлы для пространства имен, развертывания и создания сервисов, но в этом файле ниже мы разделили их с помощью --- в одном файле. Вы можете найти этот файл, расположенный здесь\r\n`\r\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: nginx\r\n  \"labels\": {\r\n    \"name\": \"nginx\"\r\n  }\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: nginx-deployment\r\n  namespace: nginx\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  replicas: 1\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      name: nginx\r\n        image: nginx\r\n        ports:\r\n        containerPort: 80\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: nginx-service\r\n  namespace: nginx\r\nspec:\r\n  selector:\r\n    app: nginx-deployment\r\n  ports:\r\n    protocol: TCP\r\n      port: 80\r\n      targetPort: 80\r\n`\r\nПроверка нашего кластера\nПеред тем как развернуть что-либо, мы должны убедиться, что у нас нет существующих пространств имен с названием nginx. Мы можем сделать это, выполнив команду kubectl get namespace, и как вы можете видеть ниже, у нас нет пространства имен с названием nginx.\r\nВремя развернуть наше приложение\r\n\r\nТеперь мы готовы развернуть наше приложение на нашем кластере minikube, этот же процесс будет работать на любом другом кластере Kubernetes.\nНам нужно перейти к расположению нашего yaml файла, а затем мы можем выполнить команду kubectl create -f nginx-stateless-demo.yaml, после чего вы увидите, что было создано 3 объекта, у нас есть пространство имен, развертывание и сервис.\nДавайте снова выполним команду, чтобы увидеть доступные пространства имен в нашем кластере kubectl get namespace, и теперь вы можете увидеть, что у нас есть наше новое пространство имен.\nЕсли мы затем проверим наше пространство имен на наличие стручков с помощью kubectl get pods -n nginx, вы увидите, что у нас есть 1 стручок в готовом и запущенном состоянии.\r\n\r\n\r\n\r\nМы также можем проверить, что наш сервис создан, выполнив команду kubectl get service -n nginx.\nНаконец, мы можем пойти и проверить наше развертывание, развертывание - это то, где и как мы сохраняем нашу желаемую конфигурацию.\nВыше приведено несколько команд, которые стоит знать, но вы также можете использовать kubectl get all -n nginx, чтобы увидеть все, что мы развернули с помощью одного YAML-файла.\nВы можете заметить, что у нас также есть replicaset, в нашем развертывании мы определяем, сколько копий нашего образа мы хотим развернуть. Изначально мы установили значение 1, но если мы хотим быстро масштабировать наше приложение, мы можем сделать это несколькими способами.\nМы можем отредактировать наш файл с помощью команды kubectl edit deployment nginx-deployment -n nginx, которая откроет текстовый редактор в вашем терминале и позволит вам изменить развертывание.\nПосле сохранения в текстовом редакторе в терминале, если не возникло проблем и было использовано правильное форматирование, вы должны увидеть дополнительное развертывание в вашем пространстве имен.\nМы также можем изменить количество реплик с помощью kubectl и команды kubectl scale deployment nginx-deployment --replicas=10 -n nginx.\nМы также можем использовать этот метод для уменьшения масштаба нашего приложения до 1 снова, если захотим, используя любой метод. Я использовал опцию edit, но вы также можете использовать команду scale выше.\nНадеюсь, здесь вы можете увидеть пример использования: не только все очень быстро запускается и выключается, но у нас есть возможность быстро увеличивать и уменьшать масштаб наших приложений. Если бы это был веб-сервер, мы могли бы увеличивать масштаб в периоды загруженности и уменьшать, когда нагрузка снижается.\nРаскрытие нашего приложения\nНо как нам получить доступ к нашему веб-серверу?\nЕсли вы посмотрите выше на наш сервис, вы увидите, что там нет внешнего IP, поэтому мы не можем просто открыть веб-браузер и ожидать, что он будет там волшебным образом. Для доступа у нас есть несколько вариантов.\nClusterIP - IP, который вы видите, является кластерным IP, он находится во внутренней сети кластера. Только объекты внутри кластера могут достичь этого IP.\nNodePort - Выставляет службу на один и тот же порт каждого из выбранных узлов в кластере с помощью NAT.\nLoadBalancer - Создает внешний балансировщик нагрузки в текущем облаке, мы используем minikube, но если вы создали свой собственный кластер Kubernetes, т.е. то, что мы сделали в VirtualBox, вам нужно будет развернуть LoadBalancer, такой как metallb, в вашем кластере, чтобы обеспечить эту функциональность.\nPort-Forward - У нас также есть возможность Port Forward, которая позволяет вам получить доступ и взаимодействовать с внутренними процессами кластера Kubernetes с вашего localhost. На самом деле эта опция используется только для тестирования и поиска неисправностей.\nТеперь у нас есть несколько вариантов на выбор, Minikube имеет некоторые ограничения или отличия от полноценного кластера Kubernetes.\nМы можем просто выполнить следующую команду, чтобы перенаправить порт для доступа, используя нашу локальную рабочую станцию.\nkubectl port-forward deployment/nginx-deployment -n nginx 8090:80.\r\n\r\n\r\n\r\nОбратите внимание, что при выполнении вышеуказанной команды терминал становится непригодным для использования, поскольку он действует как проброс порта на вашу локальную машину и порт.\nНаконец, в новом терминале запустите minikube --profile='mc-demo' service nginx-service --url -n nginx, чтобы создать туннель для нашего сервиса.\r\n\r\n\r\n\r\nОткройте браузер или программу управления и нажмите на ссылку в терминале.\r\nHelm\nHelm - это еще один способ, с помощью которого мы можем развернуть наши приложения. Известен как \"менеджер пакетов для Kubernetes\". Вы можете узнать больше здесь.\r\n\r\nHelm - это менеджер пакетов для Kubernetes. Helm можно считать аналогом yum или apt для Kubernetes. Helm развертывает диаграммы, которые можно представить как упакованное приложение. Это чертеж предварительно сконфигурированных ресурсов приложения, которые можно развернуть в виде одной простой в использовании диаграммы. Затем вы можете развернуть другую версию диаграммы с другим набором конфигураций.\r\n\r\nУ компании есть сайт, на котором можно просмотреть все доступные диаграммы Helm и, конечно, создать свою собственную. Документация также понятна и лаконична и не так пугает, как когда я впервые услышал термин Helm среди всех других новых слов в этой области.\r\n\r\nЗапустить или установить Helm очень просто. Просто. Здесь вы можете найти двоичные файлы и ссылки на загрузку практически для всех дистрибутивов, включая устройства RaspberryPi arm64.\r\n\r\nИли вы можете использовать скрипт установщика, преимущество которого в том, что будет загружена и установлена последняя версия Helm.\r\n\r\n`\r\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\r\n\r\nchmod 700 get_helm.sh\r\n\r\n./get_helm.sh\r\n`\r\n\r\nНаконец, есть также возможность использовать менеджер пакетов для менеджера приложений, homebrew для mac, chocolatey для windows, apt с Ubuntu/Debian, snap и pkg также.\r\n\r\nПока что Helm кажется наиболее удобным способом загрузки и установки различных тестовых приложений в кластере.\nХорошим ресурсом для ссылки здесь будет ArtifactHUB, который является ресурсом для поиска, установки и публикации пакетов Kubernetes. Я также порекомендую KubeApps, который представляет собой пользовательский интерфейс для отображения диаграмм штурвала.\nРесурсы\nKubernetes Documentation\r\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\r\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\r\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\r\n\r\n\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day55",
            "title": "55. State и Ingress в Kubernetes",
            "description": null,
            "content": "State и Ingress в Kubernetes\r\nВ этом заключительном разделе, посвященном Kubernetes, мы рассмотрим State и ingress.\nВсе, о чем мы говорили до сих пор, касается stateless, stateless - это когда нашим приложениям не важно, какую сеть они используют, и им не нужно постоянное хранение данных. В то время как приложения с состоянием, например, базы данных, чтобы такое приложение функционировало правильно, вам нужно убедиться, что стручки могут обращаться друг к другу через уникальную идентификацию, которая не меняется (имена хостов, IP... и т.д.). Примерами stateful-приложений являются кластеры MySQL, Redis, Kafka, MongoDB и другие. В принципе, любое приложение, которое хранит данные.\nStateful Application\nStatefulSets представляют собой набор Pods с уникальными, постоянными идентификаторами и стабильными именами хостов, которые Kubernetes поддерживает независимо от того, где они запланированы. Информация о состоянии и другие устойчивые данные для любого данного StatefulSet Pod хранятся в постоянном дисковом хранилище, связанном с StatefulSet.\r\nРазвертывание против StatefulSet\nРепликация stateful-приложений является более сложной задачей.\nРепликация наших стручков в развертывании (Stateless Application) идентична и взаимозаменяема.\nСоздаем капсулы в случайном порядке со случайными хэшами\nОдин сервис, который балансирует нагрузку на любой стручок.\nКогда дело доходит до StatefulSets или Stateful Applications, вышеописанное становится сложнее.\nНевозможно одновременно создавать и удалять.\nНе может быть случайного обращения.\nреплики Pods не являются идентичными.\r\n\r\nТо, что вы увидите в нашей демонстрации в ближайшее время, заключается в том, что каждая копия имеет свою собственную идентичность. В приложении без статического состояния вы увидите случайные имена. Например, app-7469bbb6d7-9mhxd, в то время как Stateful Application будет иметь имя mongo-0, а затем при масштабировании создаст новую капсулу под названием mongo-1.\nЭти стручки создаются на основе одной и той же спецификации, но они не взаимозаменяемы. Каждая капсула StatefulSet имеет постоянный идентификатор при любом повторном планировании. Это необходимо, потому что когда нам требуются нагрузки с учетом состояния, такие как база данных, где требуется запись и чтение в базу данных, мы не можем иметь две капсулы, пишущие в одно и то же время без осведомленности, так как это приведет к несогласованности данных. Нам нужно убедиться, что в любой момент времени только один из наших стручков записывает данные в базу данных, однако мы можем иметь несколько стручков, читающих эти данные.\nКаждый стручок в StatefulSet будет иметь доступ к своему собственному постоянному тому и копии базы данных для чтения, которая постоянно обновляется с главного сервера. Также интересно отметить, что каждый pod будет хранить свое состояние pod в этом постоянном томе, если mongo-0 умрет, то при инициализации нового pod он возьмет состояние pod, хранящееся в хранилище.\nTLDR; StatefulSets vs Deployments\r\nPredicatable pod name = mongo-0\r\nFixed individual DNS name\nPod Identity - Retain State, Retain Role\r\nReplicating stateful apps is complex\n  There are lots of things you must do:\n      Configure cloning and data synchronisation.\n      Make remote shared storage available.\r\n      Management & backup\r\n\r\nКак сохранять данные в Kubernetes?\nМы упоминали выше, что когда у нас есть приложение с состоянием, нам нужно где-то хранить состояние, и именно здесь возникает необходимость в томе, поскольку из коробки Kubernetes не обеспечивает постоянство данных.\nНам нужен уровень хранения, который не зависит от жизненного цикла стручка. Это хранилище должно быть доступно со всех наших узлов Kubernetes. Хранилище также должно находиться вне кластера Kubernetes, чтобы иметь возможность выжить, даже если кластер Kubernetes потерпит крах.\nПостоянный том\nРесурс кластера (например, процессор и оперативная память) для хранения данных.\r\nСоздается с помощью файла YAML.\nТребуется реальное физическое хранилище (NAS)\r\nВнешняя интеграция в ваш кластер Kubernetes.\r\nВ вашем хранилище могут быть доступны различные типы хранилищ.\nPV не имеют пространства имен\r\nЛокальное хранилище доступно, но оно будет специфично для одного узла в кластере\r\nПерсистентность базы данных должна использовать удаленное хранилище (NAS)\r\nУтверждение о постоянном томе\r\n\r\nПостоянный том, как описано выше, может существовать и быть доступным, но пока он не заявлен приложением, он не используется.\nСоздается с помощью файла YAML\r\nУтверждение постоянного тома используется в конфигурации стручка (атрибут volumes)\r\nPVC находятся в том же пространстве имен, что и pod\r\nТом монтируется в капсулу\r\nСтручки могут иметь несколько различных типов томов (ConfigMap, Secret, PVC).\r\n\r\nДругой способ представить PVs и PVCs заключается в следующем\nPVs создаются администратором Kubernetes Admin\nPVC создаются пользователем или разработчиком приложения.\r\n\r\nУ нас также есть два других типа томов, которые мы не будем подробно описывать, но о которых стоит упомянуть:\nConfigMaps | Secrets\nКонфигурационный файл для вашего стручка.\nФайл сертификата для вашей капсулы.\nStorageClass\nСоздается с помощью файла YAML\r\nПредоставляет постоянные тома динамически, когда PVC заявляет об этом.\nКаждый бэкенд хранилища имеет свой собственный провизор\nБэкенд хранилища определяется в YAML (через атрибут provisioner)\r\nАбстракции базового провайдера хранения\nОпределяет параметры для этого хранилища\r\nВремя просмотра\r\n\r\nВо вчерашней сессии мы рассмотрели создание приложения без статических данных, здесь мы хотим сделать то же самое, но использовать наш кластер minikube для развертывания рабочей нагрузки с статическими данными.\nНапомним команду minikube, которую мы используем, чтобы иметь возможность и аддоны для использования персистентности: minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p mc-demo --kubernetes-version=1.21.2.\nЭта команда использует драйвер csi-hostpath-driver, который дает нам наш класс хранилища, что я покажу позже.\nСборка приложения выглядит следующим образом:\nВы можете найти файл конфигурации YAML для этого приложения здесь pacman-stateful-demo.yaml\r\nКонфигурация класса хранилища\r\n\r\nЕсть еще один шаг, который мы должны выполнить перед началом развертывания нашего приложения, а именно убедиться, что наш класс хранилища (csi-hostpath-sc) является классом по умолчанию. Сначала мы можем проверить это, выполнив команду kubectl get storageclass, но из коробки кластер minikube будет показывать стандартный класс хранения по умолчанию, поэтому мы должны изменить его с помощью следующих команд.\nПервая команда сделает наш класс хранилища csi-hostpath-sc классом по умолчанию.\r\n\r\nkubectl patch storageclass csi-hostpath-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}'}''\r\n\r\nЭта команда удалит аннотацию по умолчанию из стандартного StorageClass.\nkubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}'}''\r\n\r\n\r\n\r\nНачнем с того, что в нашем кластере нет пространства имен pacman. kubectl get namespace\r\n\r\n\r\n\r\nЗатем мы развернем наш YAML-файл. kubectl create -f pacman-stateful-demo.yaml Из этой команды видно, что мы создаем ряд объектов в нашем кластере Kubernetes.\nТеперь у нас есть наше только что созданное пространство имен.\nИз следующего изображения и команды kubectl get all -n pacman видно, что в нашем пространстве имен происходит несколько вещей. У нас есть pods, запускающий наш NodeJS web front end, у нас есть mongo, запускающий нашу backend базу данных. Есть сервисы для pacman и mongo для доступа к этим стручкам. У нас есть развертывание для pacman и statefulset для mongo.\nУ нас также есть наши постоянные тома и утверждения постоянных томов. Выполнив команду kubectl get pv, мы получим наши постоянные тома, не связанные с именами, а выполнив команду kubectl get pvc -n pacman, мы получим наши утверждения постоянных томов, связанные с именами.\r\nИграем в игру | Я имею в виду доступ к нашему критически важному приложению\r\n\r\nПоскольку мы используем Minikube, как уже упоминалось в приложении без статических данных, нам предстоит преодолеть несколько препятствий, когда дело доходит до доступа к нашему приложению. Однако если бы у нас был доступ к ingress или балансировщику нагрузки в нашем кластере, служба настроена на автоматическое получение IP-адреса от него для получения доступа извне. (Вы можете видеть это выше на изображении всех компонентов в пространстве имен pacman).\nВ данном демонстрационном примере мы будем использовать метод проброса портов для доступа к нашему приложению. Открыв новый терминал и выполнив следующую команду kubectl port-forward svc/pacman 9090:80 -n pacman, открыв браузер, мы получим доступ к нашему приложению. Если вы запускаете это в AWS или в определенных местах, то это также сообщит об облаке и зоне, а также о хосте, который равен вашему стручку в Kubernetes, опять же, вы можете оглянуться назад и увидеть это имя стручка на наших скриншотах выше.\nТеперь мы можем пойти и создать высокий балл, который затем будет сохранен в нашей базе данных.\nХорошо, у нас есть высокий балл, но что произойдет, если мы удалим наш mongo-0 pod? Выполнив команду kubectl delete pod mongo-0 -n pacman, я могу удалить его, и если вы все еще находитесь в приложении, вы увидите, что высокий балл недоступен, по крайней мере, в течение нескольких секунд.\nТеперь, если я вернусь в свою игру, я смогу создать новую игру и увидеть свои высокие баллы. Единственный способ поверить мне в это - попробовать и поделиться в социальных сетях своими высокими результатами!\nС развертыванием мы можем увеличить масштаб с помощью команд, которые мы рассматривали в предыдущей сессии, но в частности здесь, особенно если вы хотите устроить огромную вечеринку pacman, вы можете увеличить масштаб с помощью kubectl scale deployment pacman --replicas=10 -n pacman.\r\nIngress объяснено\nПрежде чем мы закончим с Kubernetes, я также хотел бы затронуть важный аспект Kubernetes, и это - ingress.\nЧто такое ingress?\nДо сих пор в наших примерах мы использовали port-forward или определенные команды в minikube, чтобы получить доступ к нашим приложениям, но в производстве это не сработает. Нам нужен лучший способ доступа к нашим приложениям в масштабе с множеством пользователей.\nМы также говорили о возможности использования NodePort, но это опять же должно быть только в тестовых целях.\nIngress дает нам лучший способ открыть наши приложения, он позволяет нам определить правила маршрутизации в нашем кластере Kubernetes.\nДля ingress мы создадим запрос на внутреннюю службу нашего приложения.\nКогда вам нужен ingress?\nЕсли вы используете облачный провайдер, управляемое предложение Kubernetes, то, скорее всего, у них будет своя опция ingress для вашего кластера или они предоставят вам свой собственный балансировщик нагрузки. Вам не придется реализовывать это самостоятельно, что является одним из преимуществ управляемого Kubernetes.\nЕсли вы управляете собственным кластером, вам необходимо настроить точку входа.\nНастройка Ingress на Minikube\nНа моем конкретном запущенном кластере под названием mc-demo я могу выполнить следующую команду, чтобы включить ingress на моем кластере.\nminikube --profile='mc-demo' addons enable ingress.\r\n\r\n\r\n\r\nЕсли теперь мы проверим наши пространства имен, то увидим, что у нас есть новое пространство имен ingress-nginx. kubectl get ns\r\n\r\n\r\n\r\n\r\nТеперь мы должны создать YAML-конфигурацию ingress для запуска нашего сервиса Pacman. Я добавил этот файл в репозиторий pacman-ingress.yaml.\r\n\r\nЗатем мы можем создать его в нашем пространстве имен ingress с помощью kubectl create -f pacman-ingress.yaml.\nЗатем, если мы запустим kubectl get ingress -n pacman\nЗатем мне говорят, что поскольку мы используем minikube, работающий на WSL2 в Windows, мы должны создать туннель minikube, используя minikube tunnel --profile=mc-demo.\nНо я все еще не могу получить доступ к 192.168.49.2 и играть в свою игру pacman.\nЕсли у кого-нибудь есть или есть возможность заставить это работать под Windows и WSL, я буду благодарен за отзывы. Я подниму вопрос об этом в репозитории и вернусь к нему, как только у меня появится время и исправление.\nUPDATE: Мне кажется, что этот блог помогает определить причину того, что игра не работает на WSL Configuring Ingress to run Minikube on WSL2 using Docker runtime\r\nРесурсы\nKubernetes StatefulSet simply explained\r\nKubernetes Volumes explained\r\nKubernetes Ingress Tutorial for Beginners\r\nKubernetes Documentation\r\n[TechWorld with Nana - Kubernetes Tutorial for Beginners [FULL COURSE in 4 Hours]](https://www.youtube.com/watch?v=X48VuDVv0do)\r\nTechWorld with Nana - Kubernetes Crash Course for Absolute Beginners\r\nKunal Kushwaha - Kubernetes Tutorial for Beginners | What is Kubernetes? Architecture Simplified!\r\n\r\nНа этом мы завершаем раздел Kubernetes. Существует так много дополнительных материалов, которые мы могли бы осветить на тему Kubernetes, и 7 дней дают нам базовые знания, но есть люди, которые проходят 100DaysOfKubernetes, где вы можете погрузиться в самую гущу событий.\nДалее мы рассмотрим инфраструктуру как код и ту важную роль, которую она играет с точки зрения DevOps.",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day56",
            "title": "56. Обзор IaC",
            "description": null,
            "content": "Обзор IaC\r\n\r\nЛюди совершают ошибки! Автоматизация - это путь к успеху!\nКак вы строите свои системы сегодня?\nКаков был бы ваш план, если бы вы потеряли все, физические машины, виртуальные машины, облачные виртуальные машины, облачные PaaS и т.д. и т.п.?\nСколько времени у вас уйдет на замену всего?\nИнфраструктура как код предоставляет решение, позволяющее сделать это и одновременно протестировать, не путайте это с резервным копированием и восстановлением, но что касается вашей инфраструктуры и сред, ваших платформ, мы должны быть в состоянии раскрутить их и обращаться с ними как со скотом и домашними животными.\nTLDR; заключается в том, что мы можем использовать код для восстановления всей нашей среды.\nЕсли мы также вспомним, что с самого начала мы говорили о DevOps в целом - это способ преодоления барьеров для безопасной и быстрой доставки систем в производство.\nInfrastructure as code помогает нам поставлять системы, мы говорили о множестве процессов и инструментов. IaC предлагает нам больше инструментов, с которыми мы должны быть знакомы, чтобы обеспечить эту часть процесса.\nВ этом разделе мы сосредоточимся на инфраструктуре как коде. Вы также можете услышать упоминание этого термина как \"инфраструктура из кода\" или \"конфигурация как код\". Я думаю, что наиболее известным термином является Инфраструктура как код.\nДомашние животные против крупного рогатого скота\nЕсли мы посмотрим на до DevOps, то при необходимости создания нового приложения мы должны были подготовить наши серверы вручную.\nРазвернуть виртуальные машины | физические серверы и установить операционную систему\r\nНастроить сеть\nСоздать таблицы маршрутизации\nУстановить программное обеспечение и обновления\nНастроить программное обеспечение\nУстановка базы данных\nЭто ручной процесс, выполняемый системными администраторами. Чем больше приложение, тем больше ресурсов и серверов требуется, тем больше ручных усилий потребуется для создания этих систем. Это потребует огромного количества человеческих усилий и времени, но, кроме того, как компания, вы должны будете заплатить за эти ресурсы, чтобы создать эту среду. Как я уже говорил в начале раздела \"Люди совершают ошибки! Автоматизация - это путь к успеху!\".\r\n\r\nПосле вышеупомянутой фазы начальной установки вам предстоит обслуживание этих серверов.\nОбновление версий\nРазвертывание новых релизов\nУправление данными\nВосстановление приложений\nДобавление, удаление и масштабирование серверов\nКонфигурация сети\r\n\r\nДобавьте сюда сложность нескольких сред тестирования и разработки.\nИменно здесь на помощь приходит Infrastructure as Code. Выше было время, когда мы заботились об этих серверах, как о домашних животных, люди даже называли их домашними именами или, по крайней мере, давали им какие-то имена, потому что они должны были находиться рядом какое-то время, они должны были стать частью \"семьи\" на какое-то время.\nС Infrastructure as Code у нас есть возможность автоматизировать все эти задачи от конца до конца. Инфраструктура как код - это концепция, и есть инструменты, которые выполняют автоматическое обеспечение инфраструктуры. На данный момент, если с сервером случается что-то плохое, вы выбрасываете его и запускаете новый. Этот процесс автоматизирован, и сервер точно такой же, как определено в коде. В этот момент нам не важно, как они называются, они находятся в поле и служат своей цели до тех пор, пока их больше нет в поле, и нам нужно заменить их либо из-за сбоя, либо из-за обновления части или всего нашего приложения.\nЭто может быть использовано практически во всех платформах, виртуализации, облачных рабочих нагрузках, а также в облачной нативной инфраструктуре, такой как Kubernetes и контейнеры.\nОбеспечение инфраструктуры\nНе все IaC охватывают все перечисленное ниже, вы увидите, что инструмент, который мы будем использовать в этом разделе, охватывает только первые две области; Terraform - это тот инструмент, который мы будем рассматривать, и он позволяет нам начать с нуля и определить в коде, как должна выглядеть наша инфраструктура, а затем развернуть ее, он также позволит нам управлять этой инфраструктурой и первоначально развернуть приложение, но в этот момент он потеряет контроль над приложением, и здесь на помощь приходит следующий раздел, и что-то вроде Ansible как инструмент управления конфигурацией может работать лучше на этом фронте.\nБез забегания вперед такие инструменты, как chef, puppet и ansible, лучше всего подходят для начальной установки приложений, а затем для управления этими приложениями и их конфигурацией.\nПервоначальная установка и настройка программного обеспечения\nРазвертывание новых серверов\nКонфигурация сети\nСоздание балансировщиков нагрузки\nКонфигурация на уровне инфраструктуры\r\nКонфигурация инфраструктуры с провизией\nУстановка приложения на серверы\nПодготовьте серверы для развертывания приложения.\nРазвертывание приложения\nРазвертывание и управление приложением\nЭтап обслуживания\r\nОбновления программного обеспечения\nРеконфигурация\nРазличия инструментов IaC\r\n\r\nДекларативный и процедурный\nПроцедурный\nПошаговая инструкция\nСоздайте сервер > Добавьте сервер > Внесите это изменение\nДекларативный\nобъявить конечный результат\n2 сервера\nИзменяемые (домашние животные) против неизменяемых (крупный рогатый скот)\r\n\r\nМутабельный\nИзменение вместо замены\r\nКак правило, долгоживущие\nНеизменяемые\r\nЗамена вместо изменения\r\nВозможно, недолговечна\nИменно поэтому у нас есть множество различных вариантов Infrastructure as Code, потому что не существует одного инструмента, который бы управлял всеми.\nМы будем в основном использовать terraform и работать с ним, поскольку это лучший способ начать видеть преимущества инфраструктуры как кода в действии. Практическая работа - это также лучший способ приобрести навыки, так как вы будете писать код.\nДалее мы начнем изучать Terraform со 101-го урока, прежде чем приступим к практическому использованию.\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day57",
            "title": "57. Введение в Terraform",
            "description": null,
            "content": "\r\n\"Terraform - это инструмент для безопасного и эффективного создания, изменения и управления версиями инфраструктуры\".\n«Приведенная выше цитата взята из HashiCorp, HashiCorp - это компания, стоящая за Terraform.\n\"Terraform - это программный инструмент \"инфраструктура как код\" с открытым исходным кодом, который обеспечивает последовательный рабочий процесс CLI для управления сотнями облачных сервисов. Terraform кодирует облачные API в декларативные конфигурационные файлы\".\r\n\r\nУ HashiCorp есть отличный ресурс HashiCorp Learn, который охватывает все их продукты и дает несколько отличных демонстрационных примеров, когда вы пытаетесь достичь чего-то с помощью инфраструктуры как кода.\nВсе облачные провайдеры и локальные платформы обычно предоставляют нам доступ к консолям управления, которые позволяют нам создавать наши ресурсы с помощью пользовательского интерфейса, обычно эти платформы также предоставляют доступ к CLI или API для создания тех же ресурсов, но с API у нас есть возможность быстрого предоставления ресурсов.\nИнфраструктура как код позволяет нам подключаться к этим API для развертывания наших ресурсов в нужном состоянии.\nНиже перечислены и другие инструменты, но они не являются исключительными или исчерпывающими. Если у вас есть другие инструменты, пожалуйста, поделитесь с нами через PR.\n| Cloud Specific                  | Cloud Agnostic |\n| ------------------------------- | -------------- |\r\n| AWS CloudFormation              | Terraform      |\n| Azure Resource Manager          | Pulumi         |\n| Google Cloud Deployment Manager |                |\nЭто еще одна причина, почему мы используем Terraform, мы хотим быть независимыми от облаков и платформ, которые мы хотим использовать для наших демонстраций, а также в целом.\nОбзор Terraform\nTerraform - это инструмент, ориентированный на обеспечение, Terraform - это CLI, который предоставляет возможности для обеспечения сложных инфраструктурных сред. С помощью Terraform мы можем определить сложные требования к инфраструктуре, существующей локально или удаленно (облако). Terraform позволяет нам не только создавать вещи на начальном этапе, но и поддерживать и обновлять эти ресурсы в течение всего срока их службы.\nЗдесь мы рассмотрим основные моменты, но для получения более подробной информации и множества ресурсов вы можете посетить сайт terraform.io.\r\nЗапись\r\n\r\nTerraform позволяет нам создавать декларативные конфигурационные файлы, которые будут создавать наше окружение. Файлы пишутся с помощью языка HashiCorp Configuration Language (HCL), который позволяет кратко описывать ресурсы с помощью блоков, аргументов и выражений. Мы, конечно, будем подробно рассматривать их при развертывании виртуальных машин, контейнеров и в Kubernetes.\nПлан\r\n\r\nВозможность проверить, что вышеуказанные конфигурационные файлы развернут то, что мы хотим видеть, используя определенные функции terraform cli, чтобы иметь возможность протестировать этот план перед развертыванием чего-либо или изменением чего-либо. Помните, что Terraform - это инструмент для продолжения вашей инфраструктуры, если вы хотите изменить аспект вашей инфраструктуры, вы должны сделать это через terraform, чтобы все это было зафиксировано в коде.\nПрименить\r\n\r\nОчевидно, что когда вы будете довольны, вы сможете применить эту конфигурацию к множеству провайдеров, доступных в Terraform. Вы можете увидеть большое количество доступных провайдеров здесь.\r\n\r\nЕще одна вещь, о которой следует упомянуть, это то, что также доступны модули, и это похоже на образы контейнеров в том, что эти модули были созданы и выложены в открытый доступ, так что вам не придется создавать их снова и снова, просто используйте лучшую практику развертывания определенного ресурса инфраструктуры одинаковым способом везде. Вы можете найти доступные модули здесь.\r\n\r\nРабочий процесс Terraform выглядит следующим образом: (взято с сайта terraform)\r\nTerraform vs Vagrant\r\n\r\nВо время этого испытания мы использовали Vagrant, который является еще одним инструментом с открытым исходным кодом от Hashicorp, сконцентрированным на средах разработки.\nVagrant - это инструмент, ориентированный на управление средами разработки.\r\n\r\nTerraform - это инструмент для создания инфраструктуры.\nОтличное сравнение этих двух инструментов можно найти здесь на официальном сайте Hashicorp\r\nУстановка Terraform\nВ установке Terraform нет ничего сложного.\nTerraform является кроссплатформенным, и вы можете видеть ниже на моей Linux машине у нас есть несколько вариантов загрузки и установки CLI\nИспользование arkade для установки Terraform, arkade - это удобный инструмент для получения необходимых инструментов, приложений и clis на вашу систему. Простая команда arkade get terraform позволит обновить terraform, если он доступен, или эта же команда также установит Terraform CLI\r\n\r\n\r\n\r\nМы собираемся больше узнать о HCL, а также начать использовать Terraform для создания некоторых инфраструктурных ресурсов на различных платформах.\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day58",
            "title": "58. Язык конфигурации HashiCorp (HCL)",
            "description": null,
            "content": "Язык конфигурации HashiCorp (HCL)\r\n\r\nПрежде чем мы начнем создавать вещи с помощью Terraform, мы должны немного погрузиться в язык HashiCorp Configuration Language (HCL). До сих пор в ходе нашей задачи мы рассмотрели несколько различных языков скриптов и программирования, и вот еще один. Мы затронули язык программирования Go, затем скрипты bash, мы даже немного затронули python, когда дело дошло до автоматизации сети.\r\n\r\nТеперь мы должны рассмотреть язык конфигурации HashiCorp (HCL), если вы впервые видите этот язык, он может показаться немного пугающим, но он довольно прост и очень мощный.\r\n\r\nПо мере продвижения по этому разделу мы будем использовать примеры, которые мы можем запустить локально на нашей системе, независимо от того, какую ОС вы используете, мы будем использовать virtualbox, хотя и не инфраструктурную платформу, которую вы обычно используете с Terraform. Тем не менее, запуск этого локально, он бесплатный и позволит нам достичь того, что мы ищем в этой заметке. Мы также можем расширить концепцию этого поста на docker или Kubernetes.\nВ целом, вы будете или должны использовать Terraform для развертывания инфраструктуры в публичном облаке (AWS, Google, Microsoft Azure), а также в средах виртуализации, таких как (VMware, Microsoft Hyper-V, Nutanix AHV). В публичном облаке Terraform позволяет нам делать гораздо больше, чем просто автоматическое развертывание виртуальных машин, мы можем создавать всю необходимую инфраструктуру, такую как рабочие нагрузки PaaS, и все необходимые сетевые ресурсы, такие как VPC и группы безопасности.\nВ Terraform есть два важных аспекта: код, который мы рассмотрим в этой статье, и состояние. Оба этих аспекта вместе можно назвать ядром Terraform. Затем у нас есть среда, в которую мы хотим обратиться и развернуть, которая выполняется с помощью провайдеров Terraform, кратко упомянутых на прошлом занятии, но у нас есть провайдеры AWS, есть провайдеры Azure и т.д. Их сотни. Их сотни.\nБазовое использование Terraform\r\n\r\nДавайте посмотрим на файл Terraform .tf, чтобы увидеть, как они создаются. Первый пример, который мы рассмотрим, будет кодом для развертывания ресурсов на AWS, для этого также потребуется установить AWS CLI на вашей системе и настроить его для вашей учетной записи.\nProviders\r\n\r\nВ верхней части нашей файловой структуры .tf, обычно называемой main.tf, по крайней мере до тех пор, пока мы не сделаем все более сложным. Здесь мы определим провайдеров, о которых мы упоминали ранее. Наш источник провайдера aws, как вы видите, hashicorp/aws, это означает, что провайдер поддерживается или был опубликован самой компанией hashicorp. По умолчанию вы будете ссылаться на провайдеров, доступных в Terraform Registry, у вас также есть возможность написать свои собственные провайдеры и использовать их локально или самостоятельно опубликовать в Terraform Registry.\r\n\r\n`terraform\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp/aws\"\r\n      version = \"~> 3.0\"\r\n    }\r\n  }\r\n}\r\n`\r\n\r\nЗдесь мы также можем добавить регион, чтобы определить, какой регион AWS мы хотим предоставить, мы можем сделать это, добавив следующее:\r\n\r\n`\r\nprovider \"aws\" {\r\n  region = \"ap-southeast-1\" //region where resources need to be deployed\r\n}\r\n`\r\nResources\nДругой важный компонент конфигурационного файла terraform, который описывает один или несколько объектов инфраструктуры, таких как EC2, Load Balancer, VPC и т.д.\r\n\r\nБлок ресурсов объявляет ресурс заданного типа (\"aws_instance\") с заданным локальным именем (\"90daysofdevops\").\nТип ресурса и имя вместе служат идентификатором для данного ресурса.\r\n\r\n`\r\nresource \"aws_instance\" \"90daysofdevops\" {\r\n  ami               = data.aws_ami.instance_id.id\r\n  instance_type     = \"t2.micro\"\r\n  availability_zone = \"us-west-2a\"\r\n  security_groups   = [aws_security_group.allow_web.name]\r\n  user_data         = Deployed via Terraform\r\n\r\n\" | sudo tee /var/www/html/index.html\r\n        EOF\r\n  tags = {\r\n    Name = \"Created by Terraform\"\r\n  }\r\n}\r\n`\r\n\r\nИз вышеприведенного видно, что мы также запускаем обновление yum и устанавливаем httpd в наш экземпляр ec2.\nЕсли мы теперь посмотрим на полный файл main.tf, он может выглядеть примерно так.\r\n\r\n`terraform\r\nterraform {\r\n  required_providers {\r\n    aws = {\r\n      source  = \"hashicorp/aws\"\r\n      version = \"~> 3.27\"\r\n    }\r\n  }\r\n\r\n  required_version = \">= 0.14.9\"\r\n}\r\n\r\nprovider \"aws\" {\r\n  profile = \"default\"\r\n  region  = \"us-west-2\"\r\n}\r\n\r\nresource \"aws_instance\" \"90daysofdevops\" {\r\n  ami           = \"ami-830c94e3\"\r\n  instance_type = \"t2.micro\"\r\n  availability_zone = \"us-west-2a\"\r\n    user_data         = Deployed via Terraform\r\n\r\n\" | sudo tee /var/www/html/index.html\r\n        EOF\r\n  tags = {\r\n    Name = \"Created by Terraform\"\r\n\r\n\r\n  tags = {\r\n    Name = \"ExampleAppServerInstance\"\r\n  }\r\n}\r\n`\r\n\r\nПриведенный выше код позволит развернуть очень простой веб-сервер в качестве экземпляра ec2 в AWS. Самое замечательное в этой и любой другой подобной конфигурации то, что мы можем повторить ее и каждый раз получать один и тот же результат. Кроме вероятности того, что я испортил код, нет никакого взаимодействия с человеком.\nМы можем рассмотреть суперпростой пример, который вы, скорее всего, никогда не будете использовать, но давайте все равно пошутим. Как и во всех хороших скриптах и языках программирования, мы должны начать со скрипта приветствия мира.\r\n\r\n`terraform\r\nterraform {\r\nThis module is now only being tested with Terraform 0.13.x. However, to make upgrading easier, we are setting\r\n0.12.26 as the minimum version, as that version added support for required_providers with source URLs, making it\r\nforwards compatible with 0.13.x code.\r\n  required_version = \">= 0.12.26\"\r\n}\r\nwebsite::tag::1:: The simplest possible Terraform module: it just outputs \"Hello, World!\"\r\noutput \"hello_world\" {\r\n  value = \"Hello, 90DaysOfDevOps from Terraform\"\r\n}\r\n`\r\nВы найдете этот файл в папке IAC в разделе hello-world, но из коробки он не будет просто работать, есть несколько команд, которые необходимо выполнить, чтобы использовать наш код терраформы.\nВ терминале перейдите в папку, где был создан файл main.tf, он может быть из этого репозитория или вы можете создать новый, используя код выше.\nНаходясь в этой папке, выполните команду terraform init.\nМы должны выполнить эту команду в любой директории, где у нас есть или перед запуском любого кода terraform. Инициализация каталога конфигурации загружает и устанавливает провайдеров, определенных в конфигурации, в данном случае у нас нет провайдеров, но в примере выше это загрузит провайдера aws для этой конфигурации.\nСледующей командой будет terraform plan.\nКоманда terraform plan создает план выполнения, который позволяет вам предварительно просмотреть изменения, которые Terraform планирует внести в вашу инфраструктуру.\r\n\r\nВы можете видеть ниже, что на нашем примере hello-world мы увидим результат, если бы это был экземпляр AWS ec2, мы бы увидели все шаги, которые мы будем создавать.\nНа данном этапе мы инициализировали наш репозиторий, загрузили провайдеров, где это необходимо, запустили тестовый проход, чтобы убедиться, что это то, что мы хотим видеть, теперь мы можем запустить и развернуть наш код.\nКоманда terraform apply позволяет нам это сделать, в нее встроена мера безопасности, и это снова даст вам представление о том, что произойдет, что требует от вас ответа \"да\", чтобы продолжить.\nКогда мы вводим \"да\", чтобы ввести значение, наш код развертывается. Очевидно, это не так интересно, но вы можете видеть, что у нас есть вывод, который мы определили в нашем коде.\nТеперь мы ничего не развернули, мы ничего не добавили, не изменили и не уничтожили, но если бы мы это сделали, то мы бы увидели, что это также указано выше. Однако если мы что-то развернули и хотим избавиться от всего, что развернули, мы можем использовать команду terraform destroy. Опять же, это имеет ту безопасность, когда вы должны ввести \"да\", хотя вы можете использовать --auto-approve в конце ваших команд apply и destroy, чтобы обойти это ручное вмешательство. Но я бы посоветовал использовать это сокращение только в процессе обучения и тестирования, так как все будет исчезать иногда быстрее, чем было создано.\nТаким образом, мы рассмотрели всего 4 команды из Terraform CLI.\r\n\r\nterraform init = подготовить папку проекта с провайдерами\nterraform plan = показать, что будет создано, изменено во время следующей команды на основе нашего кода.\nterraform apply = развернет ресурсы, определенные в нашем коде.\nterraform destroy = уничтожит ресурсы, которые мы создали в нашем проекте.\r\n\r\nМы также рассмотрели два важных аспекта наших кодовых файлов.\nproviders = как terraform общается с конечной платформой через API-интерфейсы\nresources = что именно мы хотим развернуть с помощью кода\r\n\r\nЕще одна вещь, которую следует отметить, когда мы запускаем terraform init, посмотрите на дерево в папке до и после, чтобы увидеть, что происходит и где мы храним провайдеры и модули.\nTerraform state\nНам также необходимо знать о файле состояния, который создается также внутри нашей директории, и для этого примера hello world наш файл состояния прост. Это JSON-файл, который является представлением мира в соответствии с Terraform. Состояние будет радостно демонстрировать ваши конфиденциальные данные, поэтому будьте осторожны и в качестве лучшей практики помещайте файлы .tfstate в папку .gitignore перед загрузкой на GitHub.\nПо умолчанию файл состояния, как вы видите, находится в том же каталоге, что и код вашего проекта, но его можно хранить и удаленно. В производственной среде это, скорее всего, будет общее место, например, ведро S3.\nДругим вариантом может быть Terraform Cloud, это платная управляемая услуга. (Бесплатно до 5 пользователей)\r\n\r\nПлюсы хранения состояния в удаленном месте заключаются в том, что мы получаем:\n`\r\n{\r\n  \"version\": 4,\r\n  \"terraform_version\": \"1.1.6\",\r\n  \"serial\": 1,\r\n  \"lineage\": \"a74296e7-670d-0cbb-a048-f332696ca850\",\r\n  \"outputs\": {\r\n    \"hello_world\": {\r\n      \"value\": \"Hello, 90DaysOfDevOps from Terraform\",\r\n      \"type\": \"string\"\r\n    }\r\n  },\r\n  \"resources\": []\r\n}\r\n`\r\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day59",
            "title": "59. Создание виртуальной машины с помощью Terraform",
            "description": null,
            "content": "Создание виртуальной машины с помощью Terraform и переменных\r\n\r\nВ этой сессии мы будем создавать виртуальную машину или две виртуальные машины с помощью Terraform внутри VirtualBox. Это не совсем обычно, VirtualBox - это вариант виртуализации рабочих станций, и на самом деле это не было бы вариантом использования Terraform, но я сейчас нахожусь на высоте 36 000 футов в воздухе, и как бы я ни развертывал ресурсы публичного облака так высоко в облаках, гораздо быстрее сделать это локально на моем ноутбуке.\nЧисто демонстрационная цель, но концепция та же, мы собираемся иметь наш желаемый код конфигурации состояния, а затем мы собираемся запустить его против провайдера virtualbox. В прошлом мы использовали здесь vagrant, и я рассказал о различиях между vagrant и terraform в начале раздела.\nСоздание виртуальной машины в VirtualBox\nПервое, что мы сделаем, это создадим новую папку под названием virtualbox, затем мы можем создать файл virtualbox.tf, в котором мы определим наши ресурсы. Приведенный ниже код, который можно найти в папке VirtualBox под названием virtualbox.tf, создаст 2 виртуальные машины в Virtualbox.\nВы можете узнать больше о сообществе провайдера Virtualbox здесь\r\n\r\n`terraform\r\nterraform {\r\n  required_providers {\r\n    virtualbox = {\r\n      source = \"terra-farm/virtualbox\"\r\n      version = \"0.2.2-alpha.1\"\r\n    }\r\n  }\r\n}\r\nВ настоящее время нет никаких опций конфигурации для самого провайдера.\r\nresource \"virtualbox_vm\" \"node\" {\r\n  count     = 2\r\n  name      = format(\"node-%02d\", count.index + 1)\r\n  image     = \"https://app.vagrantup.com/ubuntu/boxes/bionic64/versions/20180903.0.0/providers/virtualbox.box\"\r\n  cpus      = 2\r\n  memory    = \"512 mib\"\r\n\r\n  network_adapter {\r\n    type           = \"hostonly\"\r\n    host_interface = \"vboxnet1\"\r\n  }\r\n}\r\n\r\noutput \"IPAddr\" {\r\n  value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 1)\r\n}\r\n\r\noutput \"IPAddr_2\" {\r\n  value = element(virtualbox_vm.node.*.network_adapter.0.ipv4_address, 2)\r\n}\r\n`\r\n\r\nТеперь, когда мы определили наш код, мы можем выполнить terraform init для нашей папки, чтобы загрузить провайдер для virtualbox.\nОчевидно, что в вашей системе также должен быть установлен virtualbox. Затем мы можем запустить terraform plan, чтобы посмотреть, что наш код создаст для нас. Затем следует terraform apply. На рисунке ниже показан завершенный процесс.\r\n\r\n\r\n\r\nТеперь в Virtualbox вы увидите две виртуальные машины.\nИзменение конфигурации\nДавайте добавим еще один узел в наше развертывание. Мы можем просто изменить строку count, чтобы показать новое желаемое количество узлов. Когда мы запустим нашу terraform apply, она будет выглядеть примерно так, как показано ниже.\nПосле завершения работы в virtualbox вы можете увидеть, что у нас теперь есть 3 узла.\nКогда мы закончим, мы можем очистить все это с помощью команды terraform destroy, и наши машины будут удалены.\nПеременные и выходные данные\nМы упоминали о выводах, когда выполняли пример hello-world на прошлом занятии. Но здесь мы можем остановиться на этом более подробно.\nНо есть много других переменных, которые мы можем использовать здесь, также есть несколько различных способов, которыми мы можем определить переменные.\nМы можем вручную ввести наши переменные с помощью команды terraform plan или terraform apply.\r\n\r\nМы можем определить их в .tf-файле внутри блока\nМы можем использовать переменные окружения в нашей системе, используя TF_VAR_NAME в качестве формата.\nЯ предпочитаю использовать файл terraform.tfvars в папке нашего проекта.\nСуществует опция *auto.tfvars файла\nили мы можем определить, когда запускаем terraform plan или terraform apply с помощью var или var-file.\nПорядок определения переменных будет начинаться снизу вверх.\nМы также упоминали, что файл состояния будет содержать конфиденциальную информацию. Мы можем определить нашу чувствительную информацию как переменную и определить ее как чувствительную.\n`\r\nvariable \"some resource\"  {\r\n    description = \"something important\"\r\n    type: string\r\n    sensitive = true\r\n\r\n}\r\n`\r\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day60",
            "title": "60. Контейнеры, провайдеры и модули Docker",
            "description": null,
            "content": "Контейнеры и модули Docker\nВчера мы развернули виртуальную машину с помощью Terraform в нашей локальной среде FREE virtualbox. В этом разделе мы собираемся развернуть контейнер Docker с некоторой конфигурацией в нашей локальной среде Docker.\nDocker Demo\r\n\r\nДля начала мы используем приведенный ниже блок кода, суть которого заключается в том, что мы хотим развернуть простое веб-приложение в docker и опубликовать его, чтобы оно было доступно в нашей сети. Мы будем использовать nginx и сделаем его доступным извне на нашем ноутбуке через localhost и порт 8000. Мы используем провайдера docker из сообщества, и вы можете видеть образ docker, который мы используем, также указанный в нашей конфигурации.\r\n\r\n`terraform\r\nterraform {\r\n  required_providers {\r\n    docker = {\r\n      source  = \"kreuzwerker/docker\"\r\n      version = \"2.16.0\"\r\n    }\r\n  }\r\n}\r\n\r\nprovider \"docker\" {}\r\n\r\nresource \"docker_image\" \"nginx\" {\r\n  name         = \"nginx:latest\"\r\n  keep_locally = false\r\n}\r\n\r\nresource \"docker_container\" \"nginx\" {\r\n  image = docker_image.nginx.latest\r\n  name  = \"tutorial\"\r\n  ports {\r\n    internal = 80\r\n    external = 8000\r\n  }\r\n}\r\n`\r\n\r\nПервой задачей является использование команды terraform init для загрузки провайдера на нашу локальную машину.\nЗатем мы запускаем команду terraform apply, а затем docker ps, и вы можете увидеть, что у нас есть запущенный контейнер.\nЕсли мы откроем браузер, то перейдем по адресу http://localhost:8000/ и увидим, что у нас есть доступ к нашему контейнеру NGINX.\nВы можете узнать больше информации о Docker Provider.\nВыше приведена очень простая демонстрация того, что можно сделать с помощью Terraform плюс Docker и как мы теперь можем управлять этим в состоянии Terraform. Мы рассматривали docker compose в разделе о контейнерах, и есть небольшое пересечение между этим, инфраструктурой как код, а также Kubernetes.\nДля демонстрации того, как Terraform может справиться с более сложными задачами, мы возьмем файл docker compose для wordpress и mysql, который мы создали с помощью docker compose, и поместим его в Terraform. Вы можете найти docker-wordpress.tf\r\n\r\n`tf\r\nterraform {\r\n  required_providers {\r\n    docker = {\r\n      source  = \"kreuzwerker/docker\"\r\n      version = \"2.16.0\"\r\n    }\r\n  }\r\n}\r\n\r\nprovider \"docker\" {}\r\n\r\nvariable wordpress_port {\r\n  default = \"8080\"\r\n}\r\n\r\nresource \"docker_volume\" \"db_data\" {\r\n  name = \"db_data\"\r\n}\r\n\r\nresource \"docker_network\" \"wordpress_net\" {\r\n  name = \"wordpress_net\"\r\n}\r\n\r\nresource \"docker_container\" \"db\" {\r\n  name  = \"db\"\r\n  image = \"mysql:5.7\"\r\n  restart = \"always\"\r\n  network_mode = \"wordpress_net\"\r\n  env = [\r\n     \"MYSQL_ROOT_PASSWORD=wordpress\",\r\n     \"MYSQL_PASSWORD=wordpress\",\r\n     \"MYSQL_USER=wordpress\",\r\n     \"MYSQL_DATABASE=wordpress\"\r\n  ]\r\n  mounts {\r\n    type = \"volume\"\r\n    target = \"/var/lib/mysql\"\r\n    source = \"db_data\"\r\n    }\r\n}\r\n\r\nresource \"docker_container\" \"wordpress\" {\r\n  name  = \"wordpress\"\r\n  image = \"wordpress:latest\"\r\n  restart = \"always\"\r\n  network_mode = \"wordpress_net\"\r\n  env = [\r\n    \"WORDPRESS_DB_HOST=db:3306\",\r\n    \"WORDPRESS_DB_USER=wordpress\",\r\n    \"WORDPRESS_DB_NAME=wordpress\",\r\n    \"WORDPRESS_DB_PASSWORD=wordpress\"\r\n  ]\r\n  ports {\r\n    internal = \"80\"\r\n    external = \"${var.wordpress_port}\"\r\n  }\r\n}\r\n`\r\n\r\nМы снова помещаем это в новую папку и затем запускаем команду terraform init, чтобы извлечь необходимые нам провайдеры.\nЗатем мы запускаем команду terraform apply и смотрим на вывод docker ps, мы должны увидеть наши только что созданные контейнеры.\nЗатем мы можем перейти к нашему фронт-энду WordPress. Точно так же, как мы проходили этот процесс с docker-compose в разделе о контейнерах, теперь мы можем выполнить установку, и наши посты wordpress будут жить в нашей базе данных MySQL.\nОчевидно, что теперь мы рассмотрели контейнеры и Kubernetes в некоторых деталях, мы, вероятно, знаем, что это подходит для тестирования, но если бы вы действительно собирались запустить веб-сайт, вы бы не стали делать это только с помощью контейнеров и рассмотрели бы использование Kubernetes для достижения этой цели, Далее мы рассмотрим использование Terraform с Kubernetes.\nProvisioners\nПровайдеры существуют для того, чтобы если что-то не может быть декларировано, у нас был способ разобрать это для нашего развертывания.\nЕсли у вас нет другой альтернативы, и добавление такой сложности в ваш код - это то, что вам нужно, то вы можете сделать это, выполнив что-то похожее на следующий блок кода.\n`\r\nресурс \"docker_container\" \"db\" {  # ...\r\n\r\n  provisioner \"local-exec\" {\r\n    command = \"echo The server's IP address is ${self.private_ip}\"\r\n  }\r\n}\r\n\r\n`\r\n\r\nУдаленный исполнительный провайдер вызывает скрипт на удаленном ресурсе после его создания. Это может быть использовано для чего-то специфического для ОС, или это может быть использовано для обертывания в инструмент управления конфигурацией. Хотя заметьте, что некоторые из них мы уже рассмотрели в собственных провайдерах.\r\n\r\n\r\nСредство подготовки удаленных исполняемых файлов вызывает скрипт на удаленном ресурсе после его создания. Это может быть использовано для чего-то определенного для ОС или может быть использовано для включения инструмента управления конфигурацией. Хотя обратите внимание, что у нас есть некоторые из них, покрытые их собственными провизорами.\r\nПодробнее о провизорах](https://www.terraform.io/language/resources/provisioners/syntax)\t\r\nfile\r\nlocal-exec\nremote-exec\nvendor\n    ansible\r\n    chef\r\n    puppet\nМодули\nМодули - это контейнеры для нескольких ресурсов, которые используются вместе. Модуль состоит из коллекции файлов .tf в одном каталоге.\nМодули - это хороший способ разделить ресурсы инфраструктуры, а также возможность использовать уже созданные сторонние модули, чтобы не изобретать колесо.\nНапример, если бы мы хотели использовать один и тот же проект для создания нескольких виртуальных машин, VPC, групп безопасности, а затем кластера Kubernetes, мы бы, вероятно, захотели разделить наши ресурсы на модули, чтобы лучше определить наши ресурсы и их группировку.\nЕще одним преимуществом модулей является то, что вы можете взять эти модули и использовать их в других проектах или публично поделиться ими, чтобы помочь сообществу.\nМы разбиваем нашу инфраструктуру на компоненты, компоненты известны здесь как модули.\r\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day61",
            "title": "61. Kubernetes и множественные среды",
            "description": null,
            "content": "Kubernetes и множественные среды\r\n\r\nДо сих пор в этом разделе, посвященном инфраструктуре как коду, мы рассматривали развертывание виртуальных машин, хотя и с помощью virtualbox, но суть одна и та же: мы определяем в коде, как должна выглядеть наша виртуальная машина, а затем развертываем ее. То же самое касается контейнеров Docker, и на этом занятии мы рассмотрим, как Terraform можно использовать для взаимодействия с ресурсами, поддерживаемыми Kubernetes.\r\n\r\nЯ использовал Terraform для развертывания своих кластеров Kubernetes в демонстрационных целях на трех основных облачных провайдерах, и вы можете найти репозиторий tf_k8deploy.\r\n\r\nОднако вы также можете использовать Terraform для взаимодействия с объектами внутри кластера Kubernetes, это может быть использование Kubernetes provider или Helm provider для управления развертыванием диаграмм.\nТеперь мы можем использовать kubectl, как мы показывали в предыдущих разделах. Но есть некоторые преимущества использования Terraform в вашей среде Kubernetes.\nУнифицированный рабочий процесс - если вы использовали Terraform для развертывания кластеров, вы можете использовать тот же рабочий процесс и инструмент для развертывания в кластерах Kubernetes.\r\n\r\nУправление жизненным циклом - Terraform - это не просто инструмент инициализации, он позволяет вносить изменения, обновления и удаления.\nПростая демонстрация Kubernetes\r\n\r\nПодобно демо, которое мы создали на прошлом занятии, мы можем развернуть nginx в нашем кластере Kubernetes, я снова буду использовать minikube в демонстрационных целях. Мы создаем наш файл Kubernetes.tf, который вы можете найти в папке.\r\n\r\nВ этом файле мы определим нашего провайдера Kubernetes, укажем на наш файл kubeconfig, создадим пространство имен nginx, затем создадим развертывание, содержащее 2 реплики и, наконец, сервис.\r\n\r\n`terraform\r\nterraform {\r\n  required_providers {\r\n    kubernetes = {\r\n      source  = \"hashicorp/kubernetes\"\r\n      version = \">= 2.0.0\"\r\n    }\r\n  }\r\n}\r\nprovider \"kubernetes\" {\r\n  config_path = \"~/.kube/config\"\r\n}\r\nresource \"kubernetes_namespace\" \"test\" {\r\n  metadata {\r\n    name = \"nginx\"\r\n  }\r\n}\r\nresource \"kubernetes_deployment\" \"test\" {\r\n  metadata {\r\n    name      = \"nginx\"\r\n    namespace = kubernetes_namespace.test.metadata.0.name\r\n  }\r\n  spec {\r\n    replicas = 2\r\n    selector {\r\n      match_labels = {\r\n        app = \"MyTestApp\"\r\n      }\r\n    }\r\n    template {\r\n      metadata {\r\n        labels = {\r\n          app = \"MyTestApp\"\r\n        }\r\n      }\r\n      spec {\r\n        container {\r\n          image = \"nginx\"\r\n          name  = \"nginx-container\"\r\n          port {\r\n            container_port = 80\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nresource \"kubernetes_service\" \"test\" {\r\n  metadata {\r\n    name      = \"nginx\"\r\n    namespace = kubernetes_namespace.test.metadata.0.name\r\n  }\r\n  spec {\r\n    selector = {\r\n      app = kubernetes_deployment.test.spec.0.template.0.metadata.0.labels.app\r\n    }\r\n    type = \"NodePort\"\r\n    port {\r\n      node_port   = 30201\r\n      port        = 80\r\n      target_port = 80\r\n    }\r\n  }\r\n}\r\n`\r\n\r\nПервое, что мы должны сделать в папке нашего нового проекта, это выполнить команду terraform init.\nА затем, прежде чем мы выполним команду terraform apply, позвольте мне показать вам, что у нас нет пространств имен.\nКогда мы запустим нашу команду apply, она создаст эти 3 новых ресурса, пространство имен, развертывание и сервис в нашем кластере Kubernetes.\nТеперь мы можем взглянуть на развернутые ресурсы в нашем кластере.\nТеперь, поскольку мы используем minikube, и вы видели в предыдущем разделе, это имеет свои собственные ограничения, когда мы пытаемся играть с сетью docker для ingress. Но если мы просто выполним команду kubectl port-forward -n nginx svc/nginx 30201:80 и откроем браузер на http://localhost:30201/, мы увидим нашу страницу NGINX.\nЕсли вы хотите попробовать более подробные демонстрации с Terraform и Kubernetes, то на сайте HashiCorp Learn site вы сможете ознакомиться с ними.\nМножественные окружения\nЕсли мы хотим взять любой из демонстрационных примеров, которые мы проверили, но теперь хотим, чтобы определенные среды производства, постановки и разработки выглядели одинаково и использовали этот код, есть два подхода для достижения этого с помощью Terraform\nтерраформенные рабочие пространства - несколько именованных разделов в рамках одного бэкенда\nфайловая структура - расположение каталогов обеспечивает разделение, модули обеспечивают повторное использование.\nКаждый из этих подходов имеет свои плюсы и минусы.\nterraform workspaces\nПлюсы\nЛегко начать работу\nУдобное выражение terraform.workspace\nМинимизирует дублирование кода\nМинусы\r\nСклонность к человеческим ошибкам (мы пытались устранить это, используя TF)\r\nСостояние хранится в одном бэкенде\nКодовая база не показывает однозначно конфигурации развертывания.\r\nФайловая структура\nПлюсы\nИзоляция бэкендов\n    повышенная безопасность\n    снижен потенциал для человеческих ошибок\nКодовая база полностью представляет развернутое состояние\r\n\r\nМинусы\nТребуется многократное применение terraform для обеспечения окружения\nбольше дублирования кода, но его можно минимизировать с помощью модулей.\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day62",
            "title": "62. Terraform - Тестирование, инструменты и альтернативы",
            "description": null,
            "content": "Тестирование, инструменты и альтернативы\r\n\r\nЗавершая этот раздел об инфраструктуре как коде, мы должны упомянуть о тестировании нашего кода, различных доступных инструментах, а также о некоторых альтернативах Terraform для достижения этой цели. Как я уже говорил в начале раздела, я остановился на Terraform, поскольку он, во-первых, бесплатный и с открытым исходным кодом, во-вторых, он кроссплатформенный и не зависит от окружения. Но есть и альтернативы, которые следует рассмотреть, но общая цель состоит в том, чтобы донести до людей, что это способ развертывания инфраструктуры.\nCode Rot\nПервая область, которую я хочу затронуть в этой сессии, - это гниение кода. В отличие от кода приложений, инфраструктура как код может использоваться, а затем не использоваться в течение очень долгого времени. Возьмем пример: мы собираемся использовать Terraform для развертывания нашей среды VM в AWS, все идеально, все работает с первого раза, и у нас есть наша среда, но эта среда не меняется слишком часто, поэтому код остается в состоянии, возможно, или, надеюсь, хранится в центральном месте, но код не меняется.\nА что если что-то изменится в инфраструктуре? Но это делается вне диапазона, или другие вещи меняются в нашей среде.\nВнеполосные изменения (Out of band changes)\r\nНеприкрепленные версии (Unpinned versions)\r\nУтратившие актуальность зависимости (Deprecated dependancies)\r\nНеприменимые изменения (Unapplied changes)\r\nТестирование\nЕще одна огромная область, которая следует за гниением кода и в целом, это возможность протестировать ваш IaC и убедиться, что все области работают так, как должны.\nПрежде всего, есть несколько встроенных команд тестирования, на которые мы можем взглянуть:\r\n| Command               | Description                                                                                |\n| --------------------- | ------------------------------------------------------------------------------------------ |\r\n| terraform fmt       | Rewrite Terraform configuration files to a canonical format and style.                     |\n| terraform validate  | Validates the configuration files in a directory, referring only to the configuration      |\n| terraform plan      | Creates an execution plan, which lets you preview the changes that Terraform plans to make |\n| Custom validation     | Validation of your input variables to ensure they match what you would expect them to be   |\r\n\r\nУ нас также есть некоторые инструменты тестирования, доступные вне Terraform:\ntflint\r\n\r\n    Найти возможные ошибки (Find possible errors)\r\n    Предупреждать об устаревшем синтаксисе, неиспользуемых объявлениях. (Warn about deprecated syntax, unused declarations.)\r\n    Применять лучшие практики, соглашения об именовании. (Enforce best practices, naming conventions.)\r\n\r\nИнструменты сканирования\ncheckov - сканирование конфигураций облачной инфраструктуры для поиска неправильных конфигураций до их развертывания.\r\ntfsec - сканер безопасности статического анализа для кода Terraform.\r\nterrascan - статический анализатор кода для Infrastructure as Code.\r\nterraform-compliance - легковесный тестовый фреймворк, ориентированный на безопасность и соответствие требованиям, для terraform, позволяющий проводить негативное тестирование вашей инфраструктуры как кода.\r\nsnyk - сканирует код Terraform на предмет неправильной конфигурации и проблем безопасности.\nУправляемое облачное предложение\nTerraform Sentinel - встроенный фреймворк политики как кода, интегрированный с продуктами HashiCorp Enterprise. Она позволяет принимать решения о политике на основе логики и может быть расширена для использования информации из внешних источников.\r\n\r\nАвтоматизированное тестирование\r\n\r\nTerratest - Terratest - это библиотека Go, которая предоставляет шаблоны и вспомогательные функции для инфраструктуры тестирования.\r\n\r\nСтоит упомянуть\nTerraform Cloud - Terraform Cloud - это управляемый сервис компании HashiCorp. Оно устраняет необходимость в ненужных инструментах и документации для практиков, команд и организаций для использования Terraform в производстве.\r\n\r\nTerragrunt - Terragrunt - это тонкая обертка, которая предоставляет дополнительные инструменты для сохранения DRY конфигураций, работы с несколькими модулями Terraform и управления удаленным состоянием.\nAtlantis - Terraform Pull Request Automation.\nАльтернативы\nВ день 57, когда мы начали этот раздел, мы упоминали, что есть некоторые альтернативы, и я очень планирую изучить их после завершения этой задачи.\r\n| Cloud Specific                  | Cloud Agnostic |\n| ------------------------------- | -------------- |\r\n| AWS CloudFormation              | Terraform      |\n| Azure Resource Manager          | Pulumi         |\n| Google Cloud Deployment Manager |                |\nЯ использовал AWS CloudFormation, вероятно, больше всего из вышеперечисленного списка, он является родным для AWS, но я не использовал другие, кроме Terraform. Как вы можете себе представить, версии для конкретных облаков очень хороши для конкретного облака, но если у вас несколько облачных сред, то вам будет сложно перенести эти конфигурации или у вас будет несколько плоскостей управления для ваших усилий IaC.\nЯ думаю, что следующим интересным шагом для меня будет уделить некоторое время и узнать больше о Pulumi.\nИз сравнения Pulumi на их сайте\n\"И Terraform, и Pulumi предлагают модель инфраструктуры желаемого состояния как кода, где код представляет желаемое состояние инфраструктуры, а механизм развертывания сравнивает это желаемое состояние с текущим состоянием стека и определяет, какие ресурсы должны быть созданы, обновлены или удалены\".\r\n\r\nСамое большое отличие, которое я вижу, заключается в том, что в отличие от HashiCorp Configuration Language (HCL) Pulumi позволяет использовать языки общего назначения, такие как Python, TypeScript, JavaScript, Go и .NET.\nКраткий обзор Introduction to Pulumi: Modern Infrastructure as Code Мне нравится простота и возможность выбора, которую вам предлагают, и я хочу разобраться в этом немного подробнее.\nНа этом мы завершаем раздел \"Инфраструктура как код\" и переходим к тому, что немного пересекается с управлением конфигурацией, и, в частности, по мере того, как мы переходим к общей картине управления конфигурацией, мы будем использовать Ansible для некоторых из этих задач и демонстраций.\nРесурсы\nWhat is Infrastructure as Code? Difference of Infrastructure as Code Tools\nTerraform Tutorial | Terraform Course Overview 2021\r\nTerraform explained in 15 mins | Terraform Tutorial for Beginners\nTerraform Course - From BEGINNER to PRO!\r\nHashiCorp Terraform Associate Certification Course\r\nTerraform Full Course for Beginners\r\nKodeKloud -  Terraform for DevOps Beginners + Labs: Complete Step by Step Guide!\r\nTerraform Simple Projects\r\nTerraform Tutorial - The Best Project Ideas\r\nAwesome Terraform\r\nPulumi - IaC in your favorite programming language!\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day63",
            "title": "63. Инструменты управления конфигурацией - Ansible/Terraform",
            "description": null,
            "content": "Введение: Управление конфигурацией\r\n\r\nСразу после раздела, посвященного инфраструктуре как коду, мы, вероятно, будем говорить об управлении конфигурацией или управлении конфигурацией приложений.\nУправление конфигурацией - это процесс поддержания приложений, систем и серверов в требуемом состоянии. Пересечение с Infrastructure as code заключается в том, что IaC гарантирует, что ваша инфраструктура находится в желаемом состоянии, но после этого, особенно terraform, не будет заботиться о желаемом состоянии настроек вашей ОС или приложений, и именно здесь на помощь приходят инструменты управления конфигурацией. Убедитесь, что система и приложения работают так, как ожидается, поскольку изменения происходят в Deane.\nУправление конфигурацией убережет вас от внесения мелких или крупных изменений, которые останутся недокументированными.\nПочему вы хотите использовать управление конфигурацией\r\n\r\nСценарий или почему вы хотите использовать управление конфигурацией, познакомьтесь с Дином. Он наш системный администратор, и Дин - счастливый турист, который работает над всеми своими системами.\r\nработает над всеми системами в своем окружении.\nЧто произойдет, если их система выйдет из строя, если случится пожар, сервер выйдет из строя? Дин точно знает, что делать, он может легко устранить пожар, но если несколько серверов начнут выходить из строя, особенно если у вас большая и расширяющаяся среда, вот почему Дину действительно необходимо иметь инструмент управления конфигурацией. Инструменты управления конфигурацией могут помочь Дину выглядеть как рок-звезда, все, что ему нужно сделать, это настроить правильные коды, которые позволят ему быстро, эффективно и масштабно передать инструкции по настройке каждого из серверов.\nИнструменты управления конфигурацией\nСуществует множество инструментов управления конфигурацией, и каждый из них имеет специфические особенности, которые делают его лучше для одних ситуаций, чем для других.\nНа этом этапе мы быстро рассмотрим варианты, показанные на рисунке выше, прежде чем сделать выбор, какой из них мы будем использовать и почему.\nChef**\r\n  Chef обеспечивает последовательное применение конфигурации в любой среде, в любом масштабе с помощью автоматизации инфраструктуры.\n  Chef - это инструмент с открытым исходным кодом, разработанный компанией OpsCode и написанный на Ruby и Erlang.\r\n  Chef лучше всего подходит для организаций, которые имеют гетерогенную инфраструктуру и ищут зрелые решения.\n  Рецепты и Cookbooks определяют код конфигурации для ваших систем.\n  Pro - Доступна большая коллекция рецептов\r\n  Pro - Хорошо интегрируется с Git, что обеспечивает надежный контроль версий.\r\n  Против - Крутая кривая обучения, требуется значительное количество времени.\n  Против - Главный сервер не имеет большого контроля.\n  Архитектура - сервер / клиенты\n  Простота настройки - Умеренная\r\n  Язык - Процедурный - Указать, как выполнить задачу\r\nPuppet**\r\n  Puppet - это инструмент управления конфигурацией, который поддерживает автоматическое развертывание.\n  Puppet построен на Ruby и использует DSL для написания манифестов.\n  Puppet также хорошо работает с гетерогенной инфраструктурой, где основное внимание уделяется масштабируемости.\n  За - Большое сообщество поддержки.\n  За - Хорошо развитый механизм отчетности.\n  Против - Продвинутые задачи требуют знания языка Ruby.\r\n  Против - Главный сервер не имеет большого контроля.\n  Архитектура - сервер / клиенты\n  Простота установки - Умеренная\r\n  Язык - Декларативный - указывать только то, что нужно делать\nAnsible**\r\n  Ansible - это инструмент автоматизации ИТ, который автоматизирует управление конфигурацией, предоставление облака, развертывание и оркестровку.\n  Ядро плейбуков Ansible написано на языке YAML. (Следует сделать раздел о YAML, так как мы уже несколько раз сталкивались с этим).\r\n  Ansible хорошо работает в средах, где основное внимание уделяется быстрой настройке и запуску.\n  Работает на основе плейбуков, которые предоставляют инструкции вашим серверам.\r\n  Pro - Не нужны агенты на удаленных узлах.\r\n  Pro - YAML легко изучить.\n  Против - Скорость работы часто ниже, чем у других инструментов (быстрее, чем Дин делает это сам вручную).\r\n  Против - YAML не такой мощный, как Ruby, но его легче освоить.\n  Архитектура - Только клиент\r\n  Простота настройки - Очень просто\n  Язык - Процедурный - Указать, как выполнить задачу\r\n\r\nSaltStack**\r\n  SaltStack - это инструмент на основе CLI, который автоматизирует управление конфигурацией и удаленное выполнение.\n  SaltStack основан на Python, а инструкции написаны на YAML или собственном DSL.\n  Идеально подходит для сред, где приоритетом является масштабируемость и отказоустойчивость.\n  Плюсы - Простота использования при запуске\n  Плюсы - Хороший механизм отчетности\n  Против - Фаза установки сложная\r\n  Против - Новый веб-уи, который гораздо менее проработан, чем другие.\n  Архитектура - сервер / клиенты\r\n  Простота установки - Умеренная\r\n  Язык - Декларативный - указывайте только то, что нужно делать\r\nAnsible vs Terraform\r\n\r\nИнструментом, который мы будем использовать для этого раздела, будет Ansible. (Простой в использовании и требуются основы языка).\r\n\r\nЯ думаю, что важно коснуться некоторых различий между Ansible и Terraform, прежде чем мы рассмотрим инструментарий немного подробнее. |               |Ansible                                                        |Terraform                                                          |\r\n| ------------- | ------------------------------------------------------------- | ----------------------------------------------------------------- |\r\n|Type           |Ansible is a configuration management tool                     |Terraform is a an orchestration tool                               |\r\n|Infrastructure |Ansible provides support for mutable infrastructure            |Terraform provides support for immutable infrastructure            |\r\n|Language       |Ansible follows procedural language                            |Terraform follows a declartive language                            |\r\n|Provisioning   |Ansible provides partial provisioning (VM, Network, Storage)   |Terraform provides extensive provisioning (VM, Network, Storage)   |\r\n|Packaging      |Ansible provides complete support for packaging & templating   |Terraform provides partial support for packaging & templating      |\r\n|Lifecycle Mgmt |Ansible does not have lifecycle management                     |Terraform is heavily  dependant on lifecycle and state mgmt        |\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day64",
            "title": "64. Ansible Введение",
            "description": null,
            "content": "\r\nОсновы Ansible\r\nAnsible: Начало работы\r\n\r\nМы немного рассказали о том, что такое Ansible, на вчерашней большой сессии, но здесь мы собираемся начать с более подробной информации. Во-первых, Ansible поставляется компанией RedHat. Во-вторых, это агент, подключается через SSH и выполняет команды. В-третьих, он кроссплатформенный (Linux & macOS, WSL2) и с открытым исходным кодом (есть также платный корпоративный вариант) Ansible толкает конфигурацию по сравнению с другими моделями.\nУстановка Ansible\nКак вы можете себе представить, RedHat и команда Ansible проделали фантастическую работу по документированию Ansible. Обычно это начинается с шагов по установке, которые вы можете найти здесь. Помните, мы говорили, что Ansible - это инструмент автоматизации без агентов, инструмент развертывается на системе, называемой \"узел управления\", с этого узла управления осуществляется управление машинами и другими устройствами (возможно, сетевыми) по SSH.\nВ документации по ссылке выше говорится, что ОС Windows не может использоваться в качестве узла управления.\nДля моего узла управления и, по крайней мере, для этой демонстрации я собираюсь использовать виртуальную машину Linux, которую мы создали еще в разделе Linux в качестве узла управления.\nЭта система работала под управлением Ubuntu, и для ее установки достаточно выполнить следующие команды.\n`\r\nsudo apt update\r\nsudo apt install software-properties-common\r\nsudo add-apt-repository --yes --update ppa:ansible/ansible\r\nsudo apt install ansible\r\n`\r\nТеперь у нас должна быть установлена ansible на нашем узле управления, вы можете проверить это, запустив ansible --version, и вы должны увидеть что-то похожее на это ниже.\nПрежде чем мы перейдем к управлению другими узлами в нашей среде, мы также можем проверить функциональность ansible, выполнив команду на нашей локальной машине ansible localhost -m ping будет использовать Ansible Module, и это быстрый способ выполнить одну задачу на многих различных системах. Я имею в виду, что это не очень весело только с локальным хостом, но представьте, что вы хотите получить что-то или убедиться, что все ваши системы работают, а у вас 1000+ серверов и устройств.\nИли реальное использование модуля в реальной жизни может быть чем-то вроде ansible webservers --m service -a \"name=httpd state=started\", это скажет нам, запущена ли служба httpd на всех наших веб-серверах. Я привел термин webservers, используемый в этой команде.\nhosts\nКак я использовал localhost выше для запуска простого модуля ping против системы, я не могу указать другую машину в моей сети, например, в среде, которую я использую, мой хост Windows, на котором работает VirtualBox, имеет сетевой адаптер с IP 10.0.0.1, но вы можете видеть ниже, что я могу связаться с ним с помощью ping, но я не могу использовать ansible для выполнения этой задачи.\nДля того чтобы указать наши узлы или узлы, которые мы хотим автоматизировать с помощью этих задач, нам необходимо их определить. Мы можем определить их, перейдя в каталог /etc/ansible в вашей системе.\nФайл, который мы хотим отредактировать - это файл hosts, используя текстовый редактор, мы можем зайти в него и определить наши хосты. Файл hosts содержит множество отличных инструкций по использованию и изменению файла. Мы хотим прокрутить вниз и создать новую группу под названием [windows] и добавить наш IP-адрес 10.0.0.1 для этого хоста. Сохраните файл.\nОднако помните, я говорил, что вам понадобится SSH, чтобы Ansible мог подключиться к вашей системе. Как вы можете видеть ниже, когда я запускаю ansible windows -m ping, мы получаем недостижимый результат, потому что не удалось подключиться через SSH.\nТеперь я также начал добавлять дополнительные хосты в наш инвентарь, другое название для этого файла, так как здесь вы собираетесь определить все ваши устройства, это могут быть сетевые устройства, например, коммутаторы и маршрутизаторы, которые также будут добавлены сюда и сгруппированы. В нашем файле hosts я также добавил свои учетные данные для доступа к группе систем linux.\nТеперь, если мы запустим ansible linux -m ping, мы получим успех, как показано ниже.\nДалее у нас есть требования к узлам, это целевые системы, на которых вы хотите автоматизировать конфигурацию. Мы не устанавливаем на них ничего для Ansible (то есть, мы можем установить программное обеспечение, но нам не нужен клиент Ansible). Ansible будет устанавливать соединение по SSH и отправлять все по SFTP (если вы хотите и у вас настроен SSH, вы можете использовать SCP против SFTP).\nКоманды Ansible\nВы видели, что мы смогли запустить ansible linux -m ping на нашей Linux машине и получить ответ, в принципе, с Ansible у нас есть возможность запускать множество специальных команд. Но очевидно, что вы можете запустить это против группы систем и получить эту информацию обратно. ad hoc commands\r\n\r\nЕсли вы сталкиваетесь с повторением команд или, что еще хуже, вам приходится входить в отдельные системы для выполнения этих команд, то Ansible может помочь в этом случае. Например, простая команда ниже даст нам вывод всех сведений об операционной системе для всех систем, которые мы добавим в нашу группу linux.\nansible linux -a \"cat /etc/os-release\".\r\n\r\nДругими вариантами использования могут быть перезагрузка систем, копирование файлов, управление упаковщиками и пользователями. Вы также можете объединить специальные команды с модулями Ansible.\nСпециальные команды используют декларативную модель, рассчитывая и выполняя действия, необходимые для достижения заданного конечного состояния. Они достигают идемпотентности, проверяя текущее состояние перед началом работы и ничего не делая, если текущее состояние не отличается от заданного конечного состояния.\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\n\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day65",
            "title": "65. Ansible Playbooks - Часть 1",
            "description": null,
            "content": "Ansible Playbooks\nВ этом разделе мы рассмотрим основную причину, которую я вижу, по крайней мере, для Ansible. Я имею в виду, что это здорово - взять одну команду и обратиться ко многим различным серверам для выполнения простых команд, таких как перезагрузка длинного списка серверов и избавление от необходимости подключаться к каждому из них по отдельности.\r\n\r\nНо как насчет того, чтобы взять голую операционную систему, объявить программное обеспечение и службы, которые мы хотим запустить на этой системе, и убедиться, что все они работают в нужном состоянии.\r\n\r\nЗдесь на помощь приходят учебники Ansible. Плейбук позволяет нам взять группу серверов и выполнить задачи конфигурации и установки для этой группы.\r\nФормат плейбука\r\n\r\nПлейбук > Игры > Задачи\r\n\r\nЕсли вы занимаетесь спортом, вы, возможно, сталкивались с термином \"плейбук\". Плейбук рассказывает команде о том, как вы будете играть, состоящий из различных пьес и задач. Если мы считаем пьесы декорациями в спорте или игре, а задачи связаны с каждой пьесой, у вас может быть несколько задач, составляющих пьесу, а в плейбуке может быть несколько различных пьес.\r\n\r\nЭти плейбуки написаны на YAML (YAML - это не язык разметки), вы найдете много разделов, которые мы уже рассмотрели, особенно контейнеры и Kubernetes, в которых используются файлы конфигурации в формате YAML.\r\n\r\nДавайте рассмотрим простой плейбук под названием playbook.yml.\r\n`\r\nname: Simple Play\r\n  hosts: localhost\r\n  connection: local\r\n  tasks:\r\n    name: Ping me\r\n      ping:\r\n    name: print os\r\n      debug:\r\n        msg: \"{{ ansible_os_family }}\"\r\n`\r\n\r\nВы найдете вышеуказанный файл simple_play. Если мы затем используем команду ansible-playbook simple_play.yml, то пройдем следующие шаги.\nВы видите, что первая задача \"сбор шагов\" произошла, но мы не вызывали или не просили об этом? Этот модуль автоматически вызывается плейбуками для сбора полезных переменных об удаленных хостах. ansible.builtin.setup\r\n\r\nНашей второй задачей было установить ping, это не ICMP ping, а python скрипт, который сообщает pong об успешном соединении с удаленным или локальным хостом. ansible.builtin.ping\nЗатем наша третья или на самом деле вторая определенная задача, так как первая будет выполняться, если вы не отключите печать сообщения, сообщающего нам о нашей ОС. В этой задаче мы используем условия, мы можем запустить этот плейбук на всех различных типах операционных систем, и это вернет нам имя ОС. Мы просто передаем этот вывод для удобства, но мы могли бы добавить задачу, чтобы сказать что-то вроде:\r\n`\ntasks:\n  name: \"shut down Debian flavoured systems\"\r\n    command: /sbin/shutdown -t now\n    when: ansible_os_family == \"Debian\"\r\n`\nVagrant для настройки нашего окружения\r\n\r\nМы будем использовать Vagrant для настройки нашего узлового окружения, я собираюсь оставить разумные 4 узла, но вы, надеюсь, увидите, что их может быть 300 или 3000. В этом и заключается сила Ansible и других инструментов управления конфигурацией, чтобы иметь возможность настраивать ваши серверы.\r\n\r\nВы можете найти этот файл здесь (Vagrantfile)\r\n`\r\nVagrant.configure(\"2\") do |config|\r\n  servers=[\r\n    {\r\n      :hostname => \"db01\",\r\n      :box => \"bento/ubuntu-21.10\",\r\n      :ip => \"192.168.169.130\",\r\n      :ssh_port => '2210'\r\n    },\r\n    {\r\n      :hostname => \"web01\",\r\n      :box => \"bento/ubuntu-21.10\",\r\n      :ip => \"192.168.169.131\",\r\n      :ssh_port => '2211'\r\n    },\r\n    {\r\n      :hostname => \"web02\",\r\n      :box => \"bento/ubuntu-21.10\",\r\n      :ip => \"192.168.169.132\",\r\n      :ssh_port => '2212'\r\n    },\r\n    {\r\n      :hostname => \"loadbalancer\",\r\n      :box => \"bento/ubuntu-21.10\",\r\n      :ip => \"192.168.169.134\",\r\n      :ssh_port => '2213'\r\n    }\r\n\r\n  ]\r\n\r\nconfig.vm.base_address = 600\r\n\r\n  servers.each do |machine|\r\n\r\n    config.vm.define machine[:hostname] do |node|\r\n      node.vm.box = machine[:box]\r\n      node.vm.hostname = machine[:hostname]\n      node.vm.network :public_network, bridge: \"Intel(R) Ethernet Connection (7) I219-V\", ip: machine[:ip]\r\n      node.vm.network \"forwarded_port\", guest: 22, host: machine[:ssh_port], id: \"ssh\"\r\n\r\n      node.vm.provider :virtualbox do |v|\r\n        v.customize [\"modifyvm\", :id, \"--memory\", 2048]\r\n        v.customize [\"modifyvm\", :id, \"--name\", machine[:hostname]]\r\n      end\r\n    end\r\n  end\r\n\r\nend\r\n`\r\n\r\nИспользуйте команду vagrant up, чтобы запустить эти машины в VirtualBox, Вы можете добавить больше памяти, а также определить разные частные_сетевые адреса для каждой машины, но это работает в моей среде. Помните, что наш блок управления - это рабочий стол Ubuntu, который мы установили в разделе Linux.\nЕсли вы ограничены в ресурсах, вы также можете запустить vagrant up web01 web02, чтобы поднять только веб-серверы, которые мы используем здесь.\nКонфигурация хоста Ansible\r\n\r\nТеперь, когда наша среда готова, мы можем проверить ansible, и для этого мы будем использовать наш рабочий стол Ubuntu (вы можете использовать его, но вы также можете использовать любую машину на базе Linux в вашей сети, доступную для сети ниже) в качестве нашего управления, давайте также добавим новые узлы в нашу группу в файле ansible hosts, Вы можете считать этот файл инвентаризацией, альтернативой этому может быть другой файл инвентаризации, который вызывается как часть вашей команды ansible с -i filename, это может быть полезно по сравнению с использованием файла host, так как вы можете иметь разные файлы для разных сред, например, production, test и staging. Поскольку мы используем стандартный файл hosts, нам не нужно его указывать, так как он будет использоваться по умолчанию.\r\n\r\nЯ добавил следующее в файл hosts по умолчанию.\r\n`\r\n[control]\r\nansible-control\r\n\r\n[proxy]\nloadbalancer\r\n\r\n[webservers]\nweb01\r\nweb02\r\n\r\n[database]\ndb01\r\n\r\n`\r\n\r\n\r\nПрежде чем двигаться дальше, мы хотим убедиться, что можем выполнить команду для наших узлов, давайте выполним ansible nodes -m command -a hostname, эта простая команда проверит, что у нас есть подключение и сообщит имена наших узлов.\r\n\r\nТакже обратите внимание, что я добавил эти узлы и IP на мой узел управления Ubuntu в файл /etc/hosts для обеспечения подключения. Нам также может понадобиться выполнить конфигурацию SSH для каждого узла с блока Ubuntu.\r\n`\r\n192.168.169.140 ansible-control\r\n192.168.169.130 db01\r\n192.168.169.131 web01\r\n192.168.169.132 web02\r\n192.168.169.133 loadbalancer\r\n`\r\n\r\n\r\nНа этом этапе мы хотим выполнить настройку SSH ключей между узлами управления и сервера. Это то, что мы будем делать дальше, другим способом здесь может быть добавление переменных в ваш файл hosts для указания имени пользователя и пароля. Я бы не советовал этого делать, так как это никогда не будет лучшей практикой.\nЧтобы настроить SSH и общий доступ между узлами, выполните следующие шаги, вам будет предложено ввести пароль (vagrant), и вам, вероятно, придется нажать y несколько раз, чтобы согласиться.\nssh-keygen\r\n\r\n\r\n\r\nssh-copy-id localhost\r\n\r\n\r\n\r\nТеперь, если все ваши ВМ включены, вы можете запустить команду ssh-copy-id web01 && ssh-copy-id web02 && ssh-copy-id loadbalancer && ssh-copy-id db01, которая запросит у вас пароль, в нашем случае пароль vagrant.\r\n\r\nЯ не запускаю все свои виртуальные машины, а запускаю только веб-серверы, поэтому я выдал команду sh-copy-id web01 && ssh-copy-id web02.\nПеред запуском любых плейбуков я хочу убедиться, что у меня есть простое соединение с моими группами, поэтому я запустил ansible webservers -m ping для проверки соединения.\nНаш первый \"настоящий\" плейбук Ansible\r\nНаш первый плейбук Ansible будет настраивать наши веб-серверы, мы сгруппировали их в нашем файле hosts под группировкой [webservers].\nПеред запуском нашего плейбука мы можем убедиться, что на web01 и web02 не установлен apache. В верхней части скриншота ниже показано расположение папок и файлов, которые я создал в моей системе управления ansible для запуска этого плейбука, у нас есть playbook1.yml, затем в папке templates у нас есть файлы index.html.j2 и ports.conf.j2. Вы можете найти эти файлы в папке, указанной выше в репозитории.\nЗатем мы подключаемся по SSH к web01, чтобы проверить, установлен ли у нас apache?\nИз вышеприведенного видно, что у нас не установлен apache на web01, поэтому мы можем исправить это, запустив следующий плейбук.\r\n\r\n`\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps\"\r\n  tasks:\r\n  name: ensure apache is at the latest version\r\n    apt:\r\n      name: apache2\r\n      state: latest\r\n\r\n  name: write the apache2 ports.conf config file\r\n    template:\r\n      src: templates/ports.conf.j2\r\n      dest: /etc/apache2/ports.conf\r\n    notify:\r\n    restart apache\r\n\r\n  name: write a basic index.html file\r\n    template:\r\n      src: templates/index.html.j2\r\n      dest: /var/www/html/index.html\r\n    notify:\r\n    restart apache\r\n\r\n  name: ensure apache is running\r\n    service:\r\n      name: apache2\r\n      state: started\r\n\r\n  handlers:\r\n    name: restart apache\r\n      service:\r\n        name: apache2\r\n        state: restarted\r\n`\r\nРазбираем вышеприведенный плейбук:\n- hosts: webservers означает, что наша группа, на которой будет запущен этот плейбук, называется webservers.\r\nbecome: yes означает, что наш пользователь, запускающий плейбук, станет root на наших удаленных системах. Вам будет предложено ввести пароль root.\nЗатем у нас есть vars, и это определяет некоторые переменные окружения, которые мы хотим использовать на наших веб-серверах.\nПосле этого мы приступаем к выполнению наших задач,\nЗадача 1 - убедиться, что apache работает на последней версии.\r\nЗадача 2 - написать файл ports.conf из нашего исходного файла, который находится в папке templates.\nЗадача 3 - создание базового файла index.html\nЗадача 4 - убедиться, что apache запущен.\nНаконец, у нас есть раздел обработчиков, Handlers: Running operations on change\r\n\r\n\"Иногда вы хотите, чтобы задача выполнялась только тогда, когда на машине происходят изменения. Например, вы можете захотеть перезапустить службу, если задача обновляет конфигурацию этой службы, но не перезапускать ее, если конфигурация не изменилась. Для решения этой задачи в Ansible используются обработчики. Обработчики - это задачи, которые выполняются только при получении уведомления. Каждый обработчик должен иметь глобально уникальное имя\".\r\n\r\nНа этом этапе вы можете подумать, но мы развернули 5 виртуальных машин (включая нашу машину Ubuntu Desktop, которая действует как наш Ansible Control) Остальные системы будут задействованы в оставшейся части раздела.\nЗапуск нашего плейбука\r\n\r\nТеперь мы готовы запустить наш учебник на наших узлах. Для запуска нашего плейбука мы можем использовать ansible-playbook playbook1.yml Мы определили наши узлы, на которых будет работать наш учебник, и это позволит выполнить наши задачи, которые мы определили.\nПосле завершения команды мы получим результат, показывающий наши пьесы и задачи, это может занять некоторое время, вы можете видеть на изображении ниже, что это заняло некоторое время, чтобы пойти и установить наше желаемое состояние.\nЗатем мы можем дважды проверить это, зайдя в узел и проверив, что на нашем узле установлено программное обеспечение.\r\n\r\n\r\n\r\nТеперь, когда мы развернули два автономных веб-сервера, мы можем перейти на соответствующие IP, которые мы определили, и получить наш новый веб-сайт.\nМы будем опираться на это руководство по ходу работы над остальной частью этого раздела. Мне также интересно взять наш рабочий стол Ubuntu и посмотреть, сможем ли мы загрузить наши приложения и конфигурацию с помощью Ansible, поэтому мы также можем коснуться этого. Вы видели, что мы можем использовать локальный хост в наших командах, мы также можем запускать плейбуки, например, на нашем локальном хосте.\r\n\r\nЕще одна вещь, которую следует добавить, заключается в том, что мы работаем только с виртуальными машинами Ubuntu, но Ansible не зависит от целевых систем. Альтернативы, которые мы уже упоминали ранее для управления системами, могут быть сервер за сервером (не масштабируемый, когда вы получаете большое количество серверов, плюс боль даже с 3 узлами), мы также можем использовать скрипты оболочки, которые мы рассматривали в разделе Linux, но эти узлы потенциально разные, так что да, это можно сделать, но тогда кто-то должен поддерживать и управлять этими скриптами. Ansible бесплатна и позволяет легко справиться с этой задачей по сравнению с необходимостью иметь специализированный скрипт.\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\nYour complete guide to Ansible\r\n\r\nЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day66",
            "title": "66. Ansible Playbooks - Часть 2",
            "description": null,
            "content": "Ansible Playbooks Продолжение...\r\n\r\nВ нашем последнем разделе мы начали с создания небольшой лаборатории, используя файл Vagrant для развертывания 4 машин, и мы использовали нашу Linux-машину, которую мы создали в этом разделе, в качестве нашей системы управления Ansible.\nМы также проверили несколько скриптов плейбуков, и в конце у нас был плейбук, который сделал наши web01 и web02 отдельными веб-серверами.\nНаведение порядка\r\n\r\nПрежде чем перейти к дальнейшей автоматизации и развертыванию, мы должны рассказать о том, как сохранить наш плейбук аккуратным и опрятным и как мы можем разделить наши такты и обработчики по подпапкам.\nВ основном мы собираемся копировать наши задачи в их собственный файл в папке.\r\n\r\n`\r\nname: ensure apache is at the latest version\r\n  apt: name=apache2 state=latest\r\n\r\nname: write the apache2 ports.conf config file\r\n  template:\n    src=templates/ports.conf.j2\n    dest=/etc/apache2/ports.conf\r\n  notify: restart apache\r\n\r\nname: write a basic index.html file\r\n  template:\r\n    src: templates/index.html.j2\r\n    dest: /var/www/html/index.html\r\n  notify:\r\n  restart apache\r\n\r\nname: ensure apache is running\r\n  service:\r\n    name: apache2\r\n    state: started\r\n`\r\n\r\nи то же для обработчиков.\r\n\r\n`\r\nname: restart apache\r\n  service:\r\n    name: apache2\r\n    state: restarted\r\n`\r\n\r\nЗатем в нашем плейбуке, который теперь называется playbook2.yml, мы указываем на эти файлы. Все эти файлы можно найти по адресу ansible-scenario2.\r\n\r\nВы можете проверить это на своей контрольной машине. Если вы скопировали файлы из репозитория, вы должны были заметить, что кое-что изменилось в пункте \"написать основной файл index.html\"\r\n\r\n\r\n\r\nДавайте выясним, какое простое изменение я сделал. Использование curl web01:8000\nМы только что привели в порядок наш плейбук и начали разделять области, которые могут сделать плейбук очень перегруженным в масштабе.\r\nРоли и Ansible Galaxy\r\n\r\nНа данный момент мы развернули 4 виртуальные машины и настроили 2 из них как веб-серверы, но у нас есть еще несколько специфических функций, а именно: сервер базы данных и балансировщик нагрузки или прокси. Для того чтобы сделать это и привести в порядок наш репозиторий, мы можем использовать роли в Ansible.\nДля этого мы воспользуемся командой ansible-galaxy, которая предназначена для управления ролями Ansible в общих репозиториях.\nМы собираемся использовать ansible-galaxy для создания роли для apache2, где мы собираемся разместить специфику наших веб-серверов.\nПриведенная выше команда ansible-galaxy init roles/apache2 создаст структуру папок, которую мы показали выше. Следующим шагом нам нужно переместить существующие задачи и шаблоны в соответствующие папки в новой структуре.\nКопировать и вставить легко для перемещения этих файлов, но нам также нужно внести изменения в tasks/main.yml, чтобы указать его на apache2_install.yml.\nНам также нужно изменить наш playbook, чтобы он ссылался на нашу новую роль. В playbook1.yml и playbook2.yml мы определяем наши задачи и обработчики по-разному, так как мы изменили их между двумя версиями. Нам нужно изменить наш плейбук, чтобы использовать эту роль, как показано ниже:\r\n`\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\r\n  roles:\r\n    apache2\r\n`\r\n\r\n\r\n\r\nТеперь мы можем запустить наш плейбук снова, на этот раз с новым именем плейбука ansible-playbook playbook3.yml Вы заметите обесценивание, мы можем исправить это дальше.\nХорошо, амортизация хотя наш плейбук запустился, теперь мы должны исправить наши пути, для этого я изменил опцию include в tasks/main.yml на import_tasks, как показано ниже.\nВы можете найти эти файлы в папке ansible-scenario3.\r\n\r\nМы также собираемся создать еще несколько ролей, используя ansible-galaxy, которые мы собираемся создать:\r\ncommon = for all of our servers (ansible-galaxy init roles/common)\r\nnginx = for our loadbalancer (ansible-galaxy init roles/nginx)\r\n\r\n\r\n\r\nЯ собираюсь оставить этот вариант здесь, а в следующей сессии мы начнем работать над другими узлами, которые мы развернули, но еще ничего не сделали.\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\nYour complete guide to Ansible\r\n\r\nЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day67",
            "title": "67. Роли и развертывание балансировщика нагрузки",
            "description": null,
            "content": "\r\nНа последнем занятии мы рассмотрели роли и использовали команду ansible-galaxy, чтобы помочь создать структуру папок для некоторых ролей, которые мы будем использовать. В итоге мы получили гораздо более аккуратное рабочее хранилище для нашего кода конфигурации, поскольку все спрятано в папках ролей.\nОднако мы использовали только роль apache2 и получили рабочий playbook3.yaml для работы с нашими веб-серверами.\nНа данном этапе, если вы использовали только vagrant up web01 web02, пришло время запустить vagrant up loadbalancer, который откроет другую систему Ubuntu, которую мы будем использовать в качестве балансировщика нагрузки/прокси.\nМы уже определили эту новую машину в нашем файле hosts, но у нас нет настроенного ssh-ключа, пока он не доступен, поэтому нам нужно также запустить ssh-copy-id loadbalancer, когда система будет запущена и готова.\nОбщая роль\r\nВ конце вчерашней сессии я создал роль common, роль common будет использоваться на всех наших серверах, в то время как другие роли специфичны для конкретных случаев использования, сейчас приложения, которые я собираюсь установить в качестве common, не так просты, и я не вижу много причин для этого, но это показывает цель. В структуре папок нашей общей роли перейдите в папку tasks, и у вас появится файл main.yml. В этом yaml нам нужно указать на наш файл install_tools.yml, и мы делаем это, добавляя строку - import_tasks: install_tools.yml. Раньше это был include, но он скоро будет устаревшим, поэтому мы используем import_tasks.\r\n`\r\nname: \"Install Common packages\"\r\n  apt: name={{ item }} state=latest\r\n  with_items:\r\n   neofetch\r\n   tree\r\n   figlet\r\n`\r\n\r\nЗатем в нашем плейбуке мы добавляем общую роль для каждого блока хоста.\r\n`\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\r\n  roles:\r\n    common\r\n    apache2\r\n`\r\nnginx\r\n\r\nСледующим этапом будет установка и настройка nginx на нашем виртуальном компьютере loadbalancer. Как и в общей структуре папок, у нас есть nginx, основанный на последнем сеансе.\nПрежде всего, мы добавим блок host в наш playbook. Этот блок будет включать нашу общую роль, а затем нашу новую роль nginx.\nПлейбук можно найти здесь. playbook4.yml\r\n`\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\r\n  roles:\r\n    common\r\n    apache2\r\n\r\nhosts: proxy\n  become: yes\r\n  roles:\n    common\r\n    nginx\r\n`\r\n\r\nДля того чтобы это что-то значило, мы должны определить наши задачи, которые мы хотим запустить, таким же образом мы изменим main.yml в задачах, чтобы указать на два файла, один для установки и один для конфигурации.\nЕсть и другие файлы, которые я изменил в зависимости от желаемого результата, посмотрите в папке ansible-scenario4 все измененные файлы. Вам следует проверить папки tasks, handlers и templates в папке nginx, и вы найдете эти дополнительные изменения и файлы.\nЗапуск обновленного плейбука\r\n\r\nСо вчерашнего дня мы добавили роль common, которая теперь будет устанавливать некоторые пакеты в нашей системе, а затем мы также добавили роль nginx, которая включает установку и настройку.\nДавайте запустим наш playbook4.yml, используя ansible-playbook playbook4.yml.\r\n\r\n\r\n\r\nТеперь, когда мы настроили наши веб-серверы и loadbalancer, мы должны иметь возможность перейти по адресу http://192.168.169.134/, который является IP-адресом нашего loadbalancer.\nЕсли вы следите за развитием событий и у вас нет такого состояния, то это может быть связано с IP-адресами серверов в вашем окружении. Файл находится в templates\\mysite.j2 и выглядит примерно так, как показано ниже: Вам необходимо обновить IP-адреса ваших веб-серверов.\r\n\r\n`nginx\r\n    upstream webservers {\r\n        server 192.168.169.131:8000;\r\n        server 192.168.169.132:8000;\r\n    }\r\n\r\n    server {\r\n        listen 80;\r\n\r\n        location / {\n                proxy_pass http://webservers;\r\n        }\r\n    }\r\n`\r\nЯ уверен, что все, что мы установили, в порядке, но давайте воспользуемся специальной командой с помощью ansible, чтобы проверить установку этих общих инструментов.\nansible loadbalancer -m command -a neofetch.\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\nYour complete guide to Ansible\r\n\r\nTЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day68",
            "title": "68. Теги, переменные, инвентаризация и конфигурация сервера базы данных",
            "description": null,
            "content": "Теги\nПоскольку мы оставили наш плейбук во время вчерашней сессии, нам нужно будет запустить все задачи и пьесы в рамках этого плейбука. Это означает, что нам придется запустить веб-серверы и балансировщик нагрузки до конца.\nОднако теги могут позволить нам отделить их друг от друга, если мы захотим. Это может быть эффективным шагом, если в нашей среде есть очень большие и длинные плейбуки.\nВ нашем файле плейбука, в данном случае мы используем ansible-scenario5\r\n\r\n`ansible\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 66!\"\r\n  roles:\r\n    common\r\n    apache2\r\n  tags: web\r\n\r\nhosts: proxy\n  become: yes\r\n  roles:\n    common\r\n    nginx\r\n  tags: proxy\r\n`\r\n\r\nЗатем мы можем подтвердить это с помощью команды ansible-playbook playbook5.yml --list-tags, а список тегов будет содержать теги, которые мы определили в нашем плейбуке.\nТеперь, если мы хотим нацелиться только на прокси, мы можем сделать это, выполнив ansible-playbook playbook5.yml --tags proxy, и это, как вы можете видеть ниже, запустит плейбук только против прокси.\nТеги могут быть добавлены и на уровне задач, так что мы можем получить действительно подробную информацию о том, где и что вы хотите, чтобы произошло. Это могут быть теги, ориентированные на приложения, например, мы можем пройтись по задачам и пометить наши задачи на основе установки, настройки или удаления. Еще один очень полезный тег, который вы можете использовать, это\ntag: always, который гарантирует, что независимо от того, какие -теги вы используете в вашей команде, если что-то помечено значением always, то оно всегда будет запущено при выполнении команды ansible-playbook.\nС помощью тегов мы также можем объединить несколько тегов вместе, и если мы выполним команду ansible-playbook playbook5.yml --tags proxy,web, то будут запущены все элементы с этими тегами. Очевидно, что в нашем случае это будет означать то же самое, что и запуск самого плейбука, но если бы у нас было несколько других плейбуков, то это имело бы смысл.\nВы также можете определить более одного тега.\nПеременные\nВ Ansible существует два основных типа переменных.\nСозданная пользователем (User created)\r\nФакты Ansible (Ansible Facts)\r\nФакты Ansible\r\n\r\nКаждый раз, когда мы запускали наши плейбуки, у нас была задача, которую мы не определяли, называемая \"Сбор фактов\", мы можем использовать эти переменные или факты, чтобы заставить вещи происходить с нашими задачами автоматизации.\nЕсли мы выполним следующую команду ansible proxy -m setup, то увидим много выходных данных в формате JSON. Однако на вашем терминале будет много информации, чтобы действительно использовать ее, поэтому мы хотим вывести ее в файл, используя команду ansible proxy -m setup >> facts.json, вы можете увидеть этот файл в этом репозитории, ansible-scenario5\r\n\r\n\r\n\r\nЕсли открыть этот файл, то можно увидеть всевозможную информацию для нашей команды. Мы можем получить наши IP-адреса, архитектуру, версию биоса. Много полезной информации, если мы захотим использовать ее в наших плейбуках.\nИдея заключается в том, чтобы потенциально использовать одну из этих переменных в шаблоне nginx mysite.j2, где мы жестко закодировали IP-адреса наших веб-серверов. Вы можете сделать это, создав цикл for в вашем mysite.j2, который будет проходить через группу [webservers], что позволит нам иметь более двух веб-серверов, автоматически и динамически созданных или добавленных в эту конфигурацию балансировщика нагрузки.\r\n`\r\n#Dynamic Config for server {{ ansible_facts['nodename'] }}\r\n    upstream webservers {\r\n\t{% for host in groups['webservers'] %}\r\n        server {{ hostvarshost['nodename'] }}:8000;\r\n    {% endfor %}\r\n    }\r\n\r\n    server {\r\n        listen 80;\r\n\r\n        location / {\n                proxy_pass http://webservers;\r\n        }\r\n    }\r\n`\r\nРезультат вышеописанных действий будет выглядеть так же, как и сейчас, но если мы добавим больше веб-серверов или удалим один, это динамически изменит конфигурацию прокси. Чтобы это работало, необходимо настроить разрешение имен.\nСозданные пользователем\r\n\r\nПеременные, созданные пользователем, - это то, что мы создали сами. Если вы посмотрите в наш playbook, то увидите, что у нас есть vars:, а затем список из трех переменных, которые мы используем.\n`\r\nhosts: webservers\r\n  become: yes\r\n  vars:\r\n    http_port: 8000\r\n    https_port: 4443\r\n    html_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\r\n  roles:\r\n    common\r\n    apache2\r\n  tags: web\r\n\r\nhosts: proxy\n  become: yes\r\n  roles:\n    common\r\n    nginx\r\n  tags: proxy\r\n`\r\n\r\nОднако мы можем очистить наш плейбук от переменных, переместив их в собственный файл. Мы так и сделаем, но перенесем их в папку ansible-scenario6. В корне этой папки мы создадим папку group_vars. Затем мы создадим еще одну папку под названием all (все группы получат эти переменные). В ней мы создадим файл под названием common_variables.yml и скопируем в него наши переменные из нашего плейбука. Удалим их из плейбука вместе с vars:.\r\n`\r\nhttp_port: 8000\r\nhttps_port: 4443\r\nhtml_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\r\n`\r\n\r\nПоскольку мы связываем это с глобальной переменной, мы также можем добавить сюда наши серверы NTP и DNS. Переменные устанавливаются из созданной нами структуры папок. Ниже вы можете видеть, как чисто выглядит наш Playbook.\r\n\r\n`yaml\r\nhosts: webservers\r\n  become: yes\r\n  roles:\r\n    common\r\n    apache2\r\n  tags: web\r\n\r\nhosts: proxy\n  become: yes\r\n  roles:\n    common\r\n    nginx\r\n  tags: proxy\r\n`\r\n\r\nОдной из этих переменных был http_port, мы можем использовать его снова в нашем цикле for в файле mysite.j2, как показано ниже:\r\n\r\n`\r\n#Dynamic Config for server {{ ansible_facts['nodename'] }}\r\n    upstream webservers {\r\n\t{% for host in groups['webservers'] %}\r\n        server {{ hostvarshost['nodename'] }}:{{ http_port }};\r\n    {% endfor %}\r\n    }\r\n\r\n    server {\r\n        listen 80;\r\n\r\n        location / {\n                proxy_pass http://webservers;\r\n        }\r\n    }\r\n`\r\n\r\nМы также можем определить ansible fact в нашем файле roles/apache2/templates/index.html.j2, чтобы мы могли понять, на каком веб-сервере мы находимся.\r\n\r\n`html\r\n\r\n\r\n{{ html_welcome_msg }}! I'm webserver {{ ansible_facts['nodename'] }}\n`\r\nРезультаты выполнения команды ansible-playbook playbook6.yml с нашими изменениями переменных означают, что когда мы нажимаем на наш loadbalancer, вы можете увидеть, что мы нажимаем на любой из веб-серверов, которые есть в нашей группе.\r\n\r\n\r\nМы также можем добавить папку host_vars и создать web01.yml и иметь определенное сообщение или изменить то, как это выглядит для каждого хоста, если захотим.\nФайлы инвентаризации\r\n\r\nДо сих пор мы использовали файл hosts по умолчанию в папке /etc/ansible для определения наших хостов. Однако мы можем иметь разные файлы для разных окружений, например, production и staging. Я не собираюсь создавать больше окружений. Но мы можем создавать свои собственные файлы хостов.\nМы можем создать несколько файлов для нашего различного количества серверов и узлов. Мы будем вызывать их с помощью ansible-playbook -i dev playbook.yml Вы также можете определить переменные в файле hosts и затем распечатать их или использовать эти переменные где-нибудь еще в своих плейбуках. Например, в примере и учебном курсе, за которым я слежу ниже, они добавили переменную окружения, созданную в файле host, в шаблон веб-страницы loadbalancer, чтобы показать окружение как часть сообщения веб-страницы.\nРазвертывание нашего сервера базы данных\r\n\r\nУ нас осталась еще одна машина, которую мы еще не включили и не настроили. Мы можем сделать это с помощью команды vagrant up db01 из места, где находится наш Vagrantfile. Когда машина будет запущена и доступна, нам нужно убедиться, что SSH-ключ скопирован с помощью ssh-copy-id db01, чтобы мы могли получить доступ.\nМы будем работать из папки ansible-scenario7.\r\n\r\nЗатем воспользуемся командой ansible-galaxy init roles/mysql, чтобы создать новую структуру папок для новой роли под названием \"mysql\".\nВ нашем плейбуке мы собираемся добавить новый блок для конфигурации базы данных. В файле /etc/ansible/hosts мы определили нашу группу базы данных. Затем мы указываем нашей группе базы данных роль common и новую роль mysql, которую мы создали в предыдущем шаге. Мы также помечаем нашу группу базы данных тегами database, что означает, как мы обсуждали ранее, что мы можем выбрать запуск только с этими тегами, если захотим.\r\n`\r\nhosts: webservers\r\n  become: yes\r\n  roles:\r\n    common\r\n    apache2\r\n  tags:\r\n    web\r\n\r\nhosts: proxy\r\n  become: yes\r\n  roles:\r\n    common\r\n    nginx\r\n  tags:\n    proxy\r\n\r\nhosts: database\r\n  become: yes\r\n  roles:\r\n    common\r\n    mysql\r\n  tags: database\r\n`\r\n\r\nТеперь в структуре папок с нашими ролями автоматически создается дерево, в котором нам нужно заполнить следующее:\nHandlers - main.yml\r\n`\r\nhandlers file for roles/mysql\r\nname: restart mysql\r\n  service:\r\n    name: mysql\r\n    state: restarted\r\n`\r\n\r\nTasks - install_mysql.yml, main.yml & setup_mysql.yml\r\n\r\ninstall_mysql.yml - this task is going to be there to install mysql and ensure that the service is running.\n`\r\nname: \"Install Common packages\"\r\n  apt: name={{ item }} state=latest\r\n  with_items:\r\n   python3-pip\r\n   mysql-client\r\n   python3-mysqldb\r\n   libmysqlclient-dev\r\n\r\nname: Ensure mysql-server is installed latest version\r\n  apt: name=mysql-server state=latest\r\n\r\nname: Installing python module MySQL-python\r\n  pip:\r\n    name: PyMySQL\r\n\r\nname: Ensure mysql-server is running\r\n  service:\r\n    name: mysql\r\n    state: started\r\n`\r\n\r\nmain.yml is a pointer file that will suggest that we import_tasks from these files.\n`\r\ntasks file for roles/mysql\r\nimport_tasks: install_mysql.yml\r\nimport_tasks: setup_mysql.yml\r\n`\r\n\r\nsetup_mysql.yml - This task will create our database and database user.\n`\r\nname: Create my.cnf configuration file\r\n  template: src=templates/my.cnf.j2 dest=/etc/mysql/conf.d/mysql.cnf\r\n  notify: restart mysql\r\n\r\nname: Create database user with name 'devops' and password 'DevOps90' with all database privileges\r\n  community.mysql.mysql_user:\r\n    login_unix_socket: /var/run/mysqld/mysqld.sock\r\n    login_user: \"{{ mysql_user_name }}\"\n    login_password: \"{{ mysql_user_password }}\"\n    name: \"{{db_user}}\"\r\n    password: \"{{db_pass}}\"\r\n    priv: '.:ALL'\r\n    host: '%'\r\n    state: present\r\n\r\nname: Create a new database with name '90daysofdevops'\r\n  mysql_db:\r\n    login_user: \"{{ mysql_user_name }}\"\n    login_password: \"{{ mysql_user_password }}\"\n    name: \"{{ db_name }}\"\r\n    state: present\r\n`\r\n\r\nВы можете видеть, что мы используем некоторые переменные для определения некоторых конфигураций, таких как пароли, имена пользователей и базы данных, все это хранится в файле group_vars/all/common_variables.yml.\r\n`\r\nhttp_port: 8000\r\nhttps_port: 4443\r\nhtml_welcome_msg: \"Hello 90DaysOfDevOps - Welcome to Day 68!\"\r\n\r\nmysql_user_name: root\r\nmysql_user_password: \"vagrant\"\r\ndb_user: devops\r\ndb_pass: DevOps90\r\ndb_name: 90DaysOfDevOps\r\n`\r\nУ нас также есть файл my.cnf.j2 в папке templates, который выглядит следующим образом:\n`\r\n[mysql]\nbind-address = 0.0.0.0\r\n`\nЗапуск плейбука\nТеперь наша виртуальная машина запущена и работает, и у нас есть наши конфигурационные файлы на месте, теперь мы готовы запустить наш плейбук, который будет включать все, что мы сделали раньше, если мы запустим следующий ansible-playbook playbook7.yml или мы можем выбрать просто развертывание на нашу группу баз данных с помощью команды ansible-playbook playbook7.yml --tags database, которая просто запустит наши новые конфигурационные файлы.\nЯ запустил только тег database, но наткнулся на ошибку. Эта ошибка говорит мне, что у нас не установлен pip3 (Python). Мы можем исправить это, добавив это в наши общие задачи и установив\nМы исправили вышеуказанное и запустили плейбук снова, и у нас получилось успешное изменение.\nМы должны убедиться, что на нашем новом настроенном сервере db01 все так, как мы хотим. Мы можем сделать это с нашего узла управления с помощью команды ssh db01.\nДля подключения к MySQL я использовал команду sudo /usr/bin/mysql -u root -p и указал пароль vagrant для root.\nКогда мы подключились, давайте сначала убедимся, что у нас создан пользователь devops. select user, host from mysql.user;\r\n\r\n\r\n\r\nТеперь мы можем выполнить команду SHOW DATABASES;, чтобы увидеть нашу новую базу данных, которая также была создана.\nНа самом деле я использовал root для подключения, но теперь мы можем войти в систему под учетной записью devops, используя команду sudo /usr/bin/mysql -u devops -p, но пароль здесь будет DevOps90.\nЯ обнаружил, что в нашем setup_mysql.yml мне пришлось добавить строку login_unix_socket: /var/run/mysqld/mysqld.sock для успешного подключения к моему экземпляру db01 mysql, и теперь каждый раз, когда я запускаю это, он сообщает об изменении при создании пользователя, любые предложения будут очень признательны.\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\nYour complete guide to Ansible\r\n\r\nЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day69",
            "title": "69. Ansible - контроллер автоматизации (Tower), AWX, Vault",
            "description": null,
            "content": "\r\nЗавершая раздел об управлении конфигурацией, я хотел бы рассмотреть другие области, с которыми вы можете столкнуться при работе с Ansible.\nСуществует множество продуктов, составляющих платформу Ansible Automation.\nRed Hat Ansible Automation Platform - это основа для создания и эксплуатации автоматизации в организации. Платформа включает в себя все инструменты, необходимые для внедрения автоматизации в масштабах предприятия.\r\n\r\n\r\n\r\nЯ постараюсь осветить некоторые из них в этом посте. Но для получения более подробной информации на официальном сайте Red Hat Ansible есть много другой информации. Ansible.com\r\nAnsible Automation Controller | AWX\nЯ объединил эти два продукта вместе, потому что Automation Controller и AWX очень похожи в том, что они предлагают.\nПроект AWX или сокращенно AWX - это проект сообщества с открытым исходным кодом, спонсируемый Red Hat, который позволяет вам лучше контролировать ваши проекты Ansible в ваших средах. AWX - это основной проект, из которого взят компонент контроллера автоматизации.\nЕсли вы ищете корпоративное решение, то вам нужен контроллер автоматизации, или вы могли слышать его как Ansible Tower. Контроллер автоматизации Ansible - это плоскость управления для платформы автоматизации Ansible.\nИ AWX, и контроллер автоматизации обладают следующими характеристиками, превосходящими все, что мы рассмотрели в этом разделе до сих пор.\nПользовательский интерфейс\nУправление доступом на основе ролей\nРабочие процессы\nИнтеграция CI/CD\nAutomation Controller - это корпоративное предложение, в котором вы платите за поддержку.\nМы рассмотрим развертывание AWX в нашей среде minikube Kubernetes.\nРазвертывание Ansible AWX\nAWX не нужно развертывать в кластере Kubernetes, github для AWX от ansible даст вам эту подробную информацию. Однако, начиная с версии 18.0, AWX Operator является предпочтительным способом установки AWX.\nПрежде всего, нам нужен кластер minikube. Мы можем сделать это, если вы следили за разделом Kubernetes, создав новый кластер minikube с помощью команды minikube start --cpus=4 --memory=6g --addons=ingress.\nОфициальный Ansible AWX Operator можно найти здесь. Как указано в инструкции по установке, вы должны клонировать этот репозиторий, а затем выполнить развертывание.\nЯ сделал форк вышеуказанного репозитория, а затем выполнил команду git clone https://github.com/MichaelCade/awx-operator.git. Я советую вам сделать то же самое и не использовать мой репозиторий, так как я могу что-то изменить или его там может не быть.\nВ клонированном репозитории вы найдете файл awx-demo.yml, в котором нам нужно изменить NodePort на ClusterIP, как показано ниже:\r\n`\r\napiVersion: awx.ansible.com/v1beta1\r\nkind: AWX\r\nmetadata:\r\n  name: awx-demo\r\nspec:\r\n  service_type: ClusterIP\r\n`\r\n\r\nСледующим шагом будет определение нашего пространства имен, в котором мы будем развертывать оператор awx, используя команду export NAMESPACE=awx, а затем команду make deploy, мы начнем развертывание.\nПри проверке у нас есть наше новое пространство имен, и у нас есть наш awx-operator-controller pod, запущенный в нашем пространстве имен. kubectl get pods -n awx.\r\n\r\n\r\n\r\nВ клонированном репозитории вы найдете файл awx-demo.yml. Теперь мы хотим развернуть его в нашем кластере Kubernetes и нашем пространстве имен awx. kubectl create -f awx-demo.yml -n awx.\r\n\r\n\r\n\r\nВы можете следить за прогрессом с помощью kubectl get pods -n awx -w, который будет визуально следить за происходящим.\nУ вас должно получиться что-то похожее на изображение, которое вы видите ниже, когда все работает.\nТеперь мы должны иметь доступ к нашей awx установке после запуска в новом терминале minikube service awx-demo-service --url -n $NAMESPACE, чтобы открыть ее через minikube ingress.\nЕсли мы откроем браузер по этому адресу [], вы увидите, что нам будет предложено ввести имя пользователя и пароль.\nПо умолчанию имя пользователя - admin, чтобы получить пароль, мы можем выполнить следующую команду kubectl get secret awx-demo-admin-password -o jsonpath=\"{.data.password}\" -n awx| base64 --decode.\r\n\r\n\r\nОчевидно, что это дает вам пользовательский интерфейс для централизованного управления плейбуком и задачами управления конфигурацией, а также позволяет вам работать вместе, в отличие от того, что мы делали до сих пор, когда мы работали с одной станции управления ansible.\nЭто еще одна из тех областей, где вы, вероятно, могли бы провести еще много времени, изучая возможности этого инструмента.\nЯ приведу отличный ресурс от Джеффа Гирлинга, который более подробно рассказывает об использовании Ansible AWX. Ansible 101 - Episode 10 - Ansible Tower and AWX\nВ этом видео он также подробно рассказывает о различиях между Automation Controller (ранее Ansible Tower) и Ansible AWX (Free and Open Source).\r\nAnsible Vault\nansible-vault позволяет нам шифровать и расшифровывать файлы данных Ansible. На протяжении всего этого раздела мы пропустили и поместили часть нашей конфиденциальной информации в открытый текст.\nВстроенный в двоичный файл Ansible ansible-vault позволяет нам скрыть эту конфиденциальную информацию.\nУправление секретами постепенно становится еще одной областью, которой следовало бы уделить больше времени наряду с такими инструментами, как HashiCorp Vault или AWS Key Management Service. Я отмечу эту область как ту, в которую следует погрузиться глубже.\r\n\r\nЯ собираюсь дать ссылку на отличный ресурс и демонстрационный пример от Jeff Geerling Ansible 101 - Episode 6 - Ansible Vault and Roles\r\nAnsible Galaxy (Docs)\r\n\r\nИтак, мы уже использовали ansible-galaxy для создания некоторых ролей и файловой структуры для нашего демо-проекта. Но у нас также есть документация по Ansible Galaxy\r\n\r\n\"Galaxy - это центр для поиска и обмена содержимым Ansible\".\r\nТестирование Ansible\r\n\r\nAnsible Molecule - проект Molecule предназначен для помощи в разработке и тестировании ролей Ansible.\r\n\r\nAnsible Lint - CLI-инструмент для линтинга плейбуков, ролей и коллекций.\r\nДругой ресурс\nДокументация Ansible\r\nРесурсы\nWhat is Ansible\r\nAnsible 101 - Episode 1 - Introduction to Ansible\r\nNetworkChuck - You need to learn Ansible right now!\r\nYour complete guide to Ansible\r\n\r\nЭтот последний плейлист, приведенный выше, является тем местом, откуда было взято много кода и идей для этого раздела, отличным ресурсом и руководством в видеоформате.\r\nВ этом посте мы завершаем рассмотрение управления конфигурацией, далее мы перейдем к CI/CD Pipelines и некоторым инструментам и процессам, которые мы можем увидеть и использовать для достижения этого рабочего процесса при разработке и выпуске приложений.\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day70",
            "title": "70. Конвейеры CI/CD",
            "description": null,
            "content": "\r\nРеализация конвейера CI/CD (Continous Integration/Continous Deployment) является основой современной среды DevOps.\nОн устраняет разрыв между разработкой и операциями, автоматизируя сборку, тестирование и развертывание приложений.\nМы много говорили об этой мантре Continous во вступительном разделе задачи. Но повторим еще раз:\nContinous Integration (CI) - это более современная практика разработки программного обеспечения, при которой инкрементные изменения кода вносятся чаще и надежнее. Автоматизированные шаги рабочего процесса сборки и тестирования, запускаемые Contininous Integration, обеспечивают надежность изменений кода, сливаемых в репозиторий.\nЗатем этот код / приложение быстро и беспрепятственно доставляется в рамках процесса непрерывного развертывания.\r\nВажность CI/CD?\r\n\r\nДоставка программного обеспечения быстро и эффективно\r\nОблегчает эффективный процесс вывода приложений на рынок как можно быстрее.\r\nНепрерывный поток исправлений ошибок и новых функций без ожидания месяцев или лет выпуска версии.\r\n\r\nВозможность для разработчиков регулярно вносить небольшие важные изменения означает, что мы быстрее получаем исправления и новые функции.\r\nХорошо, так что же это значит?\nВ День 5 мы рассмотрели много теории, лежащей в основе DevOps, и, как уже упоминалось здесь, CI/CD Pipeline является основой современной среды DevOps.\nDevOps\r\n\r\nЯ хочу повторить некоторые ключевые моменты на этом изображении выше, теперь, когда мы немного продвинулись в изучении основ DevOps.\nМы имеем в виду жизненный цикл разработки программного обеспечения (SDLC).\nЭтапы обычно записываются в бесконечном цикле, поскольку этот цикл повторяется вечно.\nThe steps in the cycle are, developers write the code then it gets built or all compiled together then it's tested for bugs then it's deployed into production where it's used (Operated) by end users or customers then we monitor and collect feedback and finally we plan improvements around that feedback rinse and repeat.\nДавайте немного углубимся в CI/CD\r\nCI\r\n\r\nCI - это практика разработки, которая требует от разработчиков интегрировать код в общий репозиторий несколько раз в день.\nКогда код написан и помещен в репозиторий, такой как github или gitlab, вот тут-то и начинается волшебство.\nКод проверяется автоматизированной сборкой, что позволяет командам или владельцу проекта обнаружить любые проблемы на ранней стадии.\r\n\r\n\r\n\r\nПосле этого код анализируется и подвергается серии автоматизированных тестов.\nЮнит-тестирование - тестирование отдельных частей исходного кода.\nтестирование на валидность - проверяется, что программное обеспечение удовлетворяет или соответствует предполагаемому использованию.\r\nТестирование формата проверяет синтаксис и другие ошибки форматирования.\nЭти тесты создаются как рабочий процесс и затем запускаются каждый раз, когда вы продвигаете мастер-ветку, поэтому практически каждая крупная команда разработчиков имеет какой-то рабочий процесс CI/CD, и помните, что в команде разработчиков новый код может поступать из команд по всему миру в разное время суток от разработчиков, работающих над самыми разными проектами, поэтому эффективнее построить автоматизированный рабочий процесс тестов, которые убеждаются, что все находятся на одной странице, прежде чем код будет принят. Человеку потребуется гораздо больше времени, чтобы сделать это каждый раз.\nКак только мы завершили наши тесты и они прошли успешно, мы можем скомпилировать их и отправить в наш репозиторий. Для примера я использую Docker Hub, но это может быть любое другое хранилище, которое затем будет использовано для CD-аспекта конвейера.\nИтак, этот процесс, очевидно, очень похож на процесс разработки программного обеспечения: мы создаем наше приложение, добавляем, исправляем ошибки и т.д., затем обновляем контроль исходных текстов и версионируем их, одновременно тестируя.\nПереходим к следующему этапу - элементу CD, который на самом деле все больше и больше является тем, что мы обычно видим от любого готового программного обеспечения, я бы утверждал, что мы увидим тенденцию, что если мы получим наше программное обеспечение от такого поставщика, как Oracle или Microsoft, мы будем потреблять его из репозитория типа Docker Hub, а затем мы будем использовать наши конвейеры CD для развертывания этого в наших средах.\nCD\nТеперь у нас есть протестированная версия нашего кода, и мы готовы к развертыванию на природе. Как я уже сказал, поставщик программного обеспечения пройдет через этот этап, но я твердо уверен, что именно так мы все будем развертывать готовое программное обеспечение, которое нам понадобится в будущем.\nТеперь пришло время выпустить наш код в среду. Это будет включать в себя производственную среду, но также, вероятно, и другие среды, такие как staging.\nСледующим шагом, по крайней мере, в день 1 v1 развертывания программного обеспечения, является то, что нам нужно убедиться, что мы переносим правильную кодовую базу в правильную среду. Это может быть извлечение элементов из репозитория программного обеспечения (DockerHub), но более чем вероятно, что мы также извлечем дополнительную конфигурацию из другого репозитория кода, например, конфигурацию для приложения. На диаграмме ниже мы извлекаем последний релиз программного обеспечения из DockerHub, а затем выпускаем его в нашу среду, при этом, возможно, получая конфигурацию из репозитория Git. Наш CD-инструмент выполняет это и передает все в нашу среду.\nСкорее всего, это делается не одновременно, т.е. мы переходим в промежуточную среду и запускаем ее с нашей собственной конфигурацией, чтобы убедиться, что все правильно, и это может быть ручным шагом для тестирования или автоматизированным (давайте остановимся на автоматизированном), прежде чем позволить этому коду быть развернутым в продакшн.\nПосле этого, когда выйдет v2 приложения, мы прополощем и повторим шаги, на этот раз мы убедимся, что наше приложение + конфигурация развернуты в staging, убедимся, что все хорошо, и затем развернем в production.\r\nЗачем использовать CI/CD?\nЯ думаю, мы уже неоднократно рассказывали о преимуществах, но они заключаются в том, что CI/CD автоматизирует то, что в противном случае пришлось бы делать вручную. Он находит небольшие проблемы до того, как они проникнут в основную кодовую базу. Вы, вероятно, можете себе представить, что если вы выкладываете плохой код своим клиентам, то у вас будут плохие времена!\nЭто также помогает предотвратить то, что мы называем техническим долгом - идею о том, что поскольку основные репозитории кода постоянно дорабатываются с течением времени, то быстрое исправление, сделанное в первый день, становится экспоненциально более дорогим исправлением годы спустя, потому что теперь этот пластырь исправления будет так глубоко переплетен и вплетен во все кодовые базы и логику.\nИнструментарий\r\n\r\nКак и в других разделах, мы будем работать с некоторыми инструментами, которые обеспечивают процесс конвейера CI/CD.\nЯ считаю важным отметить, что не все инструменты должны делать и CI, и CD. Мы рассмотрим ArgoCD, который, как вы догадались, отлично справляется с CD-элементом развертывания нашего программного обеспечения в кластере Kubernetes. Но что-то вроде Jenkins может работать на разных платформах.\nЯ планирую рассмотреть следующее:\nJenkins\nArgoCD\nGitHub Actions\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day71",
            "title": "71. Введение в Jenkins",
            "description": null,
            "content": "Jenkins - это инструмент непрерывной интеграции, который позволяет непрерывно разрабатывать, тестировать и развертывать вновь созданный код.\nЭтого можно достичь двумя способами: ночные сборки или непрерывная разработка. Первый вариант заключается в том, что наши разработчики в течение дня занимаются своими задачами и в конце рабочего дня вносят свои изменения в репозиторий исходного кода. Затем в течение ночи мы проводим модульные тесты и собираем программное обеспечение. Это можно считать старым способом интеграции всего кода.\nДругой вариант и более предпочтительный способ заключается в том, что наши разработчики по-прежнему фиксируют свои изменения в исходном коде, а затем, после фиксации кода, непрерывно запускается процесс сборки.\nПриведенные выше методы означают, что при распределении разработчиков по всему миру у нас нет определенного времени каждый день, когда мы должны прекратить фиксацию изменений в коде. Именно здесь на помощь приходит Jenkins, который выступает в роли CI-сервера, контролирующего тесты и процессы сборки.\nЯ знаю, что мы говорим о Jenkins, но я также хочу добавить еще несколько, которые можно будет рассмотреть позже, чтобы понять, почему я вижу Jenkins как наиболее популярный, почему это так и что другие могут сделать по сравнению с Jenkins.\nTravisCI - Размещенный, распределенный сервис непрерывной интеграции, используемый для сборки и тестирования программных проектов, размещенных на GitHub.\nBamboo - может запускать несколько сборок параллельно для более быстрой компиляции, имеет встроенную функциональность для связи с репозиториями и задачи сборки для Ant, Maven.\nBuildbot - это фреймворк с открытым исходным кодом для автоматизации процессов сборки, тестирования и выпуска программного обеспечения. Он написан на языке Python и поддерживает распределенное, параллельное выполнение заданий на нескольких платформах.\nApache Gump - специфичен для Java-проектов, разработан с целью сборки и тестирования этих Java-проектов каждую ночь. обеспечивает совместимость всех проектов как на уровне API, так и на уровне функциональности.\nПоскольку мы сейчас сосредоточимся на Jenkins - Jenkins, как и все вышеперечисленные инструменты, имеет открытый исходный код и представляет собой сервер автоматизации, написанный на Java. Он используется для автоматизации процесса разработки программного обеспечения посредством непрерывной интеграции и облегчает непрерывную доставку.\nОсобенности Jenkins\r\n\r\nКак и следовало ожидать, Jenkins имеет множество функций, охватывающих множество областей.\nПростая установка - Jenkins - это самостоятельная программа на базе java, готовая к работе с пакетами для операционных систем Windows, macOS и Linux.\nПростая конфигурация - Простая установка и настройка через веб-интерфейс, включающий проверку ошибок и встроенную помощь.\nПлагины - Множество плагинов доступно в Центре обновления и интегрируется со многими инструментами в инструментальной цепочке CI / CD.\nРасширяемость - В дополнение к доступным плагинам, Jenkins может быть расширен за счет архитектуры плагинов, что обеспечивает практически бесконечное количество вариантов того, для чего он может быть использован.\nРаспределенность - Jenkins легко распределяет работу по нескольким машинам, помогая ускорить сборку, тестирование и развертывание на различных платформах.\nJenkins Pipeline\nВы уже видели этот конвейер, но он используется гораздо шире, и мы не говорили о конкретных инструментах.\nВы собираетесь фиксировать код в Jenkins, который затем будет собирать ваше приложение со всеми автоматизированными тестами, а затем выпускать и развертывать этот код после завершения каждого этапа. Jenkins позволяет автоматизировать этот процесс.\nАрхитектура Jenkins\nВо-первых, чтобы не изобретать велосипед, всегда стоит начать с Документации Jenkins, но я собираюсь изложить свои заметки и выводы и здесь.\nJenkins может быть установлен на многих различных операционных системах, Windows, Linux и macOS, а также имеет возможность развертывания в виде контейнера Docker и в Kubernetes. Установка Jenkins\r\n\r\nПо мере изучения этого вопроса мы, вероятно, рассмотрим установку Jenkins в кластере minikube, имитируя развертывание в Kubernetes. Но это будет зависеть от скриптов, которые мы составим в оставшейся части раздела.\nТеперь давайте разберем изображение ниже.\nШаг 1 - Разработчики фиксируют изменения в репозитории исходного кода.\r\n\r\nШаг 2 - Jenkins проверяет репозиторий через регулярные промежутки времени и извлекает любой новый код.\r\n\r\nШаг 3 - Сервер сборки затем собирает код в исполняемый файл, в данном примере мы используем maven как хорошо известный сервер сборки. Еще одна область, которую необходимо охватить.\nШаг 4 - Если сборка не удалась, то разработчикам отправляется обратная связь.\nШаг 5 - Jenkins развертывает собранное приложение на тестовом сервере, в данном примере мы используем selenium как хорошо известный тестовый сервер. Еще одна область, которую необходимо охватить.\nШаг 6 - Если тест не прошел, то обратная связь передается разработчикам.\r\n\r\nШаг 7 - Если тесты прошли успешно, мы можем выпустить продукт в производство.\nЭтот цикл непрерывен, именно это позволяет обновлять приложения за минуты, а не за часы, дни, месяцы, годы!\nАрхитектура Jenkins может быть описана гораздо подробнее, если вам это нужно, у них есть возможность работы в режиме master-slave, что позволяет ведущему распределять задачи между подчиненными jenkins.\nДля справки, поскольку Jenkins является открытым исходным кодом, будет много предприятий, которым требуется поддержка, CloudBees - это корпоративная версия Jenkins, которая предоставляет поддержку и, возможно, другие функциональные возможности для платного корпоративного клиента.\nПримером такого клиента является компания Bosch, вы можете ознакомиться с примером Bosch здесь.\r\n\r\nЯ собираюсь найти пошаговый пример приложения, которое мы могли бы использовать, чтобы пройтись по Jenkins, а затем использовать его с другими инструментами.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day72",
            "title": "72. Работа с Jenkins",
            "description": null,
            "content": "\r\nСегодня мы планируем немного поработать с Jenkins и сделать что-то в рамках нашего конвейера CI, рассматривая некоторые примеры кодовых баз, которые мы можем использовать.\nЧто такое конвейер?\nПрежде чем мы начнем, нам нужно знать, что такое конвейер, когда речь идет о CI, и мы уже рассмотрели это на вчерашнем занятии с помощью следующего изображения.\nМы хотим взять процессы или шаги, описанные выше, и автоматизировать их, чтобы в итоге получить результат, то есть развернутое приложение, которое мы можем отправить нашим клиентам, конечным пользователям и т.д.\nЭтот автоматизированный процесс позволяет нам иметь контроль версий для наших пользователей и клиентов. Каждое изменение, улучшение функций, исправление ошибок и т.д. проходит через этот автоматизированный процесс, подтверждая, что все в порядке, без излишнего ручного вмешательства, чтобы убедиться, что наш код хорош.\nЭтот процесс включает в себя создание программного обеспечения надежным и повторяемым способом, а также продвижение созданного программного обеспечения (называемого \"сборкой\") через несколько этапов тестирования и развертывания.\r\n\r\nКонвейер jenkins записывается в текстовый файл Jenkinsfile. Который сам должен быть зафиксирован в репозитории контроля исходного кода. Это также известно как Pipeline as code, мы также можем сравнить это с Infrastructure as code, о которой мы рассказывали несколько недель назад.\nJenkins Pipeline Definition\nРазвертывание Jenkins\nЯ получил некоторое удовольствие от развертывания Jenkins, Вы заметите из документации, что есть много вариантов того, где вы можете установить Jenkins.\nУчитывая, что у меня под рукой есть minikube, и мы уже использовали его несколько раз, я хотел использовать его и для этой задачи. (Хотя шаги, описанные в Kubernetes Installation, привели к тому, что я уперся в стену и не смог запустить систему, вы можете сравнить эти два варианта, когда я задокументирую свои шаги здесь.\nПервым шагом будет запуск нашего кластера minikube, мы можем сделать это с помощью команды minikube start.\nЯ добавил папку со всеми конфигурациями и значениями YAML, которые можно найти здесь Теперь, когда у нас есть наш кластер, мы можем выполнить следующие действия для создания пространства имен jenkins. kubectl create -f jenkins-namespace.yml\r\n\r\n\r\n\r\nМы будем использовать Helm для развертывания jenkins в нашем кластере, о Helm мы рассказывали в разделе Kubernetes. Сначала нам нужно добавить репозиторий jenkinsci в helm helm repo add jenkinsci https://charts.jenkins.io, затем обновить наши таблицы helm repo update.\nИдея Jenkins заключается в том, что он будет сохранять состояние для своих пайплайнов, вы можете запустить вышеупомянутую установку helm без персистентности, но если эти pods будут перезагружены, изменены или модифицированы, то все пайплайны или конфигурации, которые вы создали, будут потеряны. Мы создадим том для персистентности, используя файл jenkins-volume.yml с помощью команды kubectl apply -f jenkins-volume.yml.\nНам также нужна учетная запись службы, которую мы можем создать с помощью этого yaml-файла и команды. kubectl apply -f jenkins-sa.yml\nНа этом этапе мы готовы к развертыванию с помощью схемы helm, сначала мы определим нашу схему с помощью chart=jenkinsci/jenkins, а затем развернем с помощью этой команды, где jenkins-values.yml содержит учетные записи персистентности и сервисов, которые мы ранее развернули на нашем кластере. helm install jenkins -n jenkins -f jenkins-values.yml $chart.\r\n\r\n\r\nНа этом этапе наши капсулы будут извлекать образ, но у капсулы не будет доступа к хранилищу, поэтому никакая конфигурация не может быть начата с точки зрения запуска Jenkins.\nИменно здесь документация не помогла мне понять, что должно произойти. Но мы видим, что у нас нет разрешения на запуск установки jenkins.\nДля того чтобы исправить вышеописанное или решить проблему, нам нужно убедиться, что мы предоставили доступ или правильное разрешение для того, чтобы наши jenkins pods могли писать в это место, которое мы предложили. Мы можем сделать это, используя minikube ssh, который введет нас в докер-контейнер minikube, на котором мы работаем, а затем, используя sudo chown -R 1000:1000 /data/jenkins-volume, мы можем убедиться, что у нас установлены разрешения на наш том данных.\nВышеописанный процесс должен исправить капсулы, однако если это не так, вы можете заставить капсулы обновиться с помощью команды kubectl delete pod jenkins-0 -n jenkins. На этом этапе у вас должно быть 2/2 запущенных стручка под названием jenkins-0.\nТеперь нам нужен наш пароль администратора, и мы можем сделать это с помощью следующей команды. kubectl exec --namespace jenkins -it svc/jenkins -c jenkins -- /bin/cat /run/secrets/chart-admin-password && echo\r\n\r\n\r\n\r\nТеперь откройте новый терминал, так как мы собираемся использовать команду port-forward, чтобы получить доступ с нашей рабочей станции. kubectl --namespace jenkins port-forward svc/jenkins 8080:8080.\r\n\r\n\r\n\r\nТеперь мы должны быть в состоянии открыть браузер и войти на http://localhost:8080 и аутентифицироваться с именем пользователя: admin и паролем, которые мы собрали в предыдущем шаге.\nПосле аутентификации наша страница приветствия Jenkins должна выглядеть примерно так:\nОтсюда я бы предложил перейти к \"Manage Jenkins\", и вы увидите \"Manage Plugins\", где будут доступны некоторые обновления. Выберите все эти плагины и выберите \"Загрузить сейчас и установить после перезапуска\".\nЕсли вы хотите пойти еще дальше и автоматизировать развертывание Jenkins с помощью shell-скрипта, этот замечательный репозиторий был предоставлен мне в twitter mehyedes/nodejs-k8s\r\nJenkinsfile\nТеперь у нас есть Jenkins, развернутый в нашем кластере Kubernetes, мы можем вернуться назад и подумать об этом Jenkinsfile.\nКаждый Jenkinsfile, скорее всего, будет начинаться примерно так: сначала вы определяете шаги вашего конвейера, в данном случае это Build > Test > Deploy. Но на самом деле мы не делаем ничего, кроме использования команды echo для вызова определенных этапов.\n`\r\n\r\nJenkinsfile (декларативный конвейер)\r\npipeline {\r\n    agent any\r\n\r\n    stages {\r\n        stage('Build') {\r\n            steps {\r\n                echo 'Building..'\r\n            }\r\n        }\r\n        stage('Test') {\r\n            steps {\r\n                echo 'Testing..'\r\n            }\r\n        }\r\n        stage('Deploy') {\r\n            steps {\r\n                echo 'Deploying....'\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n`\r\nВ нашей приборной панели Jenkins выберите \"New Item\" дайте элементу имя, я собираюсь \"echo1\" Я собираюсь предложить, что это Pipeline.\nНажмите Ok, и у вас появятся вкладки (General, Build Triggers, Advanced Project Options и Pipeline) для простого теста нас интересует только Pipeline. В разделе Pipeline у вас есть возможность добавить скрипт, мы можем скопировать и вставить приведенный выше скрипт в поле.\nКак мы уже говорили выше, это не даст многого, но покажет нам этапы нашей сборки > тестирования > развертывания\r\n\r\n\r\n\r\nНажмите Save, теперь мы можем запустить нашу сборку, используя сборку, показанную ниже.\nМы также должны открыть терминал и выполнить команду kubectl get pods -n jenkins, чтобы посмотреть, что произойдет.\nХорошо, очень просто, но теперь мы можем видеть, что наше развертывание и установка Jenkins работает правильно, и мы можем начать видеть здесь строительные блоки конвейера CI.\nВ следующем разделе мы будем строить конвейер Jenkins.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day73",
            "title": "73. Построение конвейера Jenkins",
            "description": null,
            "content": "\r\nВ предыдущем разделе мы развернули Jenkins на нашем кластере Minikube и создали очень простой Jenkins Pipeline, который не делал ничего особенного, кроме как повторял этапы Pipeline.\nВы также могли заметить, что в процессе создания Jenkins Pipeline нам доступны некоторые примеры скриптов для запуска.\nПервый демонстрационный скрипт - \"Declartive (Kubernetes)\", и вы можете увидеть его этапы ниже.\r\n`\r\n// Uses Declarative syntax to run commands inside a container.\r\npipeline {\r\n    agent {\r\n        kubernetes {\r\n            // Rather than inline YAML, in a multibranch Pipeline you could use: yamlFile 'jenkins-pod.yaml'\r\n            // Or, to avoid YAML:\r\n            // containerTemplate {\r\n            //     name 'shell'\r\n            //     image 'ubuntu'\r\n            //     command 'sleep'\r\n            //     args 'infinity'\r\n            // }\r\n            yaml '''\r\napiVersion: v1\r\nkind: Pod\r\nspec:\r\n  containers:\r\n  name: shell\r\n    image: ubuntu\r\n    command:\r\n    sleep\r\n    args:\r\n    infinity\r\n'''\r\n            // Can also wrap individual steps:\r\n            // container('shell') {\r\n            //     sh 'hostname'\r\n            // }\r\n            defaultContainer 'shell'\r\n        }\r\n    }\r\n    stages {\r\n        stage('Main') {\r\n            steps {\r\n                sh 'hostname'\r\n            }\r\n        }\r\n    }\r\n}\r\n`\r\nНиже показан результат того, что происходит при выполнении этого конвейера.\nСоздание задания\nЦели\nСоздать простое приложение и сохранить его в публичном репозитории GitHub (https://github.com/scriptcamp/kubernetes-kaniko.git).\r\n\r\nС помощью Jenkins собрать образ нашего docker-контейнера и выложить в docker hub. (Для этого мы будем использовать частный репозиторий).\nЧтобы добиться этого в нашем кластере Kubernetes, работающем в Minikube или с его помощью, нам нужно использовать нечто под названием Kaniko В общем, если вы используете Jenkins в реальном кластере Kubernetes или запускаете его на сервере, вы можете указать агента, который даст вам возможность выполнять команды сборки docker и загружать их в DockerHub.\nУчитывая вышесказанное, мы также собираемся развернуть секрет в Kubernetes с нашими учетными данными GitHub.\r\n`\r\nkubectl create secret docker-registry dockercred \\\r\n    --docker-server=https://index.docker.io/v1/ \\\r\n    --docker-username= \\\r\n    --docker-password=\\\r\n    --docker-email=\r\n`\r\n\r\nНа самом деле я хочу поделиться еще одним замечательным ресурсом от DevOpsCube.com, где рассматривается многое из того, о чем мы будем говорить здесь.\nДобавление учетных данных в Jenkins\nОднако если вы используете систему Jenkins, в отличие от нашей, то вы, скорее всего, захотите определить свои учетные данные в Jenkins, а затем использовать их несколько раз в своих конвейерах и конфигурациях. Мы можем ссылаться на эти учетные данные в конвейерах, используя ID, который мы определили при создании. Я пошел дальше и создал учетные данные для DockerHub и GitHub.\nСначала выберите \"Manage Jenkins\", а затем \"Manage Credentials\".\r\n\r\n\r\n\r\nВ центре страницы вы увидите магазины, предназначенные для Jenkins, нажмите на Jenkins здесь.\nТеперь выберите Global Credentials (Unrestricted).\r\n\r\n\r\n\r\nЗатем в левом верхнем углу у вас есть Добавить учетные данные\nЗаполните данные вашей учетной записи и затем выберите OK, помните, что ID - это то, на что вы будете ссылаться, когда захотите вызвать эту учетную запись. Мой совет здесь также заключается в том, что вы должны использовать специальные маркеры доступа, а не пароли.\nДля GitHub вы должны использовать Personal Access Token.\r\n\r\nЛично мне процесс создания этих учетных записей показался не очень интуитивным, поэтому, хотя мы не используем их, я хотел поделиться процессом, так как он не совсем понятен из пользовательского интерфейса.\nПостроение конвейера\r\n\r\nУ нас есть учетные данные DockerHub, развернутые как секрет в нашем кластере Kubernetes, к которым мы будем обращаться на этапе docker deploy to DockerHub в нашем конвейере.\nСценарий конвейера - это то, что вы видите ниже, это, в свою очередь, может стать нашим Jenkinsfile, расположенным в нашем репозитории GitHub, который, как вы можете видеть, также указан на этапе Get the project в конвейере.\r\n`\r\npodTemplate(yaml: '''\r\n    apiVersion: v1\r\n    kind: Pod\r\n    spec:\r\n      containers:\r\n      name: maven\r\n        image: maven:3.8.1-jdk-8\r\n        command:\r\n        sleep\r\n        args:\r\n        99d\r\n      name: kaniko\r\n        image: gcr.io/kaniko-project/executor:debug\r\n        command:\r\n        sleep\r\n        args:\r\n        9999999\r\n        volumeMounts:\r\n        name: kaniko-secret\r\n          mountPath: /kaniko/.docker\r\n      restartPolicy: Never\r\n      volumes:\r\n      name: kaniko-secret\r\n        secret:\r\n            secretName: dockercred\r\n            items:\r\n            key: .dockerconfigjson\r\n              path: config.json\r\n''') {\r\n  node(POD_LABEL) {\r\n    stage('Get the project') {\r\n      git url: 'https://github.com/scriptcamp/kubernetes-kaniko.git', branch: 'main'\r\n      container('maven') {\r\n        stage('Test the project') {\r\n          sh '''\r\n          echo pwd\r\n          '''\r\n        }\r\n      }\r\n    }\r\n\r\n    stage('Build & Test the Docker Image') {\r\n      container('kaniko') {\r\n        stage('Deploy to DockerHub') {\r\n          sh '''\r\n            /kaniko/executor --context pwd --destination michaelcade1/helloworld:latest\r\n          '''\r\n        }\r\n      }\r\n    }\r\n\r\n  }\r\n}\r\n`\r\n\r\nЧтобы начать работу на приборной панели Jenkins, нам нужно выбрать \"Новый элемент\"\nЗатем мы дадим нашему элементу имя, выберем Pipeline и нажмем OK.\nМы не будем выбирать общие триггеры или триггеры сборки, но поиграйте с ними, так как здесь есть несколько интересных расписаний и других конфигураций, которые могут быть полезны.\nНас интересует только вкладка Pipeline в конце.\nВ определении пайплайн мы скопируем и вставим скрипт пайплайна, который мы описали выше, в раздел Script и нажмем кнопку save.\nДалее мы выберем опцию \"Build Now\" в левой части страницы.\nТеперь вам нужно подождать некоторое время, меньше минуты, и вы должны увидеть в статусе этапы, которые мы определили выше в нашем скрипте.\nЧто еще более важно, если мы теперь перейдем на наш DockerHub и проверим, что у нас есть новая сборка.\nВ целом это заняло некоторое время, но я хотел придерживаться этого, чтобы получить практический опыт и проработать скрипт, который может выполнить каждый, используя minikube и доступ к github и dockerhub.\nРепозиторий DockerHub, который я использовал для этого демо, был частным. Но в следующем разделе я хочу продвинуть некоторые из этих этапов и заставить их действительно что-то делать, а не просто выводить pwd, и действительно запустить некоторые тесты и этапы сборки.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day74",
            "title": "74. Hello World - Jenkinsfile App Pipeline",
            "description": null,
            "content": "Здравствуй мир - Jenkinsfile App Pipeline\r\n\r\nВ предыдущем разделе мы построили простой конвейер в Jenkins, который будет перемещать наш образ докера из нашего dockerfile в публичном репозитории GitHub в наш частный репозиторий Dockerhub.\nВ этом разделе мы хотим сделать еще один шаг вперед и добиться следующего с помощью нашего простого приложения.\nЦель\nDockerfile (Hello World)\r\nJenkinsfile\nJenkins Pipeline для запуска при обновлении репозитория GitHub\nИспользуйте репозиторий GitHub в качестве источника.\nЗапуск - Clone/Get Repository, Build, Test, Deploy Stages\r\nРазвертывание на DockerHub с инкрементными номерами версий\r\nStretch Goal для развертывания на нашем кластере Kubernetes (для этого потребуется еще одно задание и репозиторий манифеста с использованием учетных данных GitHub).\r\nШаг первый\nУ нас есть наш GitHub репозиторий В настоящее время он содержит наш Dockerfile и наш index.html\nЭто то, что мы использовали в качестве источника в нашем конвейере, теперь мы хотим добавить этот скрипт Jenkins Pipeline в наш репозиторий GitHub.\nТеперь вернемся к нашей приборной панели Jenkins и создадим новый пайплайн, но теперь вместо вставки нашего скрипта мы будем использовать \"Pipeline script from SCM\" Мы будем использовать приведенные ниже параметры конфигурации.\nДля справки мы будем использовать https://github.com/MichaelCade/Jenkins-HelloWorld.git в качестве URL репозитория.\nНа этом этапе мы можем нажать кнопку сохранить и применить, после чего мы сможем вручную запустить наш Pipeline для сборки нового образа Docker, загруженного в наш репозиторий DockerHub.\nОднако я также хочу убедиться, что мы установили расписание, по которому при каждом изменении нашего репозитория или исходного кода будет запускаться сборка. Мы можем использовать веб-крючки или запланированное извлечение.\nЭто важный момент, потому что если вы используете дорогостоящие облачные ресурсы для хранения конвейера и у вас много изменений в репозитории кода, то вы понесете большие расходы. Мы знаем, что это демонстрационная среда, поэтому я использую опцию \"poll scm\". (Также я считаю, что при использовании minikube мне не хватает возможности использовать webhooks)\r\n\r\n\r\n\r\nОдна вещь, которую я изменил со вчерашней сессии, это то, что теперь я хочу загружать изображение в публичный репозиторий, который в данном случае будет michaelcade1\\90DaysOfDevOps, мой Jenkinsfile уже содержит это изменение. И из предыдущих разделов я удалил все существующие образы демо-контейнеров.\nДвигаясь назад, мы создали наш Pipeline, а затем, как было показано ранее, добавили нашу конфигурацию.\nНа данном этапе наш конвейер еще не запущен, и вид сцены будет выглядеть примерно так.\nТеперь нажмем кнопку \"Build Now\". и в представлении этапа будут отображены наши этапы.\nЕсли мы перейдем к нашему репозиторию DockerHub, у нас должно быть 2 новых образа Docker. У нас должен быть идентификатор сборки 1 и последняя версия, потому что каждая сборка, которую мы создаем на основе команды \"Upload to DockerHub\", отправляет версию, используя переменную окружения Jenkins Build_ID, а также выпускает последнюю версию.\nДавайте создадим обновление файла index.html в нашем репозитории GitHub, как показано ниже, я позволю вам пойти и узнать, что говорила версия 1 файла index.html.\nЕсли мы вернемся в Jenkins и снова выберем \"Build Now\". Мы увидим, что наша сборка #2 прошла успешно.\nЗатем быстро взглянув на DockerHub, мы увидим, что у нас есть наш тег версии 2 и наш последний тег.\nЗдесь стоит отметить, что я добавил в свой кластер Kubernetes секрет, который позволяет мне получить доступ и аутентификацию для отправки моих сборок docker в DockerHub. Если вы следуете этому примеру, вам следует повторить этот процесс для своей учетной записи, а также внести изменения в Jenkinsfile, связанный с моим репозиторием и учетной записью.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day75",
            "title": "75. Обзор GitHub Actions",
            "description": null,
            "content": "Обзор действий GitHub\r\n\r\nВ этом разделе я хотел бы перейти к рассмотрению, возможно, другого подхода, чем тот, на который мы только что потратили время. На этом занятии мы сосредоточимся на GitHub Actions.\nGitHub Actions - это платформа CI/CD, которая позволяет нам строить, тестировать и развертывать, помимо прочих задач, наш конвейер. В ней есть концепция рабочих процессов, которые собираются и тестируются на основе репозитория GitHub. Вы также можете использовать GitHub Actions для управления другими рабочими процессами на основе событий, происходящих в вашем репозитории.\nРабочие процессы\r\n\r\nВ целом, в GitHub Actions наша задача называется рабочий процесс.\nРабочий процесс** - это настраиваемый автоматизированный процесс.\nОпределяется как файлы YAML.\r\nСодержит и запускает одно или несколько заданий.\r\nЗапускается при срабатывании события в вашем хранилище или может быть запущен вручную.\nВы можете использовать несколько рабочих процессов для каждого хранилища.\r\nрабочий процесс* содержит *задание, а затем **шаги для достижения этого задания.\r\nВ рамках рабочего процесса у нас также будет запускающий механизм, на котором будет выполняться наш рабочий процесс.\nНапример, у вас может быть один рабочий процесс для создания и тестирования запросов, другой рабочий процесс для развертывания вашего приложения каждый раз, когда создается релиз, и еще один рабочий процесс, который добавляет метку каждый раз, когда кто-то открывает новую проблему.\r\nСобытия\nСобытия - это определенные события в хранилище, которые запускают рабочий процесс на выполнение.\nЗадания\nЗадание - это набор шагов рабочего процесса, которые выполняются на бегунке.\nШаги\r\n\r\nКаждый шаг в задании может быть скриптом оболочки, который выполняется, или действием. Шаги выполняются по порядку и зависят друг от друга.\nДействия\nПовторяющееся пользовательское приложение, используемое для часто повторяющихся задач.\nБегуны\r\n\r\nБегунок - это сервер, который запускает рабочий процесс, каждый бегунок выполняет одно задание за раз. GitHub Actions предоставляет возможность запуска бегунов для Ubuntu Linux, Microsoft Windows и macOS. Вы также можете разместить свой собственный на определенной ОС или оборудовании.\nНиже вы можете увидеть, как это выглядит: у нас есть событие, запускающее наш рабочий процесс > наш рабочий процесс состоит из двух заданий > внутри наших заданий есть шаги, а затем действия.\nYAML\nПрежде чем мы приступим к рассмотрению реального случая использования, давайте взглянем на приведенное выше изображение в виде примера YAML-файла.\nЯ добавил #, чтобы прокомментировать, где мы можем найти компоненты рабочего процесса YAML.\r\n`\r\n#Workflow\r\nname: 90DaysOfDevOps\r\n#Event\r\non: [push]\r\n#Jobs\r\njobs:\r\n  check-bats-version:\r\n    #Runners\r\n    runs-on: ubuntu-latest\r\n    #Steps\r\n    steps:\r\n        #Actions\r\n      uses: actions/checkout@v2\r\n      uses: actions/setup-node@v2\r\n        with:\r\n          node-version: '14'\r\n      run: npm install -g bats\r\n      run: bats -v\r\n`\r\nПриступаем к работе с GitHub Actions\nЯ думаю, что у GitHub Actions есть много возможностей, да, они удовлетворят ваши потребности в CI/CD, когда речь идет о сборке, тестировании, развертывании вашего кода и последующих шагах.\nЯ вижу множество вариантов и других автоматизированных задач, для которых мы могли бы использовать GitHub Actions.\nИспользование GitHub Actions для линтинга вашего кода\nОдин из вариантов - убедиться, что ваш код чист и аккуратен в вашем репозитории. Это будет наш первый демонстрационный пример.\nЯ собираюсь использовать некоторый пример кода, связанный в одном из ресурсов для этого раздела, мы будем использовать github/super-linter для проверки нашего кода.\r\n`\r\nname: Super-Linter\r\n\r\non: push\r\n\r\njobs:\r\n  super-lint:\r\n    name: Lint code base\r\n    runs-on: ubuntu-latest\r\n    steps:\r\n      name: Checkout code\r\n        uses: actions/checkout@v2\r\n\r\n      name: Run Super-Linter\r\n        uses: github/super-linter@v3\r\n        env:\r\n          DEFAULT_BRANCH: main\r\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\r\n`\r\n\r\ngithub/super-linter\r\nВы можете видеть, что для одного из наших шагов у нас есть действие под названием github/super-linter, которое ссылается на шаг, уже написанный сообществом. Вы можете узнать больше об этом здесь Super-Linter\r\n\r\n\"Этот репозиторий предназначен для GitHub Action для запуска Super-Linter. Это простая комбинация различных линтеров, написанных на bash, чтобы помочь проверить ваш исходный код.\"\r\n\r\nТакже в приведенном фрагменте кода упоминается GITHUB_TOKEN, поэтому мне было интересно узнать, зачем и для чего это нужно.\n\"ПРИМЕЧАНИЕ: Если вы передадите переменную окружения GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} в вашем рабочем процессе, то GitHub Super-Linter будет отмечать статус каждого отдельного запуска линтера в разделе \"Проверки\" запроса на выгрузку. Без этого вы будете видеть только общий статус всего прогона. Не нужно устанавливать GitHub Secret, так как он автоматически устанавливается GitHub, его нужно только передать в действие.\".\nВыделенный жирным текст важно отметить на данном этапе. Мы используем его, но нам не нужно устанавливать какую-либо переменную окружения в нашем репозитории.\nДля тестирования мы будем использовать наш репозиторий, который мы использовали в нашей демонстрации Jenkins.Jenkins-HelloWorld.\r\n\r\nВот наш репозиторий в том виде, в котором мы оставили его в сессии Jenkins.\nДля того, чтобы воспользоваться преимуществами, мы должны использовать вкладку Actions выше, чтобы выбрать из рынка, о котором я расскажу в ближайшее время, или мы можем создать наши собственные файлы, используя наш код супер-лайнера выше, чтобы создать свой собственный, вы должны создать новый файл в вашем репозитории именно в этом месте. .github/workflows/workflow_name, очевидно, убедившись, что имя workflow_name - это что-то полезное для вас, узнаваемое. Здесь мы можем иметь множество различных рабочих процессов, выполняющих различные задания и задачи в нашем репозитории.\nМы создадим .github/workflows/super-linter.yml.\r\n\r\n\r\n\r\nЗатем мы можем вставить наш код и зафиксировать его в нашем репозитории, если мы перейдем на вкладку Actions, то увидим наш рабочий процесс Super-Linter в списке, как показано ниже,\nМы определили в нашем коде, что этот рабочий процесс будет запускаться, когда мы будем перемещать что-либо в наш репозиторий, поэтому при перемещении файла super-linter.yml в наш репозиторий мы запустили рабочий процесс.\nКак вы можете видеть из вышеприведенного, у нас есть некоторые ошибки, скорее всего, из-за моих способностей к взлому и кодированию.\nХотя на самом деле это был не мой код, по крайней мере пока, запустив его и получив ошибку, я обнаружил вот это issue\r\n\r\nДубль #2 Я изменил версию Super-Linter с версии 3 на 4 и запустил задачу снова.\nКак и ожидалось, мой хакерский кодинг вызвал некоторые проблемы, и вы можете увидеть их здесь, в рабочем процессе.\r\n\r\nЯ хотел показать, как теперь выглядит наш репозиторий, когда что-то в рабочем процессе не сработало или сообщило об ошибке.\r\n\r\n\r\n\r\nТеперь, если мы решим проблему с моим кодом и внесем изменения, наш рабочий процесс снова запустится (как видно из изображения, потребовалось некоторое время, чтобы устранить наши \"ошибки\"). Удаление файла, вероятно, не рекомендуется, но это очень быстрый способ показать, что проблема решена.\nЕсли вы нажмете кнопку \"Новый рабочий процесс\", выделенную выше, это откроет вам дверь к огромному количеству действий. Вы, наверное, заметили, что мы не хотим изобретать колесо, мы хотим стоять на плечах гигантов и делиться нашим кодом, автоматизацией и навыками, чтобы сделать нашу жизнь проще.\nО, я не показал вам зеленую галочку на репозитории, когда наш рабочий процесс был успешным.\nЯ думаю, что на этом основы GitHub Actions исчерпаны, но если вы похожи на меня, то вы наверняка видите, как еще можно использовать GitHub Actions для автоматизации множества задач.\nДалее мы рассмотрим другую область CD, мы рассмотрим ArgoCD для развертывания наших приложений в наших средах.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day76",
            "title": "76. Обзор ArgoCD",
            "description": null,
            "content": "Обзор ArgoCD\r\n\r\n\"Argo CD - это декларативный инструмент непрерывной доставки GitOps для Kubernetes\".\r\n\r\nКонтроль версий - ключевой момент здесь. Вы когда-нибудь вносили изменения в вашу среду на лету и не помните об этих изменениях, а поскольку свет горит и все вокруг зеленое, вы продолжаете упорно двигаться вперед? Вы когда-нибудь вносили изменения и ломали все или часть всего? Вы могли бы знать, что внесли изменение, и вы можете быстро откатить свое изменение, тот плохой скрипт или опечатку. А теперь сделайте это в массовом масштабе, и, возможно, это были не вы, или, возможно, ошибка была обнаружена не сразу, и теперь бизнес страдает. Поэтому контроль версий очень важен. Не только это, но и \"определения приложений, конфигурации и окружения должны быть декларативными и контролируемыми по версиям\". В дополнение к этому (что взято из ArgoCD), они также упоминают, что \"развертывание приложений и управление жизненным циклом должно быть автоматизировано, проверяемо и просто для понимания\".\r\n\r\nС точки зрения операционной деятельности, но много играя с Infrastructure as Code, это следующий шаг к обеспечению того, чтобы все эти хорошие вещи были улажены по пути с помощью рабочих процессов непрерывного развертывания/доставки.\r\n\r\nЧто такое ArgoCD\r\nРазвертывание ArgoCD\nДля этого развертывания мы снова будем использовать наш надежный кластер minikube Kubernetes локально.\n`\r\nkubectl create namespace argocd\r\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\r\n`\r\n\r\n\r\n\r\nУбедитесь, что все подсистемы ArgoCD запущены и работают с помощью команды kubectl get pods -n argocd.\r\n\r\n\r\n\r\nТакже проверим все, что мы развернули в пространстве имен с помощью kubectl get all -n argocd\nКогда все выглядит хорошо, мы должны рассмотреть возможность доступа к этому через порт. Используя команду kubectl port-forward svc/argocd-server -n argocd 8080:443. Сделайте это в новом терминале.\nЗатем откройте новый веб-браузер и перейдите по адресу https://localhost:8080.\nДля входа в систему вам понадобится имя пользователя admin, а для получения созданного вами секрета в качестве пароля используйте команду kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d && echo\nПосле входа в систему у вас будет чистый холст CD.\nРазвертывание нашего приложения\nТеперь у нас есть ArgoCD, и мы можем начать использовать его для развертывания наших приложений из наших Git-репозиториев, а также Helm.\nПриложение, которое я хочу развернуть, это Pac-Man, да, именно так, знаменитая игра и то, что я использую во многих демонстрациях, когда речь идет об управлении данными, это не последний раз, когда мы видим Pac-Man.\nВы можете найти репозиторий для Pac-Man здесь.\r\n\r\nВместо того чтобы описывать каждый шаг с помощью снимков экрана, я решил, что будет проще создать видеоролик с описанием шагов, предпринятых для развертывания этого конкретного приложения.\nArgoCD Demo - 90DaysOfDevOps\r\n\r\nПримечание - Во время видео есть служба, которая никогда не удовлетворяется как здоровое приложение, это потому, что тип LoadBalancer, установленный для службы pacman, находится в состоянии ожидания, в Minikube у нас нет настроенного loadbalancer. Если вы хотите проверить это, вы можете изменить YAML для службы на ClusterIP и использовать проброс портов для игры.\nНа этом мы завершаем раздел CICD Pipelines, я считаю, что в настоящее время в индустрии уделяется большое внимание этой области, и вы также услышите термины GitOps, связанные с методологиями, используемыми в CICD в целом.\nСледующий раздел, в который мы переходим, посвящен Observability, еще одной концепции или области, которая не является новой, но становится все более важной, поскольку мы смотрим на наши среды по-другому.\r\nРесурсы\r\n\r\nJenkins is the way to build, test, deploy\r\nJenkins.io\r\nArgoCD\r\nArgoCD Tutorial for Beginners\r\nWhat is Jenkins?\r\nComplete Jenkins Tutorial\r\nGitHub Actions\r\nGitHub Actions CI/CD\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day77",
            "title": "77. Мониторинг",
            "description": null,
            "content": "Введение: Мониторинг\r\n\r\nВ этом разделе мы поговорим о мониторинге, что это такое, зачем он нам нужен?\nЧто такое мониторинг?\nМониторинг - это процесс пристального наблюдения за всей инфраструктурой.\nи зачем он нам нужен?\nПредположим, что мы управляем тысячей серверов, которые включают в себя множество специализированных серверов, таких как серверы приложений, серверы баз данных и веб-серверы. Мы также можем усложнить эту задачу за счет дополнительных сервисов и различных платформ, включая публичные облачные предложения и Kubernetes.\nМы отвечаем за то, чтобы все сервисы, приложения и ресурсы на серверах работали так, как должны.\nКак мы это делаем? Есть три способа:\nВойти вручную на все наши серверы и проверить все данные, относящиеся к процессам и ресурсам служб.\nНаписать скрипт, который заходит на серверы за нас и проверяет данные.\nОба варианта потребуют от нас значительного объема работы,\nТретий вариант проще, мы можем использовать решение для мониторинга, которое доступно на рынке.\nNagios и Zabbix - это возможные решения, которые легко доступны и позволяют нам расширить нашу инфраструктуру мониторинга, чтобы включить столько серверов, сколько мы захотим.\nNagios\r\n\r\nNagios - это инструмент мониторинга инфраструктуры, созданный одноименной компанией. Версия этого инструмента с открытым исходным кодом называется Nagios core, а коммерческая версия называется Nagios XI. Сайт Nagios\r\n\r\nЭтот инструмент позволяет нам следить за нашими серверами и видеть, достаточно ли они используются или есть какие-либо задачи, требующие решения.\nПо сути, мониторинг позволяет нам достичь этих двух целей, проверить состояние наших серверов и сервисов и определить здоровье нашей инфраструктуры. Он также дает нам возможность увидеть всю инфраструктуру с высоты 40 000 метров, чтобы увидеть, работают ли наши серверы, правильно ли работают приложения, доступны или нет веб-серверы.\nОн сообщит нам, что объем нашего диска увеличивался на 10 процентов в течение последних 10 недель на определенном сервере, что он будет полностью исчерпан в течение следующих четырех или пяти дней, и мы не сможем ответить в ближайшее время. Он предупредит нас, когда ваш диск или сервер находится в критическом состоянии, чтобы мы могли принять соответствующие меры, чтобы избежать возможных сбоев.\nВ этом случае мы можем освободить некоторое дисковое пространство и гарантировать, что наши серверы не выйдут из строя и наши пользователи не пострадают.\nСложный вопрос для большинства инженеров по мониторингу - что мы отслеживаем, а что нет?\nКаждая система имеет ряд ресурсов, за какими из них мы должны внимательно следить, а на какие можем закрыть глаза, например, нужно ли следить за использованием процессора, ответ \"да\" очевиден, тем не менее, это все равно решение, которое нужно принять, нужно ли следить за количеством открытых портов в системе, мы можем следить или не следить в зависимости от ситуации, если это сервер общего назначения, то, вероятно, не нужно, но если это веб-сервер, то, вероятно, нужно.\nПостоянный мониторинг\r\n\r\nМониторинг не является чем-то новым, и даже непрерывный мониторинг был идеалом, который многие предприятия приняли в течение многих лет.\nЕсть три ключевых области, на которых необходимо сосредоточиться, когда речь заходит о мониторинге.\nМониторинг инфраструктуры\r\nМониторинг приложений\nМониторинг сети\nВажно отметить, что существует множество доступных инструментов, мы упомянули две общие системы и инструменты в этой сессии, но их очень много. Реальная польза от решения для мониторинга появляется тогда, когда вы действительно потратили время на то, чтобы убедиться, что вы ответили на вопрос, что мы должны отслеживать, а что нет?\nМы можем включить решение мониторинга в любой из наших платформ, и оно начнет собирать информацию, но если этой информации просто слишком много, вам будет трудно извлечь пользу из этого решения, вам придется потратить время на настройку.\nНа следующем занятии мы попробуем использовать инструмент мониторинга и посмотрим, что мы можем начать отслеживать.\r\nРесурсы\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day78",
            "title": "78. Hands-On Monitoring Tools",
            "description": null,
            "content": "Инструменты мониторинга своими руками\r\n\r\nНа последнем занятии я говорил об общей картине мониторинга и рассмотрел Nagios, для этого было две причины. Во-первых, это программное обеспечение, о котором я много слышал на протяжении многих лет, поэтому хотел узнать немного больше о его возможностях.\nСегодня я буду изучать Prometheus, я все больше и больше вижу Prometheus в ландшафте Cloud-Native, но его также можно использовать для присмотра за физическими ресурсами вне Kubernetes и тому подобного.\nPrometheus - мониторинг практически всего\r\n\r\nПрежде всего, Prometheus - это Open-Source, который может помочь вам контролировать контейнеры и системы на базе микросервисов, а также физические, виртуальные и другие сервисы. За Prometheus стоит большое сообщество.\nPrometheus имеет большой набор интеграций и экспортеров Ключевым моментом является экспорт существующих метрик в метрики Prometheus. Кроме того, он также поддерживает несколько языков программирования.\nПодход Pull - Если вы работаете с тысячами микросервисов или систем и сервисов, то метод push - это метод, при котором сервис, как правило, обращается к системе мониторинга. При этом возникают некоторые проблемы, связанные с переполнением сети, высокой производительностью процессора и единой точкой отказа. Метод Pull дает нам гораздо лучший опыт, когда Prometheus будет получать данные из конечной точки метрики на каждом сервисе.\nИ снова мы видим YAML для конфигурации Prometheus.\nПозже вы увидите, как это выглядит при развертывании в Kubernetes, в частности, у нас есть PushGateway, который получает наши метрики от наших заданий/экспортеров.\nУ нас есть AlertManager, который рассылает оповещения, и именно здесь мы можем интегрироваться во внешние сервисы, такие как электронная почта, slack и другие инструменты.\nЗатем у нас есть сервер Prometheus, который управляет получением этих метрик из PushGateway, а затем отправляет эти оповещения в AlertManager. Сервер Prometheus также хранит данные на локальном диске. Хотя можно использовать решения для удаленного хранения данных.\nУ нас также есть PromQL - язык, используемый для взаимодействия с метриками, который можно увидеть позже в веб-интерфейсе Prometheus, но позже в этом разделе вы также увидите, как он используется в инструментах визуализации данных, таких как Grafana.\nСпособы развертывания Prometheus\nСуществуют различные способы установки Prometheus, Download Section Также доступны образы Docker.\ndocker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus.\r\n\r\nНо мы сосредоточим наши усилия на развертывании в Kubernetes. У которого также есть несколько вариантов.\nСоздание конфигурационных YAML-файлов\nИспользование оператора (менеджер всех компонентов prometheus)\r\nИспользование диаграммы helm для развертывания оператора\nРазвертывание в Kubernetes\nДля этой быстрой и простой установки мы снова будем использовать наш локальный кластер minikube. Как и в предыдущих случаях с minikube, мы будем использовать helm для развертывания диаграммы Prometheus helm.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts.\nКак видно из вышеприведенного, мы также выполнили обновление репо helm, теперь мы готовы развернуть Prometheus в нашей среде minikube с помощью команды helm install stable prometheus-community/prometheus.\nЧерез пару минут вы увидите, что появилось несколько новых подкастов, для этого демо я развернул их в пространство имен по умолчанию, обычно я бы развернул их в собственное пространство имен.\nПосле запуска всех подсистем мы также можем посмотреть на все развернутые аспекты Prometheus.\nТеперь, чтобы получить доступ к пользовательскому интерфейсу сервера Prometheus, мы можем использовать следующую команду для проброса портов.\n`\r\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app=prometheus,component=server\" -o jsonpath=\"{.items[0].metadata.name}\")\r\n  kubectl --namespace default port-forward $POD_NAME 9090\r\n`\r\nКогда мы впервые открываем наш браузер на http://localhost:9090, мы видим следующий очень пустой экран.\nПоскольку мы развернули наш кластер Kubernetes, мы будем автоматически получать метрики из нашего Kubernetes API, поэтому мы можем использовать некоторые PromQL, чтобы убедиться, что мы получаем метрики container_cpu_usage_seconds_total.\r\n\r\n\r\n\r\nКоротко об изучении PromQL и применении его на практике. Это очень похоже на то, о чем я говорил ранее: получение метрик - это здорово, как и мониторинг, но вы должны знать, что вы отслеживаете и почему, и что вы не отслеживаете и почему!\nЯ хочу вернуться к Prometheus, но пока я думаю, что нам нужно подумать об управлении журналами и визуализации данных, чтобы позже вернуться к Prometheus.\r\nРесурсы\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\nPromql cheat sheet with examples\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day79",
            "title": "79. Log Management",
            "description": null,
            "content": "Введение: Управление журналами\r\n\r\nВ продолжение проблем и решений в области мониторинга инфраструктуры, управление журналами - это еще один пазл в общей картине наблюдаемости.\nУправление и агрегация журналов\nДавайте поговорим о двух основных концепциях, первая из которых - агрегация журналов, это способ сбора и маркировки журналов приложений от множества различных служб в единую приборную панель, по которой можно легко осуществлять поиск.\nОдной из первых систем, которые должны быть построены в системе управления производительностью приложений, является агрегация журналов. Управление производительностью приложений - это та часть жизненного цикла devops, когда все было создано и развернуто, и вам нужно убедиться, что они постоянно работают, что им выделено достаточно ресурсов и что ошибки не показываются пользователям. В большинстве производственных развертываний существует множество связанных событий, которые передают журналы по сервисам, в google один поиск может попасть в десять различных сервисов, прежде чем будет возвращен пользователю, если вы получили неожиданные результаты поиска, это может означать логическую проблему в любом из десяти сервисов, и агрегация журналов помогает таким компаниям, как google, диагностировать проблемы в производстве.\nВ этом суть хорошей платформы для агрегации журналов, которая эффективно собирает журналы отовсюду, откуда они исходят, и делает их легко доступными для поиска в случае повторного возникновения неисправности.\nПример приложения\nНаш пример приложения - это веб-приложение, у нас есть типичный фронт-энд и бэк-энд, хранящий наши важные данные в базе данных MongoDB.\nЕсли бы пользователь сказал нам, что страница стала белой и вывела сообщение об ошибке, мы бы с трудом диагностировали проблему с помощью нашего текущего стека. Пользователь должен вручную отправить нам ошибку, а мы должны сопоставить ее с соответствующими журналами в трех других сервисах.\nELK\nДавайте посмотрим на ELK, популярный стек агрегации логов с открытым исходным кодом, названный в честь его трех компонентов elasticsearch, logstash и kibana, если мы установим его в той же среде, что и наше приложение.\nВеб-приложение подключается к фронтенду, который затем подключается к бэкенду, бэкенд отправляет журналы в logstash, а затем то, как работают эти три компонента.\nКомпоненты elk\nElasticsearch, logstash и Kibana заключается в том, что все сервисы отправляют журналы в logstash, logstash принимает эти журналы, которые являются текстом, испускаемым приложением. Например, веб-приложение, когда вы посещаете веб-страницу, может зарегистрировать доступ посетителя к этой странице в это время, и это пример сообщения журнала, которое будет отправлено в logstash.\r\n\r\nЗатем Logstash извлекает из них информацию, так что для этого сообщения пользователь сделал что-то, в время. Он извлечет время, извлечет сообщение, извлечет пользователя и включит все это в качестве тегов, так что сообщение будет объектом тегов и сообщений, так что вы можете легко искать по ним, вы можете найти все запросы, сделанные определенным пользователем, но logstash не хранит вещи самостоятельно, он хранит вещи в elasticsearch, который является эффективной базой данных для запроса текста, и elasticsearch раскрывает результаты как Kibana, а Kibana - это веб-сервер, который подключается к elasticsearch и позволяет администраторам, таким как devops или другим людям в вашей команде, дежурному инженеру просматривать журналы в производстве при возникновении серьезных неполадок. Вы, как администратор, подключаетесь к Kibana, Kibana запрашивает elasticsearch на предмет журналов, соответствующих тому, что вы хотите.\nВы можете сказать: \"Эй, Kibana, в строке поиска я хочу найти ошибки\", и Kibana скажет elasticsearch найти сообщения, которые содержат строку error, а затем elasticsearch вернет результаты, которые были заполнены logstash. Logstash получил бы эти результаты от всех других служб.\r\nкак бы мы использовали elk для диагностики производственной проблемы\r\n\r\nПользователь говорит, что я увидел код ошибки один два три четыре пять шесть семь, когда я попытался сделать это с помощью настройки elk, мы должны зайти в kibana, ввести один два три четыре пять шесть семь в строке поиска, нажать enter, а затем это покажет нам журналы, которые соответствуют этому, и один из журналов может сказать внутреннюю ошибку сервера, возвращающую один два три четыре пять шесть семь, и мы увидим, что служба, которая выдала этот журнал. и мы увидим, что служба, которая выдала этот журнал, была backend, и мы увидим, в какое время был выдан этот журнал, поэтому мы можем перейти ко времени в этом журнале и посмотреть на сообщения выше и ниже него в backend, и тогда мы сможем увидеть лучшую картину того, что произошло для запроса пользователя, и мы сможем повторить этот процесс, переходя к другим службам, пока не найдем, что на самом деле вызвало проблему у пользователя.\r\nБезопасность и доступ к журналам\nВажной частью головоломки является обеспечение того, чтобы журналы были видны только администраторам (или пользователям и группам, которым абсолютно необходим доступ). Журналы могут содержать конфиденциальную информацию, такую как токены, поэтому важно, чтобы только аутентифицированные пользователи могли получить к ним доступ. Вы не захотите выставлять Kibana в интернет без какого-либо способа аутентификации.\r\nПримеры инструментов управления журналами\r\n\r\nПримерами платформ для управления журналами являются\r\n\r\nElasticsearch\nLogstash\nKibana\nFluentd - популярный вариант с открытым исходным кодом\r\nDatadog - хостинговое предложение, обычно используется на крупных предприятиях,\nLogDNA - хостируемое предложение\nSplunk\nОблачные провайдеры также предоставляют протоколирование, например, AWS CloudWatch Logs, Microsoft Azure Monitor и Google Cloud Logging.\nУправление журналами является ключевым аспектом общей наблюдаемости ваших приложений и среды инфраструктур для диагностики проблем в производстве. Относительно просто установить готовое решение, такое как ELK или CloudWatch, и это значительно упрощает диагностику и устранение проблем в производстве.\r\nРесурсы\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\nPromql cheat sheet with examples\r\nLog Management for DevOps | Manage application, server, and cloud logs with Site24x7\r\nLog Management what DevOps need to know\r\nWhat is ELK Stack?\r\nFluentd simply explained\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day80",
            "title": "80. ELK Stack",
            "description": null,
            "content": "ELK Stack\nНа этом занятии мы немного подробнее рассмотрим некоторые из упомянутых нами опций.\nELK Stack\nELK Stack - это комбинация трех отдельных инструментов:\nElasticsearch - это распределенный, бесплатный и открытый поисковый и аналитический механизм для всех типов данных, включая текстовые, числовые, геопространственные, структурированные и неструктурированные.\r\n\r\nLogstash - свободный и открытый конвейер обработки данных на стороне сервера, который получает данные из множества источников, преобразует их, а затем отправляет в ваш любимый \"тайник\".\nKibana - это бесплатный и открытый пользовательский интерфейс, позволяющий визуализировать данные Elasticsearch и перемещаться по стеку Elastic Stack. Делайте все, что угодно: от отслеживания загрузки запросов до понимания того, как запросы проходят через ваши приложения.\nСтек ELK позволяет нам надежно и безопасно получать данные из любого источника, в любом формате, затем искать, анализировать и визуализировать их в режиме реального времени.\r\n\r\nВ дополнение к вышеперечисленным компонентам вы также можете увидеть Beats - легковесные агенты, которые устанавливаются на пограничных узлах для сбора различных типов данных для передачи в стек.\r\n\r\n\r\nЖурналы: Определяются журналы сервера, которые необходимо проанализировать.\r\n\r\nLogstash: Собирает журналы и данные о событиях. Он даже анализирует и преобразует данные.\r\n\r\nElasticSearch: Преобразованные данные из Logstash хранятся, ищутся и индексируются.\r\n\r\nKibana использует БД Elasticsearch для изучения, визуализации и обмена данными\r\n\r\n\r\n\r\nИзображение взято с сайта Guru99\r\n\r\nХороший ресурс, объясняющий это The Complete Guide to the ELK Stack\r\n\r\nС добавлением битов стек ELK теперь также известен как Elastic Stack.\nДля практического скрипта существует множество мест, где можно развернуть Elastic Stack, но мы будем использовать docker compose для локального развертывания в нашей системе.\nStart the Elastic Stack with Docker Compose\r\n\r\n\r\n\r\nОригинальные файлы и руководство, которые я использовал, вы найдете здесь  deviantony/docker-elk\r\n\r\nТеперь мы можем запустить docker-compose up -d, при первом запуске потребуется вытащить изображения.\nЕсли вы следите за этим репозиторием или за тем, который использовал я, у вас будет пароль \"changeme\" или в моем репозитории пароль \"90DaysOfDevOps\". Имя пользователя - \"elastic\".\r\n\r\nЧерез несколько минут мы можем перейти на сайт http://localhost:5601/, который является нашим сервером Kibana / Docker-контейнером.\r\n\r\n\r\n\r\nВаш начальный главный экран будет выглядеть примерно так.\nВ разделе \"Get started by adding integrations\" есть пункт \"try sample data\", нажмите на него, и мы сможем добавить одну из показанных ниже интеграций.\nЯ собираюсь выбрать \"Sample web logs\", но это действительно для того, чтобы получить представление о том, какие наборы данных можно получить в стеке ELK.\nКогда вы выбрали \"Добавить данные\", требуется некоторое время, чтобы заполнить некоторые из этих данных, а затем у вас появляется опция \"Просмотр данных\" и список доступных способов просмотра этих данных в выпадающем списке.\nКак указано в представлении приборной панели:\nОбразцы данных журналов\r\n\r\nЭта приборная панель содержит образцы данных, с которыми вы можете поиграть. Вы можете просматривать их, искать и взаимодействовать с визуализациями. Для получения дополнительной информации о Kibana ознакомьтесь с нашей документацией.\r\n\r\n\r\n\r\nЗдесь используется Kibana для визуализации данных, которые были добавлены в ElasticSearch через Logstash. Это не единственный вариант, но я лично хотел развернуть и посмотреть на это.\nВ какой-то момент мы рассмотрим Grafana, и вы увидите некоторые сходства в визуализации данных между ними, вы также видели Prometheus.\nКлючевой момент, который я уловил между Elastic Stack и Prometheus + Grafana, заключается в том, что Elastic Stack или ELK Stack сосредоточен на журналах, а Prometheus - на метриках.\nЯ читал эту статью от MetricFire Prometheus vs. ELK, чтобы лучше понять различные предложения.\r\nРесурсы\nUnderstanding Logging: Containers & Microservices\r\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\nPromql cheat sheet with examples\r\nLog Management for DevOps | Manage application, server, and cloud logs with Site24x7\r\nLog Management what DevOps need to know\r\nWhat is ELK Stack?\r\n[Fluentd simply explained](https://www.youtube.com/watch?v=5ofsNyHZwWE&t=14s\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day81",
            "title": "81. Fluentd и FluentBit",
            "description": null,
            "content": "Fluentd и FluentBit\r\n\r\nЕще одним коллектором данных, который я хотел изучить в рамках раздела о наблюдаемости, был Fluentd. Это унифицированный уровень протоколирования с открытым исходным кодом.\nFluentd имеет четыре ключевые особенности, которые делают его подходящим для создания чистых, надежных конвейеров протоколирования:\r\n\r\nУнифицированное протоколирование с JSON: Fluentd старается структурировать данные в виде JSON, насколько это возможно. Это позволяет Fluentd унифицировать все аспекты обработки данных журналов: сбор, фильтрацию, буферизацию и вывод журналов из нескольких источников и мест назначения. Последующая обработка данных намного проще с JSON, так как он имеет достаточную структуру, чтобы быть доступным без принуждения к жестким схемам.\r\n\r\nПодключаемая архитектура: Fluentd имеет гибкую систему плагинов, которая позволяет сообществу расширять его функциональность. Более 300 плагинов, созданных сообществом, соединяют десятки источников данных с десятками выходных данных, манипулируя данными по мере необходимости. Используя плагины, вы можете сразу же повысить эффективность использования ваших журналов.\r\n\r\nТребуется минимум ресурсов: Коллектор данных должен быть легким, чтобы его можно было легко запустить на загруженной машине. Fluentd написан на комбинации C и Ruby и требует минимальных системных ресурсов. Ванильный экземпляр работает на 30-40 МБ памяти и может обрабатывать 13 000 событий/секунду/ядро.\r\n\r\nВстроенная надежность: Потеря данных никогда не должна произойти. Fluentd поддерживает буферизацию на основе памяти и файлов для предотвращения потери данных между узлами. Fluentd также поддерживает надежное восстановление после отказа и может быть настроен на высокую доступность.\r\n\r\nУстановка Fluentd\r\nКак приложения записывают данные в журнал?\nЗапись в файлы. Файлы .log (трудно анализировать без инструмента и в масштабе)\r\nВести журнал непосредственно в базу данных (каждое приложение должно быть настроено на правильный формат)\r\nСторонние приложения (NodeJS, NGINX, PostgreSQL).\r\n\r\nВот почему нам нужен единый уровень логирования.\nFluentD позволяет использовать 3 типа данных, показанных выше, и дает нам возможность собирать, обрабатывать и отправлять их по назначению, это может быть отправка логов в базы данных Elastic, MongoDB, Kafka, например.\nЛюбые данные, любой источник данных может быть отправлен в FluentD, и эти данные могут быть отправлены в любое место назначения. FluentD не привязан к какому-либо конкретному источнику или месту назначения.\nИзучая Fluentd, я постоянно натыкался на Fluent bit как еще один вариант, и похоже, что если вы хотите развернуть инструмент протоколирования в среде Kubernetes, то Fluent bit даст вам такую возможность, хотя Fluentd также может быть развернут как на контейнерах, так и на серверах.\nFluentd & Fluent Bit\r\n\r\nFluentd и Fluentbit будут использовать входные плагины для преобразования данных в формат Fluent Bit, затем у нас есть выходные плагины для любой цели вывода, например, elasticsearch.\nМы также можем использовать теги и соответствия между конфигурациями.\nЯ не вижу веских причин для использования Fluentd, и кажется, что Fluent Bit - лучший способ начать работу. Хотя в некоторых архитектурах они могут использоваться вместе.\nFluent Bit в Kubernetes\nFluent Bit в Kubernetes развертывается как DaemonSet, что означает, что он будет запущен на каждом узле кластера. Каждая капсула Fluent Bit на каждом узле будет читать каждый контейнер на этом узле и собирать все доступные журналы. Он также будет собирать метаданные с сервера Kubernetes API Server.\nАннотации Kubernetes можно использовать в конфигурационном YAML наших приложений.\nПрежде всего, мы можем развернуть приложение из репозитория fluent helm. helm repo add fluent https://fluent.github.io/helm-charts, а затем установить с помощью команды helm install fluent-bit fluent/fluent-bit.\nВ моем кластере я также запускаю prometheus в моем пространстве имен по умолчанию (в тестовых целях), нам нужно убедиться, что наш fluent-bit pod запущен и работает. Мы можем сделать это с помощью команды kubectl get all | grep fluent, которая покажет нам наш запущенный pod, сервис и набор демонов, о которых мы говорили ранее.\nЧтобы Fluentbit знал, откуда получать журналы, у нас есть конфигурационный файл, в этом развертывании Fluentbit на Kubernetes у нас есть configmap, который напоминает конфигурационный файл.\nЭта ConfigMap будет выглядеть примерно так:\r\n\r\n`\r\nName:         fluent-bit\r\nNamespace:    default\r\nLabels:       app.kubernetes.io/instance=fluent-bit\r\n              app.kubernetes.io/managed-by=Helm\r\n              app.kubernetes.io/name=fluent-bit\r\n              app.kubernetes.io/version=1.8.14\r\n              helm.sh/chart=fluent-bit-0.19.21\r\nAnnotations:  meta.helm.sh/release-name: fluent-bit\r\n              meta.helm.sh/release-namespace: default\r\n\r\nData\r\n\r\ncustom_parsers.conf:\r\n[PARSER]\r\n    Name docker_no_time\r\n    Format json\r\n    Time_Keep Off\r\n    Time_Key time\r\n    Time_Format %Y-%m-%dT%H:%M:%S.%L\r\n\r\nfluent-bit.conf:\r\n[SERVICE]\r\n    Daemon Off\r\n    Flush 1\r\n    Log_Level info\r\n    Parsers_File parsers.conf\r\n    Parsers_File custom_parsers.conf\r\n    HTTP_Server On\r\n    HTTP_Listen 0.0.0.0\r\n    HTTP_Port 2020\r\n    Health_Check On\r\n\r\n[INPUT]\r\n    Name tail\r\n    Path /var/log/containers/*.log\r\n    multiline.parser docker, cri\r\n    Tag kube.*\r\n    Mem_Buf_Limit 5MB\r\n    Skip_Long_Lines On\r\n\r\n[INPUT]\r\n    Name systemd\r\n    Tag host.*\r\n    Systemd_Filter _SYSTEMD_UNIT=kubelet.service\r\n    Read_From_Tail On\r\n\r\n[FILTER]\r\n    Name kubernetes\r\n    Match kube.*\r\n    Merge_Log On\r\n    Keep_Log Off\r\n    K8S-Logging.Parser On\r\n    K8S-Logging.Exclude On\r\n\r\n[OUTPUT]\r\n    Name es\r\n    Match kube.*\r\n    Host elasticsearch-master\r\n    Logstash_Format On\r\n    Retry_Limit False\r\n\r\n[OUTPUT]\r\n    Name es\r\n    Match host.*\r\n    Host elasticsearch-master\r\n    Logstash_Format On\r\n    Logstash_Prefix node\r\n    Retry_Limit False\r\n\r\nEvents:\n`\r\n\r\nТеперь мы можем перенаправить наш pod на наш localhost, чтобы убедиться, что у нас есть соединение. Сначала узнайте имя вашего pod с помощью kubectl get pods | grep fluent и затем используйте kubectl port-forward fluent-bit-8kvl4 2020:2020 откройте веб-браузер на http://localhost:2020/.\nЯ также нашел эту замечательную статью на Medium, в которой рассказывается о Fluent Bit.\r\nРесурсы\nUnderstanding Logging: Containers & Microservices\r\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\nPromql cheat sheet with examples\r\nLog Management for DevOps | Manage application, server, and cloud logs with Site24x7\r\nLog Management what DevOps need to know\r\nWhat is ELK Stack?\r\nFluentd simply explained\n Fluent Bit explained | Fluent Bit vs Fluentd )\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day82",
            "title": "82. EFK Stack",
            "description": null,
            "content": "EFK Stack\n\nВ предыдущем разделе мы говорили о ELK Stack, который использует Logstash в качестве сборщика логов в стеке, в EFK Stack мы меняем его на FluentD или FluentBit.\n\nНаша задача в этом разделе - отслеживать журналы Kubernetes с помощью EFK.\n\nОбзор EFK\n\nМы развернем следующее в нашем кластере Kubernetes.\n\nСтек EFK представляет собой набор из 3 программ, объединенных вместе, включая:\n\nElasticsearch : NoSQL база данных используется для хранения данных и предоставляет интерфейс для поиска и журнал запросов.\n\nFluentd : Fluentd - это сборщик данных с открытым исходным кодом для унифицированного уровня логирования. Fluentd позволяет унифицировать сбор и потребление данных для лучшего использования и понимания данных.\n\nKibana : Интерфейс для управления и статистики журналов. Отвечает за чтение информации из elasticsearch .\n\nРазвертывание EFK на Minikube\n\nМы будем использовать наш надежный кластер minikube для развертывания нашего стека EFK. Давайте запустим кластер с помощью minikube start на нашей системе. Я использую ОС Windows с включенным WSL2.\n\nЯ создал efk-stack.yaml, который содержит все необходимое для развертывания стека EFK в нашем кластере, используя команду kubectl create -f efk-stack.yaml мы видим, что все развернуто.\n\n\n\nВ зависимости от вашей системы и если вы уже выполняли эту процедуру и получили изображения, теперь вам нужно посмотреть, как стручки переходят в состояние готовности, прежде чем мы сможем двигаться дальше, вы можете проверить прогресс с помощью следующей команды. kubectl get pods -n kube-logging -w Это может занять несколько минут.\n\nПриведенная выше команда позволяет нам следить за ситуацией, но я люблю уточнять, все ли в порядке, выполняя следующую команду kubectl get pods -n kube-logging, чтобы убедиться, что все pods теперь работают.\n\nПосле того, как мы запустили все наши pods, и на этом этапе мы должны увидеть\n3 стручка, связанные с ElasticSearch\n1 стручок, связанный с Fluentd\n1 стручок, связанный с Kibana\n\nМы также можем использовать kubectl get all -n kube-logging, чтобы показать все в нашем пространстве имен, fluentd, как объяснялось ранее, развернут как набор демонов, kibana как развертывание и ElasticSearch как набор состояний.\n\nТеперь все наши pods работают, и мы можем ввести в новом терминале команду port-forward, чтобы мы могли получить доступ к нашей приборной панели kibana. Обратите внимание, что имя вашего pod будет отличаться от команды, которую мы видим здесь. kubectl port-forward kibana-84cf7f59c-v2l8v 5601:5601 -n kube-logging.\n\n\n\nТеперь мы можем открыть браузер и перейти по этому адресу, http://localhost:5601 вас встретит либо экран, который вы видите ниже, либо вы можете увидеть экран с примерами данных, либо продолжить и настроить самостоятельно. В любом случае и непременно посмотрите на эти тестовые данные, это то, что мы рассмотрели при изучении стека ELK в предыдущей сессии.\n\nДалее нам нужно перейти на вкладку \"discover\" в левом меню и добавить \"*\" к нашему шаблону индекса. Перейдите к следующему шагу, нажав кнопку \"Следующий шаг\".\n\nНа шаге 2 из 2 мы будем использовать опцию @timestamp из выпадающего списка, так как это позволит отфильтровать наши данные по времени. Когда вы нажмете кнопку создать шаблон, это может занять несколько секунд.\n\nЕсли через несколько секунд мы вернемся на вкладку \"discover\", вы должны увидеть данные, поступающие с вашего кластера Kubernetes.\n\nТеперь, когда у нас установлен и работает стек EFK и мы собираем журналы с нашего кластера Kubernetes через Fluentd, мы можем взглянуть на другие источники, которые мы можем выбрать. Если вы перейдете на главный экран, нажав на логотип Kibana в левом верхнем углу, вас встретит та же страница, которую мы видели при первом входе в систему.\n\nУ нас есть возможность добавить APM, данные журнала, метрические данные и события безопасности из других плагинов или источников.\n\nЕсли мы выберем \"Добавить данные журнала\", то увидим ниже, что у нас есть большой выбор, откуда мы хотим получать наши журналы, вы можете увидеть, что там упоминается Logstash, который является частью стека ELK.\n\nПод данными метрик вы увидите, что можно добавить источники для Prometheus и многих других сервисов.  Переведено с помощью www.DeepL.com/Translator (бесплатная версия)\n\nAPM (Мониторинг производительности приложений)\n\nТакже есть возможность собрать APM (мониторинг производительности приложений), который собирает подробные показатели производительности и ошибки изнутри вашего приложения. Он позволяет отслеживать производительность тысяч приложений в режиме реального времени.\n\nЯ не буду здесь углубляться в APM, но вы можете узнать больше на сайте Elastic.\n\nРесурсы\n\nUnderstanding Logging: Containers & Microservices\nThe Importance of Monitoring in DevOps\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\nPromql cheat sheet with examples\nLog Management for DevOps | Manage application, server, and cloud logs with Site24x7\nLog Management what DevOps need to know\nWhat is ELK Stack?\nFluentd simply explained\n\nSee you on Day 83\n\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day83",
            "title": "83. Визуализация данных - Grafana",
            "description": null,
            "content": "Визуализация данных - Grafana\r\n\r\nМы много говорили о Kibana в этом разделе, посвященном Observability. Но мы также должны уделить некоторое время Grafana. Но это не одно и то же, и они не полностью конкурируют друг с другом.\nОсновной функцией Kibana является запрос и анализ данных. Используя различные методы, пользователи могут искать в данных, проиндексированных в Elasticsearch, определенные события или строки в данных для анализа и диагностики первопричин. На основе этих запросов пользователи могут использовать функции визуализации Kibana, которые позволяют визуализировать данные различными способами, используя графики, таблицы, географические карты и другие виды визуализации.\r\n\r\nGrafana фактически началась как форк Kibana, целью Grafana была поддержка метрик и мониторинга, которые в то время Kibana не предоставляла.\nGrafana - это бесплатный инструмент визуализации данных с открытым исходным кодом. Обычно мы видим Prometheus и Grafana вместе в полевых условиях, но мы также можем увидеть Grafana вместе с Elasticsearch и Graphite.\nКлючевое различие между этими двумя инструментами - это логирование и мониторинг. В начале раздела мы рассмотрели мониторинг с помощью Nagios, затем Prometheus и перешли к логированию, где мы рассмотрели стеки ELK и EFK.\nGrafana предназначена для анализа и визуализации таких показателей, как использование системного процессора, памяти, дисков и ввода-вывода. Платформа не позволяет выполнять полнотекстовые запросы данных. Kibana работает поверх Elasticsearch и используется в основном для анализа сообщений журнала.\nКак мы уже выяснили, Kibana довольно проста в развертывании, а также в выборе места установки, то же самое можно сказать и о Grafana.\nОба поддерживают установку на Linux, Mac, Windows, Docker или сборку из исходников.\nНесомненно, есть и другие, но Grafana - это инструмент, который, по моим наблюдениям, охватывает виртуальные, облачные и облачно-нативные платформы, поэтому я хотел рассказать о нем в этом разделе.\nОператор Prometheus + развертывание Grafana\nМы уже рассказывали о Prometheus в этом разделе, но поскольку мы так часто видим эти пары, я хотел создать среду, которая позволила бы нам хотя бы увидеть, какие метрики мы могли бы отображать в визуализации. Мы знаем, что мониторинг наших сред очень важен, но просмотр этих метрик в Prometheus или любом другом метрическом инструменте будет громоздким и не будет масштабироваться. Именно здесь на помощь приходит Grafana, которая предоставляет нам интерактивную визуализацию этих метрик, собранных и сохраненных в базе данных Prometheus.\nС помощью этой визуализации мы можем создавать пользовательские графики, диаграммы и оповещения для нашей среды. В этом руководстве мы будем использовать наш кластер minikube.\nДля начала мы клонируем его в нашу локальную систему. Используя git clone https://github.com/prometheus-operator/kube-prometheus.git и cd kube-prometheus.\r\n\r\n\r\n\r\nПервая задача - создать наше пространство имен в кластере minikube kubectl create -f manifests/setup, если вы не следили за предыдущими разделами, мы можем использовать minikube start для создания нового кластера.\nДалее мы собираемся развернуть все необходимое для нашего демо с помощью команды kubectl create -f manifests/, как вы можете видеть, это развернет множество различных ресурсов в нашем кластере.\nЗатем нам нужно подождать, пока наши стручки поднимутся, и, находясь в запущенном состоянии, мы можем использовать команду kubectl get pods -n monitoring -w, чтобы следить за стручками.\nКогда все запущено, мы можем проверить, что все pods находятся в рабочем и здоровом состоянии, используя команду kubectl get pods -n monitoring.\nПри развертывании мы развернули ряд сервисов, которые мы будем использовать позже в демо, вы можете проверить их с помощью команды kubectl get svc -n monitoring.\nИ, наконец, давайте проверим все ресурсы, развернутые в нашем новом пространстве имен мониторинга, используя команду kubectl get all -n monitoring.\nОткрыв новый терминал, мы готовы получить доступ к нашему инструменту Grafana и начать собирать и визуализировать некоторые из наших метрик, команда для использования - kubectl --namespace monitoring port-forward svc/grafana 3000.\r\n\r\n\r\n\r\nОткройте браузер и перейдите по адресу http://localhost:3000, вам будет предложено ввести имя пользователя и пароль.\nПо умолчанию имя пользователя и пароль для доступа следующие\r\n`\r\nИмя пользователя: admin\nПароль: admin\r\n`\r\nОднако при первом входе в систему вам будет предложено ввести новый пароль. На начальном экране или домашней странице вы увидите несколько областей для изучения, а также некоторые полезные ресурсы для ознакомления с Grafana и ее возможностями. Обратите внимание на виджеты \"Добавить свой первый источник данных\" и \"Создать свою первую приборную панель\", мы будем использовать их позже.\nВы увидите, что источник данных prometheus уже добавлен в источники данных Grafana, однако, поскольку мы используем minikube, нам нужно также перенаправить prometheus, чтобы он был доступен на нашем localhost, открыв новый терминал, мы можем выполнить следующую команду. kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090 если на главной странице Grafana мы теперь заходим в виджет \"Add your first data source\" и отсюда выбираем Prometheus.\nДля нашего нового источника данных мы можем использовать адрес http://localhost:9090, и нам также нужно будет изменить выпадающий список на браузер, как показано ниже.\r\n\r\n\r\n\r\nВнизу страницы мы можем нажать кнопку сохранить и протестировать. Это должно дать нам результат, который вы видите ниже, если проброс порта для prometheus работает.\nВернитесь на главную страницу и найдите опцию \"Create your first dashboard\", выберите \"Add a new panel\".\r\n\r\n\r\n\r\nНиже вы увидите, что мы уже собираем данные из нашего источника данных Grafana, но мы хотели бы собирать метрики из нашего источника данных Prometheus, выберите выпадающий список источников данных и выберите наш недавно созданный \"Prometheus-1\"\nЕсли затем выбрать браузер Metrics, то появится длинный список метрик, собираемых из Prometheus, связанных с нашим кластером minikube.\nДля целей демонстрации я собираюсь найти метрику, которая дает нам некоторые данные о наших системных ресурсах, cluster:node_cpu:ratio{} дает нам некоторые подробности об узлах в нашем кластере и доказывает, что эта интеграция работает.\nЕсли вас устраивает такая визуализация, нажмите кнопку \"Применить\" в правом верхнем углу, и вы добавите этот график на свою приборную панель. Разумеется, вы можете добавлять дополнительные графики и другие диаграммы, чтобы обеспечить нужную вам визуализацию.\nОднако мы можем воспользоваться тысячами ранее созданных приборных панелей, которые мы можем использовать, чтобы не изобретать велосипед.\nЕсли мы выполним поиск по Kubernetes, то увидим длинный список готовых приборных панелей, из которых мы можем выбирать.\nМы выбрали приборную панель Kubernetes API Server и изменили источник данных, чтобы соответствовать нашему недавно добавленному источнику данных Prometheus-1, и мы видим некоторые метрики, отображаемые как показано ниже.\nОповещение\r\n\r\nВы также можете использовать развернутый нами alertmanager для отправки оповещений в slack или другие интеграции, для этого вам нужно перенести сервис alertmanager, используя следующие данные.\nkubectl --namespace monitoring port-forward svc/alertmanager-main 9093\r\nhttp://localhost:9093\r\n\r\nНа этом мы завершаем наш раздел о наблюдаемости. Лично я считаю, что этот раздел подчеркнул, насколько широка эта тема, но в равной степени, насколько она важна для наших ролей, и что будь то метрика, логирование или трассировка, вам необходимо иметь хорошее представление о том, что происходит в наших широких средах в будущем, особенно когда они могут так сильно измениться благодаря автоматизации, которую мы уже рассмотрели в других разделах.\nДалее мы рассмотрим управление данными и то, как принципы DevOps также необходимо учитывать, когда речь идет об управлении данными.\r\nРесурсы\nUnderstanding Logging: Containers & Microservices\r\nThe Importance of Monitoring in DevOps\r\nUnderstanding Continuous Monitoring in DevOps?\nDevOps Monitoring Tools\nTop 5 - DevOps Monitoring Tools\r\nHow Prometheus Monitoring works\nIntroduction to Prometheus monitoring\r\nPromql cheat sheet with examples\r\nLog Management for DevOps | Manage application, server, and cloud logs with Site24x7\r\nLog Management what DevOps need to know\r\nWhat is ELK Stack?\r\nFluentd simply explained\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day84",
            "title": "84. Управление данными",
            "description": null,
            "content": "Введение: Управление данными\r\n\r\n\r\n\r\nУправление данными - это далеко не новая стена, на которую нужно карабкаться, хотя мы знаем, что данные стали более важными, чем несколько лет назад. Ценные и постоянно меняющиеся, они также могут стать огромным кошмаром, когда мы говорим об автоматизации и непрерывной интеграции, тестировании и развертывании частых выпусков программного обеспечения. Вводим постоянные данные и базовые службы данных, которые часто являются главным виновником, когда что-то идет не так.\nНо прежде чем я перейду к управлению данными в облаке, нам нужно подняться на уровень выше. В ходе этой задачи мы затронули множество различных платформ. Будь то физические, виртуальные, облачные и Cloud-Native, включая Kubernetes, ни одна из этих платформ не обеспечивает отсутствие требований к управлению данными.\nКаким бы ни был наш бизнес, более чем вероятно, что вы найдете базу данных, скрывающуюся где-то в среде, будь то для самой критически важной системы в бизнесе или, по крайней мере, какой-то винтик в цепи хранит эти постоянные данные на каком-то уровне системы.\nDevOps и данные\nКак и в самом начале этой серии статей, где мы говорили о принципах DevOps, для улучшения процесса работы с данными вам необходимо привлечь нужных людей. Это могут быть DBA, но в равной степени это должны быть и люди, которые заботятся о резервном копировании этих сервисов данных.\nВо-вторых, нам также необходимо определить различные типы данных, домены, границы, которые мы связываем с нашими данными. Таким образом, данные не будут рассматриваться изолированно среди администраторов баз данных, инженеров по хранению данных или инженеров, специализирующихся на резервном копировании. Таким образом, вся команда может определить наилучший маршрут действий при разработке и размещении приложений для более широкого бизнеса и сосредоточиться на архитектуре данных, а не на том, о чем подумали позже.\nЭто может охватывать множество различных областей жизненного цикла данных, мы можем говорить о вводе данных, где и как данные будут вводиться в наш сервис или приложение? Как сервис, приложение или пользователи будут получать доступ к этим данным. Но затем нам также необходимо понять, как мы будем защищать данные, и как мы будем защищать эти данные.\nУправление данными 101\nУправление данными, согласно Data Management Body of Knowledge, - это \"разработка, выполнение и контроль планов, политик, программ и практик, которые контролируют, защищают, предоставляют и повышают ценность данных и информационных активов\".\nДанные - самый важный аспект вашего бизнеса - Данные - это только одна часть вашего бизнеса в целом. Я встречал выражение \"Данные - это жизненная сила нашего бизнеса\", и, скорее всего, это абсолютно верно. Это заставило меня задуматься о том, что кровь очень важна для организма, но сама по себе она ничего не значит, нам все еще нужны аспекты организма, чтобы сделать кровь чем-то другим, кроме жидкости.\nКачество данных важно как никогда - Мы должны относиться к данным как к бизнес-активу, что означает, что мы должны уделять им должное внимание, чтобы они работали с нашими принципами автоматизации и DevOps.\nСвоевременный доступ к данным - Ни у кого не хватит терпения не иметь доступа к нужным данным в нужное время для принятия эффективных решений. Данные должны быть доступны в упорядоченном и своевременном виде независимо от формы представления.\nУправление данными должно стать помощником DevOps - я уже упоминал о рационализации, мы должны включить требования к управлению данными в наш цикл и обеспечить не только доступность этих данных, но и другие важные политические меры защиты этих точек данных, а также полностью протестированные модели восстановления.\nDataOps\nDataOps и DevOps применяют лучшие практики разработки и эксплуатации технологий для повышения качества, увеличения скорости, снижения угроз безопасности, восхищения клиентов и обеспечения значимой и сложной работы для квалифицированных специалистов. DevOps и DataOps имеют общие цели - ускорить доставку продукта путем автоматизации как можно большего количества этапов процесса. Для DataOps целью является устойчивый конвейер данных и надежные выводы из аналитики данных.\nНекоторые из наиболее распространенных областей более высокого уровня, которые фокусируются на DataOps, - это машинное обучение, большие данные и аналитика данных, включая искусственный интеллект.\nУправление данными - это управление информацией\r\n\r\nВ этом разделе я не буду углубляться в машинное обучение или искусственный интеллект, а сосредоточусь на защите данных с точки зрения защиты информации. Этот подраздел называется \"Управление данными - это управление информацией\", и мы можем считать, что информация = данные.\nТри ключевые области, которые мы должны рассмотреть на этом пути с данными, следующие:\r\n\r\nТочность - Убедитесь в том, что производственные данные точны, также нам необходимо убедиться в том, что наши данные в виде резервных копий также работают и протестированы на восстановление, чтобы быть уверенными в том, что в случае сбоя или возникновения причины нам необходимо иметь возможность восстановить работоспособность как можно быстрее.\nПоследовательность - Если наши службы данных расположены в нескольких местах, то для производства нам необходимо обеспечить последовательность во всех местах расположения данных, чтобы мы получали точные данные. Это также относится к защите данных, когда речь идет о защите этих служб данных, особенно служб данных, нам необходимо обеспечить последовательность на разных уровнях, чтобы убедиться, что мы делаем хорошую чистую копию этих данных для наших резервных копий, реплик и т. д.\nБезопасность - контроль доступа, а также просто хранение данных в целом - актуальная тема в настоящее время во всем мире. Убедиться в том, что нужные люди имеют доступ к вашим данным, - первостепенная задача, и это опять же относится к защите данных, где мы должны убедиться, что только необходимый персонал имеет доступ к резервным копиям и возможность восстановления из них, а также клонирования и предоставления других версий бизнес-данных.\nЛучшие данные = лучшие решения\nДни управления данными\nВ течение следующих 6 занятий мы рассмотрим базы данных, резервное копирование и восстановление, аварийное восстановление, мобильность приложений с элементами демонстрации и практической работы.\r\nРесурсы\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day85",
            "title": "85. Службы данных",
            "description": null,
            "content": "Службы данных\r\n\r\nБазы данных являются наиболее распространенными службами данных, с которыми мы сталкиваемся в наших средах. На этом занятии я хотел бы рассмотреть некоторые из этих различных типов баз данных и некоторые случаи их использования. Некоторые из них мы уже использовали и видели в ходе решения задачи.\nС точки зрения разработки приложений выбор правильной службы данных или базы данных будет иметь огромное значение для производительности и масштабируемости вашего приложения.\nhttps://www.youtube.com/watch?v=W2Z7fbCLSTw\r\nКлюч-значение\r\n\r\nБаза данных \"ключ-значение\" - это тип нереляционной базы данных, которая использует простой метод \"ключ-значение\" для хранения данных. База данных \"ключ-значение\" хранит данные в виде набора пар \"ключ-значение\", в которых ключ служит уникальным идентификатором. И ключи, и значения могут быть любыми, от простых объектов до сложных составных объектов. Базы данных \"ключ-значение\" хорошо поддаются разделению и позволяют горизонтальное масштабирование в таких масштабах, которые недостижимы для других типов баз данных.\r\n\r\nПримером базы данных типа \"ключ-значение\" является Redis.\n*Redis - это хранилище структур данных в памяти, используемое как распределенная база данных ключей-значений в памяти, кэш и брокер сообщений с возможностью долговечности. Redis поддерживает различные виды абстрактных структур данных, таких как строки, списки, карты, множества, сортированные множества, HyperLogLogs, растровые изображения, потоки и пространственные индексы.\r\n\r\n\r\n\r\nКак вы можете видеть из описания Redis, это означает, что наша база данных работает быстро, но мы ограничены в пространстве в качестве компромисса. Также нет запросов или объединений, что означает, что возможности моделирования данных очень ограничены.\nЛучше всего подходит для:\nКэширование\nPub/Sub\r\nЛидерборды\nкорзины покупок\r\n\r\nОбычно используется в качестве кэша над другим постоянным слоем данных.\nШирокий столбец\r\n\r\nБаза данных с широкими колонками - это база данных NoSQL, которая организует хранение данных в гибких колонках, которые могут быть распределены по нескольким серверам или узлам базы данных, используя многомерное отображение для ссылки на данные по столбцам, строкам и временным меткам.\r\n\r\nCassandra - это бесплатная система управления базами данных NoSQL с открытым исходным кодом, распределенная, с широким хранилищем колонок, разработанная для обработки больших объемов данных на множестве серверов, обеспечивающая высокую доступность без единой точки отказа.\r\n\r\n\r\n\r\nНет схемы, что означает возможность работы с неструктурированными данными, однако это может рассматриваться как преимущество для некоторых рабочих нагрузок.\nЛучше всего подходит для:\nВременные ряды\nИсторические записи\nВысокая запись, низкий уровень чтения\nДокумент\r\n\r\nБаза данных документов (также известная как документо-ориентированная база данных или хранилище документов) - это база данных, которая хранит информацию в документах.\nMongoDB - это кросс-платформенная кросс-платформенная программа базы данных, ориентированная на документы. Классифицируемая как NoSQL база данных, MongoDB использует JSON-подобные документы с необязательными схемами. MongoDB разработана компанией MongoDB Inc. и лицензирована по лицензии Server Side Public License..\r\n\r\n\r\n\r\nДокументальные базы данных NoSQL позволяют предприятиям хранить простые данные без использования сложных кодов SQL. Быстрое хранение без ущерба для надежности.\nЛучше всего подходит для:\nБольшинство приложений\nИгры\nИнтернет вещей\r\nРеляционная\r\n\r\nЕсли вы новичок в области баз данных, но знаете о них, то, скорее всего, вы сталкивались с реляционной базой данных.\nРеляционная база данных - это цифровая база данных, основанная на реляционной модели данных, предложенной Э. Ф. Коддом в 1970 году. Система, используемая для ведения реляционных баз данных, - это система управления реляционными базами данных. Многие системы реляционных баз данных имеют возможность использования SQL для запросов и ведения базы данных.\r\n\r\nMySQL - это система управления реляционными базами данных с открытым исходным кодом. Ее название представляет собой комбинацию слов \"My\", имя дочери соучредителя Майкла Видениуса, и \"SQL\", аббревиатура для языка структурированных запросов.\r\n\r\nMySQL является одним из примеров реляционной базы данных, существует множество других вариантов.\nПри изучении реляционных баз данных часто упоминается термин или аббревиатура ACID (atomicity, consistency, isolation, durability) - это набор свойств транзакций базы данных, призванных гарантировать достоверность данных, несмотря на ошибки, сбои питания и другие казусы. В контексте баз данных последовательность операций с базой данных, удовлетворяющая свойствам ACID (которую можно воспринимать как одну логическую операцию над данными), называется транзакцией. Например, перевод средств с одного банковского счета на другой, даже включающий несколько изменений, таких как дебетование одного счета и кредитование другого, является одной транзакцией.\nЛучше всего подходит для:\nБольшинство приложений (существует уже много лет, но это не значит, что он лучший).\r\n\r\nОна не идеальна для неструктурированных данных или способности к масштабированию - некоторые из других NoSQL обеспечивают лучшую способность к масштабированию для определенных рабочих нагрузок.\nGraph\r\n\r\nГрафовая база данных хранит узлы и отношения вместо таблиц или документов. Данные хранятся так же, как вы можете набросать идеи на доске. Ваши данные хранятся без ограничения их заранее определенной моделью, что позволяет очень гибко подходить к их осмыслению и использованию.\r\n\r\nNeo4j - это система управления графовыми базами данных, разработанная компанией Neo4j, Inc. Разработчики описывают ее как ACID-совместимую транзакционную базу данных со встроенными средствами хранения и обработки графов.\r\n\r\nЛучшая для:\nГрафы\r\nГрафы знаний\r\nРекомендательные движки\r\nПоисковая система\r\n\r\nВ предыдущем разделе мы фактически использовали базу данных поисковой системы на пути к Elasticsearch.\nБаза данных поисковой системы - это тип нереляционной базы данных, предназначенной для поиска данных. Базы данных поисковых систем используют индексы для категоризации схожих характеристик данных и облегчения поиска.\r\n\r\nElasticsearch - это поисковая система, основанная на библиотеке Lucene. Она представляет собой распределенную полнотекстовую поисковую систему с поддержкой многопользовательского доступа, веб-интерфейсом HTTP и документами JSON без схем.\r\n\r\nЛучшее для:\nПоисковые системы\nTypeahead\nПоиск по журналу\r\nМультимодель\r\n\r\nМногомодельная база данных - это система управления базой данных, разработанная для поддержки нескольких моделей данных на основе единого интегрированного бэкенда. В отличие от этого, большинство систем управления базами данных организованы вокруг одной модели данных, которая определяет, как данные могут быть организованы, храниться и манипулироваться. Документ, граф, реляционная модель и модель ключ-значение - это примеры моделей данных, которые могут поддерживаться многомодельной базой данных.\nFauna - это гибкая, удобная для разработчиков, транзакционная база данных, предоставляемая в виде безопасного и масштабируемого облачного API со встроенным GraphQL..\r\n\r\nЛучшее решение для:\nВы не привязаны к выбору модели данных.\r\nСоответствует стандарту ACID\r\nБыстрая\nОтсутствие накладных расходов на инициализацию\r\nКак вы хотите использовать свои данные и предоставить облаку выполнять всю работу.\r\n\r\nНа этом мы закончим обзор баз данных, независимо от того, в какой отрасли вы работаете, вы обязательно столкнетесь с одной из областей баз данных. Далее в этом разделе мы рассмотрим некоторые из этих примеров и управление данными и, в частности, защиту и хранение этих сервисов данных.\nСуществует масса ресурсов, ссылки на которые я привел ниже, и вы можете потратить 90 лет на глубокое погружение во все типы баз данных и все, что с этим связано.\r\nРесурсы\nRedis Crash Course - the What, Why and How to use Redis as your primary database\r\nRedis: How to setup a cluster - for beginners\r\nRedis on Kubernetes for beginners\r\nIntro to Cassandra - Cassandra Fundamentals\r\nMongoDB Crash Course\r\nMongoDB in 100 Seconds\r\nWhat is a Relational Database?\r\nLearn PostgreSQL Tutorial - Full Course for Beginners\r\n[MySQL Tutorial for Beginners [Full Course]](https://www.youtube.com/watch?v=7S_tz1z_5bA)\r\nWhat is a graph database? (in 10 minutes)\r\nWhat is Elasticsearch?\r\nFaunaDB Basics - The Database of your Dreams\r\nFauna Crash Course - Covering the Basics\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day86",
            "title": "86. Резервное копирование всех платформ",
            "description": null,
            "content": "Резервное копирование всех платформ\r\n\r\nВ ходе всего этого задания мы обсудили множество различных платформ и сред. Всех их объединяет то, что все они нуждаются в определенном уровне защиты данных!\r\n\r\nЗащита данных существует уже много лет, но богатство данных, которые мы имеем сегодня, и ценность, которую эти данные приносят, означает, что мы должны быть уверены не только в устойчивости к сбоям инфраструктуры за счет наличия нескольких узлов и высокой доступности приложений, но мы также должны учитывать, что нам нужна копия этих данных, этих важных данных в безопасном и надежном месте, если произойдет сбой.\nВ наши дни мы часто слышим о киберпреступности и программах-выкупах, и не поймите меня неправильно - это серьезная угроза, и я уверен, что вы подвергнетесь атаке программ-выкупов. Это не вопрос \"если\", это вопрос \"когда\". Поэтому еще больше причин убедиться в том, что ваши данные надежно защищены на тот случай, если такое время настанет. Однако самой распространенной причиной потери данных является не выкупное ПО или киберпреступность, а просто случайное удаление!\nМы все это делали, удаляли то, что не должны были удалять, и тут же сожалели об этом.\nНесмотря на все технологии и автоматизацию, о которых мы говорили в этой статье, требование защищать любые данные с состоянием или даже сложные конфигурации без состояния все еще существует, независимо от платформы.\r\n\r\n\r\n\r\nНо мы должны быть в состоянии выполнить эту защиту данных с учетом автоматизации и возможности интеграции в наши рабочие процессы.\nЕсли мы посмотрим, что такое резервное копирование:\nВ информационных технологиях резервная копия или резервное копирование данных - это копия компьютерных данных, снятая и сохраненная в другом месте, чтобы ее можно было использовать для восстановления оригинала после потери данных. Глагольная форма, обозначающая процесс создания такой копии, - \"резервное копирование\", а существительное и прилагательное - \"резервное копирование\".\r\n\r\nЕсли мы разберем это в самой простой форме, то резервное копирование - это копирование и вставка данных в новое место. Проще говоря, я могу сделать резервную копию прямо сейчас, скопировав файл с диска C: на диск D:, и у меня будет копия на случай, если что-то случится с диском C: или что-то будет неправильно отредактировано в файлах. Я могу вернуться к копии, которая находится на диске D:. Теперь, если мой компьютер умрет, где находятся оба диска C и D, я не буду защищен, поэтому мне придется искать решение или копировать данные вне моей системы, может быть, на NAS-накопитель у себя дома? Но тогда что произойдет, если что-то случится с моим домом, может быть, мне нужно подумать о хранении данных на другой системе в другом месте, может быть, облако - это вариант. Может быть, я могу хранить копии важных файлов в нескольких местах, чтобы снизить риск сбоя?\n3-2-1 Методика резервного копирования\nСейчас самое время поговорить о правиле 3-2-1 или методологии резервного копирования. На самом деле я провел lightening talk, посвященный этой теме.\nМы уже упоминали о некоторых крайностях того, почему нам нужно защищать наши данные, но ниже перечислены еще несколько:\nЭто позволяет мне рассказать о методологии 3-2-1. Моя первая копия или резервная копия данных должна быть как можно ближе к моей производственной системе, причина этого заключается в скорости восстановления и, опять же, возвращаясь к исходному пункту о случайном удалении, это будет наиболее распространенной причиной для восстановления. Но я хочу хранить эти данные на подходящем втором носителе за пределами исходной или рабочей системы.\nЗатем мы хотим убедиться, что мы также отправляем копию наших данных на внешний носитель или за пределы системы, и здесь нам на помощь приходит второе место, будь то другой дом, здание, центр обработки данных или публичное облако.\nОтветственность за резервное копирование\nМы, скорее всего, слышали все мифы о том, что резервное копирование не нужно, например, такие как \"Все не имеет состояния\". Если все не имеет состояния, то что тогда бизнес? Нет баз данных? документов? Очевидно, что каждый человек в компании несет определенную ответственность за обеспечение своей защиты, но, скорее всего, именно операционные команды должны обеспечить процесс резервного копирования критически важных приложений и данных.\nЕще одна хорошая фраза: \"Высокая доступность - это моя резервная копия, мы встроили несколько узлов в наш кластер, поэтому он ни за что не выйдет из строя!\", кроме тех случаев, когда вы допускаете ошибку в базе данных, и она реплицируется на все узлы кластера, или когда происходит пожар, наводнение, что означает, что кластер больше недоступен, а вместе с ним и важные данные. Речь идет не об упрямстве, а о том, чтобы быть в курсе данных и сервисов, абсолютно все должны учитывать высокую доступность и отказоустойчивость в своей архитектуре, но это не заменяет необходимости резервного копирования!\nРепликация также может дать нам копию данных вне офиса, и, возможно, упомянутый выше кластер действительно живет в нескольких местах, однако первая случайная ошибка все равно будет реплицирована туда. Но, опять же, требование резервного копирования должно стоять в одном ряду с репликацией приложений или системной репликацией в среде.\nТеперь, учитывая все вышесказанное, можно впасть в крайность и отправить копии данных в слишком большое количество мест, что приведет не только к большим затратам, но и к увеличению риска подвергнуться атаке, поскольку площадь вашей поверхности теперь значительно увеличилась.\nВ любом случае, кто заботится о резервном копировании? В каждом предприятии это будет по-разному, но кто-то должен понимать требования к резервному копированию. Но также необходимо понимать план восстановления!\nНикому нет дела, пока всем нет дела\nРезервное копирование является ярким примером: никто не заботится о резервном копировании, пока вам не понадобится что-то восстановить. Наряду с требованием резервного копирования данных нам также необходимо подумать о том, как мы будем восстанавливать данные!\nВ нашем примере с текстовыми документами речь идет об очень маленьких файлах, поэтому возможность копирования туда и обратно является простой и быстрой. Но если речь идет о файлах размером более 100 ГБ, то на это потребуется время. Также необходимо учитывать уровень, на котором требуется восстановление, например, если мы возьмем виртуальную машину.\nУ нас есть вся виртуальная машина, у нас есть операционная система, установка приложений, а если это сервер баз данных, то у нас есть и некоторые файлы баз данных. Если мы допустили ошибку и вставили неправильную строку кода в нашу базу данных, мне, вероятно, не нужно восстанавливать всю виртуальную машину, я хочу быть детальным в том, что я восстанавливаю.\nСценарий резервного копирования\nТеперь я хочу начать строить скрипт защиты некоторых данных, в частности, я хочу защитить некоторые файлы на моей локальной машине (в данном случае Windows, но инструмент, который я собираюсь использовать, на самом деле не только бесплатный и с открытым исходным кодом, но и кроссплатформенный). Я хочу убедиться, что они защищены на устройстве NAS, которое у меня есть дома, а также в облачном хранилище Object Storage bucket.\nЯ хочу сделать резервную копию этих важных данных, так получилось, что это репозиторий для 90DaysOfDevOps, который, да, также отправляется на GitHub, где вы, вероятно, сейчас это читаете, но что, если моя машина умрет, а GitHub будет закрыт? Как бы кто-нибудь смог прочитать содержимое, а также как бы я мог восстановить эти данные на другом сервисе.\nСуществует множество инструментов, которые могут помочь нам достичь этого, но я собираюсь использовать инструмент под названием Kopia - это инструмент резервного копирования с открытым исходным кодом, который позволит нам шифровать, дедупировать и сжимать наши резервные копии, а также отправлять их во многие места.\nВы найдете релизы для загрузки здесь на момент написания статьи я буду использовать версию 0.10.6.\nУстановка Kopia\nСуществует Kopia CLI и GUI, мы будем использовать GUI, но знайте, что вы можете иметь и CLI версию для тех Linux серверов, которые не дают вам GUI.\nЯ буду использовать KopiaUI-Setup-0.10.6.exe.\r\n\r\nДействительно быстрая установка, а затем, когда вы откроете приложение, вам предложат выбрать тип хранилища, которое вы хотите использовать в качестве хранилища резервных копий.\nНастройка хранилища\nСначала мы хотим создать хранилище на локальном NAS-устройстве и собираемся сделать это с помощью SMB, но можно использовать и NFS.\nНа следующем экране мы собираемся определить пароль, этот пароль используется для шифрования содержимого хранилища.\nТеперь, когда хранилище настроено, мы можем запустить adhoc snapshot, чтобы начать запись данных в хранилище.\r\n[18:26, 16.06.2022] evgschegolkova:\nПрежде всего, нам нужно ввести путь к тому, что мы хотим сделать снимок, и в нашем случае мы хотим сделать копию папки 90DaysOfDevOps. Вскоре мы вернемся к аспекту планирования.\nМы можем определить хранение наших снимков.\nВозможно, есть файлы или типы файлов, которые мы хотим исключить.\nЕсли бы мы хотели определить расписание, мы могли бы сделать это на следующем экране, когда вы впервые создаете этот снимок, это начальная страница для определения.\nИ вы увидите ряд других настроек, которые могут быть обработаны здесь.\nВыберите snapshot now, и данные будут записаны в ваше хранилище.\nВнесетевое резервное копирование на S3\nС помощью Kopia мы можем настроить только одно хранилище одновременно. Но через пользовательский интерфейс мы можем проявить творческий подход и, по сути, иметь несколько файлов конфигурации хранилища на выбор для достижения нашей цели - иметь локальную и внесетевую копию в Object Storage.\nХранилище Object Storage, в которое я решил отправить свои данные, будет Google Cloud Storage. Сначала я вошел в свой аккаунт Google Cloud Platform и создал себе ведро хранения. В моей системе уже был установлен Google Cloud SDK, но выполнение команды gcloud auth application-default login позволило мне аутентифицироваться в моей учетной записи.\nЗатем я использовал CLI Kopia, чтобы показать мне текущее состояние моего хранилища после того, как мы добавили наше SMB хранилище в предыдущих шагах. Я сделал это с помощью команды \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status.\nТеперь мы готовы заменить конфигурацию хранилища для целей демонстрации. Если бы мы хотели получить долгосрочное решение для обоих хранилищ, мы бы создали файл smb.config и файл object.config и могли бы запускать обе эти команды для отправки наших копий данных в каждое место. Для добавления нашего хранилища мы выполнили команду \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository create gcs --bucket 90daysofdevops.\r\n\r\nПриведенная выше команда учитывает, что ведро Google Cloud Storage, которое мы создали, называется 90daysofdevops.\r\n\r\n\r\n\r\nТеперь, когда мы создали наше новое хранилище, мы можем снова запустит\r\n[18:27, 16.06.2022] evgschegolkova:\nПрежде всего, нам нужно ввести путь к тому, что мы хотим сделать снимок, и в нашем случае мы хотим сделать копию папки 90DaysOfDevOps. Вскоре мы вернемся к аспекту планирования.\nМы можем определить хранение наших снимков.\nВозможно, есть файлы или типы файлов, которые мы хотим исключить.\nЕсли бы мы хотели определить расписание, мы могли бы сделать это на следующем экране, когда вы впервые создаете этот снимок, это начальная страница для определения.\nИ вы увидите ряд других настроек, которые могут быть обработаны здесь.\nВыберите snapshot now, и данные будут записаны в ваше хранилище.\nВнесетевое резервное копирование на S3\nС помощью Kopia мы можем настроить только одно хранилище одновременно. Но через пользовательский интерфейс мы можем проявить творческий подход и, по сути, иметь несколько файлов конфигурации хранилища на выбор для достижения нашей цели - иметь локальную и внесетевую копию в Object Storage.\nХранилище Object Storage, в которое я решил отправить свои данные, будет Google Cloud Storage. Сначала я вошел в свой аккаунт Google Cloud Platform и создал себе ведро хранения. В моей системе уже был установлен Google Cloud SDK, но выполнение команды gcloud auth application-default login позволило мне аутентифицироваться в моей учетной записи.\nЗатем я использовал CLI Kopia, чтобы показать мне текущее состояние моего хранилища после того, как мы добавили наше SMB хранилище в предыдущих шагах. Я сделал это с помощью команды \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status.\nТеперь мы готовы заменить конфигурацию хранилища для целей демонстрации. Если бы мы хотели получить долгосрочное решение для обоих хранилищ, мы бы создали файл smb.config и файл object.config и могли бы запускать обе эти команды для отправки наших копий данных в каждое место. Для добавления нашего хранилища мы выполнили команду \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository create gcs --bucket 90daysofdevops.\r\n\r\nПриведенная выше команда учитывает, что ведро Google Cloud Storage, которое мы создали, называется 90daysofdevops.\r\n\r\n\r\n\r\nТеперь, когда мы создали наше новое хранилище, мы можем снова запустить команду \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config repository status, которая теперь покажет конфигурацию хранилища GCS.\nСледующее, что нам нужно сделать, это создать снимок и отправить его в наш только что созданный репозиторий. Используя команду \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config kopia snapshot create \"C:\\Users\\micha\\demo\\90DaysOfDevOps\" мы можем запустить этот процесс. В браузере ниже вы можете увидеть, что в нашем ведре Google Cloud Storage теперь есть файлы kopia, основанные на нашей резервной копии.\nС помощью вышеописанного процесса мы смогли решить нашу задачу по отправке важных данных в 2 разных места, одно из которых находится вне помещения в Google Cloud Storage, и, конечно же, у нас все еще есть наша производственная копия данных на другом типе носителя.\nВосстановление\r\n\r\nВосстановление - это еще один важный момент, Kopia дает нам возможность не только восстанавливать данные в существующее местоположение, но и в новое.\nЕсли мы выполним команду \"C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe\" --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config snapshot list, это приведет к списку снимков, которые в настоящее время находятся в нашем настроенном хранилище (GCS).\nЗатем мы можем смонтировать эти снимки непосредственно из GCS, используя команду `C:\\Program Files\\KopiaUI\\resources\\server\\kopia.exe'' --config-file=C:\\Users\\micha\\AppData\\Roaming\\kopia\\repository.config mount all Z:.\r\n\r\n\r\n\r\nМы также можем восстановить содержимое снимка с помощью команды kopia snapshot restore kdbd9dff738996cfe7bcf99b45314e193.\nОчевидно, что приведенные выше команды очень длинные, и это потому, что я использовал KopiaUI версию kopia.exe, как объяснялось в верхней части руководства, вы можете скачать kopia.exe и поместить в путь, чтобы вы могли просто использовать команду kopia.\nНа следующем занятии мы сосредоточимся на защите рабочих нагрузок в Kubernetes.\r\nРесурсы\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day87",
            "title": "87. Резервное копирование и восстановление",
            "description": null,
            "content": "Резервное копирование и восстановление своими руками\r\n\r\nНа прошлом занятии мы рассмотрели Kopia - инструмент резервного копирования с открытым исходным кодом, который мы использовали для переноса важных данных на локальный NAS и в облачное хранилище объектов.\nВ этом разделе я хочу погрузиться в мир резервного копирования Kubernetes. Это платформа, которую мы рассматривали в The Big Picture: Kubernetes ранее в этой задаче.\nМы снова будем использовать наш кластер minikube, но на этот раз мы воспользуемся некоторыми из доступных аддонов.\nНастройка кластера Kubernetes\nДля настройки нашего кластера minikube мы выполним команду minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p 90daysofdevops --kubernetes-version=1.21.2. Вы заметите, что мы используем volumesnapshots и csi-hostpath-driver, поскольку мы будем использовать их для создания резервных копий.\nНа данном этапе я знаю, что мы еще не развернули Kasten K10, но мы хотим выполнить следующую команду, когда ваш кластер будет запущен, но мы хотим аннотировать класс volumesnapshotclass, чтобы Kasten K10 мог использовать его.\n`\r\nkubectl annotate volumesnapshotclass csi-hostpath-snapclass \\\r\n    k10.kasten.io/is-snapshot-class=true\r\n`\r\n\r\nМы также собираемся изменить класс хранения по умолчанию со стандартного класса хранения по умолчанию на класс хранения csi-hostpath, используя следующее.\n`\r\nkubectl patch storageclass csi-hostpath-sc -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"true\"}}}}''\r\n\r\nkubectl patch storageclass standard -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\": \"false\"}}}}''\r\n`\r\nРазвертывание Kasten K10\nДобавьте репозиторий Kasten Helm\r\n\r\nhelm repo add kasten https://charts.kasten.io/.\r\n\r\nМы могли бы использовать arkade kasten install k10 и здесь, но для целей демонстрации мы выполним следующие шаги. Подробнее\r\n\r\nСоздайте пространство имен и разверните K10, обратите внимание, что это займет около 5 минут\nhelm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace.\r\n\r\n\r\n\r\nВы можете наблюдать за появлением стручков, выполнив следующую команду.\r\n\r\nkubectl get pods -n kasten-io -w\r\n\r\n\r\n\r\nЧтобы получить доступ к приборной панели K10, откройте новый терминал и выполните следующую команду\r\n\r\nkubectl --namespace kasten-io port-forward service/gateway 8080:8000.\r\n\r\nПриборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/\r\n\r\n\r\n\r\nДля аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.\n`\r\nTOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\r\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\r\n\r\necho \"Значение токена: \"\r\necho $TOKEN\r\n`\r\n\r\n\r\n\r\nТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.\nЗатем мы получаем доступ к приборной панели Kasten K10.\nРазвертывание нашего stateful-приложения\nИспользуйте stateful-приложение, которое мы использовали в разделе Kubernetes.\nВы можете найти конфигурационный файл YAML для этого приложения здесьpacman-stateful-demo.yaml\r\n\r\n\r\n\r\nМы можем использовать kubectl get all -n pacman, чтобы проверить появление наших стручков.\nВ новом терминале мы можем перенаправить фронт-енд pacman. kubectl port-forward svc/pacman 9090:80 -n pacman.\r\n\r\nОткройте другую вкладку в браузере на http://localhost:9090/\nНайдите время, чтобы записать несколько высоких результатов в базе данных backend MongoDB.\nЗащитите наши высокие баллы\nТеперь у нас есть некоторые важные данные в нашей базе данных, и мы не хотим их потерять. Мы можем использовать Kasten K10 для защиты всего приложения.\nЕсли мы вернемся на вкладку приборной панели Kasten K10, вы увидите, что количество наших приложений увеличилось с 1 до 2 с добавлением нашего приложения pacman в наш кластер Kubernetes.\nЕсли вы нажмете на карточку Applications, вы увидите автоматически обнаруженные приложения в нашем кластере.\nВ Kasten K10 у нас есть возможность использовать моментальные снимки на основе хранилища, а также экспортировать наши копии в объектные хранилища.\nДля целей демонстрации мы создадим ручной снимок хранилища в нашем кластере, а затем добавим некоторые неавторизованные данные в наши высокие результаты, чтобы имитировать случайную ошибку или нет?\nДля начала мы можем воспользоваться приведенным ниже вариантом ручного снапшота.\nДля демонстрации я собираюсь оставить все по умолчанию\nВернувшись на приборную панель, вы получите отчет о состоянии задания в процессе его выполнения, а после завершения оно должно выглядеть так же успешно, как и здесь.\nСценарий неудачи\nТеперь мы можем внести фатальное изменение в наши критически важные данные, просто добавив предписывающее плохое изменение в наше приложение.\nКак вы можете видеть ниже, у нас есть два входа, которые мы, вероятно, не хотим видеть в нашей производственной критически важной базе данных.\r\nВосстановление данных\r\n\r\nОчевидно, что это простая демонстрация и в некотором роде нереалистичная, хотя вы видели, как легко можно сбросить базы данных?\nТеперь мы хотим, чтобы список высоких результатов выглядел немного чище и как он выглядел до того, как были допущены ошибки.\nВернемся в карточку приложений и на вкладку pacman, теперь у нас есть 1 точка восстановления, которую мы можем использовать для восстановления.\nПри выборе восстановления вы можете увидеть все связанные снимки и экспорты для этого приложения.\nВыберите восстановление и появится боковое окно, мы сохраним настройки по умолчанию и нажмем восстановить.\nПодтвердите, что вы действительно хотите, чтобы это произошло.\nЗатем вы можете вернуться на приборную панель и просмотреть ход восстановления. Вы должны увидеть что-то вроде этого.\nНо более важно то, как выглядит наш список High-Score в нашем критически важном приложении. Вам придется снова запустить проброс портов в pacman, как мы уже рассказывали ранее.\nЭто очень простая демонстрация, которая лишь слегка касается того, чего Kasten K10 может достичь в области резервного копирования. В будущем я буду создавать более подробные видеоматериалы по некоторым из этих областей. Мы также будем использовать Kasten K10 для освещения некоторых других важных областей управления данными, когда речь идет об аварийном восстановлении и мобильности ваших данных.\nДалее мы рассмотрим согласованность приложений.\nРесурсы\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day88",
            "title": "88. Резервное копирование, ориентированное на приложения",
            "description": null,
            "content": "Резервное копирование, ориентированное на приложения\r\n\r\nВ День 85 мы уже потратили некоторое время на обсуждение служб данных или приложений с интенсивным использованием данных, таких как базы данных. Для этих служб данных мы должны подумать о том, как управлять согласованностью, особенно когда речь идет о согласованности приложений.\nВ этой статье мы рассмотрим требования к защите данных приложения в последовательной манере.\nДля этого мы выберем инструмент Kanister\r\nПредставляем Kanister\nKanister - это проект с открытым исходным кодом от Kasten, который позволяет нам управлять (резервное копирование и восстановление) данными приложений на Kubernetes. Вы можете развернуть Kanister как helm-приложение в своем кластере Kubernetes.\nKanister использует пользовательские ресурсы Kubernetes, основные пользовательские ресурсы, которые устанавливаются при развертывании Kanister, следующие\nProfile - целевое место для хранения резервных копий и восстановления. Чаще всего это объектное хранилище.\nBlueprint - шаги, которые необходимо предпринять для резервного копирования и восстановления базы данных, должны быть сохранены в Blueprint.\r\nActionSet - действия по перемещению целевой резервной копии в наш профиль, а также действия по восстановлению.\nОписание выполнения\nПрежде чем приступить к работе, мы должны рассмотреть рабочий процесс, который использует Kanister для защиты данных приложения. Во-первых, наш контроллер развертывается с помощью helm в нашем кластере Kubernetes, Kanister живет в своем собственном пространстве имен. Мы берем наш Blueprint, для которого существует множество поддерживаемых сообществом Blueprint, мы рассмотрим это более подробно в ближайшее время. Затем у нас есть рабочая нагрузка базы данных.\nЗатем мы создаем наш ActionSet.\nActionSet позволяет нам запускать действия, определенные в чертеже, против конкретной службы данных.\nActionSet, в свою очередь, использует функции Kanister (KubeExec, KubeTask, Resource Lifecycle) и выталкивает нашу резервную копию в целевое хранилище (Profile).\nЕсли действие выполнено/не выполнено, соответствующий статус обновляется в наборе действий.\r\nРазвертывание Kanister\nИ снова мы будем использовать кластер minikube для создания резервной копии приложения. Если у вас он все еще работает с предыдущей сессии, то мы можем продолжать использовать его.\nНа момент написания статьи мы имеем версию образа 0.75.0. С помощью следующей команды helm мы установим kanister в наш кластер Kubernetes.\nhelm install kanister --namespace kanister kanister/kanister-operator --set image.tag=0.75.0 --create-namespace.\r\n\r\n\r\n\r\nМы можем использовать kubectl get pods -n kanister, чтобы убедиться, что pod запущен и работает, а также проверить, что наши пользовательские определения ресурсов теперь доступны (Если вы только установили Kanister, то вы увидите выделенные 3)\r\nРазвертывание базы данных\nРазвертывание mysql через helm:\r\n\r\n`\r\nAPP_NAME=my-production-app\r\nkubectl create ns ${APP_NAME}\r\nhelm repo add bitnami https://charts.bitnami.com/bitnami\r\nhelm install mysql-store bitnami/mysql --set primary.persistence.size=1Gi,volumePermissions.enabled=true --namespace=${APP_NAME}\r\nkubectl get pods -n ${APP_NAME} -w\r\n`\r\n\r\n\r\nЗаполните базу данных mysql исходными данными, выполнив следующее:\r\n\r\n`\r\nMYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace ${APP_NAME} mysql-store -o jsonpath=\"{.data.mysql-root-password}\" | base64 --decode)\r\nMYSQL_HOST=mysql-store.${APP_NAME}.svc.cluster.local\r\nMYSQL_EXEC=\"mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\"\r\necho MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}\r\n`\r\nСоздание MySQL CLIENT\nМы запустим другой образ контейнера, который будет выступать в качестве нашего клиента\r\n\r\n`\r\nAPP_NAME=my-production-app\r\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image docker.io/bitnami/mysql:latest --command -- bash\r\n`\r\n`\r\nПримечание: если у вас уже запущен существующий mysql client pod, удалите его с помощью команды\r\n\r\nkubectl delete pod -n ${APP_NAME} mysql-client\r\n`\r\nДобавление данных в MySQL\r\n`\r\necho \"create database myImportantData;\" | mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD}\r\nMYSQL_EXEC=\"mysql -h ${MYSQL_HOST} -u root --password=${MYSQL_ROOT_PASSWORD} -DmyImportantData -t\"\r\necho \"drop table Accounts\" | ${MYSQL_EXEC}\r\necho \"create table if not exists Accounts(name text, balance integer); insert into Accounts values('nick', 0);\" |  ${MYSQL_EXEC}\r\necho \"insert into Accounts values('albert', 112);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('alfred', 358);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('beatrice', 1321);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('bartholomew', 34);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('edward', 5589);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('edwin', 144);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('edwina', 233);\" | ${MYSQL_EXEC}\r\necho \"insert into Accounts values('rastapopoulos', 377);\" | ${MYSQL_EXEC}\r\necho \"select * from Accounts;\" |  ${MYSQL_EXEC}\r\nexit\r\n`\r\nВы должны увидеть некоторые данные, как показано ниже.\nСоздание профиля Kanister\r\n\r\nKanister предоставляет CLI, kanctl и другую утилиту kando, которая используется для взаимодействия с провайдером объектного хранилища из blueprint и обе эти утилиты.\nCLI Download\r\n\r\nЯ пошел и создал AWS S3 Bucket, который мы будем использовать в качестве цели профиля и места восстановления. Я буду использовать переменные окружения, чтобы иметь возможность показать вам команды, которые я выполняю с помощью kanctl для создания нашего профиля kanister.\nkanctl create profile s3compliant --access-key $ACCESS_KEY --secret-key $SECRET_KEY --bucket $BUCKET --region eu-west-2 --namespace my-production-app.\r\nВремя чертежа\r\n\r\nНе волнуйтесь, вам не нужно создавать свой собственный с нуля, если только ваш сервис данных не указан в Примерах Канистера, но, конечно, вклад сообщества - это то, как этот проект становится известным.\nМы будем использовать следующую схему.\r\n\r\n`\r\napiVersion: cr.kanister.io/v1alpha1\r\nkind: Blueprint\r\nmetadata:\r\n  name: mysql-blueprint\r\nactions:\r\n  backup:\r\n    outputArtifacts:\r\n      mysqlCloudDump:\r\n        keyValue:\r\n          s3path: \"{{ .Phases.dumpToObjectStore.Output.s3path }}\"\r\n    phases:\r\n    func: KubeTask\r\n      name: dumpToObjectStore\r\n      objects:\r\n        mysqlSecret:\r\n          kind: Secret\r\n          name: '{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}'\r\n          namespace: '{{ .StatefulSet.Namespace }}'\r\n      args:\r\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\r\n        namespace: \"{{ .StatefulSet.Namespace }}\"\r\n        command:\r\n        bash\r\n        -o\r\n        errexit\r\n        -o\r\n        pipefail\r\n        -c\r\n        |\r\n          s3_path=\"/mysql-backups/{{ .StatefulSet.Namespace }}/{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}/{{ toDate \"2006-01-02T15:04:05.999999999Z07:00\" .Time  | date \"2006-01-02T15-04-05\" }}/dump.sql.gz\"\r\n          root_password=\"{{ index .Phases.dumpToObjectStore.Secrets.mysqlSecret.Data \"mysql-root-password\" | toString }}\"\r\n          mysqldump --column-statistics=0 -u root --password=${root_password} -h {{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }} --single-transaction --all-databases | gzip - | kando location push --profile '{{ toJson .Profile }}' --path ${s3_path} -\r\n          kando output s3path ${s3_path}\r\n  restore:\r\n    inputArtifactNames:\r\n    mysqlCloudDump\r\n    phases:\r\n    func: KubeTask\r\n      name: restoreFromBlobStore\r\n      objects:\r\n        mysqlSecret:\r\n          kind: Secret\r\n          name: '{{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}'\r\n          namespace: '{{ .StatefulSet.Namespace }}'\r\n      args:\r\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\r\n        namespace: \"{{ .StatefulSet.Namespace }}\"\r\n        command:\r\n        bash\r\n        -o\r\n        errexit\r\n        -o\r\n        pipefail\r\n        -c\r\n        |\r\n          s3_path=\"{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\"\r\n          root_password=\"{{ index .Phases.restoreFromBlobStore.Secrets.mysqlSecret.Data \"mysql-root-password\" | toString }}\"\r\n          kando location pull --profile '{{ toJson .Profile }}' --path ${s3_path} - | gunzip | mysql -u root --password=${root_password} -h {{ index .Object.metadata.labels \"app.kubernetes.io/instance\" }}\r\n  delete:\r\n    inputArtifactNames:\r\n    mysqlCloudDump\r\n    phases:\r\n    func: KubeTask\r\n      name: deleteFromBlobStore\r\n      args:\r\n        image: ghcr.io/kanisterio/mysql-sidecar:0.75.0\r\n        namespace: \"{{ .Namespace.Name }}\"\r\n        command:\r\n        bash\r\n        -o\r\n        errexit\r\n        -o\r\n        pipefail\r\n        -c\r\n        |\r\n          s3_path=\"{{ .ArtifactsIn.mysqlCloudDump.KeyValue.s3path }}\"\r\n          kando location delete --profile '{{ toJson .Profile }}' --path ${s3_path}\r\n`\r\n\r\nЧтобы добавить его, мы воспользуемся командой kubectl create -f mysql-blueprint.yml -n kanister.\nСоздаем наш ActionSet и защищаем наше приложение\nТеперь мы создадим резервную копию данных MySQL с помощью ActionSet, определяющего резервное копирование для этого приложения. Создайте ActionSet в том же пространстве имен, что и контроллер.\r\n\r\nkubectl get profiles.cr.kanister.io -n my-production-app Эта команда покажет нам профиль, который мы ранее создали, здесь может быть настроено несколько профилей, поэтому мы можем захотеть использовать определенные профили для разных ActionSet'ов.\nЗатем мы создадим наш ActionSet следующей командой с помощью kanctl.\nkanctl create actionset --action backup --namespace kanister --blueprint mysql-blueprint --statefulset my-production-app/mysql-store --profile my-production-app/s3-profile-dc5zm --secrets mysql=my-production-app/mysql-store.\r\n\r\nИз приведенной выше команды видно, что мы определяем blueprint, который мы добавили в пространство имен, statefulset в нашем пространстве имен my-production-app, а также секреты для входа в приложение MySQL.\nПроверьте состояние ActionSet, взяв имя ActionSet и используя эту команду kubectl --namespace kanister describe actionset backup-qpnqv.\r\n\r\nНаконец, мы можем пойти и подтвердить, что теперь у нас есть данные в нашем ведре AWS S3.\nВосстановление\nНам нужно нанести некоторый ущерб, прежде чем мы сможем что-либо восстановить, мы можем сделать это, уронив нашу таблицу, возможно, это был несчастный случай, а возможно и нет.\nПодключитесь к нашему MySQL pod.\r\n`\r\nAPP_NAME=my-production-app\r\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image  docker.io/bitnami/mysql:latest --command -- bash\r\n`\r\n\r\nВы можете увидеть, что наша база данных importantdata находится там с помощью echo \"SHOW DATABASES;\" | ${MYSQL_EXEC}.\nЗатем для удаления мы запустили echo \"DROP DATABASE myImportantData;\" | ${MYSQL_EXEC}.\r\n\r\nИ подтвердили, что все исчезло, сделав несколько попыток показать нашу базу данных.\nТеперь мы можем использовать Kanister, чтобы вернуть наши важные данные в рабочее состояние, используя команду kubectl get actionset -n kanister, чтобы узнать имя нашего ActionSet, который мы взяли ранее. Затем мы создадим ActionSet восстановления для восстановления наших данных, используя kanctl create actionset -n kanister --action restore --from \"backup-qpnqv\".\r\n\r\n\r\n\r\nМы можем подтвердить, что наши данные восстановлены, используя следующую команду для подключения к нашей базе данных.\r\n`\r\nAPP_NAME=my-production-app\r\nkubectl run mysql-client --rm --env APP_NS=${APP_NAME} --env MYSQL_EXEC=\"${MYSQL_EXEC}\" --env MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD} --env MYSQL_HOST=${MYSQL_HOST} --namespace ${APP_NAME} --tty -i --restart='Never' --image  docker.io/bitnami/mysql:latest --command -- bash\r\n`\r\nТеперь мы находимся внутри клиента MySQL, мы можем выполнить команду echo \"SHOW DATABASES;\" | ${MYSQL_EXEC} и мы увидим, что база данных восстановлена. Мы также можем выполнить команду echo \"select * from Accounts;\" | ${MYSQL_EXEC} для проверки содержимого базы данных, и наши важные данные будут восстановлены.\nВ следующем посте мы рассмотрим аварийное восстановление в Kubernetes.\r\nРесурсы\nKanister Overview - An extensible open-source framework for app-lvl data management on Kubernetes\r\nApplication Level Data Operations on Kubernetes\r\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day89",
            "title": "89. Аварийное восстановление",
            "description": null,
            "content": "Аварийное восстановление\r\n\r\nМы уже упоминали о том, что различные скрипты сбоев требуют различных требований к восстановлению. Когда речь идет о скриптах пожара, наводнения и крови, мы можем рассматривать их как аварийные ситуации, в которых нам может потребоваться, чтобы наши рабочие нагрузки были запущены в совершенно другом месте как можно быстрее или, по крайней мере, с почти нулевым временем восстановления (RTO).\nЭтого можно достичь только в масштабе, если автоматизировать репликацию всего стека приложений в резервную среду.\nЭто позволяет быстро переходить от одного облачного региона к другому, облачным провайдерам или между локальной и облачной инфраструктурой.\nПродолжая тему, мы сосредоточимся на том, как этого можно достичь с помощью Kasten K10, используя наш кластер minikube, который мы развернули и настроили несколько занятий назад.\nЗатем мы создадим еще один кластер minikube с установленным Kasten K10 в качестве резервного кластера, который теоретически может находиться в любом месте.\nKasten K10 также имеет встроенную функциональность для обеспечения того, что если что-то случится с кластером Kubernetes, на котором он работает, данные каталога будут реплицированы и доступны на новом K10 Disaster Recovery.\r\nДобавление объектного хранилища в K10\nПервое, что нам нужно сделать, это добавить ведро объектного хранилища в качестве целевого местоположения для наших резервных копий. Это не только выступает в качестве удаленного хранилища, но мы также можем использовать его в качестве исходных данных для аварийного восстановления.\nЯ очистил ведро S3, которое мы создали для демонстрации Kanister на прошлом занятии.\nЧтобы получить доступ к приборной панели K10, откройте новый терминал и выполните следующую команду:\r\n\r\nkubectl --namespace kasten-io port-forward service/gateway 8080:8000.\r\n\r\nПриборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/\r\n\r\n\r\n\r\nДля аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.\r\n`\r\nTOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\r\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\r\n\r\necho \"Token value: \"\r\necho $TOKEN\r\n`\r\n\r\n\r\n\r\nТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.\nЗатем мы получаем доступ к приборной панели Kasten K10.\nТеперь, когда мы вернулись в приборную панель Kasten K10, мы можем добавить наш профиль местоположения, выберите \"Настройки\" в верхней части страницы и \"Новый профиль\".\r\n\r\n\r\n\r\nНа изображении ниже видно, что у нас есть выбор, где будет находиться этот профиль местоположения, мы выбираем Amazon S3, и добавляем наши учетные данные доступа, регион и имя ведра.\nЕсли мы прокрутим окно создания нового профиля вниз, то увидим, что у нас также есть возможность включить неизменяемое резервное копирование, которое использует API блокировки объектов S3. В данном демо мы не будем использовать эту возможность.\nНажмите \"Сохранить профиль\", и теперь вы можете увидеть наш только что созданный или добавленный профиль местоположения, как показано ниже.\nСоздание политики для защиты приложения Pac-Man в объектном хранилище\r\n\r\nВ предыдущем сеансе мы создали только специальный снимок нашего приложения Pac-Man, поэтому нам нужно создать политику резервного копирования, которая будет отправлять резервные копии нашего приложения в наше недавно созданное объектное хранилище.\nЕсли вы вернетесь на приборную панель и выберете карточку Policy, вы увидите окно, как показано ниже. Выберите \"Создать новую политику\".\nВо-первых, мы можем дать нашей политике полезное имя и описание. Мы также можем определить частоту резервного копирования, для демонстрационных целей я использую \"по требованию\".\nДалее мы хотим включить резервное копирование через Snapshot exports, что означает, что мы хотим отправлять наши данные в наш профиль местоположения. Если у вас их несколько, вы можете выбрать, в какой из них вы хотите отправлять резервные копии.\r\n\r\n\r\n\r\nДалее выбираем приложение по имени или по меткам, я собираюсь выбрать по имени и все ресурсы.\nВ разделе Advanced settings мы не будем использовать ничего из этого, но, основываясь на нашем вчерашнем walkthrough of Kanister, мы можем использовать Kanister как часть Kasten K10 для создания согласованных с приложением копий наших данных.\nНаконец, выберите \"Создать политику\", и теперь вы увидите политику в нашем окне политики.\nВ нижней части созданной политики появится \"Show import details\", нам нужна эта строка, чтобы иметь возможность импортировать в наш резервный кластер. Скопируйте ее в безопасное место.\nПрежде чем двигаться дальше, нам нужно выбрать \"run once\", чтобы получить резервную копию, отправленную нашему ведру объектного хранилища.\nНиже, на скриншоте просто показано успешное резервное копирование и экспорт наших данных.\nСоздание нового кластера MiniKube и развертывание K10\r\n\r\nЗатем нам нужно развернуть второй кластер Kubernetes, и где это может быть любая поддерживаемая версия Kubernetes, включая OpenShift, в целях обучения мы будем использовать очень бесплатную версию MiniKube с другим названием.\nИспользуя minikube start --addons volumesnapshots,csi-hostpath-driver --apiserver-port=6443 --container-runtime=containerd -p standby --kubernetes-version=1.21.2 мы можем создать наш новый кластер.\nЗатем мы можем развернуть Kasten K10 в этом кластере, используя:\r\n\r\nhelm install k10 kasten/k10 --namespace=kasten-io --set auth.tokenAuth.enabled=true --set injectKanisterSidecar.enabled=true --set-string injectKanisterSidecar.namespaceSelector.matchLabels.k10/injectKanisterSidecar=true --create-namespace.\r\n\r\nЭто займет некоторое время, но тем временем мы можем использовать kubectl get pods -n kasten-io -w, чтобы наблюдать за прогрессом перехода наших pods в статус запущенных.\nСтоит отметить, что поскольку мы используем MiniKube, наше приложение будет запущено, когда мы запустим политику импорта, наш класс хранилища будет таким же на этом резервном кластере. Однако то, что мы рассмотрим на последнем занятии, касается мобильности и трансформации.\nКогда капсулы запущены, мы можем выполнить шаги, которые мы проделали в предыдущих шагах на другом кластере.\nПеренесите порт вперед для доступа к приборной панели K10, откройте новый терминал и выполните следующую команду\r\n\r\nkubectl --namespace kasten-io port-forward service/gateway 8080:8000.\r\n\r\nПриборная панель Kasten будет доступна по адресу: http://127.0.0.1:8080/k10/#/\r\n\r\n\r\n\r\nДля аутентификации на приборной панели нам теперь нужен токен, который мы можем получить с помощью следующих команд.\r\n`\r\nTOKEN_NAME=$(kubectl get secret --namespace kasten-io|grep k10-k10-token | cut -d \" \" -f 1)\r\nTOKEN=$(kubectl get secret --namespace kasten-io $TOKEN_NAME -o jsonpath=\"{.data.token}\" | base64 --decode)\r\n\r\necho \"Token value: \"\r\necho $TOKEN\r\n`\r\n\r\n\r\n\r\nТеперь мы берем этот токен и вводим его в браузер, после чего вам будет предложено ввести email и название компании.\nЗатем мы получаем доступ к приборной панели Kasten K10.\nИмпортируем Pac-Man в новый кластер MiniKube\r\n\r\nНа данном этапе мы можем создать политику импорта в резервном кластере, подключиться к резервным копиям объектного хранилища и определить, что и как мы хотим, чтобы выглядело.\nВо-первых, мы добавляем наш профиль местоположения, который мы рассмотрели ранее на другом кластере, используя темный режим, чтобы показать разницу между нашей производственной системой и резервным местоположением DR.\nТеперь вернемся к приборной панели и перейдем на вкладку политик, чтобы создать новую политику.\nСоздайте политику импорта в соответствии с приведенным ниже изображением. После завершения мы можем создать политику. Здесь есть опция восстановления после импорта, и некоторые люди могут захотеть воспользоваться этой опцией, которая будет восстановлена в нашем резервном кластере по завершении. У нас также есть возможность изменить конфигурацию приложения при восстановлении, и это то, что я описал в Day 90.\nЯ выбрал импорт по требованию, но вы, очевидно, можете установить расписание, когда вы хотите, чтобы этот импорт происходил. В связи с этим я собираюсь выполнить один раз.\nНиже вы можете видеть успешное выполнение задания политики импорта.\nЕсли мы теперь вернемся на приборную панель и зайдем в карточку Applications, мы можем выбрать выпадающий список, где вы видите ниже \"Removed\", здесь вы увидите наше приложение. Выберите \"Восстановить\nЗдесь мы видим доступные нам точки восстановления; это было задание резервного копирования, которое мы выполнили на первичном кластере для нашего приложения Pac-Man.\nЯ не буду менять никаких настроек по умолчанию, так как хочу рассмотреть это более подробно на следующем занятии.\nКогда вы нажмете кнопку \"Восстановить\", появится запрос на подтверждение.\nНиже мы видим, что мы находимся в резервном кластере, и если мы проверим наши pods, мы увидим, что у нас есть наше запущенное приложение.\nЗатем мы можем перенаправить порт (в реальной жизни/производственной среде вам не понадобится этот шаг для доступа к приложению, вы будете использовать ingress)\r\n\r\n\r\n\r\nДалее мы рассмотрим мобильность и трансформацию приложений.\r\nРесурсы\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\n",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/day90",
            "title": "90. Мобильность данных и приложений",
            "description": null,
            "content": "Мобильность данных и приложений\r\n\r\nДень 90 из #90DaysOfDevOps Challenge! В этой заключительной сессии я собираюсь рассказать о мобильности наших данных и приложений. Я сосредоточусь конкретно на Kubernetes, но потребность в мобильности между платформами и между платформами - это то, что является постоянно растущей потребностью и встречается на практике.\nСценарий использования таков: \"Я хочу переместить рабочую нагрузку, приложение и данные из одного места в другое\" по разным причинам, будь то стоимость, риск или предоставление бизнесу более качественных услуг.\nНа этом занятии мы возьмем нашу рабочую нагрузку и рассмотрим перемещение рабочей нагрузки Kubernetes с одного кластера на другой, но при этом мы изменим то, как наше приложение находится в целевом месте.\nФактически, здесь используются многие характеристики, которые мы рассмотрели в статье Аварийное восстановление\r\nТребование\r\n\r\nНаш текущий кластер Kubernetes не справляется со спросом, а наши затраты стремительно растут, поэтому мы хотим переместить наш производственный кластер Kubernetes в место аварийного восстановления, расположенное в другом публичном облаке, которое обеспечит возможность расширения, но при этом будет дешевле. Мы также сможем воспользоваться некоторыми собственными облачными сервисами, доступными в целевом облаке.\nНаше текущее критически важное приложение (Pac-Man) имеет базу данных (MongoDB) и работает на медленном хранилище, мы хотели бы перейти на новый более быстрый уровень хранения.\nТекущий фронтенд Pac-Man (NodeJS) не очень хорошо масштабируется, и мы хотели бы увеличить количество доступных стручков в новом месте.\nПриступаем к ИТ\r\n\r\nУ нас есть бриф, и на самом деле мы уже импортировали наши импорты в кластер Disaster Recovery Kubernetes.\nПервое, что нам нужно сделать, это удалить операцию восстановления, которую мы выполнили в день 89 для тестирования Disaster Recovery.\nМы можем сделать это с помощью команды kubectl delete ns pacman на \"резервном\" кластере minikube.\nЧтобы начать работу, зайдите в Kasten K10 Dashboard, выберите карточку Applications. Из выпадающего списка выберите \"Удаленные\"\r\n\r\n\r\n\r\nЗатем мы получим список доступных точек восстановления. Мы выберем ту, которая доступна, так как она содержит важные данные. (В этом примере у нас только одна точка восстановления).\r\n\r\n\r\n\r\nКогда мы работали над процессом аварийного восстановления, мы оставили все по умолчанию. Однако эти дополнительные опции восстановления существуют, если у вас есть процесс Disaster Recovery, который требует преобразования вашего приложения. В данном случае нам требуется изменить хранилище и количество реплик.\r\n\r\n\r\nВыберите опцию \"Применить преобразования к восстановленным ресурсам\".\nТак получилось, что два встроенных примера преобразования, которые мы хотим выполнить, соответствуют нашим требованиям.\nПервое требование заключается в том, что на нашем основном кластере мы использовали класс хранения под названием csi-hostpath-sc, а в нашем новом кластере мы хотим использовать standard, поэтому мы можем сделать это изменение здесь.\nВыглядит хорошо, нажимаем кнопку create transform внизу.\nСледующее требование заключается в том, что мы хотим масштабировать развертывание нашего фронтенда Pac-Man до \"5\"\r\n\r\n\r\n\r\nЕсли вы следите за развитием событий, вы должны увидеть оба наших преобразования, как показано ниже.\nТеперь вы можете видеть на изображении ниже, что мы собираемся восстановить все артефакты, перечисленные ниже, если бы мы захотели, мы могли бы также детализировать то, что мы хотим восстановить. Нажмите кнопку \"Восстановить\"\r\n\r\n\r\n\r\nСнова нам будет предложено подтвердить действия.\nИ последнее, что мы покажем, если мы вернемся в терминал и посмотрим на наш кластер, вы увидите, что у нас теперь 5 стручков для стручков pacman и наш класс хранения теперь установлен на стандартный, а не на csi-hostpath-sc\nСуществует множество различных вариантов, которые могут быть достигнуты с помощью трансформации. Это может охватывать не только миграцию, но и аварийное восстановление, скрипты типа тестирования и разработки и т.д.\nAPI и автоматизация\nЯ не говорил о возможности использовать API и автоматизировать некоторые из этих задач, но эти опции присутствуют, и во всем пользовательском интерфейсе есть хлебные крошки, которые предоставляют наборы команд для использования API для задач автоматизации.\nВажно отметить, что при развертывании Kasten K10 развертывается внутри кластера Kubernetes и затем может быть вызван через API Kubernetes.\nНа этом мы завершаем раздел о хранении и защите данных.\nРесурсы\r\n\r\nKubernetes Backup and Restore made easy!\r\nKubernetes Backups, Upgrades, Migrations - with Velero\r\n7 Database Paradigms\r\nDisaster Recovery vs. Backup: What's the difference?\r\nVeeam Portability & Cloud Mobility\r\nЗакрытие\r\n\r\nЗаканчивая эту задачу, я хочу продолжить просить об обратной связи, чтобы убедиться, что информация всегда актуальна.\nЯ также ценю, что есть много тем, которые я не смог охватить или не смог глубже погрузиться в тему DevOps.\nЭто означает, что мы всегда можем предпринять еще одну попытку в следующем году и найти еще 90 дней контента и прохождений для работы.\nЧто дальше?\nВо-первых, немного отдохнем от писанины, я начал этот вызов 1 января 2022 года и закончил 31 марта 2022 года в 19:50 BST! Это был тяжелый труд. Но, как я говорю и говорил уже давно, если этот контент поможет одному человеку, то всегда стоит учиться публично!\r\nУ меня есть несколько идей, куда двигаться дальше, и я надеюсь, что у него будет жизнь за пределами репозитория GitHub, и мы сможем рассмотреть возможность создания электронной книги и, возможно, даже физической книги.\nЯ также знаю, что нам нужно пересмотреть каждый пост и убедиться, что все грамматически правильно, прежде чем делать что-то подобное. Если кто-то знает о том, как использовать формат markdown для печати или создания электронной книги, я буду очень признателен за ответ.\nКак всегда, продолжайте обсуждать вопросы и PR.\nСпасибо!",
            "tags": [
                "devops"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/apm-server/README",
            "content": "APM Server extension\n\nThe APM Server receives data from APM agents and transforms them into Elasticsearch documents that can be visualised in\nKibana.\n\nUsage\n\nTo include APM Server in the stack, run Docker Compose from the root of the repository with an additional command line\nargument referencing the apm-server-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/apm-server/apm-server-compose.yml up\n\nMeanwhile, you can navigate to the APM application in Kibana and follow the setup instructions to get started.\n\nConnecting an agent to APM Server\n\nThe most basic configuration to send traces to APM server is to specify the SERVICE_NAME and SERVICE_URL. Here is an\nexample Python Flask configuration:\n\nimport elasticapm\nfrom elasticapm.contrib.flask import ElasticAPM\n\nfrom flask import Flask\n\napp = Flask(name)\napp.config['ELASTIC_APM'] = {\nSet required service name. Allowed characters:\na-z, A-Z, 0-9, -, _, and space\n    'SERVICE_NAME': 'PYTHON_FLASK_TEST_APP',\n\nSet custom APM Server URL (default: http://localhost:8200)\n    'SERVER_URL': 'http://apm-server:8200',\n\n    'DEBUG': True,\n}\n\nConfiguration settings for each supported language are available in the APM documentation: APM Agents.\n\nChecking connectivity and importing default APM dashboards\n\nOn the Kibana home page, click Add APM under the Observability panel.\nClick Check APM Server status to confirm the server is up and running.\nClick Check agent status to verify your agent has registered properly.\nClick Load Kibana objects to create an index pattern for APM.\nClick Launch APM to be taken to the APM dashboard.\n\nSee also\n\nRunning APM Server on Docker\n\n[apm-agents]: https://www.elastic.co/guide/en/apm/guide/current/components.html\n[apm-docker]: https://www.elastic.co/guide/en/apm/guide/current/running-on-docker.html\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/curator/README",
            "content": "Curator\n\nElasticsearch Curator helps you curate or manage your indices.\n\nUsage\n\nIf you want to include the Curator extension, run Docker Compose from the root of the repository with an additional\ncommand line argument referencing the curator-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/curator/curator-compose.yml up\n\nThis sample setup demonstrates how to run curator every minute using cron.\n\nAll configuration files are available in the config/ directory.\n\nDocumentation\n\nCurator Reference\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/enterprise-search/README",
            "content": "Enterprise Search extension\n\nElastic Enterprise Search is a suite of products for search applications backed by the Elastic Stack.\n\nRequirements\n\n2 GB of free RAM, on top of the resources required by the other stack components and extensions.\n\nEnterprise Search exposes the TCP port 3002 for its Web UI and API.\n\nUsage\n\nGenerate an encryption key\n\nEnterprise Search requires one or more encryption keys to be configured before the\ninitial startup. Failing to do so prevents the server from starting.\n\nEncryption keys can contain any series of characters. Elastic recommends using 256-bit keys for optimal security.\n\nThose encryption keys must be added manually to the config/enterprise-search.yml file. By\ndefault, the list of encryption keys is empty and must be populated using one of the following formats:\n\nsecret_management.encryption_keys:\n  my_first_encryption_key\n  my_second_encryption_key\n  ...\n\nsecret_management.encryption_keys: [my_first_encryption_key, my_second_encryption_key, ...]\n:information_source: To generate a strong encryption key, for example using the AES-256 cipher, you can use the\nOpenSSL utility or any other online/offline tool of your choice:\n> $ openssl enc -aes-256 -P\nenter aes-256-cbc encryption password:\nVerifying - enter aes-256-cbc encryption password:\n...\nkey=\n\nEnable Elasticsearch's API key service\n\nEnterprise Search requires Elasticsearch's built-in API key service to be enabled in order to start.\nUnless Elasticsearch is configured to enable TLS on the HTTP interface (disabled by default), this service is disabled\nby default.\n\nTo enable it, modify the Elasticsearch configuration file in elasticsearch/config/elasticsearch.yml and\nadd the following setting:\n\nxpack.security.authc.api_key.enabled: true\n\nConfigure the Enterprise Search host in Kibana\n\nKibana acts as the management interface to Enterprise Search.\n\nTo enable the management experience for Enterprise Search, modify the Kibana configuration file in\nkibana/config/kibana.yml and add the following setting:\n\nenterpriseSearch.host: http://enterprise-search:3002\n\nStart the server\n\nTo include Enterprise Search in the stack, run Docker Compose from the root of the repository with an additional command\nline argument referencing the enterprise-search-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml up\n\nAllow a few minutes for the stack to start, then open your web browser at the address  to see the\nEnterprise Search home page.\n\nEnterprise Search is configured on first boot with the following default credentials:\n\nuser: enterprise_search\npassword: changeme\n\nSecurity\n\nThe Enterprise Search password is defined inside the Compose file via the ENT_SEARCH_DEFAULT_PASSWORD environment\nvariable. We highly recommend choosing a more secure password than the default one for security reasons.\n\nTo do so, change the value ENT_SEARCH_DEFAULT_PASSWORD environment variable inside the Compose file **before the first\nboot**:\n\nenterprise-search:\n\n  environment:\n    ENT_SEARCH_DEFAULT_PASSWORD: {{some strong password}}\n:warning: The default Enterprise Search password can only be set during the initial boot. Once the password is\npersisted in Elasticsearch, it can only be changed via the Elasticsearch API.\n\nFor more information, please refer to User Management and Security.\n\nConfiguring Enterprise Search\n\nThe Enterprise Search configuration is stored in config/enterprise-search.yml. You can\nmodify this file using the Default Enterprise Search configuration as a reference.\n\nYou can also specify the options you want to override by setting environment variables inside the Compose file:\n\nenterprise-search:\n\n  environment:\n    ent_search.auth.source: standard\n    worker.threads: '6'\n\nAny change to the Enterprise Search configuration requires a restart of the Enterprise Search container:\n\n$ docker-compose -f docker-compose.yml -f extensions/enterprise-search/enterprise-search-compose.yml restart enterprise-search\n\nPlease refer to the following documentation page for more details about how to configure Enterprise Search inside a\nDocker container: Running Enterprise Search Using Docker.\n\nSee also\n\nEnterprise Search documentation\n\n[config-enterprisesearch]: ./config/enterprise-search.yml\n\n[enterprisesearch-encryption]: https://www.elastic.co/guide/en/enterprise-search/current/encryption-keys.html\n[enterprisesearch-security]: https://www.elastic.co/guide/en/workplace-search/current/workplace-search-security.html\n[enterprisesearch-config]: https://www.elastic.co/guide/en/enterprise-search/current/configuration.html\n[enterprisesearch-docker]: https://www.elastic.co/guide/en/enterprise-search/current/docker.html\n[enterprisesearch-docs]: https://www.elastic.co/guide/en/enterprise-search/current/index.html\n[enterprisesearch-ui]: https://www.elastic.co/guide/en/enterprise-search/current/user-interfaces.html\n\n[es-security]: https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#api-key-service-settings\n[config-es]: ../../elasticsearch/config/elasticsearch.yml\n[config-kbn]: ../../kibana/config/kibana.yml\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/filebeat/README",
            "content": "Filebeat\n\nFilebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers,\nFilebeat monitors the log files or locations that you specify, collects log events, and forwards them either to\nElasticsearch or Logstash for indexing.\n\nUsage\n\nTo include Filebeat in the stack, run Docker Compose from the root of the repository with an additional command line\nargument referencing the filebeat-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up\n\nConfiguring Filebeat\n\nThe Filebeat configuration is stored in config/filebeat.yml. You can modify this file with\nthe help of the Configuration reference.\n\nAny change to the Filebeat configuration requires a restart of the Filebeat container:\n\n$ docker-compose -f docker-compose.yml -f extensions/filebeat/filebeat-compose.yml restart filebeat\n\nPlease refer to the following documentation page for more details about how to configure Filebeat inside a Docker\ncontainer: Run Filebeat on Docker.\n\nSee also\n\nFilebeat documentation\n\n[filebeat-config]: https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-reference-yml.html\n[filebeat-docker]: https://www.elastic.co/guide/en/beats/filebeat/current/running-on-docker.html\n[filebeat-doc]: https://www.elastic.co/guide/en/beats/filebeat/current/index.html\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/logspout/README",
            "content": "Logspout extension\n\nLogspout collects all Docker logs using the Docker logs API, and forwards them to Logstash without any additional\nconfiguration.\n\nUsage\n\nIf you want to include the Logspout extension, run Docker Compose from the root of the repository with an additional\ncommand line argument referencing the logspout-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/logspout/logspout-compose.yml up\n\nIn your Logstash pipeline configuration, enable the udp input and set the input codec to json:\n\ninput {\n  udp {\n    port  => 5000\n    codec => json\n  }\n}\n\nDocumentation\n\n\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/metricbeat/README",
            "content": "Metricbeat\n\nMetricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the\noperating system and from services running on the server. Metricbeat takes the metrics and statistics that it collects\nand ships them to the output that you specify, such as Elasticsearch or Logstash.\n\nUsage\n\nTo include Metricbeat in the stack, run Docker Compose from the root of the repository with an additional command line\nargument referencing the metricbeat-compose.yml file:\n\n$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml up\n\nConfiguring Metricbeat\n\nThe Metricbeat configuration is stored in config/metricbeat.yml. You can modify this file\nwith the help of the Configuration reference.\n\nAny change to the Metricbeat configuration requires a restart of the Metricbeat container:\n\n$ docker-compose -f docker-compose.yml -f extensions/metricbeat/metricbeat-compose.yml restart metricbeat\n\nPlease refer to the following documentation page for more details about how to configure Metricbeat inside a\nDocker container: Run Metricbeat on Docker.\n\nSee also\n\nMetricbeat documentation\n\n[metricbeat-config]: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-reference-yml.html\n[metricbeat-docker]: https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-docker.html\n[metricbeat-doc]: https://www.elastic.co/guide/en/beats/metricbeat/current/index.html\n",
            "tags": []
        },
        {
            "uri": "/tracks/90daysofdevops/Monitoring/Elastic Stack/extensions/README",
            "content": "Extensions\n\nThird-party extensions that enable extra integrations with the Elastic stack.\n",
            "tags": []
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/_index",
            "title": "Data Structures",
            "description": null,
            "content": "\nTree\n\nclass Node:\n    def init(self, value):\n        self.value = value\n        self.children = {}\n\n    def insert(self, s, idx):\nidx: index of the current character in s\n        if idx != len(s):\n            self.children.setdefault(s[idx], Node(s[idx]))\n            self.children.get(s[idx]).insert(s, idx + 1)\n\nFenwick Tree\n\nclass Fenwick: #also known as Binary Indexed Tree (BIT)\n    def init(self, n):\n        self.n = n\n        self.bit = [0] * (n+1)\n\n    def add(self, idx, val):\n        while idx  0:\n            ret += self.bit[idx]\n            idx -= idx & -idx\n        return ret\n\n    def range_sum(self, l, r):\nReturn the sum of the elements from l (inclusive) to r (exclusive)\n        return self.prefix_sum(r - 1) - self.prefix_sum(l - 1)\n\n    def prefix_sum(self, x):\nreturn sum upto and including element x\n        z = x\n        res = 0\n        while z >= 0:\n            res += self.bit[z]\nStrip trailing zeros from z, and then take away one\n            z = (z & (z + 1)) - 1\n        return res\n\n\nA Visual Introduction to Fenwick Tree | medium\nFenwick Tree\nДерево Фенвика | algorithmica\nДерево Фенвика | habr\n\nResources\n\ndata structures\nCompetitive Programming Library\nAlgorithms for Competitive Programming\n",
            "tags": [
                "Data Structures"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/fenwick-tree",
            "title": "Дерево Фенвика",
            "description": "Дерево Фенвика",
            "content": "\nДерево Фенвика, также известное как двоичное индексированное дерево (Binary Indexed Tree, BIT).\n\nТерминология:\n\n    a - исходный массив\n    tree - массив дерева, полученный в результате преобразований массива 'a'\n    i - индекс массива\n    k - индекс массива\n    F(i) - еще не определенный индекс, полученный в реузльтате преобразования индекса i. F(i)  Как формируется массив дерева | youtube\n\nПодсчет префиксных сумм:\n\n    def prefix_sum(k):\n        result = 0\n        i = k\n        while i >= 0:\n            result += tree[i]\n            i = F(i) - 1\n\nРесурсы\n\nhttps://www.youtube.com/watch?v=BzFN9YwR-NM\nДерево Фенвика | wiki",
            "tags": [
                "Data Structures"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/prefix-sum",
            "title": "Префиксные суммы",
            "description": "Префиксные суммы",
            "content": "\nПодсчет префиксных сумм\n\nСумма в текущей ячейке равна сумме в предыдущей + значение текущей.\n\n   0  1  2  3  4  5  6  7       0  1   2   3   4   5   6   7\na[ 5, 7, 9, 3, 8, 2, 4, 6] => b[5, 12, 21, 24, 32, 34, 38, 44]\n\n0    5              [5,]\n1    5 + 7 = 12     [5,12,]\n2    9 + 12 = 21    [5,12,21,]\n3    3 + 21 = 24    [5,12,21,24,]\n...\n\nЧтобы посчитать сумму на любом отрезок [L, R], достаточно вычесть сумму с предыдущего индекса от `L'\n\nL = 3, R = 5: [3, 8, 2] = 13\n\nАлгоритм:\n\nНаходим сумму (значение) в правой части отрезка: b[R] = 34\nВычитаем из значения b[R] значение b[L-1]\n\nВ новом массиве:\n\nb[L - 1] = b[2] = 21\nb[R] = 34\n\n34 - 21 = 13\n\nРесурсы\n\nhttps://www.youtube.com/watch?v=BzFN9YwR-NM&t=797s",
            "tags": [
                "Data Structures"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/segment-tree",
            "title": "Дерево отрезков",
            "description": "Дерево отрезков",
            "content": "\nДля изучения темы \"дерево отрезков\" необходимо знать следующие понятия:\n\nмассивы\nциклы\nусловные операторы\nбитовые операции\n\nДерево отрезков (Segment Tree) - это динамическая структура данных, используемая для выполнения операций над интервалами и их обновления. Оно поддерживает две операции: обновление элементов (update) в заданном диапазоне и запрос (query) на сумму элементов в заданном диапазоне.\n\nВыполним следующую задачу: у нас есть массив, и мы хотим находить сумму элементов в определенном диапазоне.\n\nДля этой задачи мы можем использовать дерево отрезков. Оно построено как бинарное дерево, где каждый узел представляет интервал, а значение узла - это сумма элементов в этом интервале.\n\nОсновы:\n\nОпределение элемента суммы в дереве отрезков:\n\nЭлемент суммы в дереве отрезков является суммой всех элементов в диапазоне, который он представляет.\n\nКонструирование дерева отрезков:\n\nДерево отрезков может быть построено на базе массива чисел. Каждый узел дерева представляет диапазон элементов в массиве и хранит сумму элементов в этом диапазоне.\n\nРеализация операций:\n\nРеализация различных операций в дереве отрезков по сути зависит от его структуры. Однако, существует несколько операций, которые часто используются в различных задачах:\n\nОбновление** значения в массиве: Эта операция позволяет изменять значение элемента в массиве. Обычно она реализуется с помощью рекурсивного прохода по дереву.\n\nЗапрос на значение**: Эта операция позволяет запрашивать значение элемента в массиве. Обычно она также реализуется с помощью рекурсивного прохода по дереву.\n\nЗапрос на сумму**: Эта операция позволяет запрашивать сумму значений в массиве на заданном интервале. Она обычно реализуется с помощью рекурсивного прохода по дереву и подсчета суммы\n\nПостроение дерева отрезков\n\nТак как дерево бинарное, у каждой вершины будет до двух потомков.\n\nГрафически это выглядит следующим образом (для массива из 8 элементов):\n\nsegment-tree-structure.ru.png\n\nВ самой верхней вершине зафиксирован отрезок от начала массива до последнго элемента.\n\nСлева - лева половина от родителя ([0 1 2 3]). Справа - правая половина ()[4 5 6 7]). И так далее до последнего узла с отрезком из одного элемента.\n\nВозьмем массив a = [1, 3, -2, 8, -7]. На его базе построим дерево отрезкови запишем суммы этих отрезков в каждый узел.\n\nСтруктура такого дерево выглядит следующим образом:\n\nsegment-tree-sum.ru.png\n\n💡 Дерево содержит менее 2n вершин. 2*n-1\n\nЧисло вершин в худшем случае оценивается суммой $n + \\frac{n}{2} + \\frac{n}{4} + \\frac{n}{8} + \\ldots + 1 > array = [1, 3, -2, 8, -7]\n> build_tree(array)\n\n  4 [0, 0, 0, 0, 1, 1, 3, -2, 8, -7]\n  3 [0, 0, 0, 1, 1, 1, 3, -2, 8, -7]\n  2 [0, 0, 2, 1, 1, 1, 3, -2, 8, -7]\n  1 [0, 3, 2, 1, 1, 1, 3, -2, 8, -7]\n  0 [3, 3, 2, 1, 1, 1, 3, -2, 8, -7]\n\nПодсчет суммы на отрезке:\n\nФункция получает индексы исходного массива.\n\nПри создании дерева из изходного массива мы помещали каждый отдельный элемент на новый индекс [n + i].\n\n💡 Поэтому когда функция принимает индекс, сначала мы найдет самый нижний элемент в дереве. Он расположен в новом массиве по индексу [длинаисходногомассива + index]\n\nподсчет суммы на отрезке\ndef query_tree(l, r):\n  global tree, n\n\n  sum = 0\n  l += n # индекс текущего элемента\n  r += n\n  while l > a = [1, 3, -2, 8, -7]\n> n = len(a)\n> tree = build_tree(a)\n> query_tree(0, 4) # sum([1, 3, -2, 8, -7])\n3\n> query_tree(1, 3) # sum([3, -2, 8])\n9\n> query_tree(4, 4)\n-7\n\nПолучаем класс SegmentTree:\n\nФункцию суммирования или любую другую можно включить в момент генерации дерева\n\nclass SegmentTree:\n    def init(self, a):\n        self.n = len(a)\n        self.tree = [0] * 2 * self.n\n        for i in range(self.n):\n            self.tree[self.n + i] = a[i]\n        for i in range(self.n - 1, 0, -1):\n            self.tree[i] = self.tree[2i] + self.tree[2i+1]\n\n    def calculate_sum(self, l, r):\n        sum = 0\n        l += self.n\n        r += self.n\n        while l >= 1\n    while idx:\n        self.data[idx] = self._func(self.data[2 * idx], self.data[2 * idx + 1])\n        idx >>= 1\n\ndef len(self):\n    return self._len\n\ndef query(self, start, stop):\n    \"\"\"func of data[start, stop)\"\"\"\n    start += self._size\n    stop += self._size\n    if start==stop:\n        return self._default\n    res_left = res_right = self._default\n    while start >= 1\n        stop >>= 1\n\n    return self._func(res_left, res_right)\n\ndef repr(self):\n    return \"SegmentTree({0})\".format(self.data)\n\nМетод build_tree строит дерево отрезков, а query позволяет выполнять операции запроса.\n\nРесурсы\n\nhttps://ru.algorithmica.org/cs/segment-tree/",
            "tags": [
                "Data Structures"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/11-container-with-most-water.en",
            "title": "11. Container With Most Water",
            "description": "LeetCode 11. Container With Most Water",
            "content": "\nLeetCode problem\n\nYou are given an integer array height of length n. There are n vertical lines drawn such that the two endpoints of the ith line are (i, 0) and (i, height[i]).\n\nFind two lines that together with the x-axis form a container, such that the container contains the most water.\n\nReturn the maximum amount of water a container can store.\n\nNotice that you may not slant the container.\n\nLeetCode 11\n\nExample 1:\n\n    Input: height = [1,8,6,2,5,4,8,3,7]\n    Output: 49\n    Explanation: The above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. In this case, the max area of water (blue section) the container can contain is 49.\n\nExample 2:\n\n    Input: height = [1,1]\n    Output: 1\n\nFirst accepted\n\nIdea:\n\nTwo Pointers\n\nMax water area is limited by the height of the shorter line\nGet most left pointer and most right\nLoop until left==right\n\nclass Solution:\n    def maxArea(self, height: List[int]) -> int:\n        p1 = 0 # indexes\n        p2 = len(height) - 1\n\n        max_water = 0\n        while p1 < p2:\n            area = (p2 - p1) * min(height[p1], height[p2])\n            max_water = max(area, max_water)\n\n            if height[p1] <= height[p2]:\n                p1 += 1\n            else:\n                p2 -= 1\n        return max_water\n",
            "tags": [
                "Array",
                "Two Pointers",
                "Greedy"
            ]
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/15-3sum",
            "title": "15. 3Sum",
            "description": "LeetCode 15. 3Sum",
            "content": "\nLeetCode problem\n\nGiven an integer array nums, return all the triplets [nums[i], nums[j], nums[k]] such that i != j, i != k, and j != k, and nums[i] + nums[j] + nums[k] == 0.\n\nNotice that the solution set must not contain duplicate triplets.\n\nExample 1:\n\n    Input: nums = [-1,0,1,2,-1,-4]\n    Output: [[-1,-1,2],[-1,0,1]]\n    Explanation:\n    nums[0] + nums[1] + nums[2] = (-1) + 0 + 1 = 0.\n    nums[1] + nums[2] + nums[4] = 0 + 1 + (-1) = 0.\n    nums[0] + nums[3] + nums[4] = (-1) + 2 + (-1) = 0.\n    The distinct triplets are [-1,0,1] and [-1,-1,2].\n    Notice that the order of the output and the order of the triplets does not matter.\n\nExample 2:\n\n    Input: nums = [0,1,1]\n    Output: []\n    Explanation: The only possible triplet does not sum up to 0.\n\nFirst accepted\n\nIdea:\n\nclass Solution:\n    def threeSum(self, nums: List[int]) -> List[List[int]]:\n        nums.sort()\n\n        x = 0 # index\n        ll = len(nums)\n\n        res = []\n        while x  y and nums[z] == nums[z-1]:\n                            z -= 1\n                        y += 1\n                        z -= 1\n                    elif s > 0:\n                        z -= 1\n                    else:\n                        y += 1\n            x += 1\n\n        return res\n",
            "tags": [
                "Array",
                "Two Pointers",
                "Sorting"
            ]
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/28-find-the-index-of-the-first-occurrence-in-a-string.en",
            "title": "28. Find the Index of the First Occurrence in a String",
            "description": "LeetCode 28. Find the Index of the First Occurrence in a String",
            "content": "\nLeetCode problem\n\nGiven two strings needle and haystack, return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.\n\nExample 1:\n\n  Input: haystack = \"sadbutsad\", needle = \"sad\"\n  Output: 0\n  Explanation: \"sad\" occurs at index 0 and 6.\n  The first occurrence is at index 0, so we return 0.\n\nExample 2:\n\n  Input: haystack = \"leetcode\", needle = \"leeto\"\n  Output: -1\n  Explanation: \"leeto\" did not occur in \"leetcode\", so we return -1.\n\nCode\n\nclass Solution:\n    def strStr(self, haystack: str, needle: str) -> int:\n      return haystack.find(needle)\n\nclass Solution:\n    def strStr(self, haystack: str, needle: str) -> int:\n        start = 0\n        end = len(needle)\n\n        while end <= len(haystack):\n            if haystack[start:end] == needle:\n                return start\n            start += 1\n            end += 1\n\n        return -1\n",
            "tags": [
                "String",
                "Two pointers",
                "String Matching"
            ]
        },
        {
            "uri": "/tracks/archive/",
            "title": "Docs",
            "content": "\nDocs EN | RU\nPosts EN | RU\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/_index",
            "title": "AWS Certified Developer (DVA-C01 -> DVA-C02)",
            "description": "My plan for preparing for and taking the AWS Certified Developer exam",
            "content": "\nTL;DR\n\nPassed exam in one month.\nCreated an app with questions and progress that helped me a lot\nLQVKGDT16BF1Q6CX\n\nNote\nThe AWS Certified Developer - Associate exam is changing February 28, 2023. The last date to take the current exam is February 27, 2023.\nTo keep the docs up to date I will add new and latest information.\n\nDVA-C01 vs DVA-C02\n\nnew domain: Domain 3: Deployment\n  focus will be on testing and deploying your code into different environments including development, test, and production environments. You’ll need to know how CloudFormation, the AWS Cloud Development Kit (CDK), and AWS SAM are used to deploy applications.\nDomains 4 and 5 (“Refactoring” along with “Monitoring and Troubleshooting”) from the DVA-C01 exam guide have been consolidated into Domain 4 (“Troubleshooting and Optimization”) in the DVA-C02 exam guide\nPlus 2% questions in Development with AWS Services domain\n\nTest questions for DVA-C02 - here and here\n\nCriteria\n\nIn order to pass the exam, you must score more than 720/1000 (unspecified) points. Criterion will be a minimum threshold of 75/100%, unless conditions change in preparation.\n\nStudy Plan\n\nFind out what the exam requirements are\nHave a list of topics that will be on the exam\nPractice each service for comprehension\nRead extra theory that will not be covered during practice\nGo through the test general questions\nRepeat 3-5 repeat until the result of failed block greater than 80 points\n\nEntrypoint:\n\nAWS Certified Developer Exam Information\n\nPrepare\n\nThe AWS website has:\nExam Preparation Guide DVA-C01\nFrom 27 Feb 2023 Exam Preparation Guide DVA-C02\n\n\nTo pass the exam, you need to know certain services from the 4 domains: Development with AWS Services, Security, Deployment, Refactoring, Monitoring and Troubleshooting\n\nList of services on the exam\nVersion 2.1 DVA-C01\nVersion 1.0 DVA-C02\n\n\nAnalytics:\n\nAmazon Athena (new in DVA-C02)\nAmazon OpenSearch Service (Amazon Elasticsearch Service)\nAmazon Kinesis\n\nApplication Integration:\n\nAWS AppSync (new in DVA-C02)\nAmazon EventBridge (Amazon CloudWatch Events)\nAmazon Simple Notification Service (Amazon SNS)\nAmazon Simple Queue Service (Amazon SQS)\nAWS Step Functions\n\nCompute:\n\nAmazon EC2\nAWS Elastic Beanstalk\nAWS Lambda\nAWS Serverless Application Model (AWS SAM) (new in DVA-C02)\n\nContainers:\n\nAWS Copilot (new in DVA-C02)\nAmazon Elastic Container Registry (Amazon ECR)\nAmazon Elastic Container Service (Amazon ECS)\nAmazon Elastic Kubernetes Services (Amazon EKS)\n\nDatabase:\n\nAmazon Aurora (new in DVA-C02)\nAmazon DynamoDB\nAmazon ElastiCache\nAmazon MemoryDB for Redis (new in DVA-C02)\nAmazon RDS\n\nDeveloper Tools:\nAWS Amplify (new in DVA-C02)\nAWS Cloud9 (new in DVA-C02)\nAWS CloudShell (new in DVA-C02)\nAWS CodeArtifact\nAWS CodeBuild\nAWS CodeCommit\nAWS CodeDeploy\nAmazon CodeGuru\nAWS CodePipeline\nAWS CodeStar\nAWS Fault Injection Simulator\nAWS X-Ray\n\nManagement and Governance:\n\nAWS AppConfig (new in DVA-C02)\nAWS Cloud Development Kit (AWS CDK) (new in DVA-C02)\nAWS CloudFormation\nAWS CloudTrail (new in DVA-C02)\nAmazon CloudWatch\nAmazon CloudWatch Logs (new in DVA-C02)\nAWS Command Line Interface (AWS CLI) (new in DVA-C02)\nAWS Systems Manager (new in DVA-C02)\n\nNetworking and Content Delivery:\n\nAmazon API Gateway\nAmazon CloudFront\nElastic Load Balancing\nAmazon Route 53 (new in DVA-C02)\nAmazon VPC (new in DVA-C02)\n\nSecurity, Identity, and Compliance:\n\nAWS Certificate Manager (ACM) (new in DVA-C02)\nAWS Certificate Manager Private Certificate Authority (new in DVA-C02)\nAmazon Cognito\nAWS Identity and Access Management (IAM)\nAWS Key Management Service (AWS KMS)\nAWS Secrets Manager (new in DVA-C02)\nAWS Security Token Service (AWS STS) (new in DVA-C02)\nAWS WAF (new in DVA-C02)\n\nStorage:\n\nAmazon Elastic Block Store (Amazon EBS) (new in DVA-C02)\nAmazon Elastic File System (Amazon EFS) (new in DVA-C02)\nAmazon S3\nAmazon S3 Glacier (new in DVA-C02)\n\nTraining plan\n\nOpened a training plan for any tutorial to understand where to start learning. Have chosen cloudacademy service (but for example FreeCodeCamp has a free course with content).\n\nAnother option is to use free AWS Workshops\n\nAWS Developer - Associate (DVA-C01) Certification Preparation\n\nDon't see coverage of the following services, so I add them to the block when related topics are covered:\n\nAnalytics:\n\nAmazon Elasticsearch Service (Amazon ES) -> OpenSearch Service\n\nDeveloper Tools:\n\nAWS CodeArtifact\nAWS Fault Injection Simulator\n\nMy roadmap\n\nThe following is my roadmap for the study. There may be adjustments.\n\nAWS Identity and Access Management (IAM)\nAmazon EC2\nAWS Elastic Beanstalk\nAWS Lambda\nAmazon S3\nAmazon DynamoDB\nAmazon ElastiCache\nAmazon RDS\nAmazon API Gateway\nAmazon CloudFront\nElastic Load Balancing (ELB)\nAmazon Kinesis\nAmazon OpenSearch Service (Amazon Elasticsearch Service)\nAmazon CloudWatch\nAWS CloudFormation\nAWS CodeCommit\nAWS CodeDeploy\nAWS CodeBuild\nAWS CodePipeline\nAmazon CodeGuru\nAWS CodeStar\nAWS CodeArtifact\nAWS X-Ray\nAWS Fault Injection Simulator\nAmazon Elastic Container Registry (Amazon ECR)\nAmazon Elastic Container Service (Amazon ECS)\nAWS Fargate\nAmazon Elastic Kubernetes Services (Amazon EKS)\nAmazon Cognito\nRoute 53\nAWS Key Management Service (AWS KMS)\nAmazon EventBridge (Amazon CloudWatch Events)\nAmazon Simple Notification Service (Amazon SNS)\nAmazon Simple Queue Service (Amazon SQS)\nAWS Step Functions\n\nResources\n\nAWS Certified Developer\nA brief overview of the official documentation\nExam Preparation Guide\nSample Exam Questions\nhttps://github.com/itsmostafa/certified-aws-developer-associate-notes\nhttps://github.com/arnaudj/mooc-aws-certified-developer-associate-2020-notes\nFreeCodeCamp Youtube - AWS Certified Developer - Associate 2020\nHow-To Labs from AWS\nAWS Ramp-Up guides: Downloadable AWS Ramp-Up Guides offer a variety of resources to help you build your skills and knowledge of the AWS Cloud.\nCoursera's AWS Courses(Free to enroll via audit): AWS also provides various specializations in partnership with coursera\nAWS Architecture center: Provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more. This expert guidance was contributed by cloud architecture experts from AWS, including AWS Solutions Architects, Professional Services Consultants, and Partners.\nAWS Whitepapers: Expand your knowledge of the cloud with AWS technical content authored by AWS and the AWS community, including technical whitepapers, technical guides, reference material, and reference architecture diagrams.\nBack to Basics: Back to Basics' is a video series that explains, examines, and decomposes basic cloud architecture pattern best practices.\nAWS Heroes Content Library: AWS Hero authored content including blogs, videos, slide presentations, podcasts, and more.\nhttps://amazon.qwiklabs.com/catalog\nAWS Workshops: This website lists workshops created by the teams at Amazon Web Services (AWS). Workshops are hands-on events designed to teach or introduce practical skills, techniques, or concepts which you can use to solve business problems.\nhttps://wellarchitectedlabs.com/\nhttps://testseries.edugorilla.com/tests/1359/aws-certified-developer-associate\n\nCommunity posts\n\nhttps://dev.to/romankurnovskii/aws-certified-developer-associate-prepare-2np\nhttps://www.reddit.com/user/romankurnovskii/comments/x8rgig/what_is_the_topics_order_to_cover_to_get_prepared/?utm_source=share&utm_medium=web2x&context=3\nhttps://twitter.com/romankurnovskii/status/1567746601136832512\n",
            "tags": [
                "AWS"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/ec2/",
            "title": "EC2",
            "description": "Пошаговое руководство по Amazon EC2",
            "content": "\nAmazon EC2\n\nДокументация Amazon EC2 - 1\nДокументация Amazon EC2 - 2\n\nAmazon Elastic Compute Cloud (EC2) - одна из самых популярных служб AWS. EC2 позволяет запускать различные типы облачных экземпляров и оплачивать их по модели \"оплата за использование\". EC2 позволяет контролировать вычислительные ресурсы на уровне операционной системы, работая в вычислительной среде Amazon.\n\nЦены\n\nАктуальный прайс\n\nПрактика\n\nСоздание EC2 инстанса\n\nЗаходим на страницу EC2 -> Launch Instance\n\n01.png\nОбраз EC2\n\nВыбираем нужный нам образ\n\nСоздание ключей\n\nСоздадим ключ, чтобы использовать его для подключения к инстансу извне\n\n\nВводим любое имя. Остальные параметры оставляю по умолчанию\n\nПосле создания ключа начнется автоматическое скачивание. Ключ понадобится далее для подключения к EC2  с локального терминала\nСетевые настройки\n\nВ разделе Network Settings оставляю включенным Allow SSH traffic from\n\nСоздание\n\nНажимаем Launch Instance\n\nИнстанс создан и доступен для подключения\n\nПодключение к EC2 с терминала\n\nПодключимся к EC2 с локального терминала\n\nПеренесем созданный и скачанный ранее ключ mykey в папку home текущего пользователя и дадим права на файл CHMOD 400\n\ncd ~\ncd Downloads/\nmv mykey.pem $HOME\ncd ..\nchmod 400 mykey.pem\n\nДля подключения нам необходим публичный iPv4 адрес. Находим на странице с инстансом\n\n\n\nПодключаемся с помощью команды ssh\nssh -i mykey.pem ec2-user@52.24.109.78\n\nВопросы\n\nВопрос 1\n\nA company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\n\nWhich approach meets these requirements?\n\nA) Store the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI.\nB) Store the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials.\nC) Store the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances.\nD) Store the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials\n\n\nОтвет\n\nПравильный ответ: D\n\nD – AWS Secrets Manager helps to protect the credentialsneeded to access databases, applications,services, and other IT resources. The service enables users to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Users and applications retrieve secrets with a call to the Secrets Manager APIs, eliminating the need to hardcode sensitive information in plaintext. Secrets Manager offers secret rotation with built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB.\n\n\n",
            "tags": [
                "aws",
                "ec2"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/elasticbeanstalk/",
            "title": "Elastic Beanstalk",
            "description": "Пошаговое руководство по AWS Elastic Beanstalk. Разворачиваем приложение с AWS Elastic Beanstalk",
            "content": "\nAWS Elastic Beanstalk\n\nДокументация AWS Elastic Beanstalk\nДокументация AWS Elastic Beanstalk\n\nЦены\n\nДополнительная плата за AWS Elastic Beanstalk не взимается. Оплате подлежат только ресурсы AWS, необходимые для хранения и работы приложений.\n\nПрактика\n\nКонтролируемое развертывание с AWS Elastic Beanstalk\n\nСсылка на лабораторную работу\n\nВ этой лабораторной работе  развернем несколько обновлений версий приложения в среде с балансировкой нагрузки и автоматическим масштабированием.\n\nПервое обновление развертывается с помощью простого развертывания. Второе обновление развертывается с помощью blue-green развертывания, когда создается отдельная среда для запуска новой версии приложения, а DNS свитчер переключает входящий трафик на новую среду.\n\nИтоговая архитектура развертывания будет выглядеть следующим образом\n02.png\n\nЗагрузка приложения\n\nВ данном обзоре я использую код, который предоставил мне Cloudacademy, но у меня есть готовый скрипт для запуска, который вы можете загрузить в Elastic Beanstalk: скачать\n\nСоздание\n\nЗаходим на страницу Elastic Beanstalk и нажимаем Create Application\n03.png\n\nНазвание\n\nУказываем название нового приложения\n04.png\nВыбор платформы\n\nВ разделе Platform выбираем нужную платформу приложения. В нашем случае - Node.js\n05.png\n\nЗагрузка исходников\n\nВ разделе Source code origin указываем версию приложения и загружаем архив с приложением.  Пример\n\n06.png\n\nКонфигурация приложения\n\nИзменяем предустановку Configuration на Custom configuration:\n07.png\n\nНажимаем Edit в разделе Rolling updates and deployments\n\nВ конфигурации по умолчанию обновления распространяются на все экземпляры одновременно. Это приводит к простою приложения, что неприемлемо для производственных сред.\n\nМы установим Rolling и Batch size 30%\n08.png\n\nСеть\nВернувшись к основной форме приложения, нажмите Edit в конфигурации Network.\n\nНа форме Modify network настроим следующие значения, затем  Save. \n\nVPC: Выберите VPC с блоком CIDR **10.0.0.0/16. Это не будет VPC по умолчанию.\nLoad balancer settings**:\n    Load balancer subnets: Выберите подсети с блоками CIDR 10.0.100.0/24 **(us-west-2a)и **10.0.101.0/24 (us-west-2b). Это публичные подсети. Балансировщику нагрузки приложений требуется как минимум две подсети в разных зонах доступности\nInstance settings**:\n    Instance subnets: Выберите подсеть с блоком CIDR 10.0.1.0/24. Это частная подсеть.\n09.png\n\n10.png\nПодтверждение\nНажимаем Create app\n\nПроцесс создания приложения занимает от 5 минут.\n\nДалее переходим в раздел Dasboard\n11.png\n\nНа этом этап загрузки приложения в Elastic Beanstalk закончен. Далее разберем как переключать загрузку новой версии приложения клиентам.\n\nЗагрузка 2-й версии приложения\n\nЗагрузка версии 2.0\n\nНажимаем Upload and deploy и загружаем обновленный код. _Например, можно том же исходнике изменить текст для сравнения.\n12.png\n\nУказываем новую версию и настройки публикации\n13.png\n\nСравнение версий\nТеперь можем сверить обе версии, пройдя по ссылкам. В моем случае приложения выглядят следующим образом\n15.png\n14.png\n\nСмена url у приложений\nТеперь поменяем приложения местами. Чтобы пользователь, который ранее заходил по одному адресу, теперь видел 2-ю версию приложения.\n\nВ разделе Actions нажимаем Swap environment URLs и далее выбираем приложение с которым происходит смена\n16.png\n\nУдаление ресурсов Elastic Beanstalk\n\nElastic Beanstalk для развертывания приложений запускает EC2 инстансы, а также прочие сервисы. Но удалить все сервисы можно из одного окна.\n\nИдем в раздел Applications\nВыбираем приложение\nНажимаем Actions -> Terminate environment\n\n17.png\n\nРесурсы\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html\n",
            "tags": [
                "aws",
                "Elastic Beanstalk"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/iam/",
            "title": "IAM",
            "description": "Пошаговое руководство по настройке AWS Identity and Access Management (IAM)",
            "content": "\nAWS Identity and Access Management\n\nДокументация AWS IAM\nДокументация AWS IAM\n\nAWS Identity and Access Management (IAM) позволяет безопасно контролировать доступ пользователей к службам и ресурсам AWS. Эта услуга предназначена для организаций с множеством пользователей или систем, использующих такие продукты AWS, как Amazon EC2, Amazon RDS и AWS Management Console. С помощью IAM вы можете централизованно управлять пользователями, учетными данными безопасности, такими как ключи доступа, и разрешениями, контролирующими доступ пользователей к ресурсам AWS.\n\nПрактика\n\nПереходим на страницу IAM\n\nСоздание групп IAM\nНа странице User Groups нажимаем Create group\n\n01.png\n\nУказываем имя группы. Мое: DevOps\nДобавляем разрешение на просмотр EC2: AmazonEC2ReadOnlyAccess\nCreate\nAmazon EC2 Read Only Access\n\nГруппа создана\n\nIAM Group\n\nСоздание пользователей IAM\n\nНа странице Users нажимаем Create user\n\n\nВводим имя пользователя(логин)\n\nPermissions\nДобавляем пользователя в созданную группу\n\nTags\nПропускаем раздел или ставим tags. Полезно и популярно устанавливать теги ресурсам в компаниях, где много подключено ресурсов AWS\n\nЛогин/Пароль\nНа последнем этапе скачиваем .csv файл с логином, ключами и паролем. Пароль понадобится далее, чтобы войти в систему под данным пользователем.\nНа данной странице имеется ссылка для входа. Ею воспользуемся на следующем шаге\n\nВход новым пользователем\n\nПроверка прав\nУ данного пользователя есть доступ на просмотр EC2 инстансов. Проверим наличие/отсутствие доступа к S3 корзинам.\n\nПопробуем создать S3 корзину\n\n\nПосле попытки создать корзину получаем окно с указанием на отсутствие прав\n\n",
            "tags": [
                "aws",
                "iam"
            ],
            "lang": "ru"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/lambda/",
            "title": "Lambda",
            "description": "Пошаговое руководство по AWS Lambda",
            "content": "\nhttps://docs.aws.amazon.com/lambda/?id=docs_gateway\n\nhttps://aws.amazon.com/lambda/\n\nAWS Lambda – это сервис бессерверных вычислений, который запускает программный код в ответ на определенные события и отвечает за автоматическое выделение необходимых вычислительных ресурсов.\n\nAWS Lambda автоматически запускает программный код в ответ на различные события, такие как HTTP‑запросы через Amazon API Gateway, изменение объектов в корзинах Amazon Simple Storage Service (Amazon S3), обновление таблиц в Amazon DynamoDB или смена состояний в AWS Step Functions.\n\nПоддержка языков Java, Go, PowerShell, Node.js, C#, Python и Ruby\n\nЦены\n\nАктуальный прайс\n\nЦена x86\n0,0000166667 USD за каждую гигабайт-секунду\n0,20 USD за 1 млн запросов\n\nЦена Arm\n0,0000133334 USD за каждую гигабайт-секунду\n0,20 USD за 1 млн запросов\n\nПрактика\n\nВ строке поиска Консоли управления AWS введите Lambda и выбираем Lambda в разделе «Services»:\n\nDsбор сервиса Lambda\n\nhttps://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#\n\nНа странице Functions нажимаем Create a function\n\n\n\nAuthor from scratch is selected and enter the following values in the bottom form:\n\nFunction name*: *MyCustomFunc\nRuntime**: Node.js 16.X\n\nЯ выбираю этот раздел, потому что использую аккаунт cloudacademy. Данная роль дает разрешение на создание функций\n\nPermissions: **Change default execution role\n    Execution Role: Select Use an existing role\n    Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole\n\n\n\n→ Create function\n\nПишу функцию, чтобы просмотреть лог, добавлю печать в терминал. А также добавлю обработку получаемого сообщения (В следующем шаге в разделе тестирования)\n\nфункция принимает в качестве объекта event который содержит массив Records. На 1-й (0) позиции Объект Sns (название сервиса SNS Notifications).\n\nВ самом объекте будет 2 значения:\n\ncook_secs - время варки (микроволновки)\nreq_secs - время приготовления\n\nconsole.log('Loading function');\nexports.handler = function(event, context) {\n  console.log(JSON.stringify(event, null, 2));\n  const message = JSON.parse(event.Records[0].Sns.Message);\n  if (message.cook_secs < message.req_secs) {\n    if (message.pre) {\n      context.succeed(\"User ended \" + message.pre + \" preset early\");\n    }\n    else  {\n      context.succeed(\"User ended custom cook time early\");\n    }\n  }\n  context.succeed();\n};\n\n→ Deploy\n\n\n\n→ Test\n\nДанная функциональность позволяет протестировать как функция реагирует на определенные события. Попробуем добавить событие от SNS Notifications.\n\nВыберем из списка\n\n\n\nПолучаем шаблон, в котором внесем правки, подправим поле Message - то самое, которое мы будем обрабатывать в нашей функции.\n\nПоле Message- строка, поэтому наш объект надо будет обернуть в кавычки\n\nЧтобы обработчик понимал, что мы ставим кавычки внутри кавычек, необходимо поставить специальный символ \\ перед кавычкой.\n\nВ итоге обновляем одну строку и сохраняем → Create\n\n\n\nТеперь нажимаем на кнопку Test\n\nТак как cook_secs в нашем евенте был меньше, чем req_secs, то функция распечатала первое условие, а ниже в разделе Function Logs видим сообщение, которые мы распечатываем при инициализации функции Loading function\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/_index",
            "title": "Диссертация",
            "description": "Подготовка диссертации",
            "content": "\nПлан\n\nПубликационный профиль\n\nСледует вовремя обновлять идентификаторы автора (ORCID, ResearcherID, Scopus AuthorId)\nЗарегистрироваться как автор:\nScience Index\nScopus - Профиль создается автоматически при первой публикации в изданиях, индексируемых Scopus\nhttps://scienceadmin.rudn.ru/ru/auth/ - (с корпоративной почты @rudn.ru)\n\nРесурсы:\nПубликационные профили автора. Ресурсы РУДН (Бит.Наука, Science Admin, Репозиторий РУДН)\n\nОформление ссылок / источников литературы\n\nПО, редназначенное для хранения и систематизации библиографических данных, полных текстов документов, а также оформления ссылок в тексте и списков литературы.\n\nАвтоматический подбор/выбор стилей цитирования из базы:\nMendeley - Бесплатно - идеально совместим с базами Scopus, ScienceDirect\nZotero - Бесплатно - раcпознает публикации с eLibrary\n\nЗадачи:\n\n• создание личной электронной библиотеки публикаций\n• хранение и систематизация библиографической информации и полных текстов публикаций\n• возможность настраивать нужный стиль цитирования (библиографических ссылок)\n• синхронизация работы со всех устройств\n• возможность интегрировать библиографические ссылки в текст исследовательской работы в Word\n\nОформление публикации. Библиографические менеджеры Zotero, Mendeley\nВарианты расположения\n\nАлфавитное.** Как правило, сначала приводится перечень документов на кириллице, затем на латинице, далее – на других иностранных языках (при наличии)\n\nСистематическое.** Источники подбираются по типам документа (сначала – официальные документы, нормативно-правовые акты, затем – учебная и научная литература (монографии, статьи), потом – интернет-ресурсы\n\nХронологическое:** применяют, как правило, в исследованиях историографического плана, посвященных развитию науки, проблемы и т.д. В пределах каждого года записи дают в алфавитном порядке\n\nКомбинированное.** например, внутри систематического списка источники располагаются в алфавитном порядке\n\nОфициальные документы ставятся в начале списка и располагаются по значимости\nБиблиографическая ссылка оформляется в соответствии с ГОСТ Р 7.0.5-2008 и ГОСТ Р 7.0.108-2022\n\nЗарубежные стили цитирования\n\nТребования к оформлению статей в зарубежных изданиях, как правило, основаны\nна руководствах (Manuals of Styles) различных профессиональных сообществ.\n\nНаиболее распространены следующие стили:\n\nChicago - используется для социально-научных публикаций и в большинстве исторических журналов\nVancouver - популярен в медицине и физических науках\nAPA (American Psychological Association) Style - используется для оформления публикаций по психологии, образованию, социальным наукам\nMLA (Modern Language Association) Style - используется при написании работ в области гуманитарных наук\nInstitute of Electrical & Electronics Engineers Style (IEEE) Style - широко используется во всех областях техники, компьютерных наук и других технологических областях\n\nБиблиографические ссылки служат для выделения в тексте цитат\n\nПри цитировании наибольшего внимания должна заслуживать современная\nлитература и первоисточники.\n\nВозможны два способа цитирования:\n\nпрямое — в этом случае в кавычках дословно повторяется текст из\nсоответствующего источника (в ссылке на источник через запятую необходимо\nуказать страницу, на которой находится цитируемый текст);\nкосвенное — когда одна или несколько мыслей из источника излагаются автором\nсвоии словами, но близко к оригинальному тексту.\n\n\nВсе источники транслитерируются с помощью сайта http://translit-online.ru/.\n\nИсточники\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/articles-notes",
            "title": "Написание статей",
            "description": "Заметки по написанию статей",
            "content": "\nПроверить журнал в диссернет\n\nВ аннотацию прописывать предложения, которые могут быть запрошены как поисковый запрос в поисковиках. Анотации индексируются поисковиками яндекс, гугл.\n\nhttps://www.semanticscholar.org/\nhttps://www.researchgate.net/\n\n\nПосле выхода в сборнике:\nДобавить на сайт pdf из журнала\nДобавить в ResearchGate\n\n\nIMF Report: September 16, 2022: West Bank and Gaza: Report to the AD HOC Liaison Committee\n\nКонференции\n\nhttps://sciencen.org/konferencii/grafik-konferencij/\nhttps://na-konferencii.ru/\nhttps://konferencii.ru/\nhttps://www.kon-ferenc.ru/econom.html\nhttps://www.hse.ru/science/HSEconf",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/_index",
            "title": "Кандидатский минимум Мировая экономика 08.00.14",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "\nВопросы к кандидатскому минимуму:\nРУДН\nВШЭ\nМГИМО\n\n\n  Перечень вопросов, выносимых на кандидатский минимум по специальности 08.00.14 Мировая экономика\n\nРаздел 1. Экономическая теория.\n\nМеркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле.\nАдам Смит: теория абсолютного преимущества в торговле.\nДавид Рикардо: теория сравнительного преимущества в торговле.\nТеорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары.\n\nРаздел 2. Международная торговля.\n\nПонятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России.\nОсобенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры.\nСущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием.\nМесто и роль России в международной торговле. Конкурентные преимущества России.\nСтоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры.\nМеждународный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами.\n\nРаздел 3. Международная торговая политика. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)\n\nПолитика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики.\nТаможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение.\nНетарифные барьеры в международной торговле.\nВсемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров.\nРоссия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО.\nСистема внешнеторгового регулирования в ЕАЭС и в России.\n\nРаздел 4. Международное движение капитала. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nФормы, структура и масштабы международного движения капитала.\nМасштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны.\nМеждународная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций.\n\nРаздел 5. Международный валютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)\n\nПонятие иностранной валюты. Валютный курс и паритет покупательной способности валюты.\nМировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития.\nВиды операций на валютном рынке. Хеджирование валютных рисков.\nФакторы, влияющие на формирование валютного курса.\n\nРаздел 6. Эволюция мировой валютной системы.\n\nСтруктурные принципы и функции мировой валютной системы. Механизм золотого стандарта.\nБреттон-Вудская валютная система: основные принципы построения. Роль и функции МВФ. Причины и особенности кризиса Бреттон-Вудской системы.\nПринципы Ямайской валютной системы. Стандарт СДР.\nСовременные режимы валютных курсов. Роль доллара и евро в современной валютной системе.\nЕвропейская валютная система: этапы создания и структурные принципы. Развитие еврозоны.\nЛиберализация валютной политики России.\n\nРаздел 7. Евровалютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)\n\nОсновные характеристики рынка евровалют: размеры, валютная структура, виды операций, процентные ставки.\nОсновные характеристики рынка еврооблигаций: валютная структура, основные эмитенты, виды облигаций. Россия на рынке еврооблигаций\n\nРаздел 8. Внешняя задолженность.(ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nВнешняя задолженность как общемировая проблема. Особенности внешней задолженности развитых и развивающихся стран.\nДолговой характер экономики США: показатели, структура, динамика и способы урегулирования американской внешней задолженности.\nРоль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран.\nВнешний долг России: динамика, структура и особенности управления. Задолженность иностранных государств перед Россией.\n\nРаздел 9. Международная трудовая миграция.(МАНЬШИН РОМАН ВЛАДИМИРОВИЧ)\n\nОсновные направления и структура международной трудовой миграции.\nМеждународная трудовая миграция в России.\n\nРаздел 10. Региональная экономическая интеграция. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)\n\nХарактеристика этапов экономической интеграции (на примере Европейского союза).\nОсобенности интеграционные процессов на постсоветском пространстве.\nСовременные цели, направления, проблемы и перспективы развития интеграционных процессов в ЕАЭС.\nНАФТА (ЮСМКА): особенности интеграции и влияние на экономики стран-участниц.\nОсобенности интеграционных процессов в Азии (на примере АСЕАН).\nОсобенности интеграционных процессов в Африке (на примере Западноафриканского экономического и валютного союза).\n\nРаздел 11. Международные корпорации (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)\nРоль ТНК в мировой экономике. Показатели транснационализации и рейтинг крупнейших финансовых и нефинансовых ТНК мира.\nОсобенности ТНК развивающихся стран (на примере Китая).\nДеятельность зарубежных ТНК в экономике современной России.\n\nРаздел 12. Международные финансовые организации. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)\n\nМВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях.\nГруппа Всемирного Банка: цели, направления, динамика и география предоставления финансирования.\nРоль региональных банков развития в финансировании развивающихся стран.\n\nРаздел 13. Свободные экономические зоны.\n\nСвободные экономические зоны в мировой экономике: эволюция и современное состояние.\nОсобые экономические зоны в РФ как фактор привлечения иностранных инвестиций.\n\nРаздел 14. Платежный баланс. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nПонятие платежного баланса и основные принципы его составления. Классификация статей платежного баланса по методике МВФ.\nСостояние платежного баланса России в 2020 г. Основные факторы, определяющие платежный баланс.\n\nРаздел 15. Внешнеэкономическая безопасность. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)\nВнешнеэкономическая безопасность: характер и типология угроз, показатели и их пороговые значения для России.\nВнешнеэкономическая безопасность: механизмы обеспечения национальных интересов в странах и регионах мира (на примере США и РФ).\nЭкономические санкции как механизм реализации национальных интересов в международных экономических отношениях.\n\n\n\n\nРаздел 1. Экономическая теория.\n  Меркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле.\n  Адам Смит: теория абсолютного преимущества в торговле.\n  Давид Рикардо: теория сравнительного преимущества в торговле.\n  Теорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары.\nРаздел 2. Международная торговля\n  Понятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России.\n  Особенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры.\n  Сущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием.\n  Место и роль России в международной торговле. Конкурентные преимущества России.\n  Стоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры.\n  Международный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами.\nРаздел 3. Международная торговая политика\n  Политика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики.\n  Таможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение.\n  Нетарифные барьеры в международной торговле.\n  Всемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров.\n  Россия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО.\n  Система внешнеторгового регулирования в ЕАЭС и в России.\nРаздел 4. Международное движение капитала\n  Формы, структура и масштабы международного движения капитала.\n  Масштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны.\n  Международная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций.\nРаздел 5. Международный валютный рынок\nРаздел 6. Эволюция мировой валютной системы\nРаздел 7. Евровалютный рынок\nРаздел 8. Внешняя задолженность\n  Роль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран.\nРаздел 9. Международная трудовая миграция\nРаздел 10. Региональная экономическая интеграция\nРаздел 11. Международные корпорации\nРаздел 12. Международные финансовые организации\n  МВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях.\nРаздел 13. Свободные экономические зоны\nРаздел 14. Платежный баланс\nРаздел 15. Внешнеэкономическая безопасность\n\n\nРаздел 3. Международная торговая политика. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)\n\nРоссия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО.\nСистема внешнеторгового регулирования в ЕАЭС и в России.\n\nРаздел 4. Международное движение капитала. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nФормы, структура и масштабы международного движения капитала.\nМасштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны.\nМеждународная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций.\n\nРаздел 5. Международный валютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)\n\nПонятие иностранной валюты. Валютный курс и паритет покупательной способности валюты.\nМировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития.\nВиды операций на валютном рынке. Хеджирование валютных рисков.\nФакторы, влияющие на формирование валютного курса.\n\nРаздел 6. Эволюция мировой валютной системы.\n\nСтруктурные принципы и функции мировой валютной системы. Механизм золотого стандарта.\nБреттон-Вудская валютная система: основные принципы построения. Роль и функции МВФ. Причины и особенности кризиса Бреттон-Вудской системы.\nПринципы Ямайской валютной системы. Стандарт СДР.\nСовременные режимы валютных курсов. Роль доллара и евро в современной валютной системе.\nЕвропейская валютная система: этапы создания и структурные принципы. Развитие еврозоны.\nЛиберализация валютной политики России.\n\nРаздел 7. Евровалютный рынок. (МАДИЯРОВА ДИАНА МАКАЕВНА)\n\nОсновные характеристики рынка евровалют: размеры, валютная структура, виды операций, процентные ставки.\nОсновные характеристики рынка еврооблигаций: валютная структура, основные эмитенты, виды облигаций. Россия на рынке еврооблигаций\n\nРаздел 8. Внешняя задолженность.(ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nВнешняя задолженность как общемировая проблема. Особенности внешней задолженности развитых и развивающихся стран.\nДолговой характер экономики США: показатели, структура, динамика и способы урегулирования американской внешней задолженности.\nРоль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран.\nВнешний долг России: динамика, структура и особенности управления. Задолженность иностранных государств перед Россией.\n\nРаздел 9. Международная трудовая миграция.(МАНЬШИН РОМАН ВЛАДИМИРОВИЧ)\n\nОсновные направления и структура международной трудовой миграции.\nМеждународная трудовая миграция в России.\n\nРаздел 10. Региональная экономическая интеграция. (ДЮЖЕВА НАТАЛИЯ ВАЛЕРЬЕВНА)\n\nХарактеристика этапов экономической интеграции (на примере Европейского союза).\nОсобенности интеграционные процессов на постсоветском пространстве.\nСовременные цели, направления, проблемы и перспективы развития интеграционных процессов в ЕАЭС.\nНАФТА (ЮСМКА): особенности интеграции и влияние на экономики стран-участниц.\nОсобенности интеграционных процессов в Азии (на примере АСЕАН).\nОсобенности интеграционных процессов в Африке (на примере Западноафриканского экономического и валютного союза).\n\nРаздел 11. Международные корпорации (ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)\nРоль ТНК в мировой экономике. Показатели транснационализации и рейтинг крупнейших финансовых и нефинансовых ТНК мира.\nОсобенности ТНК развивающихся стран (на примере Китая).\nДеятельность зарубежных ТНК в экономике современной России.\n\nРаздел 12. Международные финансовые организации. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)\n\nМВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях.\nГруппа Всемирного Банка: цели, направления, динамика и география предоставления финансирования.\nРоль региональных банков развития в финансировании развивающихся стран.\n\nРаздел 13. Свободные экономические зоны.\n\nСвободные экономические зоны в мировой экономике: эволюция и современное состояние.\nОсобые экономические зоны в РФ как фактор привлечения иностранных инвестиций.\n\nРаздел 14. Платежный баланс. (ФЕДЯКИНА ЛОРА НИКОЛАЕВНА)\n\nПонятие платежного баланса и основные принципы его составления. Классификация статей платежного баланса по методике МВФ.\nСостояние платежного баланса России в 2020 г. Основные факторы, определяющие платежный баланс.\n\nРаздел 15. Внешнеэкономическая безопасность. (АНДРОНОВА ИННА ВИТАЛЬЕВНА)\nВнешнеэкономическая безопасность: характер и типология угроз, показатели и их пороговые значения для России.\nВнешнеэкономическая безопасность: механизмы обеспечения национальных интересов в странах и регионах мира (на примере США и РФ).\nЭкономические санкции как механизм реализации национальных интересов в международных экономических отношениях.\n\nРаздел 5. Международный валютный рынок\n\nПонятие иностранной валюты. Валютный курс и паритет покупательной способности валюты.\nМировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития.\nВиды операций на валютном рынке. Хеджирование валютных рисков.\nФакторы, влияющие на формирование валютного курса.\n\nРаздел 6. Эволюция мировой валютной системы\nРаздел 7. Евровалютный рынок\nРаздел 8. Внешняя задолженность\n\nРоль МВФ, Всемирного банка, Лондонского и Парижского клубов в урегулировании внешней задолженности развивающихся стран.\n\nМВФ\n\nТри основные функции МВФ:\n\nЭкономический надзор\n\nПредоставление государствам-членам рекомендаций\nотносительно принятия мер политики для достижения\nмакроэкономической стабильности, ускорения\nэкономического роста и уменьшения бедности\n\nКредитование\n\nПредоставление финансирования государствам-членам,\nчтобы помочь им в решении проблем платежного\nбаланса, включая ситуации нехватки иностранной валюты,\nвозникающие, когда их внешние платежи превышают\nих поступления в иностранной валюте\n\nРазвитие потенциала\n\nОбеспечение развития потенциала (включая техническую\nпомощь и подготовку кадров), по просьбе государствчленов, для оказания им помощи в укреплении\nэкономических институтов в целях разработки\nи проведения обоснованной экономической политики.\n\nРаздел 9. Международная трудовая миграция\nРаздел 10. Региональная экономическая интеграция\nРаздел 11. Международные корпорации\nРаздел 12. Международные финансовые организации\n\nМВФ: цели, динамика, география финансирования, характеристика кредитных механизмов и условия финансирования стран-заемщиков. Трансформация деятельности МВФ в современных условиях.\n\nБуклет о МФВ (pdf)\nТермины (pdf 500стр)\nГодовой отчет МВФ 2022 (pdf)\n\nМВФ — это организация, представляющая 184 страны. Целями егоработы являются укреплениемеждународного сотрудничествав валютно-финансовой сфере, обеспечение финансовой стабильности, развитие международной торговли, содействие высокой занятостии устойчивому экономическому росту, а также сокращение бедности.\n\nБыл учрежден на основе международногодоговора в1945 году для содействияоздоровлению мировой экономики.\n\nОдновременно с МВФ был образован Международный банк реконструкции и развития (МБРР), болеешироко известный как Всемирныйбанк, для содействия долгосрочному экономическому развитию, в частности с помощью финансирования инфраструктурных проектов, таких как строительство дорог и улучшение системыводоснабжения.\n\nЦели:\n\nПроводит наблюдение за изменениями в экономической и финансовой ситуации и политикев государствах-членах и на глобальном уровне идает государствам-членам рекомендации по вопросам экономической политики\nПредоставляет кредиты государствам-членам, испытывающим проблемы платежного баланса\n\nОсновная деятельность МВФ: макроэкономическая политика и политика в отношении финансового сектора\n\nВ центре внимания МВФ находится макроэкономическая политика страны — то есть политика, связанная с государственным бюджетом, регулированием процентных ставок, денежнокредитной сферы и обменного курса, а также политика в отношении финансового сектора, включая регулирование банков и других финансовых учреждений и надзор за ними.\n\nв МВФ существует система взвешенного распределения голосов: чем больше квота государства в МВФ, — определяемая в основном размером его экономики, — тем большимчислом голосов оно располагает.\n\nКредитные механизмы МВФ:\n\nДоговоренности о кредите «стэнд-бай» составляют ядро политики кредитования МВФ. Договоренность о кредите «стэнд-бай» гарантирует государству-члену возможность получения средств по кредиту в пределах установленной суммы, обычно в течение 12–18 месяцев, для урегулирования краткосрочной проблемы платежного баланса.\nМеханизм расширенного кредитования МВФ**. Поддержка, оказываемая МВФ государствам-членам в рамках механизма расширенного кредитования, гарантирует государству-члену возможностьполучения средств по кредиту в пределах установленной суммы, обычно в течение трех-четырех лет, с тем чтобы помочь ему в решении экономических проблем структурного характера, которые существенно ослабляют состояние его платежного баланса.\nМеханизм финансирования на цели сокращения бедностии содействия экономическому росту** (введенный в ноябре 1999 годана смену механизму расширенного финансирования структурной перестройки). Это механизм кредитования с низкими процентными став-ками, призванный помочь беднейшим государствам-членам, испытывающим долговременные проблемы в области платежного баланса. Проценты для заемщиков субсидируются за счет средств, полученных ранее от продажи принадлежащего МВФ золота, а такжезаймов и грантов, предоставленных МВФ на эти цели государствами-членами.\nМеханизм финансирования дополнительных резервов.** Предоставляет дополнительное краткосрочное финансирование государствам-членам, испытывающим чрезвычайные трудности в области платежного баланса вследствие внезапной и дестабилизирующей утраты доверия рынка, проявляющейся в оттоках капитала. Процентная ставка по кредитам СРФ включает надбавку к обычнойставке по кредитам МВФ.\nЧрезвычайная помощь.** Форма помощи, введенная в 1962 годудля содействия государствам-членам в преодолении проблем платежного баланса, вызванных внезапными и непредвиденными стихийными бедствиями, в 1995 году была распространена наситуации, когда государства-члены пережили военные конфликты, подорвавшие их институциональный и административный потенциал\n\nСсылки:\nIMF Regional Reports\n\nРаздел 13. Свободные экономические зоны\nРаздел 14. Платежный баланс\nРаздел 15. Внешнеэкономическая безопасность\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/01-economic-theory",
            "title": "Раздел 1. Экономическая теория",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "(ВОЛГИНА НАТАЛЬЯ АНАТОЛЬЕВНА)\n\nМеркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле.\nАдам Смит: теория абсолютного преимущества в торговле.\nДавид Рикардо: теория сравнительного преимущества в торговле.\nТеорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары.\n\nМеркантилизм как внешнеторговая теория и политика. «Игра с нулевой суммой» в торговле.\n\nМеркантилизм (15-17 вв) - это экономическая политика, цель которой — накопление в стране драгоценных металлов, средство достижения цели – активный торговый баланс, то есть превышение экспорта над импортом.\n\nНеобходимость активного вмешательства государства в хозяйственную деятельность, в основном в форме протекционизма: установления высоких импортных пошлин, выдачи субсидий национальным производителям и так далее.\n\n1. Ранний меркантилизм (конец XV — середина XVI века).\nПредставители: У. Стаффорд, Де Сантис, Г. Скаруффи.\nВ этот период в учении преобладает теория денежного баланса, в рамках которой было закреплено увеличение национального благосостояния законодательным путем: устанавливался запрет на вывоз золота и серебра за границу. Деньги выполняли только функцию средства накопления.\n\n2. Поздний меркантилизм (вторая половина XVI — начало XVII века).\nПредставители: Т. Ман, А. Серра, А. де Монкретьен.\n\nИми была создана теория торгового баланса, который обеспечивался путем активной внешней торговли. Главенствовал принцип: покупать дешевле в одной стране и продавать дороже в другой. Вывоз денежных средств за границу был разрешен. Деньгам отводились функции средства накопления и средства обращения — поздний меркантилизм трактовал деньги как капитал и признавал их товаром.\n\nПоздний меркантилизм был прогрессивным. Он содействовал развитию торговли, судостроения, экспортной промышленности, международного разделения труда.\n\nОсновные принципы:\n\n— регулирование внешней торговли с целью притока в страну золота и серебра;\n— поддержка промышленности путем импорта дешевого сырья;\n— протекционизм;\n— поощрение экспорта готовой продукции;\n— рост населения для поддержания низкого уровня зарплаты;\n— рассмотрение проблем сферы обращения в отрыве от сферы производства;\n— достижение экономического роста путем приумножения денежного богатства страны через государственное регулирование внешней торговли и достижение положительного сальдо торгового баланса.\n\nПреобладал в странах Западной Европы (преимущественно Франции, Италии и Англии). В России одним из приверженцев идей меркантилизма был выдающийся государственный деятель А. Л. Ордын-Нащекин (1605—1680).\n\nИгра с нулевой суммой — это противоположность беспроигрышным ситуациям — таким как торговое соглашение, которое значительно увеличивает торговлю между двумя странами — или проигрышным ситуациям, таким как война, например. В реальной жизни, однако, не всегда все так очевидно, и зачастую сложно измерить прибыли и убытки.\n\nИгра с нулевой суммой — это ситуация, когда, если одна сторона проигрывает, другая сторона выигрывает, а чистое изменение богатства равно нулю.\n\nИсточники:\n\nhttps://www.banki.ru/wikibank/merkantilizm/\nМеркантелизм / годы /страны\n\nАдам Смит: теория абсолютного преимущества в торговле.\n\nИсследование о природе и причинах богатства народов (1776 г.) - основная работа шотландского экономиста Адама Смита.\n\nА. Смит (1723—1790) распространил и на мирохозяйственную сферу, впервые теоретически обосновав принцип абсолютных преимуществ (или абсолютных издержек)\n«Основное правило каждого благоразумного главы семьи состоит в том, чтобы не пытаться изготовить дома такие предметы, изготовление которых обойдется дороже, чем при покупке их на стороне... То, что представляется разумным в образе действия любой частной семьи, вряд ли может оказаться неразумным для всего королевства. Если какая-либо чужая страна может снабдить нас каким-нибудь товаром по более дешевой цене, чем мы в состоянии изготовить его, гораздо лучше покупать его у нее на некоторую часть продукта нашего собственного промышленного труда, прилагаемого в той области, в которой мы обладаем некоторым преимуществом»\n\nОсновой развития международной торговли служит различие в абсолютных издержках. Торговля будет приносить экономический эффект, если товары будут ввозиться из страны, где издержки абсолютно меньше, а вывозиться те товары, издержки которых в данной стране ниже, чем за рубежом.\n\nБлагосостояние наций зависит не столько от количества накопленного ими золота, сколько от их способностей производить конечные товары и услуги.\n\nОсновные положения А.Смита в теории международной торговли:\n\nправительствам не следует вмешиваться во внешнюю торговлю, поддерживая режим открытых рынков и свободы торговли;\nнации, так же как и частные лица, должны специализироваться на изготовлении товаров, в производстве которых у них есть абсолютные преимущества, и торговать ими в обмен на товары, абсолютным преимуществом в производстве которых обладают другие нации;\nконцентрация усилий (ресурсов) стран на производстве товаров, по которым страны имеют абсолютное преимущество, приводит к увеличению общих объемов производства, росту обмена между странами продуктами своего труда;\nсвободная торговля между странами обусловливает эффективное распределение мировых ресурсов, обеспечивая прибыль любой и каждой торгующей стране.\n\nДавид Рикардо: теория сравнительного преимущества в торговле.\n\nТеория сформулированна Давидом Рикардо (1772-1823) (классик политической экономии, последователь и одновременно оппонент Адама Смита) в начале XIX века.\n\nДавид Рикардо развил теорию абсолютных преимуществ Адама Смита и показал, что торговля выгодна каждой из двух стран, даже если одна из них не обладает абсолютным преимуществом в производстве любых конкретных товаров. Специализация на производстве товара, имеющего максимальные сравнительные преимущества, выгодна, даже если нет абсолютных преимуществ.\n\nТеория сравнительных преимуществ на примере двух стран и двух товаров\n\nВременные затраты на производство единицы товара:\n|         | Сыр (в ед. Вина) | Вино (в ед. Сыра) |\n| :-----: | ---------------- | ----------------- |\n| Франция | 2                | 1                 |\n| Испания | 4                | 3                 |\n\nВ данном случае во Франции затраты времени в производстве обоих товаров меньше (она обладает абсолютным преимуществом). Согласно А. Смиту, торговля между странами принесёт выгоды только Франции. Однако, с точки зрения теории сравнительных преимуществ Д. Рикардо, при определённом соотношении цен между товарами, торговля может приводить к взаимной выгоде обеих стран даже при абсолютном преимуществе только одной из них.\n\nРассчитаем альтернативные цены производства каждого из товаров в каждой стране:\n\nАльтернативная цена производства единицы товара:\n|         | Сыр (в ед. Вина) | Вино (в ед. Сыра) |\n| :-----: | ---------------- | ----------------- |\n| Франция | 2 / 1            | 1 / 2             |\n| Испания | 4 / 3            | 3 / 4             |\n\nВ данном случае одна единица сыра (например, килограмм) во Франции стоит 2 единицы вина (2 бутылки), а в Испании единица сыра стоит дешевле (4 / 3 единицы вина). В то же время единица вина в Испании стоит 3 / 4 единицы сыра, что дороже чем во Франции. Таким образом, если Франция будет производить вино для Испании, а Испания — сыр для Франции, то обе страны выиграют трудовые ресурсы. На каждой закупленной единице сыра Франция будет экономить 2 - 4 /3 = 2/3 единицы вина, а Испания 3/4-1/2=1/4 единицы сыра на каждой закупленной единице вина.\n\nТеорема Хекшера-Олина и выравнивание относительных цен на торгуемые товары.\n\nТеория Хекшера — Олина (теория соотношения факторов производства) - каждая страна экспортирует товары, для производства которых она обладает относительно избыточными факторами производства, и импортирует товары, для производства которых она испытывает относительный недостаток факторов производства.\n\nВыравнивание относительных цен на торгуемые товары - это процесс, который происходит когда цены на торгуемые товары становятся ближе к ценам на неторгуемые товары. Это может происходить из-за снижения транспортных расходов, изменения в налоговой политике или снижения тарифов на товары. Это может повлиять на решения потребителей и производителей и в конечном итоге повлиять на развитие экономики.\n\nИсточники:\n\nТеория выравнивания цен на факторы производства (теория Хекшера-Олина-Самуэльсона)\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/02-international-trade",
            "title": "Раздел 2. Международная торговля",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "БЕЛОВА ИРИНА НИКОЛАЕВНА\n\nПонятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России.\nОсобенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры.\nСущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием.\nМесто и роль России в международной торговле. Конкурентные преимущества России.\nСтоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры.\nМеждународный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами.\n\nПонятие и критерии «открытости» национальной экономики. Показатели открытости экономик США, стран Европы, России.\n\nСтатья макроэкономика. Вопрос 3\nСтатистика по нешней торговле РФ 2021\n\n\nОткрытая национальная экономика (полностью открытое хозяйство) характеризуется полностью открытыми внутренними рынками природных ресурсов, товаров, услуг, капиталов, рабочей силы, идей, информации. Такая экономика способна обеспечить углубление специализации и кооперации в национальной экономике, рост ее конкурентоспособности за счет постоянного соперничества с иностранными фирмами на внутреннем рынке, использование позитивного мирового опыта через систему международных экономических отношений, эффективное использование принципа сравнительных преимуществ в международном разделении труда.\n\nКритерии:\n\nКритерии, которыми можно оценивать открытость экономики, включают в себя уровень торговой интеграции, уровень иностранной наличности, и уровень иностранных инвестиций.\n\nфакт наличия влияния внешней среды на динамику основных показателей национального экономического развития, а именно: на объем и темпы роста производства, состояние внутренних товарных рынков, занятости населения.\n\nВ настоящее время стремление к открытости национальных экономик в большей степени обусловливается объективными процессами интернационализации и глобализации производства, обмена, капиталов, потребления, чем экспансией американских корпораций.\n\nСтепень открытости экономики\n\nРассчитывается как показатель экспортной квоты, который понимается как отношение объема экспорта (за год) к ВВП в следующей формуле: $Эк = Экспорт / ВВП * 100%$. Чем больше показатель Эк, тем более высока степень открытости экономики.\nпоказатель импортной квоты. Рассчитывается как отношение объема импорта (за год) к ВВП. $Ик = Импорт / ВВП * 100%$\nНаиболее точный показатель - внешнеторговой квоты. Отношение суммы объемов экспорта и импорта страны (за год) к ее ВВП: $ВТк = (Экспорт+Импорт) / ВВП * 100%$. Чем больше ВТК, тем более открытой является экономика страны\n\nПоказатели Экспорт+Импорт по странам\n\nОсобенности развития мировой торговли товарами в 2000-2010-е годы: стоимостная динамика и товарно-географическая структура. Крупнейшие страны-экспортеры и страны-импортеры.\n\nРазвитие мировой торговли товарами в 2000-2010 годах было связано с несколькими особенностями, включая стоимостную динамику и товарно-географическую структуру.\n\nСтоимостная динамика в этот период была определена преимущественно повышением цен на энергоносители и металлы. Это привело к росту средней цены на экспортируемые товары и повышению доходов стран-экспортеров.\n\nТоварно-географическая структура мировой торговли также изменилась в этот период. Страны Азии, в частности Китай, значительно увеличили свою долю в мировой торговле, став самыми крупными экспортерами и импортерами. Страны Европы и Северной Америки также оставались важными участниками мировой торговли, но их доля снизилась.\n\nКрупнейшие страны-экспортеры в этот период были Китай, США, Германия и Япония. Они составляли более трети мирового экспорта. Китай был крупнейшим экспортером, а США и Германия занимали второе и третье место соответственно.\n\nКрупнейшими странами-импортерами были США, Китай, Япония и Германия. Они также составляли более трети мирового импорта. США был крупнейшим импортером, а Китай и Япония занимали второе и третье место соответственно.\n\nВ целом, развитие мировой торговли в 2000-2010 годах было связано с ростом цен на энергоносители и металлы, что привело к существенному увеличению стоимости товаров и усилило конкуренцию на мировом рынке. Это также привело к смещению товарно-географической структуры торговли, с ростом торговли между развитыми странами и развивающимися странами, в частности Китаем и другими странами Азии.\n\nОдним из важных факторов, который способствовал росту мировой торговли в этот период, был рост инвестиций в инфраструктуру и технологии в развивающихся странах, что позволило увеличить их производительность и способствовало росту экспорта. Также рост интеграции развивающихся стран в мировую экономику, в частности Китая в мировой торговле и производстве, сыграл важную роль в развитии мировой торговли.\n\nКрупнейшими экспортерами в 2000-2010 годах были США, Китай, Германия, Япония и Нидерланды. Эти страны вносили значительный вклад в мировой экспорт, особенно в такие отрасли как автомобилестроение, машиностроение, электроника и текстильная промышленность. США также были крупнейшим импортером, а Китай занимал второе место. Россия и другие страны СНГ также имели значительный уровень экспорта нефти и газа, но их роль в мировой торговле все еще была несущественной.\n\nВ целом, мирововая торговля товарами в 2000-2010 годах была существенно обусловлена стоимостной динамикой и товарно-географической структурой. Крупнейшие экспортеры и импортеры были США, Китай, Европейский союз и Япония. Эти страны имели значительное влияние на мировой рынок товаров, и их роль в торговле была ключевой в этот период. Китай и Европейский союз были особенно важными игроками, так как они стали одними из крупнейших импортеров и экспортеров товаров в мире. Россия и другие страны СНГ также имели значительный уровень экспорта нефти и газа, но их роль в мировой торговле все еще была несущественной.\n\nСущность и причины структурных сдвигов в мировой торговле промышленными товарами, сырьем, топливом и продовольствием.\n\nСтруктурные сдвигы в мировой торговле - это изменения в распределении торговли между различными группами товаров и странами. Это может быть вызвано различными факторами, включая экономические и политические изменения, технологические инновации и изменения в потребительском спросе.\n\nОдной из главных причин структурных сдвигов является изменение технологий и инноваций. Например, развитие информационных технологий и интернета привело к сокращению торговли физическими товарами и росту торговли цифровыми товарами и услугами. Также, развитие новых технологий в области энергетики, таких как солнечная и ветроэнергетика, может привести к сдвигу в торговле топливом и энергоресурсами.\n\nДругой важной причиной является изменение в мировой экономике и политике. Например, рост новых индустрий и развитие стран в Азии может привести к сдвигу в торговле с участием этих стран. Также, изменения в международной политике, такие как тарифы или ограничения на торговлю, могут привести к сдвигам в торговле между странами.\n\nВ целом, структурные сдвигы в мировой торговле промышленными товарами, сырьем, топливом и продовольствием являются необходимой частью эволюции мировой экономики. Они отражают изменения в технологиях, инновациях, демографии и мировой политике, которые в свою очередь оказывают влияние на развитие и эволюцию различных отраслей и рынков. Однако, необходимо учитывать, что эти сдвиги могут иметь как положительные, так и отрицательные последствия для развития стран и общества в целом. Поэтому важно проводить мониторинг и анализ сдвигов, а также разрабатывать международные инициативы и политики, которые способствуют сохранению стабильности и равновесия в мировой торговле.\n\nМесто и роль России в международной торговле. Конкурентные преимущества России.\n\nРоссия является одной из крупнейших икономик мира, и ее место и роль в международной торговле имеет значительное значение. Россия является крупным производителем нефти, газа, металлургической продукции и других промышленных товаров, которые являются важными экспортными товарами.\n\nКонкурентными преимуществами России являются ее богатые природные ресурсы, как нефть, газ и металлы, а также высокоразвитая инфраструктура и квалифицированная рабочая сила. Россия также имеет развитую систему образования и науки, что позволяет ей развивать инновационные технологии и новые продукты.\n\nОднако, Россия также сталкивается с некоторыми проблемами в международной торговле, такими как сложности с доступом к западным рынкам и ограничения на экспорт некоторых товаров. Россия также сталкивается с конкуренцией на мировом рынке от других крупных икономик, таких как Китай и США.\n\nВ целом, Россия имеет значителелое место и роль в международной торговле из-за ее богатых природных ресурсов и высокоразвитой инфраструктуры. Ее конкурентные преимущества включают в себя нефть, газ, металлы, а также развитое сельское хозяйство и развитую систему образования и науки. Однако, Россия должна справиться с некоторыми проблемами в международной торговле, такими как сложности с доступом к западным рынкам и ограничения на экспорт некоторых товаров, а также с конкуренцией с другими крупными икономиками.\n\nВ целом, Россия имеет значительный потенциал для роста и развития в международной торговле, но для этого необходимо сделать существенные усилия.\n\nСтоимостная динамика, структура и география международной торговли услугами. Крупнейшие страны-экспортеры и страны-импортеры.\n\nМеждународная торговля услугами является важной составляющей мировой экономики, и ее стоимостная динамика, структура и география имеют значительное влияние на мировой рынок торговли.\n\nСтоимостная динамика международной торговли услугами зависит от множества факторов, включая экономическое развитие стран, изменения в ценах на товары и услуги, изменения в общей ситуации на рынке, а также изменения в тарифах и торговых барьерах. В последние годы международная торговля услугами стала расти быстрее, чем торговля товарами, что свидетельствует о росте значимости услуг в мировой экономике.\n\nСтруктура международной торговли услугами также имеет свои особенности. Крупнейшими секторами являются туризм, транспортные услуги, коммуникационные услуги и профессиональные услуги. Кроме того, международная торговля услугами часто ассоциируется с интеллектуальной собственностью и лицензиями.\n\nГеография международной торговли услугами также имеет свои особенности. Крупнейшими странами-экспортерами являются США, Германия и Великобритания, а странами-импортерами - Китай, Япония и Индия. Одна из важных особенностей международной торговли услугами является то, что она часто происходит между развитыми странами и развивающимися странами. Это может быть связано с тем, что развитые страны имеют более высокий уровень технологического развития и более высокий уровень образования, которые позволяют им предлагать более высококачественные услуги.\n\nМеждународный трансферт технологий: современные каналы, формы и показатели технологического обмена между странами.\n\nМеждународный трансфер технологий - это процесс передачи знаний, инноваций и технологий между странами. Современные каналы трансфера технологий включают множество форм, таких как инвестиции в исследование и разработку, лицензирование, совместное производство и совместное исследование.\n\nИнвестиции в исследование и разработку являются одним из наиболее эффективных способов трансфера технологий. Они позволяют компаниям и организациям приобретать новые знания и технологии, которые могут быть использованы для улучшения их продуктов и услуг.\n\nЛицензировование также является важным каналом трансфера технологий. Это позволяет компаниям и организациям использовать запатентованные технологии и инновации других сторон в обмен на определенную плату. Лицензирование может быть как на использование технологии, так и на ее дальнейшую разработку.\n\nСовместное производство и совместное сотрудничество также являются важными формами международного трансфера технологий. Это позволяет компаниям и организациям сотрудничать на месте, чтобы создавать и использовать новые технологии. Совместное производство может включать в себя совместное исследование и разработку, а также производство и маркетинг продуктов и услуг, использующих новые технологии.\n\nМеждународный трансферт технологий является важным фактором экономического роста и развития стран. Он позволяет странам получить доступ к новым технологиям и знаниям, которые помогают улучшить конкурентносспособность и снизить затраты. Одним из главных каналов технологического обмена является инвестиция, которая позволяет фирмам иностранного капитала инвестировать в развитие новых технологий в стране-партнере. Другими каналами являются сотрудничество в области науки и исследований, обмен специалистами и студентами, а также лицензирование и франшизинг.\n\nКрупнейшими странами-экспортерами технологий являются США, Япония и Германия, а крупнейшими странами-импортерами - Китай, Индия и Россия.\n\nОсновными формами технологического обмена между странами являются лицензирование, франчайзинг, контрактное производство, совместное исследование и разработка, партнерство и инвестиции.\n\nОсновные показатели технологического обмена между странами включают:\n\nОбъем инвестиций в исследования и разработки;\nЧисло патентов, зарегистрированных в международных организациях;\nЧисло международных стандартов, которым следуют страны;\nЧисло международных контрактов на лицензирование технологий;\nЧисло студентов, обучающихся за границей, и число иностранных студентов, обучающихся в стране;\nЧисло международных научных сотрудничеств и конференций.\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/03-international-policy",
            "title": "Раздел 3. Международная торговая политика",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "\nПолитика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики.\nТаможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение.\nНетарифные барьеры в международной торговле.\nВсемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров.\nРоссия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО.\nСистема внешнеторгового регулирования в ЕАЭС и в России.\n\nПолитика «свободной торговли» и политика протекционизма в исторической перспективе. Цели и инструменты внешнеторговой политики.\n\nПолитика \"свободной торговли\" и политика протекционизма - это две различные стратегии, которые используются государствами для регулирования их экономики.\n\nПолитика \"свободной торговли\" опирается на идею, что открытость торговли поможет создать большую эффективность и рост экономики. В то же время, политика протекционизма стремится защитить отечественное производство и рабочие места с помощью таможенных пошлин, квот и других форм регулирования.\n\nПолитика протекционизма в исторической перспективе - это политика, которая направлена на защиту отечественной промышленности и торговли от иностранной конкуренции. Это может быть достигнуто с помощью таких мер, как таможенные пошлины, квоты, субсидии и иные ограничения на импорт.\n\nИнструменты внешнеторговой политики, используемые в протекционизме, могут включать:\n  Таможенные пошлины: таможенные сборы, которые увеличивают цену на импортные товары, защищая отечественное производство.\n  Квоты и ограничения на импорт: ограничения на количество импортируемых товаров или специальные требования, которые делают импорт менее конкурентным.\n  Субсидии для отечественного производства: государственная поддержка для отечественных производителей, чтобы сделать их более конкурентными на местном рынке.\n  Национализация или контроль над ключевыми отраслями: государство объединяет контроль над ключевыми отраслями экономики, чтобы обеспечить их сохранность и развитие.\n\nПолитика свободной торговли:\n  Основная идея заключается в отсутствии тарифных и нетарифных барьеров на импорт и экспорт товаров и услуг между странами;\n  Приводит к экономическому росту за счет увеличения объемов торговли и специализации производства;\n  Основана на принципах компаративных преимуществ и абсолютной выгоды.\n\nПолитика протекционизма:\n  Предусматривает введение тарифов и нетарифных барьеров для защиты от импорта и поддержки отечественных производителей;\n  Может привести к снижению объемов торговли, росту цен, нарушению конкуренции и инфляции;\n  Используется государствами для достижения своих экономических и политических целей, например, защиты отечественной промышленности, увеличения экспорта или сокращения импорта.\n\nЦели внешнеторговой политики:\n  Расширение рынков сбыта для национальной экономики;\n  Увеличение объемов экспорта;\n  Защита отечественной промышленности от конкуренции;\n  Повышение уровня занятости и доходов населения;\n  Решение политических задач, например, улучшение отношений с другими странами.\n\nИнструменты внешнеторговой политики:\n  Тарифы и квоты на импорт и экспорт товаров;\n  Субсидирование отечественных производителей;\n  Различные виды лицензирования и контроля за импортом и экспортом;\n  Валютные манипуляции;\n  Соглашения о свободной торговле.\n\nТаможенно-тарифное регулирование: характеристика основных институтов и их экономическое значение.\n\nТаможенно-тарифное регулирование внешнеторговой деятельности:\n  Совокупность методов государственного регулирования внешнеторговой деятельности, основанных на применении пошлин, таможенных процедур, правил.\n  Осуществляется путем установления таможенных пошлин, налогов и других ограничений на импорт и экспорт товаров.\n\nОсновные институты таможенно-тарифного регулирования включают в себя:\nтарифы\nквоты\nлицензирование\nинспекционные и контрольные меры\nсанкции и международное сотрудничество в области таможенного регулирования.\n\nТарифы:\n  Таможенные пошлины, которые устанавливаются на импорт и экспорт товаров для регулирования объемов торговли и защиты национальных производителей;\n  Имеются два типа тарифов: специфические (фиксированная сумма на единицу товара) и адвалорные (процент от стоимости товара).\n  Тарифы - это пошлины, которые налагаются на импортируемые товары, в то время как квоты ограничивают количество импортируемых товаров.\n\nЦели и задачи таможенно-тарифного регулирования:\n\nТаможенно-тарифное регулирование является основным методом регулирования государством сферы внешней торговли.\n\nЦелями могут быть:\nПротекционистская функция — защита национальных товаропроизводителей от иностранной конкуренции.\nФискальная функция — обеспечение поступления средств в бюджет\n\nИнституты таможенно-тарифного регулирования:\n  Таможенная служба - осуществляет контроль за перемещением товаров через таможенную границу и взимает таможенные пошлины;\n  Таможенные брокеры - предоставляют услуги по таможенному оформлению товаров и оказанию консультационной помощи по таможенному регулированию\n  Международные организации - например, ВТО, которые регулируют правила международной торговли и снижают уровень протекционизма.\n\nЭкономическое значение таможенно-тарифного регулирования:\n  Позволяет государствам защитить отечественных производителей от конкуренции;\n  Способствует повышению доходов государственного бюджета за счет сбора таможенных пошлин;\n  Снижает уровень импорта некоторых товаров, что может способствовать развитию отечественной промышленности;\n  Может ограничивать объемы торговли и вызывать рост цен для потребителей.\n\nРесурсы:\n\nВикипедия - Таможенно-тарифное регулирование внешнеторговой деятельности\nВики - Таможенный тариф\n\nНетарифные барьеры в международной торговле.\n\nНетарифные барьеры - при­ни­мае­мые го­су­дар­ст­вен­ны­ми и му­ни­ци­паль­ны­ми ор­га­на­ми не­та­мо­жен­но-та­риф­ные ме­ры по ре­гу­ли­ро­ва­нию внеш­ней тор­гов­ли, ко­то­рые спо­соб­ны пря­мо или кос­вен­но воз­дей­ст­во­вать на им­порт и экс­порт то­ва­ров – на их объ­ё­мы, струк­ту­ру и це­ны. Не­та­риф­ное ре­гу­ли­ро­ва­ние на­прав­ле­но на соз­да­ние для отеч. то­ва­ров бо­лее бла­го­при­ят­ных ус­ло­вий, чем для то­ва­ров ино­стр. про­ис­хо­ж­де­ния. Оно мо­жет ис­поль­зо­вать­ся так­же для диф­фе­рен­циа­ции тор­го­во­го ре­жи­ма для то­ва­ров, про­ис­хо­дя­щих из разл. стран.\n\nНетарифные барьеры (НБ):\n   Это меры, которые не являются таможенными пошлинами, но ограничивают объемы торговли и влияют на конкуренцию на рынке;\n   НБ могут быть установлены для защиты национальных производителей, обеспечения безопасности и здоровья населения, охраны окружающей среды и других целей.\nВиды НБ:\n   Технические преграды - например, стандарты качества, упаковки и маркировки, которые могут ограничивать ввоз товаров;\n   Санитарные и фитосанитарные меры - контроль за качеством и безопасностью продуктов питания, животных и растений;\n   Антидемпинговые меры - установление дополнительных пошлин на импорт товаров, которые продаются на международном рынке ниже рыночной стоимости;\n   Субсидирование отечественных производителей;\n   Лицензирование импорта товаров;\n   Дискриминационные меры, направленные против конкретных стран или регионов.\nЭкономические последствия НБ:\n   Могут приводить к снижению объемов торговли и сокращению выгоды от международной специализации;\n   Увеличивают издержки для потребителей и ограничивают доступ к более качественным и дешевым товарам;\n   Могут приводить к возникновению торговых споров и противодействию со стороны других государств;\n   Использование НБ ограничивает конкуренцию на рынке, что может привести к росту цен и снижению инновационной активности.\n\nНа­ря­ду с по­ня­ти­ем «Н. б.» (non-tariff barriers) в лит-ре ис­поль­зу­ют­ся и иные, близ­кие ему по смыс­лу – «не­та­риф­ные ог­ра­ни­че­ния» (non-tariff restraints), «не­та­риф­ные пре­пят­ст­вия» (non-tariff obstacles), «не­та­риф­ные ис­ка­же­ния» (non-tariff distortions) и др. В офи­ци­аль­ной и нор­ма­тив­но-пра­во­вой лит-ре ис­поль­зу­ет­ся по­ня­тие «не­та­риф­ные ме­ры» (non-tariff measures). Ис­то­ри­че­ски фор­ми­рова­ние ин­ст­ру­мен­та­рия ре­гу­ли­ро­ва­ния ме­ж­ду­нар. тор­гов­ли шло по пу­ти вы­де­ле­ния (раз­ли­че­ния) мер та­мо­жен­но-та­риф­но­го ре­гу­ли­ро­ва­ния (та­мо­жен­ных по­шлин) и всех иных спо­со­бов, с по­мо­щью ко­то­рых мож­но бы­ло воз­дей­ст­во­вать на неё. Про­ти­во­пос­тав­ле­ние с са­мо­го на­ча­ла Н. б. та­мо­жен­но­му та­ри­фу вы­ра­зи­лось в их на­зва­нии.\n\nВ до­ку­мен­тах Все­мир­ной тор­го­вой ор­га­ни­за­ции уточ­ня­ет­ся, что Н. б. мо­гут при­ме­нять­ся на ос­но­ве дос­тиг­ну­тых в рам­ках этой ор­га­ни­за­ции спец. со­гла­ше­ний (в т. ч. по ли­цен­зи­ро­ва­нию им­пор­та, за­щит­ным ме­рам, ан­ти­дем­пин­го­вым про­це­ду­рам, та­мо­жен­ной оцен­ке, стра­не про­ис­хо­ж­де­ния, с. х-ву, ин­фор­мац. тех­но­ло­ги­ям, тек­сти­лю, тех­нич. барь­е­рам в тор­гов­ле, са­ни­тар­ным и фи­то­са­ни­тар­ным ме­рам).\n\nНаиболее распространенные в настоящее время нетарифные меры, воздействующие на международную торговлю, охватывают следующие категории:\nколичественные ограничения и сходные административные меры (импортные и экспортные квоты, лицензии, меры валютного контроля, запреты и др.);\nнетарифные сборы, финансовые меры, скользящие налоги, антидемпинговые и компенсационные пошлины, защитные меры;\nограничительная практика правительственных органов;\nсубсидии и другие дотации экспортерам или импортозамещающим отраслям, предпочтительная для национальных предприятий система размещения правительственных заказов, транспортные мероприятия, дискриминирующие иностранные грузы и предприятия, региональная политика развития, устанавливающая льготы отдельным регионам, дискриминационная политика в отношении иностранных инвестиций, дискриминационная налоговая, финансовая и валютная политика;\nтаможенные и другие пограничные формальности в том случае, когда они превышают нормальные общепринятые нормы, что превращает их в дополнительный барьер в торговле, в частности: методы таможенной оценки, система тарифной классификации, завышенные и произвольные требования к документам, необходимым для таможенного оформления, недостаточная правовая и судебная защита в спорах, возникающих по этому кругу вопросов;\nтехнические барьеры в торговле и стандарты в тех случаях, когда они затрудняют экспорт или импорт товаров или прямо дискриминируют иностранные товары;\nсанитарные и фитосанитарные нормы. Практика последних десятилетий показывает, что применение этих норм постепенно становится все более заметным направлением их использования не столько по прямому назначению, а в качестве нетарифных мер. Анализ деятельности системы разрешения споров ВТО за последние 15 лет – яркое тому подтверждение.\n\nВлияние наиболее распространенных видов нетарифных барьеров на международную торговлю:\nЛицензия**. Страны могут использовать лицензии для ограничения импорта\nтоваров определенными предприятиями. Если компании выдана торговая лицензия, ей разрешается импортировать товары, которые в противном случае\nбыли бы ограничены для торговли в стране. Использование лицензионных систем в качестве инструмента регулирования внешней торговли основано на\nмногих международных стандартах. В частности, эти соглашения включают\nнекоторые положения генерального соглашения по тарифам и торговле, а также\nсоглашения о процедурах лицензирования импорта, заключенные в рамках\nГАТТ(Генеральное соглашение по тарифам и торговле).\nКвоты**. Страны часто выдают квоты на экспорт и импорт товаров и услуг.\nПри помощи этих квот страны договариваются об определенных лимитах на\nтовары и услуги, разрешенные к ввозу в ту или иную страну. В большинстве\nслучаев нет никаких ограничений на импорт этих товаров и услуг до тех пор,\nпока страна не достигнет своей квоты, которую она может установить на определенный период времени. Кроме того, квоты часто используются в международных торговых лицензионных соглашениях.\nЭмбарго**. Это когда одна или несколько стран официально запрещают торговлю определенными товарами и услугами с другой страной. Правительства могут принимать такие меры в поддержку своих конкретных политических или\nэкономических целей.\nСтандарты**. Одним из наиболее распространенных видов нетарифных барьеров являются стандарты. Страны вводят определенный стандарт маркировки,\nклассификации и тестирования продуктов для обеспечения соответствия отечественных и иностранных продуктов внутренним стандартам. В случае несоответствия этим стандартам идет ограничение продаж соответствующего продукта.\nВалютный контроль и валютные ограничения**. Данные ограничения занимают важное место среди методов нетарифного регулирования внешнеэкономической деятельности. Валютные ограничения представляют собой управление операциями между национальными и иностранными операторами либо с помощью ограничения предложения иностранной валюты, тем самым ограничивая импорт, либо путем государственного манипулирования обменными курсами (в целях стимулирования экспорта и ограничения импорта).\nСанкции**. Страны вводят санкции против других стран, чтобы ограничить их торговую деятельность. Санкции могут включать усиленные административные меры или дополнительные таможенные и торговые процедуры, которые ограничивают или замедляют способность страны вести торговлю.\n\nОсновные каналы воздействия на торговлю:\nнетарифные барьеры могут ограничить полный доступ к рынкам (как в случае квот).\nони могут увеличить стоимость ведения бизнеса. Нетарифные барьеры, которые увеличивают издержки бизнеса, могут быть специфическими – например, соблюдение определенных стандартов на продукцию- или более общими, такими как более строгие документальные и таможенные процедуры.\n\nРесурсы:\nНетарифные меры в современной международной торговле: некоторые вопросы теории, практика и правила ВТО, интересы России\nНЕТАРИФНЫЕ БАРЬЕРЫ И ИХ ВЛИЯНИЕ НА МЕЖДУНАРОДНУЮ ТОРГОВЛЮ\n\nВсемирная торговая организация (ВТО): функции, задачи, система соглашений, новые направления многосторонних торговых переговоров.\n\nФункции Всемирной торговой организации (ВТО):\n   Регулирование международной торговли, обеспечение свободной и честной конкуренции;\n   Разрешение торговых споров между государствами-членами;\n   Предоставление форума для переговоров по вопросам международной торговли;\n   Обе спечение развития экономических связей между государствами.\nЗадачи ВТО:\n   Создание условий для расширения международной торговли и увеличения объемов экспорта;\n   Снижение тарифных и нетарифных барьеров на международную торговлю;\n   Предотвращение использования протекционистских мер в торговле;\n   Защита прав интеллектуальной собственности.\nСистема соглашений ВТО:\n   Соглашение о создании ВТО, основной документ, определяющий задачи и функции организации;\n   Соглашение по тарифам и торговле (ГАТТ), устанавливающее правила торговли и ограничивающее уровень тарифов;\n   Соглашение по аспектам прав интеллектуальной собственности, которое регулирует защиту интеллектуальной собственности, включая патенты, авторские - права и товарные знаки.\nНовые направления многосторонних торговых переговоров:\n   Расширение сферы деятельности ВТО на новые секторы экономики, включая услуги и интеллектуальную собственность;\n   Разработка новых мер по регулированию международной торговли, таких как решение проблем, связанных с электронной коммерцией;\n   Поиск более эффективных механизмов разрешения торговых споров;\n   Развитие взаимодействия между ВТО и другими международными организациями.\n\nРоссия в ВТО: сложности вступления, принятые обязательства, экономические последствия. Характеристика современного участия РФ в ВТО.\n\nСложности вступления России в ВТО:\n   Вступление России в ВТО заняло более 18 лет и было связано с различными сложностями, такими как переговоры с другими государствами, в том числе с США, Евросоюзом и Китаем, а также с противодействием со стороны отдельных отраслей российской экономики.\nПринятые обязательства России при вступлении в ВТО:\n   Снижение таможенных пошлин на большинство товаров и услуг;\n   Согласование правил и процедур импорта и экспорта товаров;\n   Разрешение торговых споров в соответствии с правилами ВТО;\n   Подписание многосторонних соглашений и протоколов с другими государствами.\nЭкономические последствия вступления России в ВТО:\n   Рост объемов международной торговли;\n   Увеличение конкуренции на внутреннем рынке;\n   Снижение тарифных барьеров и других препятствий для въезда и выезда товаров и услуг;\n   Развитие экспортной ориентации российской экономики.\nХарактеристика современного участия России в ВТО:\n   Россия активно участвует в работе организации, принимает участие в переговорах и разрешении торговых споров;\n   Однако российские экспортеры продолжают сталкиваться с рядом препятствий при экспорте товаров, таких как технические преграды и санитарные нормы;\n   Россия выступает за сохранение правил многосторонней торговой системы, однако также разрабатывает свою более протекционистскую торговую политику.\n\nСистема внешнеторгового регулирования в ЕАЭС и в России.\n\nСистема внешнеторгового регулирования в ЕАЭС:\n   ЕАЭС (Евразийский экономический союз) - таможенный союз, созданный Россией, Беларусью, Казахстаном, Арменией и Киргизией;\n   Члены ЕАЭС осуществляют единую внешнеторговую политику, включая согласование ставок таможенных пошлин, проведение внешнеторговых переговоров и координацию мер по борьбе с контрабандой и незаконной торговлей;\n   ЕАЭС имеет единую систему таможенного регулирования и таможенного контроля.\nСистема внешнеторгового регулирования в России:\n   Россия имеет свою систему внешнеторгового регулирования, включая правила и процедуры импорта и экспорта товаров, включая таможенное оформление и таможенную очистку;\n   Россия также регулирует ввоз и вывоз отдельных товаров, включая продукты питания, лекарства и товары, связанные с национальной безопасностью;\n   Россия может устанавливать тарифы на импорт и экспорт товаров, а также другие нетарифные меры регулирования внешней торговли.\nРазличия в системах внешнеторгового регулирования:\n   В России и в ЕАЭС имеются различные правила и процедуры внешнеторгового регулирования;\n   В рамках ЕАЭС действуют общие правила таможенного регулирования и контроля, а также единые ставки таможенных пошлин;\n   Россия имеет больше свободы в установлении собственных тарифов и нетарифных мер регулирования внешней торговли.\nСотрудничество России и ЕАЭС:\n   Россия активно сотрудничает с государствами-членами ЕАЭС в области внешнеторгового регулирования;\n   Россия и другие государства-члены ЕАЭС стремятся к согласованию тарифных ставок и прочих мер регулирования торговли;\n   Россия поддерживает развитие\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/04-international-capital-movement",
            "title": "Раздел 4. Международное движение капитала",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "\n\nФормы, структура и масштабы международного движения капитала.\nМасштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны.\nМеждународная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций.\n\nМеждународное движение капитала - это процесс переноса финансовых ресурсов из одной страны в другую в целях получения прибыли или увеличения доходности инвестиций. Международное движение капитала имеет важное значение для экономического развития и роста стран, позволяя привлекать капитал для инвестирования в различные отрасли экономики.\n\nОднако, международное движение капитала также может приводить к некоторым рискам, таким как волатильность финансовых рынков, кризисы и паники, а также экономическую зависимость различных стран друг от друга. В условиях глобализации и ускоренного развития технологий международное движение капитала становится все более интенсивным, а страны развивающегося мира становятся все более зависимыми от притока иностранных инвестиций.\n\nОдним из важных инструментов регулирования международного движения капитала является налоговая политика, тарифное регулирование, контроль капитальных операций, международное сотрудничество и координация мер макроэкономической политики.\n\nФормы, структура и масштабы международного движения капитала.\n\nФормы международного движения капитала:\n   Прямые иностранные инвестиции (ПИИ), когда иностранные компании инвестируют в зарубежные предприятия;\n   Портфельные инвестиции, когда инвесторы покупают ценные бумаги зарубежных компаний;\n   Кредитование и заемы, когда банки и другие финансовые институты предоставляют заемные средства зарубежным компаниям и правительствам;\n   Инвестиции в недвижимость и другие активы.\nСтруктура международного движения капитала:\n   Большая часть международного движения капитала сосредоточена в развитых странах, таких как США, Великобритания, Германия и Япония;\n   Страны с развивающимися экономиками также получают значительные инвестиции, но общий объем этих инвестиций ниже, чем в развитых странах;\n   Инвестиции в различные отрасли экономики распределены неравномерно: наибольшие объемы инвестиций приходятся на финансовый сектор, производство и добычу полезных ископаемых, а также на недвижимость.\nМасштабы международного движения капитала:\n   Общий объем международного движения капитала достигает нескольких триллионов долларов ежегодно;\n   Портфельные инвестиции составляют значительную долю международного движения капитала, особенно в развитых странах;\n   Прямые иностранные инвестиции являются важным источником капитала для многих развивающихся стран, особенно для стран с низким уровнем инвестиционной активности.\n\nМасштабы, динамика и география прямых иностранных инвестиций. Основные инвестирующие и принимающие страны.\n\nМасштабы и динамика ПИИ:\n   Объем ПИИ составляет многие миллиарды долларов ежегодно;\n   Общий объем ПИИ постепенно растет на протяжении последних десятилетий;\n   Развитые страны являются наиболее активными инвесторами в ПИИ, однако страны с развивающимися экономиками также увеличивают свою долю в общем - объеме ПИИ.\nГеография ПИИ:\n   Европа, Северная Америка и Азия являются наиболее активными регионами по привлечению ПИИ;\n   Крупнейшими принимающими странами являются США, Китай, Великобритания, Германия, Франция, Индия, Бразилия, Россия и др.;\n   Основными инвестирующими странами являются США, Япония, Германия, Великобритания, Франция, Нидерланды и др.\nОсновные направления ПИИ:\n   Основные - это производство, добыча полезных ископаемых, финансовый сектор, телекоммуникации, информационные технологии и другие - секторы экономики;\n   Отрасли с высокой доходностью и быстрым ростом обычно привлекают большую часть инвестиций.\nРоссия и ПИИ:\n   Россия привлекает значительные объемы ПИИ, особенно в нефтегазовом секторе, добыче полезных ископаемых, транспортной инфраструктуре, финансовом - секторе и других секторах экономики;\n   Россия также активно инвестирует в другие страны, в том числе в страны СНГ, Европу, Азию и Африку;\n   Однако, политическая нестабильность, низкая инвестиционная активность в отдельных секторах экономики и низкий уровень инновационности могут - снижать привлекательность России для иностранных инвесторов.\n\nМеждународная инвестиционная позиция России: динамика и состав иностранных активов и обязательств. Условия для масштабного привлечения в Россию иностранных инвестиций.\n\nМеждународная инвестиционная позиция России:\n   Международная инвестиционная позиция России включает в себя иностранные активы и обязательства;\n   В 2021 году иностранные активы России составили около $520 млрд, а обязательства - около $535 млрд;\n   Состав иностранных активов России включает преимущественно прямые иностранные инвестиции, портфельные инвестиции и резервы Центрального банка.\nДинамика международной инвестиционной позиции России:\n   С 2014 года международная инвестиционная позиция России снижается, главным образом из-за сокращения объемов внешней торговли и падения цен на нефть и газ;\n   Однако в последнее время инвестиционная активность в России начала постепенно восстанавливаться.\nУсловия для масштабного привлечения иностранных инвестиций в Россию:\n   Устойчивость макроэкономической ситуации и рост экономики;\n   Политическая стабильность и предсказуемость;\n   Улучшение инвестиционного климата и снижение административных барьеров;\n   Развитие инфраструктуры и увеличение эффективности бизнес-процессов;\n   Разработка и реализация национальных программ по развитию ключевых отраслей экономики;\n   Сотрудничество с международными организациями и инвесторами.\nОсобенности инвестирования в Россию:\n   Высокий уровень риска, связанный с политической цнестабильностью, санкциями и неопределенностью законодательной базы;\n   Высокие инвестиционные риски, связанные с изменением экономической ситуации и колебаниями цен на нефть и газ;\n   Высокий уровень коррупции и бюрократии, который может затруднять реализацию инвестиционных проектов;\n   Высокий потенциал доходности, особенно в некоторых отраслях экономики, таких как нефтегазовая, транспортная.\n\nРесурсы:\n\nКонференция ООН по торговле и развитию (ЮНКТАД/UNCTAD - United Nations Conference on Trade and Development):\nДоклады о мировых инвестициях\n\nОрганизация экономического сотрудничества и развития (ОЭСР/Organisation for Economic Co-operation and Development, OECD)):\n\nОтчеты по РФ\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/05-international-foreign-exchange-market",
            "title": "Раздел 5. Международный валютный рынок",
            "description": "Кандидатский минимум 08.00.14 «Мировая экономика» - Тезисы ответов",
            "content": "\n\nПонятие иностранной валюты. Валютный курс и паритет покупательной способности валюты.\nМировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития.\nВиды операций на валютном рынке. Хеджирование валютных рисков.\nФакторы, влияющие на формирование валютного курса.\n\nПонятие иностранной валюты. Валютный курс и паритет покупательной способности валюты.\n\nПонятие иностранной валюты:\nИностранная валюта - это денежные единицы других стран, которые используются в международных финансовых - операциях;\nИностранная валюта может быть использована для оплаты товаров и услуг, инвестирования, торговли на - международном валютном рынке и других целей.\n\nВалютный курс:\nВалютный курс - это отношение стоимости одной валюты к другой;\nКурс валют может изменяться в зависимости от спроса и предложения на международном валютном рынке, а - также от макроэкономической ситуации в странах-участниках;\nНекоторые факторы, которые могут влиять на курс валют, включают в себя инфляцию, процентные ставки, - политическую стабильность, торговый баланс и т.д.\n\nПаритет покупательной способности валюты:\nПаритет покупательной способности (PPP) - это теоретический концепт, который описывает ситуацию, когда - две разные валюты имеют одинаковую стоимость в разных странах;\nВ рамках PPP цена товаров и услуг должна быть одинаковой в разных странах при использовании курса - валют, который учитывает стоимость товаров в каждой стране;\nPPP может использоваться для оценки действительного курса валюты и прогнозирования изменений курса в - долгосрочной перспективе.\n\nИнструменты на международном валютном рынке:\nОперации на межбанковском валютном рынке;\nВалютные фьючерсы и опционы;\nСпот-рынок валют, т.е. операции на продажу и покупку валюты с моментальным исполнением;\nКредиты в иностранной валюте;\nИнвестирование в ценные бумаги и фонды, выраженные в иностранной валюте;\nКонверсионные операции и др.\n\nМировой валютный рынок: понятие, функции, размер, институциональная структура, тенденции развития.\n\nПонятие мирового валютного рынка:\nМировой валютный рынок - это рынок, на котором торгуются валюты разных стран;\nРынок включает в себя банки, биржи, фондовые рынки, государственные резервы и другие участники.\n\nФункции мирового валютного рынка:\nОбеспечение доступности иностранной валюты для международных торговых операций;\nОбеспечение ликвидности на международном уровне;\nОпределение валютных курсов;\nФормирование цен на различные активы, связанные с валютами, такие как ценные бумаги и фьючерсы.\n\nРазмер мирового валютного рынка:\nРазмер мирового валютного рынка включает в себя все валюты мира и может быть оценен в несколько триллионов долларов;\nОсновными валютами на мировом валютном рынке являются доллар США, евро, японская йена, британский фунт стерлингов и швейцарский франк.\n\nИнституциональная структура мирового валютного рынка:\nМировой валютный рынок не имеет централизованной структуры и состоит из множества финансовых институтов и участников;\nОсновные участники рынка включают в себя центральные банки, коммерческие банки, инвестиционные фонды, хедж-фонды, фондовые биржи, брокерские фирмы и другие финансовые институты.\n\nТенденции развития мирового валютного рынка:\nГлобализация и расширение международной торговли;\nРазвитие электронной торговли на международном валютном рынке;\nИзменение роли национальных валют на мировом уровне;\nУсиление регулирования и надзора на мировом валютном рынке;\nРазвитие новых финансовых инструментов на мировом валютном рынке;\nИзменение конкурентной среды и структуры рынка на мировом уровне.\n\nРесурсы:\n\nДанные Банка международных расчетов (BIS) предоставляют информацию о международном валютном рынке, его размере и участниках: https://www.bis.org/statistics/rpfx19.htm\n\nВиды операций на валютном рынке. Хеджирование валютных рисков.\n\nВиды операций на валютном рынке:\nОперации на спот-рынке валют, включая операции покупки/продажи валюты на моментальной основе;\nФорвардные операции, которые позволяют сторонам договориться о покупке/продаже валюты в будущем по фиксированной цене;\nВалютные фьючерсы, которые позволяют инвесторам купить/продать валюту по заранее установленной цене в будущем;\nОпционы на валюту, которые позволяют покупателю опции купить/продать валюту по определенной цене в определенный момент времени;\nСвопы, которые являются договоренностью между двумя сторонами на обмен валютами с последующим возвратом валют.\n\nХеджирование валютных рисков:\nХеджирование - это стратегия защиты инвесторов от потенциальных убытков, связанных с колебаниями валютных курсов;\nХеджирование валютных рисков - это использование финансовых инструментов для снижения рисков, связанных с изменениями валютных курсов;\nПримеры инструментов хеджирования валютных рисков включают форвардные контракты, опционы, свопы и другие финансовые инструменты;\nХеджирование валютных рисков широко используется на международном уровне, особенно в бизнесе, где компании имеют множество обязательств и активов, выраженных в различных валютах;\nХеджирование валютных рисков может помочь компаниям снизить потенциальные риски, связанные с изменением валютных курсов, и защитить свои прибыли и активы.\n\nОдним из примеров компаний, использующих хеджирование валютных рисков, является международная сеть ресторанов McDonald's. Компания использует свою программу хеджирования для защиты себя от колебаний курсов валют, которые могут значительно влиять на ее операционную прибыль, в частности, на закупку продуктов и услуг в разных странах.\n\nКонкретнее, McDonald's использует финансовые деривативы, такие как валютные форварды и опционы, чтобы зафиксировать определенный курс обмена валют на будущее время. Это позволяет компании избежать возможных потерь при колебаниях курсов валют и защитить свою операционную прибыль. Например, если компания заключила сделку на закупку продуктов из другой страны на будущее время, она может использовать финансовые деривативы, чтобы защитить себя от валютных рисков, связанных с колебаниями курсов обмена валют.\n\nФакторы, влияющие на формирование валютного курса.\n\nЭкономические факторы:\nИнфляция - рост инфляции может привести к понижению стоимости валюты;\nПроцентные ставки - высокие процентные ставки могут привлечь иностранных инвесторов и увеличить спрос на валюту;\nБаланс платежей - неравновесие между экспортом и импортом может привести к изменению валютного курса;\nЭкономический рост - высокий экономический рост может привести к увеличению спроса на валюту.\n\nПолитические факторы:\nСтабильность правительства - нестабильность политической ситуации может привести к понижению стоимости валюты;\nГеополитические события - например, военные конфликты или террористические акты могут привести к понижению стоимости валюты;\nРегулирование валютного рынка - правительственные органы могут вводить различные меры, такие как интевенции на валютном рынке, которые могут повлиять на валютный курс.\n\nФакторы спроса и предложения:\nИзменения в спросе на валюту и предложении валюты могут привести к изменению валютного курса;\nНаличие большого спроса на валюту может привести к повышению ее стоимости, тогда как избыток валюты на рынке может привести к ее понижению.\n\nТехнические факторы:\nТехнические анализы - основанные на данных о прошлых изменениях валютного курса, а также на технических показателях, могут помочь предсказать будущие изменения валютного курса;\nНаличие больших игроков на рынке - таких как крупные банки или хедж-фонды, может оказывать значительное влияние на валютный курс.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/canditate-minimum/languages-requirements",
            "title": "Требования по иностранным языкам",
            "description": "Основные требования к кандидатскому экзамену по иностранным языкам",
            "content": "\nНеобходимые документы:\n\nЗаявление на имя заведующего кафедрой. В з-и указать название и шифр специальности\nМонография на иностранном языке.\nРеферат на русском языке (объем 21-8 стр) по прочитанной лит-ре объемом 300 стр.\n   Реферат, подписанный автором, должен иметь заключения, а также библиографию (список использованной литературы).\nГлосарий (словарь специальных терминов) - ен менее 300 единиц.\nОтзыв от научного руководителя или специалиста по данной дисциплине.\n\nСодержание экзамена:\n\nВышеуказанные документы сдаются н а кафедру иностраных яызков за 10 дней до экзамена.\nЧтение, перевод со (словарем) на руский язык оп специальности оригинального текста. Объем 2500 печ знаков. Время на подготовку 45 мин.\nФорма проверки - чтение части текста вслух, выборочная проверка подготовленного перевода (Если не\nвыполнен минимум 2тыс знаков - экзамен не продолжается).\nЧтение (просмотровое без словаря) оригинального газетного публицистического текста по специальности. Объем 2500 печ знаков. Время на подготовку - 5 мин.\nФорма проверки - передача содержания текста на русском языке (реферирование).\nЧтение оригинального газетно-публицистического текста без словаря. Объем - 2500 печатных знаков. Время н а подготовку - 15-20 мин.\nФорма проверки - передача содержания текста на иностранном языке и беседа на иностранном языке по прочитанному тексту.\nБеседа на иностранном по вопросам, связынным со специальностью и научной работой аспиранта (защита реферата по теме исследования; требования к реферату см. выше)\n\n\nПримечание: вышеуказанные документы иностранных языков за 14 дней до экзамена тексту.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/israel-notes",
            "title": "Заметки по Израилю",
            "description": "Заметки по Израилю",
            "content": "\nСтатистика - показатели по Израилю",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/disser/utils/text_2_short",
            "title": "Генерация аннотации",
            "description": "Генерация краткого содержания текста",
            "content": "\n\n            Сгенерировать\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/_index",
            "title": "Карманная книга по Python",
            "description": "Карманная книга по Python + Топ 100 вопросов по Python 2023",
            "content": "\nСкачать | PDF обновление 2023/02/27\n\nЗдравствуйте!\n\nЯ рад приветствовать вас на страницах этого хендбука по языку программирования Python.\n\nPython - это мощный и гибкий язык программирования, который находит применение в самых различных областях, от разработки веб-приложений и научных исследований до создания игр и машинного обучения. Однако, как и любой другой язык программирования, Python может показаться довольно сложным для начинающих.\n\nЭтот хендбук призван помочь вам изучить основы языка Python, научиться использовать различные библиотеки и инструменты, а также дать ответы на самые часто задаваемые вопросы о языке. Я постарался сделать материал доступным и понятным для всех, даже для тех, кто никогда не программировал.\n\nХендбук по Python - это идеальное решение для тех, кто хочет быстро овладеть основами языка и начать программировать. Книга содержит более 50 тем, которые охватывают все основные конструкции языка, от простых типов данных и операторов до продвинутых тем, таких как объектно-ориентированное программирование и обработка исключений.\n\nКаждая тема описывается кратко и доступно, чтение каждой занимает около 2 минут. Это означает, что вы можете быстро пройти через всю книгу и получить краткий, но полный обзор языка.\n\nХендбук по Python - это идеальный выбор для тех, кто хочет быстро начать программировать на языке Python. Вы сможете быстро овладеть основными конструкциями и перейти к созданию своих собственных программ.\n\nЯ уверен, что этот хендбук поможет вам стать более уверенными в использовании Python, и поможет вам создавать более эффективные программы. Не стесняйтесь задавать вопросы и искать помощи, если вам что-то не ясно - только так можно добиться настоящих результатов в программировании. Удачи!\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/_index",
            "title": "I - Основы",
            "description": "Python 101",
            "content": "\nДанная глава посвящена изучению основ языка программирования Python, который используется для решения различных задач в области науки, инженерии, экономики и многих других областях. В ходе обучения вы познакомитесь с различными типами данных, операторами, условиями, циклами, функциями и классами, а также научитесь работать с файлами и модулями.\n\nЭтот материал будет полезен как новичкам, так и опытным программистам, желающим расширить свои знания в области Python.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/classes",
            "title": "Классы",
            "description": "Python 101",
            "content": "\n\nВсе в Python является объектом. Это означает, что каждая сущность в Python имеет методы и значения. Причина в том, что в основе всего лежит класс.\n>> x = \"Some String\"\n>> dir(x)\n['add', 'class', 'contains', 'delattr', 'doc', 'eq',\n'format', 'ge', 'getattribute', 'getitem', 'getnewargs',\n'getslice', 'gt', 'hash', 'init', 'le', 'len', 'lt',\n'mod', 'mul', 'ne', 'new', 'reduce', 'reduce_ex', 'repr',\n'rmod', 'rmul', 'setattr', 'sizeof', 'str', 'subclasshook',\n'_formatter_field_name_split', '_formatter_parser', 'capitalize', 'center', 'count',\n'decode', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'index', 'isalnum',\n'isalpha', 'isdigit', 'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust',\n'lower', 'lstrip', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition',\n'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title',\n'translate', 'upper', 'zfill']\n\nЗдесь у нас есть строка, присвоенная переменной x. Может показаться, что это не так много, но у этой строки есть много методов. Если вы используете ключевое слово dir в Python, то сможете получить список всех методов, которые можно вызвать для вашей строки.\n\nТехнически мы не должны напрямую вызывать методы, начинающиеся с символов подчеркивания, но их можно вызвать.\n\nЭто значит, что строка основана на классе, а x- это экземпляр этого класса!\n\nВ Python мы можем создавать свои собственные классы.\n\nСоздание класса\n\nСоздать класс в Python очень просто. Вот очень простой пример:\n\nclass MyClass:\n    my_attribute = 42\n\n    def my_method(self):\n        print(\"Hello, world!\")\n\nЗдесь мы создали класс MyClass, который имеет атрибут my_attribute со значением 42 и метод my_method, который просто выводит сообщение в консоль.\n\nАтрибуты класса могут быть доступны как через экземпляр класса, так и напрямую через класс:\n\nprint(MyClass.my_attribute)  # 42\n\nmy_object = MyClass()\nprint(my_object.my_attribute)  # 42\n\nМетоды класса принимают в качестве первого аргумента экземпляр класса (self) и могут иметь доступ к атрибутам класса и вызывать другие методы класса:\n\nclass MyClass:\n    my_attribute = 42\n\n    def my_method(self):\n        print(self.my_attribute)\n\n    def my_other_method(self):\n        self.my_method()\n\nЗдесь мы добавили метод my_other_method, который просто вызывает метод my_method.\n\nВ Python существуют специальные методы, которые определяются с помощью двойного подчеркивания в начале и в конце названия метода. Например, метод init используется для инициализации экземпляра класса при его создании (конструкторы):\n\nclass MyClass:\n    def init(self, name):\n        self.name = name\n\n    def say_hello(self):\n        print(\"Hello, \" + self.name + \"!\")\n\nЗдесь мы определили метод init, который принимает аргумент name и сохраняет его в атрибуте name. Метод say_hello использует этот атрибут для вывода сообщения.\n\nКлассы могут наследовать друг от друга, позволяя переопределять и расширять функциональность базового класса. Для этого используется ключевое слово super:\n\nЧто такое self?\n\nКлассы Python нуждаются в способе обращения к самим себе. Это не какое-то самовлюбленное созерцание класса. Напротив, это способ отличить один экземпляр от другого.\n\nСлово self - это способ самоописания любого объекта, в буквальном смысле.\n\nclass Person:\n    def init(self, name, age):\n        self.name = name\n        self.age = age\n\n    def introduce(self):\n        print(\"My name is {} and I'm {} years old.\".format(self.name, self.age))\n\nperson1 = Person(\"Alice\", 25)\nperson1.introduce()\n\nЗдесь self.name и self.age представляют атрибуты объекта Person, который вызывает метод introduce. Без использования self мы не могли бы получить доступ к атрибутам объекта из метода.\n\nНаследование\n\nНаследование - это механизм, который позволяет создавать новый класс на основе уже существующего, наследуя его свойства и методы. Класс, от которого наследуется новый класс, называется родительским классом, а новый класс - дочерним классом.\n\nДочерний класс может использовать свойства и методы родительского класса, а также добавлять свои собственные свойства и методы. Это позволяет создавать более сложные иерархии классов, где дочерние классы наследуют общие свойства и методы от родительского класса, но могут быть уникальными в других отношениях.\n\nclass Animal:\n    def init(self, name, species):\n        self.name = name\n        self.species = species\n\n    def speak(self):\n        print(\"I am an animal.\")\n\nclass Dog(Animal):\n    def init(self, name, breed):\n        super().init(name, species=\"Canis\")\n        self.breed = breed\n\n    def speak(self):\n        print(\"Woof!\")\n\nclass Cat(Animal):\n    def init(self, name, color):\n        super().init(name, species=\"Felis\")\n        self.color = color\n\n    def speak(self):\n        print(\"Meow!\")\n\ndog = Dog(\"Buddy\", \"Golden Retriever\")\ncat = Cat(\"Luna\", \"Black\")\n\nprint(dog.name)   # Output: Buddy\nprint(dog.breed)  # Output: Golden Retriever\ndog.speak()       # Output: Woof!\n\nprint(cat.name)   # Output: Luna\nprint(cat.color)  # Output: Black\ncat.speak()       # Output: Meow!\n\nВ этом примере класс Animal является родительским классом для классов Dog и Cat.\n\nКласс Dog наследует свойства name и species от класса Animal и добавляет свой собственный атрибут breed.\n\nКласс Cat также наследует свойства name и species от класса Animal и добавляет свой собственный атрибут color.\n\nУ каждого класса есть свой метод speak, который переопределяет метод speak родительского класса Animal. Когда вызывается метод speak для экземпляра класса Dog, выводится строка \"Woof!\", а когда вызывается для экземпляра класса Cat, выводится строка \"Meow!\". (Полиморфизм)\n\nРесурсы:\n\nhttps://vegibit.com/python-abstract-base-classes/\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/comprehensions",
            "title": "Генераторы",
            "description": "Python 101",
            "content": "\nВ языке Python есть несколько методов создания списков и словарей, которые известны как генераторы.\n\nГенераторы списков\n\nГенератор списка - это выражение, которое генерирует список значений на основе каких-то правил. Вместо того, чтобы создавать список целиком и хранить его в памяти, генератор списка генерирует значения по мере их запроса.\n\nsquares = [x*x for x in range(10)]\n\nЭта строка создает генератор списка, который генерирует квадраты чисел от 0 до 9. Затем можно перебрать элементы этого генератора с помощью цикла:\n\nВ Python есть функция range, которая может возвращать список чисел. По умолчанию она возвращает целые числа, начиная с 0 и заканчивая числом, которое вы ей передали, но не включая его. В данном случае она возвращает список, содержащий целые числа 0-9.\n\nfor square in squares:\n    print(square)\n\nГенераторы словарей\n\nГенератор словаря работает аналогично генератору списка, но вместо списка мы создаем словарь с помощью фигурных скобок и пары \"ключ: значение\".\n\nmy_dict = {x: x**2 for x in range(5)}\nprint(my_dict)\nВывод: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\nГенератор множеств\n\nГенератор множества используется аналогично генератору списка, но вместо списка мы создаем множество с помощью фигурных скобок.\n\nmy_set = {x**2 for x in range(5)}\nprint(my_set)\nВывод: {0, 1, 4, 9, 16}\n\nЗдесь мы создаем множество my_set с элементами, равными квадратам чисел от 0 до 4.\n\nРесурсы:\n\nhttps://vegibit.com/python-comprehension-tutorial/\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/conditionals",
            "title": "Условия",
            "description": "Python 101",
            "content": "\nВ каждом компьютерном языке есть хотя бы один условный оператор. Чаще всего этот оператор представляет собой структуру if/elif/else.\n\nВ Python 3.10 добавилась структура match/case\n\nОператор if\n\nПозволяет выполнить блок кода, если определенное условие истинно\n\nx = 5\nif x > 0:\n    print(\"x is positive\")\nelif x  0 and y > 0:\n    print(\"Both x and y are positive\")\nif x > 0 or y > 0:\n    print(\"At least one of x and y is positive\")\nif not x < 0:\n    print(\"x is not negative\")\n\nmy_list = [1, 2, 3, 4]\nx = 10\nif x not in my_list:\n    print(\"'x' is not in the list, so this is True!\")\n\nПроверка на ничто (None)\n\nВ Python None используется, чтобы обозначить отсутствие значения. Это можно использовать в условных операторах, чтобы проверить, имеет ли переменная значение None.\n\nНапример, если мы хотим проверить, имеет ли переменная x значение None, мы можем написать:\n\nif x is None:\n    print(\"x is None\")\n\nМы также можем использовать оператор is not для проверки, имеет ли переменная значение, отличное от None:\n\nif x is not None:\n    print(\"x is not None\")\n\nЗдесь мы используем условный оператор if, чтобы проверить, имеет ли переменная x значение None. Если это так, мы выводим сообщение \"x is None\". Если переменная x имеет какое-то другое значение, мы ничего не выводим.\n\nЭто может быть полезно, если мы не знаем, какое значение будет иметь переменная, или если переменная может быть пустой.\n\nif name == “main”\n\nОператор if name == \"main\" используется для определения, запущен ли файл напрямую или импортирован как модуль. Если файл запущен напрямую, блок кода внутри этого условия будет выполнен, если же файл импортирован как модуль, этот блок кода не будет выполнен:\n\nif name == \"main\":\nкод, который будет выполнен только при запуске файла напрямую\n\nРасполагается в конце файла. Это говорит Python, что вы хотите выполнить следующий код, только если эта программа будет выполнена как отдельный файл.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/dict",
            "title": "Словари",
            "description": "Python 101",
            "content": "\nСловарь - это коллекция, которая позволяет хранить пары ключ-значение. В отличие от списков, словари не имеют порядка, и доступ к элементам словаря осуществляется по ключу, а не по индексу.\n\nСоздание\n\nДля создания словаря используется фигурная скобка {} с ключами и значениями, разделенными двоеточием. Можно также использовать функцию dict() для создания словаря.\n\nПример создания словаря:\n\nmy_dict = {'name': 'John', 'age': 25, 'city': 'New York'}\nmy_dict_2 = dict(name='Mary', age=30, city='London')\n\nДля доступа к элементам словаря используется ключ. Например, чтобы получить значение, связанное с ключом \"name\", можно использовать следующий синтаксис:\n\nname = my_dict['name']\nname = my_dict.get('name', None) # вернет None если такого ключа нету\n\nЧтобы добавить новый элемент в словарь, просто создайте новую пару ключ-значение:\n\nmy_dict['occupation'] = 'engineer'\n\nМетоды\n\nkeys(): возвращает все ключи словаря\nvalues(): возвращает все значения словаря\nitems(): возвращает все пары ключ-значение словаря в виде кортежей\n\nА также: 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'\n\nkeys = my_dict.keys() # Получение всех ключей словаря\nvalues = my_dict.values() # Получение всех значений словаря\nitems = my_dict.items() # Получение всех пар ключ-значение словаря\n\nПрименение\n\nСловари - это очень мощный инструмент, который часто используется в программировании для хранения и управления данными.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/exception_handling",
            "title": "Обработка исключений",
            "description": "Python 101",
            "content": "\nОбработка исключений - это механизм, который позволяет обработать возможную ошибку, которая может возникнуть в процессе выполнения программы.\n\nВ Python эта конструкция исключений обычно обернута в так называемый try/except.\n\nОператор try-except - это основной инструмент для обработки исключений. Код, который может вызвать исключение, помещается в блок try. Если исключение возникает, то Python переходит в блок except, где вы можете обработать исключение и выполнить соответствующий код.\n\ntry:\n    x = int(input(\"Введите число: \"))\n    result = 100 / x\nexcept ZeroDivisionError:\n    print(\"Деление на ноль!\")\nelse:\n    print(f\"Результат: {result}\")\nfinally:\n    print(\"Конец программы\")\n\nВ этом примере программа просит пользователя ввести число, которое будет использоваться в делении на 100. Если пользователь вводит 0, то возникает исключение ZeroDivisionError, которое обрабатывается блоком except.\n\nВ случае, если исключение не возникает, программа выполняет блок else. Независимо от того, возникает исключение или нет, блок finally всегда будет выполнен.\n\nКроме того, вы можете использовать несколько блоков except для обработки разных типов исключений.\n\ntry:\n    x = int(input(\"Введите число: \"))\n    result = 100 / x\nexcept ZeroDivisionError:\n    print(\"Деление на ноль!\")\nexcept ValueError:\n    print(\"Неверный формат числа!\")\nelse:\n    print(f\"Результат: {result}\")\nfinally:\n    print(\"Конец программы\")\n\nПомимо этого, можно использовать операторы try-except внутри функций, чтобы обрабатывать исключения, возникающие во время их выполнения.\n\nВ Python используются операторы raise и assert, которые позволяют вызвать исключение в явном виде, когда это необходимо.\n\nПример использования оператора raise:\n\nx = -1\nif x < 0:\n    raise ValueError(\"Число должно быть положительным!\")\n\n\nПример использования оператора assert:\n\nx = 10\nassert x < 0, \"Число должно быть отрицательным!\"\n\nОператор assert проверяет истинность заданного выражения, и если оно является ложным, вызывает исключение AssertionError.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/file_io",
            "title": "Работа с файлами",
            "description": "Python 101",
            "content": "\nЧтение файла\n\nЧтобы прочитать файл в Python, вам нужно сначала открыть файл. Вы можете сделать это, используя функцию open(). Эта функция принимает два аргумента: имя файла и режим открытия файла.\n\nРежим открытия файла может быть \"r\" (чтение), \"w\" (запись) или \"a\" (добавление).\n\nПример, который читает файл \"example.txt\" в режиме чтения и выводит его содержимое на экран:\n\nwith open(\"example.txt\", \"r\") as f:\n    content = f.read()\n    print(content)\n\nМы используем оператор with, который автоматически закрывает файл после его использования. Функция read() читает содержимое файла и возвращает его в виде строки.\n\nКак читать файлы по частям\n\nЕсли файл очень большой, то может быть более эффективным читать его по частям.\n\nСамый простой способ читать файл по частям - использовать цикл. Для первого примера мы будем использовать цикл for:\n\nhandle = open(\"test.txt\", \"r\")\n\nfor line in handle:\n    print(line)\n\nhandle.close()\n\nЗдесь мы открываем файл в дескрипторе в режиме \"только чтение\", а затем используем цикл for для итерации по нему.\n\nВот пример, который читает файл по 100 байтов за раз:\n\nwith open(\"example.txt\", \"r\") as f:\n    while True:\n        chunk = f.read(100)\n        if not chunk:\n            break\n        print(chunk)\n\nЗдесь мы используем цикл while для чтения файла по частям. Функция read() читает 100 байтов за раз и возвращает их в виде строки. Если возвращаемая строка пустая, значит, мы достигли конца файла, и мы выходим из цикла.\n\nЗапись файлов\n\nЧтобы записать данные в файл в Python, вам также нужно открыть файл с помощью функции open(), но в режиме записи (\"w\") или добавления (\"a\"). Затем вы можете записать данные в файл, используя функцию write().\n\nВот пример, который записывает строку в файл \"example.txt\":\n\nwith open(\"example.txt\", \"w\") as f:\n    f.write(\"Hello, world!\")\n\nЗдесь мы используем функцию write(), чтобы записать строку в файл.\n\nИспользование оператора with\n\nВ Python есть небольшой встроенный оператор with, который можно использовать для упрощения чтения и записи файлов. Оператор with создает то, что в Python известно как менеджер контекста, который автоматически закроет файл, когда вы закончите его обработку. Давайте посмотрим, как это работает:\n\nwith open(\"test.txt\") as file_handler:\n    for line in file_handler:\n        print(line)\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/functions",
            "title": "Функции",
            "description": "Python 101",
            "content": "\nФункция - это структура, которую вы определяете. Вы можете решать, есть ли у них аргументы или нет. Вы можете добавить аргументы в виде ключевых слов и аргументы по умолчанию.\n\nФункция - это блок кода, который начинается с ключевого слова def, имени функции и двоеточия. Вот простой пример:\n\ndef a_function():\n    print(\"You just created a function!\")\n\nЭта функция ничего не делает, кроме вывода какого-то текста.\n\ndef add(a, b):\n    result = a + b\n    return result\n\nВ этом примере мы создали функцию add, которая принимает два аргумента a и b и возвращает их сумму.\n\nВызов функции происходит путем указания имени функции, за которым следуют аргументы в скобках.\n\nПример:\n\nresult = add(2, 3)\nprint(result) # выводит 5\n\nПустая функция (заглушка)\n\nИногда, когда вы пишете код, вы просто хотите написать определения функций, не вставляя в них никакого кода.\n\n def empty_function():\n        pass\n\nВсе функции что-то возвращают. Если не указать ей, что она должна что-то вернуть, то она вернет None.\n\nАргументы с ключевыми словами\n\nФункции также могут принимать аргументы в виде ключевых слов! На самом деле они могут принимать как обычные аргументы, так и аргументы с ключевыми словами. Значит, вы можете указать, какие ключевые слова какими являются, и передать их. Вы видели такое поведение в предыдущем примере.\n\n def keyword_function(a=1, b=2):\n        return a+b\n\nkeyword_function(b=4, a=5) # 9\n\nВы также могли бы вызвать эту функцию без указания ключевых слов. Эта функция также демонстрирует концепцию аргументов по умолчанию. Каким образом? Ну, попробуйте вызвать функцию вообще без аргументов!\n\nkeyword_function() # 3\n\nargs и *kwargs\n\nТакже функции могут принимать переменное число аргументов или аргументы с произвольными именами (как в словарях). Это делается с помощью операторов ` и *`.\n\n\ndef myfunc(*args):\n    for arg in args:\n        print(arg)\n\nЭта функция принимает переменное число аргументов и выводит их все на экран.\n\nФункции в Python также могут иметь аргументы со значениями по умолчанию. Если аргумент не передан при вызове функции, то будет использовано значение по умолчанию. Например:\n\ndef myfunc(a, b=10):\n    result = a + b\n    return result\n\nВ этом примере мы создали функцию myfunc, которая принимает два аргумента: a и b (по умолчанию равный 10). Если при вызове функции не указан второй аргумент, то он будет равен 10.\n\n\nФункции также могут принимать аргументы с ключевыми словами, которые представляют собой пары \"ключ-значение\". Эти аргументы позволяют явно указать, какое значение должно быть использовано для каждого параметра функции. Для определения аргументов с ключевыми словами используются двойные звездочки (**).\n\ndef print_values(**kwargs):\n    for key, value in kwargs.items():\n        print(key, value)\n\nprint_values(name='John', age=25, city='New York')\n\nВ этом примере функция print_values() принимает произвольное количество аргументов с ключевыми словами и выводит их на экран. При вызове функции передаются аргументы с ключевыми словами name, age, и city, и функция выводит их значения.\n\nАргументы с ключевыми словами особенно полезны, когда у функции есть множество параметров, и вы хотите явно указать, какое значение должно быть использовано для каждого параметра. Это также может быть полезно, если вы используете библиотеку, которая принимает множество аргументов, и вы хотите быть уверены, что вы передаете значения правильно.\n\nРесурсы:\n\nhttps://vegibit.com/python-function-tutorial/",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/ide",
            "title": "Среда разработки",
            "description": "Python 101",
            "content": "\n\nДля удобной работы с Python требуется хорошо настроенная рабочая среда. Я предпочитаю использовать Visual Studio Code - бесплатный редактор кода, разработанный Microsoft.\n\nДля начала, нужно установить Visual Studio Code на свой компьютер. Это можно сделать с помощью официального сайта https://code.visualstudio.com/.\n\nУстановка Python на MacOS и Linux очень проста. Для MacOS можно использовать менеджер пакетов brew, который позволяет установить последнюю версию Python одной командой:\n\nbrew install --cask visual-studio-code\n\nДля Linux, в зависимости от дистрибутива, используется свой менеджер пакетов. Например, для Ubuntu это можно сделать командой:\n\nsudo apt-get install code\n\n\nПосле установки необходимо установить расширение Python. Для этого необходимо перейти во вкладку \"Extensions\", найти \"Python\" и нажать кнопку \"Install\".\n\n\n\nСоздайте файл для проекта, например, example_1.py.\n\nНапишите код print(\"Hello, world!\") и запустите его, нажав на кнопку с треугольником справа вверху:\n\n\n\nVSCode запустит код и в нижнем окне программы в терминале вы увидите результат:\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/imports",
            "title": "Импорт модулей",
            "description": "Python 101",
            "content": "\nPython поставляется с большим количеством готового кода. Эти части кода известны как модули и пакеты.\n\nМодуль - это один импортируемый файл Python, а пакет состоит из двух или более модулей. Пакет может быть импортирован так же, как и модуль.\n\nВ Python вы можете импортировать модули из других файлов, чтобы использовать функции и переменные, определенные в этих модулях.\n\nimport\n\nPython предоставляет ключевое слово import для импорта модулей.\n\nДопустим, у нас есть два файла:\n\nФайл dog.py, содержащий следующий код:\n\ndef bark():\n    print('Гав-гав!')\nФайл main.py, в котором мы хотим использовать функцию bark из dog.py:\n\nimport dog\n\ndog.bark()\n\nМы импортируем модуль dog в main.py с помощью оператора import и затем можем вызывать функцию bark() через точку и имя модуля.\n\nfrom X import Y\n\nМы также можем импортировать определенные функции или переменные из модуля с помощью оператора from.\n\nДопустим, у нас есть файл math.py, содержащий функцию square, которая возводит число в квадрат:\n\ndef square(x):\n    return x ** 2\n\nВ файле main.py мы можем импортировать только функцию square из math.py:\n\nfrom math import square\n\nresult = square(5)\nprint(result)\n\nМы можем использовать square, как будто она была определена в main.py, и не нужно вызывать ее через точку и имя модуля.\n\nОбратите внимание, что если мы попытаемся вызвать какую-то другую функцию из math.py, которая не была импортирована, мы получим ошибку:\n\nfrom math import square\n\nОшибка: name 'add' is not defined\nresult = add(5, 6)\n\nimport *\n\nВ Python можно импортировать все функции из модуля одной командой. Для этого используется символ звездочки (*).\n\nВот пример:\n\nfrom math import *\n\nЭта команда импортирует все функции и константы из модуля math, и мы можем использовать их в нашем коде без префикса math.\n\nОднако, такой подход не рекомендуется, так как может привести к конфликту имен и ухудшить читаемость кода. Вместо этого, лучше явно указывать, какие функции и константы нужны для нашей программы.\n\n\nМодуль csv\nМодуль configparser\nЛогирование\nМодуль sys\nМодуль os\nМодуль email / smtplib\nМодуль sqlite\nМодуль subprocess\nМодуль потоков Thread\nМодуль asyncio",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/inputs",
            "title": "Ввод данных пользователем",
            "description": "Python 101",
            "content": "В Python вы можете запросить у пользователя ввод данных во время выполнения программы. Для этого используется функция input(), которая приостанавливает выполнение программы, ожидает ввода от пользователя и возвращает введенные данные в виде строки.\n\nname = input(\"Введите ваше имя: \")\nprint(\"Привет, \" + name + \"!\")\n\nПри запуске этого кода пользователь увидит приглашение \"Введите ваше имя:\", после чего он может ввести свое имя и нажать клавишу Enter. Затем программа поприветствует пользователя по имени.\n\nВы также можете использовать функцию int() для преобразования введенной строки в целое число. Например:\n\nage = int(input(\"Сколько вам лет? \"))\nprint(\"В следующем году вам будет\", age + 1)\n\nЭтот код запросит у пользователя возраст, преобразует его в целое число и выводит сообщение о том, сколько ему будет лет в следующем году.\n\nОбратите внимание, что функция input() всегда возвращает строку, поэтому необходимо преобразовывать введенные данные в нужный тип, если это необходимо.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/install",
            "title": "Установка Python",
            "description": "Python 101",
            "content": "\nДля установки Python на MacOS можно использовать менеджер пакетов brew. Для этого необходимо выполнить команду:\n\n\nbrew install python\n\nДля операционных систем на базе Linux также существуют менеджеры пакетов, которые можно использовать для установки Python. Например, для Ubuntu можно использовать команду:\n\nsudo apt-get install python\n\nДля Windows можно загрузить установочный пакет с официального сайта https://www.python.org/downloads/.\n\nПосле установки Python необходимо убедиться, что версия Python, установленная на компьютере, соответствует требованиям для запуска необходимых библиотек и инструментов, которые будут использоваться в процессе разработки.\n\nПроверить версию можно в терминале или командной строке набрав команду python.\n\n\n\nКак только мы запутили Python, можно писать код в терминале.\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/lists",
            "title": "Списки",
            "description": "Python 101",
            "content": "\nСписок в Python похож на массив в других языках.\n\nСоздание\n\nВ Python пустой список может быть создан следующими способами.\n\nmy_list = []\n>> my_list = list()\n\nМожно обращаться к элементам списка и кортежа по индексу, начиная с нуля. Например, чтобы получить доступ к первому элементу списка, можно использовать индекс 0:\n\nmy_list = [1, 2, 3, \"four\", 5.0]\nprint(my_list[0]) # выводит 1\n\nМожно также использовать срезы (slices) для получения подмножества элементов списка или кортежа. Например, чтобы получить первые три элемента списка, можно использовать срез [0:3]:\n\nmy_list = [1, 2, 3, \"four\", 5.0]\nprint(my_list[0:3]) # выводит [1, 2, 3]\n\nВы также можете создавать списки списков следующим образом:\n>> my_nested_list = [my_list, my_list2]\n>> my_nested_list\n[[1, 2, 3], ['a', 'b', 'c']]\n\nИногда возникает необходимость объединить два списка вместе. Первый способ - использовать метод extend:\n>> combo_list = []\n>> one_list = [4, 5]\n>> combo_list.extend(one_list)\n>> combo_list\n[4, 5]\n\nМожно просто сложить два списка вместе:\n>> my_list = [1, 2, 3]\n>> my_list2 = [\"a\", \"b\", \"c\"]\n>> combo_list = my_list + my_list2\n>> combo_list\n[1, 2, 3, 'a', 'b', 'c']\n\nМетоды\n\nМетоды списков - это функции, которые могут быть применены к спискам. Некоторые из наиболее распространенных методов:\n\nappend(): добавляет элемент в конец списка.\ninsert(): добавляет элемент в указанное место списка.\npop(): удаляет последний элемент списка и возвращает его.\nremove(): удаляет первый элемент списка с указанным значением.\nsort(): сортирует элементы списка по возрастанию.\nreverse(): переворачивает порядок элементов списка.\n\nПримеры использования методов:\n\nfruits = ['apple', 'banana', 'cherry']\nfruits.append('orange') # ['apple', 'banana', 'cherry', 'orange']\nfruits.insert(1, 'grape') # добавить по индексу 1: ['apple', 'grape', 'banana', 'cherry', 'orange']\nfruits.pop() # ['apple', 'grape', 'banana', 'cherry']\nfruits.remove('banana')\nfruits.sort() #['apple', 'cherry', 'grape']\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/loops",
            "title": "Циклы",
            "description": "Python 101",
            "content": "\nЦикл while\n\nЦикл while повторяет набор инструкций, пока заданное условие истинно. Каждый раз, когда выполняется набор инструкций, условие проверяется снова, и если оно продолжает быть истинным, то набор инструкций выполняется снова.\n\ni = 1\nwhile i < 6:\n    print(i)\n    i += 1\n\n1\n2\n3\n4\n5\n\nЦикл for\n\nЦикл for используется для прохождения через элементы в последовательности, такой как список или строка. В отличие от цикла while, в цикле for не нужно определять начальное условие или шаг увеличения.\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor x in fruits:\n    print(x)\n\napple\nbanana\ncherry\n\nОператоры break и continue\n\nОператор break позволяет выйти из цикла, когда выполнено определенное условие, даже если условие продолжает оставаться истинным.\nОператор continue позволяет пропустить определенные итерации цикла, когда выполняется определенное условие, и продолжить следующую итерацию.\n\nПример:\n\ni = 0\nwhile i < 6:\n    i += 1\n    if i == 3:\n        continue\n    print(i)\n    if i == 5:\n        break\n\n1\n2\n4\n5\n\nelse в циклах\n\nКонструкция else в циклах в Python выполняется, когда цикл завершается нормально, то есть без использования оператора break. Если оператор break используется в цикле, то блок кода, указанный после else, не будет выполняться.\n\nВ цикле while, конструкция else будет выполнена, когда условие цикла станет ложным, и все итерации будут выполнены.\n\nВ цикле for, конструкция else будет выполнена после последней итерации, когда больше нет элементов для итерации.\n\nnumbers = [1, 2, 3, 4, 5]\n\nfor num in numbers:\n    if num == 3:\n        print(\"Found 3\")\n        break\nelse:\n    print(\"3 not found\")\n\nВ этом примере, если число 3 найдено в списке numbers, то будет выведено \"Found 3\". Если число 3 не найдено в списке, то после окончания цикла будет выведено \"3 not found\".\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/numbers",
            "title": "Числа",
            "description": "Python 101",
            "content": "\nЧисловые типы данных в Python могут быть целыми числами (int), числами с плавающей запятой (float) и комплексными числами (complex).\n\nЦелые числа - это числа без дробной части, а числа с плавающей запятой - это числа с дробной частью.\n\nКомплексные числа представляются парой вещественных чисел и используются в математических расчетах.\n\n\na = 5  # целое число\nb = 3.14  # вещественное число\nc = 2 + 3j  # комплексное число\n\nprint(type(a))  # выведет\nprint(type(b))  # выведет\nprint(type(c))  # выведет\n\nPython поддерживает все стандартные арифметические операции: сложение, вычитание, умножение, деление, возведение в степень, целочисленное деление и остаток от деления.\n\na = 10\nb = 3\n\nprint(a + b)  # сложение, выведет 13\nprint(a - b)  # вычитание, выведет 7\nprint(a * b)  # умножение, выведет 30\nprint(a / b)  # деление, выведет 3.3333333333333335\nprint(a ** b)  # возведение в степень, выведет 1000\nprint(a // b)  # целочисленное деление, выведет 3\nprint(a % b)  # остаток от деления, выведет 1\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/operators",
            "title": "Операторы",
            "description": "Python 101",
            "content": "\nОператор присваивания\n\nОператор присваивания \"=\" используется для присвоения значения переменной. Например:\n\nx = 5\n\nАрифметические операторы\n\nАрифметические операторы используются для выполнения математических операций над числами.\n\nСложение\nВычитание\nУмножение\n/  # Деление\n%  # Остаток от деления\n** # Возведение в степень\n// # Целочисленное деление\n\nПример:\n\nx = 5\ny = 2\nprint(x + y)  # 7\nprint(x - y)  # 3\nprint(x * y)  # 10\nprint(x / y)  # 2.5\nprint(x % y)  # 1\nprint(x ** y) # 25\nprint(x // y) # 2\n\nОператоры сравнения\n\nОператоры сравнения используются для сравнения значений.\n\nРавно\n!= # Не равно\nБольше\n= # Больше или равно\n y)   # True\nprint(x = y)  # True\nprint(x  y and x > z) # True\nprint(x > y or x  y)       # False\n\nПобитовые операторы\n\nПобитовые операторы используются для выполнения операций с двоичными числами.\n\n&  # Побитовое И\n|  # Побитовое ИЛИ\n^  # Побитовое исключающее ИЛИ\n~  # Побитовое НЕ\nСдвиг вправо\n\nx = 5  # 0b101\ny = 3  # 0b011\nprint(x & y)  # 1  (0b001)\nprint(x | y)  # 7\n\nОператоры \"is\" и \"in\"\n\nОператор is используется для проверки, являются ли два объекта одним и тем же объектом в памяти.\n\nx = [1, 2, 3]\ny = [1, 2, 3]\n\nprint(x is y)  # False, потому что это два разных объекта в памяти\nprint(x == y)  # True, потому что содержание списков одинаковое\n\nОператор in используется для проверки, находится ли элемент в последовательности.\n\nx = [1, 2, 3]\nprint(2 in x)  # True\nprint(4 in x)  # False\n\nТернарный оператор\n\nТернарный оператор - это оператор, который позволяет записать короткое условие в одну строку. Он имеет следующий синтаксис: value_if_true if condition else value_if_false.\n\nx = 10\ny = 20\nmax_value = x if x > y else y\nprint(max_value)  # 20\n\nВ этом примере, если x больше y, то max_value будет равен x, иначе y.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/scope",
            "title": "Область видимости",
            "description": "Python 101",
            "content": "\nОбласть видимости, или scope, определяет, где переменные могут быть использованы в программе. В Python есть две основные области видимости: глобальная и локальная.\n\nПеременные, определенные внутри функции, имеют локальную область видимости. Это означает, что они могут быть использованы только внутри этой функции. Если попытаться использовать их вне функции, будет вызвано исключение.\n\nПеременные, определенные вне функции, имеют глобальную область видимости. Они могут быть использованы в любом месте программы, в том числе и внутри функций. Если внутри функции определить переменную с тем же именем, что и глобальная переменная, то функция будет использовать локальную переменную.\n\nПример:\n\nx = 10  # глобальная переменная\n\ndef my_func():\n    x = 5  # локальная переменная\n    print(\"Значение x внутри функции:\", x)\n\nmy_func()\nprint(\"Значение x вне функции:\", x)\n\nВывод:\n\nЗначение x внутри функции: 5\nЗначение x вне функции: 10\n\nВ этом примере мы создали глобальную переменную x со значением 10, а затем определили функцию my_func(), в которой мы создали локальную переменную x со значением 5. Внутри функции мы выводим значение локальной переменной x, а затем вызываем функцию и выводим значение глобальной переменной x.\n\n💡 Если мы попробуем изменить значение глобальной переменной x внутри функции, то мы получим ошибку:\n\nx = 10  # глобальная переменная\n\ndef my_func():\n    x = x + 5  # ошибка: переменная x не определена\n    print(\"Значение x внутри функции:\", x)\n\nmy_func()\nprint(\"Значение x вне функции:\", x)\n\nВ этом примере мы пытаемся изменить значение глобальной переменной x внутри функции, но получаем ошибку, так как переменная x не определена внутри функции.\n\nЧтобы изменить значение глобальной переменной, нужно использовать оператор global.\n\nx = 10  # глобальная переменная\n\ndef my_func():\n    global x\n    x = x + 5\n    print(\"Значение x внутри функции:\", x)\n\nmy_func()\nprint(\"Значение x вне функции:\", x)\n\nВ этом примере мы используем оператор global для того, чтобы указать, что мы хотим использовать глобальную переменную x.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/sets",
            "title": "Множества",
            "description": "Python 101",
            "content": "\nСоздание\n\nМножество можно создать, используя фигурные скобки {} или функцию set():\n\nmy_set = {1, 2, 3}\nprint(my_set) # {1, 2, 3}\n\nmy_set = set([1, 2, 3])\nprint(my_set) # {1, 2, 3}\n\nМетоды\n\nadd(): добавляет элемент в множество.\nremove(): удаляет элемент из множества. Если элемента нет в множестве, возбуждается исключение.\ndiscard(): удаляет элемент из множества. Если элемента нет в множестве, ничего не происходит.\nunion(): возвращает объединение двух множеств.\nintersection(): возвращает пересечение двух множеств.\ndifference(): возвращает разность двух множеств.\nsymmetric_difference(): возвращает симметрическую разность двух множеств.\n\nТакже: 'copy', 'difference', 'difference_update', 'discard', 'intersection', 'intersection_update', 'isdisjoint', 'issubset', 'issuperset', 'pop', 'remove', 'symmetric_difference', 'symmetric_difference_update', 'union', 'update'\n\nmy_set = {1, 2, 3}\nprint(my_set) # {1, 2, 3}\n\nДобавление элемента\nmy_set.add(4)\nprint(my_set) # {1, 2, 3, 4}\n\nУдаление элемента\nmy_set.remove(2)\nprint(my_set) # {1, 3, 4}\n\nОбъединение множеств\nother_set = {3, 4, 5}\nunion_set = my_set.union(other_set)\nprint(union_set) # {1, 3, 4, 5}\n\nПересечение множеств\nintersection_set = my_set.intersection(other_set)\nprint(intersection_set) # {3, 4}\n\nРазность множеств\ndifference_set = my_set.difference(other_set)\nprint(difference_set) # {1}\n\nСимметрическая разность множеств\nsymmetric_difference_set = my_set.symmetric_difference(other_set)\nprint(symmetric_difference_set) # {1, 5}\n\nПрименение\n\nМножества могут использоваться для проверки наличия элемента или для удаления дубликатов из списка:\n\nmy_list = [1, 2, 2, 3, 4, 4, 5]\nmy_set = set(my_list)\nprint(my_set) # {1, 2, 3, 4, 5}\n\nПроверка наличия элемента\nif 3 in my_set:\n    print(\"3 есть в множестве\")\n\nУдаление дубликатов из списка\nmy_list = list(my_set)\nprint(my_list) # [1, 2, 3, 4, 5]\n\nРесурсы:\n\nМножества в Python",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/strings",
            "title": "Строки",
            "description": "Python 101",
            "content": "\nВ Python существует несколько типов данных. Основные типы данных, с которыми вы, вероятно, будете чаще всего встречаться, - это строка, целое число, плавающая цифра, список, словарь и кортеж. В этой главе мы рассмотрим строковый тип данных. Вы удивитесь, как много вещей можно делать со строками в Python прямо из коробки. Существует также модуль string, который можно импортировать для получения доступа к еще большей функциональности, но мы не будем рассматривать его в этой главе. Вместо этого мы рассмотрим следующие темы:\n\nКак создавать строки\nКонкатенация строк\nМетоды работы со строками\nНарезка строк\nПодстановка строк\n\nСоздание\n\nСтроки обычно создаются одним из трех способов. Вы можете использовать одинарные, двойные или тройные кавычки. Давайте посмотрим!\n>> text1 = 'Привет, мир!'\n>> text2 = \"Python - это замечательный язык программирования\"\n>> text3 = '''Строка с тройными кавычками может быть выполнена с помощью трех одинарных или трех двойных кавычек.\nПри выводе сохраняются разрывы строк.'''\n\nСуществует еще один способ создания строки - это использование метода str:\n>> my_number = 123\n>> my_string = str(my_number)\n>>\n>> my_string\n'123'\n\nМетоды\n\nСтрока - это объект в Python. Фактически, все в Python является объектом.\n\nСтроки в Python поддерживают множество операций, включая конкатенацию (объединение строк), повторение, индексацию, извлечение срезов и многое другое.\n\nstring1 = 'Привет, '\nstring2 = 'мир!'\nstring3 = string1 + string2  # конкатенация строк\nprint(string3)  # выведет 'Привет, мир!'\n\nstring4 = 'Python '\nstring5 = string4 * 3  # повторение строки\nprint(string5)  # выведет 'Python Python Python'\n\nstring6 = 'Hello, world!'\nprint(string6[7])  # индексация символов, выведет 'w'\n\nstring7 = 'Python is awesome'\nprint(string7[0:6])  # извлечение среза, выведет 'Python'\n\n\nСуществует множество других методов работы со строками. Например, если бы вы хотели, чтобы все было в нижнем регистре, вы бы использовали метод lower(). Если бы вы хотели удалить все пробелы в начале и в конце строки, вы бы использовали метод strip(). Чтобы получить список всех методов работы со строками, введите в интерпретатор следующую команду:\n>> dir(my_string)\nВ итоге вы должны увидеть нечто похожее на это:\n\n['add', 'class', 'contains', 'delattr', 'dir', 'doc', 'eq', 'format',\n'ge', 'getattribute', 'getitem', 'getnewargs', 'gt', 'hash', 'init',\n'init_subclass', 'iter', 'le', 'len', 'lt', 'mod', 'mul', 'ne',\n'new', 'reduce', 'reduce_ex', 'repr', 'rmod', 'rmul', 'setattr',\n'sizeof', 'str', 'subclasshook', 'capitalize', 'casefold', 'center', 'count', 'encode',\n'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii',\n'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle',\n'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix',\n'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines',\n'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n\nВы можете смело игнорировать методы, начинающиеся и заканчивающиеся двойными знаками, такие как add. Они не используются в повседневном кодировании на Python. Вместо этого сосредоточьтесь на других методах.\n\nЕсли вы хотите узнать, что делает один из них, просто попросите помощи. Например, вы хотите узнать, для чего нужна capitalize. Чтобы узнать это, введите\n>> help(my_string.capitalize)\n\nЭто вернет следующую информацию:\n\nHelp on built-in function capitalize:\n\ncapitalize() method of builtins.str instance\n    Return a capitalized version of the string.\n\n    More specifically, make the first character have upper case and the rest lower\n    case.\n\nВозвращает копию строки S, в которой заглавным является только первый символ.\n\nВы только что узнали немного о теме, называемой интроспекцией. Python позволяет легко проводить интроспекцию всех своих объектов, что делает его очень удобным в использовании. По сути, интроспекция позволяет вам спрашивать Python о самом себе. В одном из предыдущих разделов вы узнали о преобразовании. Возможно, вы задавались вопросом, как определить тип переменной (например, int или string). Вы можете попросить Python рассказать вам об этом!\n>> type(my_string)\n\n\nПеременная my_string имеет тип str.\n\nРазделение строки на подстроки\n\nОдной из задач с которой вы будете часто заниматься в реальном мире, - это разделение строк.\nДавайте посмотрим, как работает нарезка на примере следующей строки:\n>> my_string = \"I like Python!\"\n\nКаждый символ в строке может быть доступен с помощью нарезки. Например, если я хочу получить только первый символ, я могу сделать следующее:\n>> my_string[0:4]\n'I li'\n\nЭто захватит первый символ в строке до 4-го символа, но не включая его. Да, Python основан на нулях. Это будет немного проще понять, если мы обозначим позицию каждого символа в таблице:\n\n0 |\t1 |\t2 |\t3 |\t4 |\t5 |\t6 |\t7 |\t8 |\t9 |\t10 | 11 |\t12 | 13\nI |\t  |\tl |\ti |\tk |\te |\t  |\tP |\ty |\tt |\t h |\to |\tn  |\t!\n\nТаким образом, у нас есть строка длиной 14 символов, начинающаяся с нуля и заканчивающаяся тринадцатью. Давайте рассмотрим еще несколько примеров, чтобы лучше закрепить эти понятия в голове.\n>> my_string[:1]\n'I'\n>> my_string[0:12]\n'I like Pytho'\n>> my_string[0:13]\n'I like Python'\n>> my_string[0:14]\n'I like Python!'\n>> my_string[0:-5]\n'I like Py'\n>> my_string[:]\n'I like Python!'\n>> my_string[2:]\n'like Python!'\n\nФорматирование строк\n\nДля форматирования строк в Python есть несколько способов, но одним из наиболее распространенных является метод format(). Он позволяет объединять строки и значения переменных, заданных в скобках {}. Например:\n\nname = \"Alice\"\nage = 30\nprint(\"Меня зовут {}, и мне {} лет\".format(name, age))\n\nВ этом примере мы использовали фигурные скобки для обозначения места, где нужно вставить переменные name и age. Метод format() позволяет использовать несколько переменных, их значения будут подставлены в порядке следования внутри скобок.\n\nКроме того, можно задать формат вывода для каждой переменной. Например, чтобы вывести значение переменной age в шестнадцатеричном формате, можно использовать следующий код:\n\nage = 30\nprint(\"Мне {} лет, что в шестнадцатеричной системе счисления равно {}\".format(age, hex(age)))\n\nВ результате мы получим вывод: 'Мне 30 лет, что в шестнадцатеричной системе счисления равно 0x1e'.\n\nТакже в Python 3.6 и выше есть более удобный способ форматирования строк, называемый \"f-strings\" (форматированные строки). В этом случае мы используем символ f перед открывающей кавычкой, а переменные вставляем прямо внутрь фигурных скобок. Например:\n\nname = \"Alice\"\nage = 30\nprint(f\"Меня зовут {name}, и мне {age} лет\")\n\nЭтот код даст тот же результат, что и предыдущий.\n\nРесурсы:\n\nОфициальная документация Python по типу str\nФорматирование строк\nПодробнее о форматировании строк\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/tuples",
            "title": "Кортежи",
            "description": "Python 101",
            "content": "\nКортежи в Python - это неизменяемые последовательности элементов, очень похожие на списки.\n\nСоздание\n\nСоздаются с использованием круглых скобок и могут содержать любые типы данных, в том числе и другие кортежи.\n\nСоздание кортежей очень похоже на создание списков, только используются круглые скобки вместо квадратных скобок. Например:\n\nt = (1, 2, 3)\nanother_tuple = tuple()\nabc = tuple([4, 5, 6])\n\nМы создали кортеж t, содержащий три элемента. Теперь мы можем обратиться к каждому элементу этого кортежа по его индексу, так же как и в списках:\n\nprint(t[0])  # выведет 1\nprint(t[1])  # выведет 2\nprint(t[2])  # выведет 3\n\nКортежи также могут содержать элементы разных типов данных:\n\nt = (\"apple\", 42, True)\n\nКак и в списках, мы можем использовать отрицательные индексы для обращения к элементам кортежа с конца:\n\nprint(t[-1])  # выведет True\n\nКортежи поддерживают срезы (slicing). Например, мы можем получить подкортеж, состоящий из элементов с индексами от 1 до 2:\n\nprint(t[1:3])  # выведет (42, True)\n\nМетоды\n\nКортежи имеют ряд методов, которые позволяют производить некоторые операции с ними. Однако, поскольку они неизменяемы, многие методы, доступные для списков, недоступны для кортежей. Вот несколько примеров доступных методов:\n\ncount(x) - возвращает количество элементов в кортеже, равных x.\nindex(x) - возвращает индекс первого элемента в кортеже, равного x.\n\nНапример, мы можем использовать метод count() для подсчета количества элементов \"apple\" в кортеже:\n\nt = (\"apple\", 42, True, \"apple\", \"banana\")\nprint(t.count(\"apple\"))  # выведет 2\n\nИли мы можем использовать метод index() для поиска индекса первого вхождения элемента \"banana\" в кортеже:\n\nt = (\"apple\", 42, True, \"apple\", \"banana\")\nprint(t.index(\"banana\"))  # выведет 4\n\nПрименение\n\nКортежи могут быть очень полезны, когда вам нужно создать неизменяемый набор данных. Они также могут быть использованы в качестве ключей словаря, потому что они неизменяемы.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/basis/types",
            "title": "Типы данных",
            "description": "Python 101",
            "content": "\nPython - это язык программирования, который обладает динамической типизацией, что означает, что тип переменной может меняться в процессе выполнения программы.\n\nВ Python есть несколько основных типов данных:\n\nСтроковые типы** (string)\nЧисловые типы** (целые числа, числа с плавающей запятой, комплексные числа)\nЛогический тип** (True/False)\nСписки** (list) - это упорядоченная коллекция элементов, которые могут быть различных типов данных.\n\n    Списки создаются при помощи квадратных скобок [ ] и элементы списка разделяются запятыми.\n\nКортежи** (tuple) - это упорядоченная коллекция элементов, которые могут быть различных типов данных.\n\n    Кортежи создаются при помощи круглых скобок ( ) и элементы кортежа разделяются запятыми.\n\nСловари** (dictionary) - это неупорядоченная коллекция пар \"ключ-значение\", где каждый ключ связан со значением.\n\n    Словари создаются при помощи фигурных скобок { } и пары \"ключ-значение\" разделяются двоеточием, а элементы словаря разделяются запятыми.\n\nМножества** (set) - это неупорядоченная коллекция уникальных элементов.\n\n    Множества создаются при помощи фигурных скобок { } и элементы множества разделяются запятыми.\n\nНапример, вот как можно создать списки, кортежи, словари и множества в Python:\n\nmy_list = [1, 2, 3, \"four\", 5.0]\nmy_tuple = (1, \"two\", 3.0, \"four\", 5)\nmy_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\nmy_set = {1, 2, 3, 4, 5}\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/_index",
            "title": "III - Расширенные возможности",
            "description": "Python 101",
            "content": "\nВ третьей части вы узнаете о некоторых внутренних компонентах Python, которые многие относят к владению Python среднего уровня. Вы перешли от молока и готовы к мясу! В этой части мы рассмотрим следующие темы:\n\nОтладка\nДекораторы\nОператор лямбда\nПрофилирование кода\nТестирование\n\nВ первой главе этого раздела вы познакомитесь с модулем отладки Python, pdb, и узнаете, как использовать его для отладки кода. Следующая глава посвящена декораторам. Вы узнаете о том, как их создавать, и о некоторых декораторах, встроенных в Python. В третьей главе мы рассмотрим оператор лямбда, который, по сути, создает однострочную анонимную функцию. Это немного странно, но весело! В четвертой главе речь пойдет о том, как профилировать свой код. Эта дисциплина дает вам возможность найти возможные узкие места в вашем коде, чтобы вы знали, на чем сосредоточиться для оптимизации кода. Последняя глава этого раздела посвящена тестированию кода. В ней вы узнаете, как тестировать свой код с помощью нескольких встроенных модулей Python.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/closure",
            "title": "Замыкания",
            "description": "Python 101",
            "content": "\nЗамыкание в Python - это функция, которая запоминает значения из внешней области видимости, даже если эта область видимости больше не существует. Таким образом, замыкание позволяет функции использовать переменные, которые были определены вне самой функции.\n\nПример:\n\ndef outer_func(x):\n    def inner_func(y):\n        return x + y\n    return inner_func\n\nclosure = outer_func(10)\nresult = closure(5)\nprint(result)  # выводит 15\n\nВ этом примере outer_func возвращает inner_func, которая запоминает значение x. Затем outer_func вызывается, и возвращаемая функция сохраняется в closure. Затем closure вызывается с аргументом 5, и она использует сохраненное значение x (которое равно 10), чтобы вернуть результат 15.\n\nЗамыкания могут быть полезны для создания функций, которые сохраняют состояние между вызовами, а также для создания функций, которые могут быть адаптированы к различным сценариям использования, например для создания функций, которые возвращают другие функции в зависимости от переданных аргументов.\n\nНиже приведен другой пример замыкания, который возвращает функцию, которая будет умножать аргумент на заданное число:\n\n\ndef multiply_by(num):\n    def multiplier(n):\n        return n * num\n    return multiplier\n\ndouble = multiply_by(2)\ntriple = multiply_by(3)\nprint(double(5))  # выводит 10\nprint(triple(5))  # выводит 15\n\nВ этом примере multiply_by возвращает функцию multiplier, которая запоминает значение num. Затем мы вызываем multiply_by два раза с аргументами 2 и 3 соответственно, и сохраняем возвращаемые функции в переменных double и triple.\n\nЗатем мы вызываем каждую из этих функций с аргументом 5, и каждая функция использует сохраненное значение num (которое равно 2 для double и 3 для triple) для умножения аргумента и возврата результата.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/debugging",
            "title": "Отладка Python",
            "description": "Python 101",
            "content": "\nPython поставляется с собственным модулем отладчика, который называется pdb. Этот модуль предоставляет интерактивный отладчик исходного кода для ваших программ на Python. Вы можете устанавливать брейкпоинты, просматривать код, изучать кадры стека и многое другое. Мы рассмотрим следующие аспекты этого модуля:\n\nНапример, чтобы установить точку останова в коде, можно вставить следующую строку в месте, где вы хотите остановить выполнение программы:\n\nimport pdb; pdb.set_trace()\n\nПосле запуска программы выполнение остановится на этой строке, и вы сможете использовать различные команды отладчика для изучения переменных и выполнения других операций.\n\nТакже можно запустить python модуль в режиме отладчика:\n\npython3 -m pdb myscript.py\n\nКроме встроенного отладчика Python, есть также сторонние инструменты, такие как PyCharm, Visual Studio Code и Eclipse, которые предоставляют расширенные функции отладки, такие как автоматическое определение ошибок и возможность управления отладкой из пользовательского интерфейса.\n\nНекоторые из основных команд pdb:\n\nbreak: установить точку останова в коде\ncontinue: продолжить исполнение программы до следующей точки останова\nstep: перейти к следующей строке в коде, вызванной из текущей строки\nnext: перейти к следующей строке в коде, не вызывая функции, если таковые имеются\nreturn: выполнить оставшуюся часть текущей функции и вернуться к вызывающей функции\nlist: отобразить несколько строк кода вокруг текущей строки\nprint: напечатать значение переменной\n\n\nРесурсы:\n\nThe Python Debugger\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/decorators",
            "title": "Декораторы",
            "description": "Python 101",
            "content": "\nДекораторы в Python позволяют изменять поведение функций и методов, оборачивая их в другую функцию. В этом разделе мы рассмотрим несколько встроенных декораторов и создание собственного декоратора.\n\n@classmethod\n\nДекоратор @classmethod используется для создания методов класса в Python. Методы класса имеют доступ к состоянию класса и могут использоваться без необходимости создания экземпляра класса. Методы класса можно вызывать как от самого класса, так и от его экземпляров.\n\nДекоратор @classmethod применяется к методам класса. Он принимает первым аргументом класс (cls) вместо экземпляра класса (self).\n\nclass MyClass:\n    @classmethod\n    def my_class_method(cls, arg1, arg2):\n        print('Class:', cls, 'arg1:', arg1, 'arg2:', arg2)\n\nMyClass.my_class_method('a', 'b')\n\n@staticmethod\n\nДекоратор @staticmethod используется для создания статических методов в Python. Статические методы не имеют доступа к состоянию класса и могут использоваться без необходимости создания экземпляра класса. Статические методы можно вызывать как от самого класса, так и от его экземпляров.\n\nДекоратор @staticmethod также применяется к методам класса. Он не принимает первый аргумент, связанный с классом.\n\nclass MyClass:\n    @staticmethod\n    def my_static_method(arg1, arg2):\n        print('arg1:', arg1, 'arg2:', arg2)\n\nMyClass.my_static_method('a', 'b')\n\n@property\n\nДекоратор @property используется для создания свойств класса в Python. Свойства класса обеспечивают доступ к закрытым переменным класса, так что они могут быть использованы без необходимости создания экземпляра класса. Доступ к свойствам можно получить как чтением, так и записью.\n\nДекоратор @property используется для превращения метода в атрибут объекта. Метод, декорированный @property, может быть вызван как атрибут объекта, а не как метод.\n\nclass MyClass:\n    def init(self, x):\n        self._x = x\n\n    @property\n    def x(self):\n        return self._x\n\nmy_obj = MyClass(10)\nprint(my_obj.x) # 10\n\n@contextmanager\n\nДекоратор @contextmanager используется для создания менеджера контекста в Python. Менеджеры контекста позволяют определять блоки кода, которые должны быть выполнены с определенными контекстными условиями, такими как открытие и закрытие файлов, установка и восстановление состояния объекта и т. д.\n\n@contextmanager позволяет использовать функцию как менеджер контекста с использованием ключевого слова with.\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef my_context():\n    print('entering context')\n    yield\n    print('exiting context')\n\nwith my_context():\n    print('inside context')\n\n@lru_cache\n\nДекоратор @lru_cache используется для кэширования результатов функции. Он сохраняет результаты вызовов функции в памяти, чтобы избежать повторных вычислений.\n\n@lru_cache использует алгоритм LRU (least recently used) для автоматического удаления наиболее неиспользуемых элементов из кэша.\n\nfrom functools import lru_cache\n\n@lru_cache(maxsize=128)\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(30))\n\nСоздание декоратора\n\nДля создания собственного декоратора в Python нужно определить функцию-обертку, которая будет принимать функцию в качестве аргумента и возвращать новую функцию, изменяющую поведение исходной функции.\n\nНапример, создадим декоратор, который будет выводить время выполнения функции:\n\nimport time\n\ndef timer(func):\n    def wrapper(args, *kwargs):\n        start_time = time.time()\n        result = func(args, *kwargs)\n        end_time = time.time()\n        print(f\"Function '{func.name}' executed in {end_time - start_time:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef my_func():\n    time.sleep(2)\n\nmy_func()\n\nЗдесь мы определили функцию-обертку wrapper, которая принимает любое количество позиционных и именованных аргументов и вызывает исходную функцию func с этими аргументами. Затем мы измеряем время выполнения функции, выводим результат и возвращаем его.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/lambda",
            "title": "Лямбда",
            "description": "Python 101",
            "content": "\nЛямбда-функции в Python - это безымянные функции, которые можно определить в одной строке и не требуют ключевого слова def. Они используются для написания коротких функций внутри других функций или выражений, где требуется функция в качестве аргумента.\n\nЛямбда-функция определяется ключевым словом lambda, за которым следуют параметры функции, после чего через двоеточие указывается выражение, которое нужно вернуть из функции.\n\nПример:\n\nadd = lambda x, y: x + y\nprint(add(2, 3)) # Output: 5\n\nЗдесь мы определяем лямбда-функцию add, которая принимает два аргумента x и y и возвращает их сумму. Затем мы вызываем эту функцию, передав ей аргументы 2 и 3, и выводим результат, который равен 5.\n\nЛямбда-функции могут использоваться в качестве аргументов для функций высшего порядка, таких как map, filter или reduce. Например, следующий код использует лямбда-функцию для фильтрации списка:\n\nnumbers = [1, 2, 3, 4, 5, 6]\neven_numbers = list(filter(lambda x: x % 2 == 0, numbers))\nprint(even_numbers) # Output: [2, 4, 6]\n\nЗдесь мы используем функцию filter, чтобы отфильтровать только четные числа из списка numbers. В качестве первого аргумента передаем лямбда-функцию, которая проверяет, является ли число четным. Результат фильтрации преобразуем в список и выводим на экран.\n\nЛямбда-функции также могут использоваться для создания простых обработчиков событий или для задания ключей сортировки. В целом, лямбда-функции могут быть удобным инструментом для написания коротких функций на лету.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/enhance_python/testing",
            "title": "Тестирование",
            "description": "Python 101",
            "content": "\nunittest\n\nPython поставляется со встроенным модулем для тестирования - unittest.\n\nПример теста:\n\nimport unittest\n\ndef square(x):\n    return x * x\n\nclass TestSquare(unittest.TestCase):\n    def test_positive(self):\n        self.assertEqual(square(2), 4)\n        self.assertEqual(square(3), 9)\n        self.assertEqual(square(4), 16)\n\n    def test_negative(self):\n        self.assertEqual(square(-2), 4)\n        self.assertEqual(square(-3), 9)\n        self.assertEqual(square(-4), 16)\n\nif name == 'main':\n    unittest.main()\n\nВ этом примере мы создаем тестовый класс TestSquare, который наследуется от unittest.TestCase. В этом классе мы определяем два метода: test_positive и test_negative. Эти методы используют метод assertEqual для проверки ожидаемых результатов.\n\nМетод assertEqual сравнивает два значения и генерирует исключение, если они не равны. Если тест проходит успешно, то мы не получаем никаких сообщений.\n\nЗапуск тестов можно выполнить из командной строки с помощью следующей команды:\n\npython test_square.py\n\nДа, в модуле unittest есть возможность делать моки с помощью встроенного класса unittest.mock.Mock. Это позволяет заменить реальный объект на имитацию, чтобы упростить тестирование и избежать внешних зависимостей.\n\nВот пример, который демонстрирует, как можно использовать моки в unittest для тестирования функции, которая зависит от внешнего сервиса:\n\nfrom unittest import TestCase, mock\n\ndef get_external_data():\nЭто внешний сервис, который может вернуть много данных\nНо для тестирования нас интересует только первый элемент\n    return ['data1', 'data2', 'data3']\n\ndef process_data():\n    data = get_external_data()\n    return data[0]\n\nclass TestProcessData(TestCase):\n\n    @mock.patch('main.get_external_data')\n    def test_process_data(self, mock_get_external_data):\n        mock_get_external_data.return_value = ['test_data1', 'test_data2', 'test_data3']\n        result = process_data()\n        self.assertEqual(result, 'test_data1')\n\nЗдесь мы используем декоратор @mock.patch для замены реального get_external_data на имитацию. В тесте мы устанавливаем возвращаемое значение имитации и проверяем, что функция process_data вернула ожидаемый результат.\n\nРесурсы:\n\nДокументация по unittest\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/external_packages/_index",
            "title": "IV - Внешние модули",
            "description": "Python 101",
            "content": "\nНекоторые из самых популярных пакетов Python по количеству загрузок через PyPI (Python Package Index) за последнее время включают:\n\nrequests: библиотека для HTTP-запросов и взаимодействия с веб-серверами.\nnumpy: библиотека для работы с массивами и матрицами в Python.\npandas: библиотека для обработки и анализа данных в Python.\nmatplotlib: библиотека для создания графиков и визуализации данных в Python.\nFlask: легковесный веб-фреймворк для создания веб-приложений на Python.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/external_packages/install_packages",
            "title": "Установка пакетов",
            "description": "Python 101",
            "content": "\n\nPython-пакеты можно искать на официальном репозитории PyPI (Python Package Index) по адресу https://pypi.org/. В PyPI представлены большинство сторонних пакетов для Python, их можно устанавливать с помощью менеджера пакетов pip.\n\nТакже существуют другие источники для поиска и установки Python-пакетов, например, Anaconda, Conda-forge и т.д.\n\nДля установки пакетов в Python существует несколько способов. Рассмотрим наиболее распространенные из них:\n\nУстановка с помощью pip\n\npip - это менеджер пакетов для Python, который упрощает установку, удаление и обновление пакетов. Чтобы установить пакет с помощью pip, необходимо выполнить команду в терминале:\n\npip install package_name\n\nЗдесь package_name - название пакета, который вы хотите установить. Можно также указать конкретную версию пакета:\n\npip install package_name==version_number\n\nКроме того, можно установить пакет из файла, используя команду:\n\npip install path/to/package.whl\n\nУстановка с помощью Anaconda\n\nAnaconda - это дистрибутив Python, который включает в себя множество научных пакетов и библиотек. Установка пакетов в Anaconda происходит с помощью менеджера пакетов conda. Для установки пакета необходимо выполнить команду:\n\nconda install package_name\n\nУстановка из исходников\n\nПри установке пакета из исходников необходимо скачать исходный код пакета, распаковать его, перейти в папку с исходниками и выполнить команду:\n\npython setup.py install\n\nЭта команда выполнит установку пакета.\n\nВажно отметить, что при установке пакетов необходимо убедиться в том, что используется правильная версия Python и что пакеты совместимы с используемой версией Python.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/external_packages/requests",
            "title": "Пакет requests",
            "description": "Python 101",
            "content": "\nМодуль requests - это сторонняя библиотека Python для отправки HTTP-запросов. Он предоставляет удобный и простой API для отправки GET-, POST-, PUT-, DELETE- и других типов запросов.\n\nУстановить requests можно с помощью менеджера пакетов pip:\n\npip install requests\n\nПример GET-запроса:\n\nimport requests\n\nresponse = requests.get(\"https://www.example.com\")\nprint(response.status_code)\nprint(response.text)\n\nПример POST-запроса:\n\nimport requests\n\npayload = {'key1': 'value1', 'key2': 'value2'}\nresponse = requests.post(\"https://site.org/post\", data=payload)\nprint(response.status_code)\nprint(response.json())\n\nМодуль requests также поддерживает отправку запросов с использованием сессий, установку заголовков, аутентификацию и другие полезные функции для работы с HTTP-запросами.\n\nРесурсы:\n\nhttps://www.w3schools.com/python/module_requests.asp\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/frameworks/_index",
            "title": "V - Фреймворки",
            "description": "Python 101",
            "content": "\nРассмотрим основные фреймворки Python.\n\nFlask - легковесный микрофреймворк, который предоставляет необходимые инструменты для быстрой разработки веб-приложений. Flask использует принцип \"минимальности\", позволяя разработчикам выбирать только необходимые компоненты, что делает его очень гибким и простым в использовании.\n\nDjango - это полноценный фреймворк для создания веб-приложений на языке Python. Он предоставляет широкий спектр инструментов и функциональности, которые облегчают разработку, тестирование и масштабирование приложений. Django имеет встроенную административную панель и ORM-систему, что делает его особенно удобным для разработки сложных веб-приложений.\n\nFastAPI - это быстрый и современный веб-фреймворк, который использует Python 3.7+ типы данных и асинхронную синтаксическую модель. Он предоставляет автоматическую документацию API и мощный систему валидации входных данных. FastAPI быстрый и прост в использовании, что делает его особенно полезным для создания высокопроизводительных и масштабируемых веб-приложений.\n\nTornado - это фреймворк для создания асинхронных веб-приложений на Python. Он предоставляет быструю и масштабируемую платформу для создания высокопроизводительных веб-приложений. Tornado также обеспечивает возможность использования сокетов, что делает его особенно полезным для создания приложений, которые должны быть связаны с другими приложениями или службами.\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/frameworks/django",
            "title": "Django",
            "description": "Python 101",
            "content": "\nDjango - это высокоуровневый фреймворк для веб-приложений на языке Python. Он предоставляет множество инструментов для разработки сайтов, начиная от автоматического создания административного интерфейса до работы с базами данных. Основными принципами, которыми руководствуется Django, являются: быстрота разработки, возможность переиспользования кода и расширяемость.\n\nУстановим необходимые пакеты:\n\npip install django\n\nДля начала работы с Django нужно создать проект. Для этого в командной строке нужно ввести команду:\n\ndjango-admin startproject project_name\n\nПосле этого будет создан проект с именем \"project_name\". Внутри проекта есть файлы настроек и приложения. Приложение - это часть проекта, которая отвечает за определенную функциональность.\n\nДля создания приложения нужно ввести команду:\n\npython manage.py startapp app_name\n\nДалее можно начинать разработку функциональности внутри приложения.\n\nПример реализации CRUD операций с использованием Django:\n\nfrom django.shortcuts import render, get_object_or_404\nfrom django.http import HttpResponseRedirect\nfrom django.urls import reverse\nfrom .models import Book\n\ndef index(request):\n    books = Book.objects.all()\n    return render(request, 'index.html', {'books': books})\n\ndef create(request):\n    if request.method == 'POST':\n        book = Book(\n            title=request.POST.get('title'),\n            author=request.POST.get('author'),\n            published_date=request.POST.get('published_date')\n        )\n        book.save()\n        return HttpResponseRedirect(reverse('index'))\n    return render(request, 'create.html')\n\ndef update(request, book_id):\n    book = get_object_or_404(Book, pk=book_id)\n    if request.method == 'POST':\n        book.title = request.POST.get('title')\n        book.author = request.POST.get('author')\n        book.published_date = request.POST.get('published_date')\n        book.save()\n        return HttpResponseRedirect(reverse('index'))\n    return render(request, 'update.html', {'book': book})\n\ndef delete(request, book_id):\n    book = get_object_or_404(Book, pk=book_id)\n    book.delete()\n    return HttpResponseRedirect(reverse('index'))\n\nВ данном примере определены функции для отображения списка книг (index), создания новой книги (create), обновления существующей книги (update) и удаления книги (delete). Все эти функции используют модель Book, которая определена в файле models.py. Шаблоны (templates) для каждой из функций находятся в отдельных html-файлах.\n\n\nРесурсы:\n\nОфициальная документация Django\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/frameworks/fastapi",
            "title": "FastAPI",
            "description": "Python 101",
            "content": "\nFastAPI - это фреймворк для создания веб-приложений на Python, использующий современный подход к созданию API и основанный на ASGI-серверах. Он разработан с упором на скорость и быстродействие, предоставляя возможности асинхронного выполнения запросов, автоматического документирования API и многие другие.\n\nДля установки FastAPI нужно выполнить команду pip install fastapi. Для запуска приложения можно использовать стандартный инструмент uvicorn, который также необходимо установить: pip install uvicorn.\n\nПример CRUD приложения на FastAPI:\n\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Dict\n\napp = FastAPI()\n\nИмитация базы данных\ndb = {}\n\nМодель для создания/редактирования задачи\nclass Task(BaseModel):\n    title: str\n    description: str\n\nМодель для ответа со списком задач\nclass TaskList(BaseModel):\n    tasks: Dict[int, Task]\n\nПолучение списка задач\n@app.get(\"/tasks/\", response_model=TaskList)\nasync def get_tasks():\n    return TaskList(tasks=db)\n\nПолучение одной задачи по id\n@app.get(\"/tasks/{task_id}\")\nasync def get_task(task_id: int):\n    if task_id not in db:\n        raise HTTPException(status_code=404, detail=\"Task not found\")\n    return db[task_id]\n\nСоздание новой задачи\n@app.post(\"/tasks/\")\nasync def create_task(task: Task):\n    task_id = max(db.keys(), default=0) + 1\n    db[task_id] = task\n    return {\"id\": task_id}\n\nРедактирование задачи\n@app.put(\"/tasks/{task_id}\")\nasync def update_task(task_id: int, task: Task):\n    if task_id not in db:\n        raise HTTPException(status_code=404, detail=\"Task not found\")\n    db[task_id] = task\n    return {\"message\": \"Task has been updated\"}\n\nУдаление задачи\n@app.delete(\"/tasks/{task_id}\")\nasync def delete_task(task_id: int):\n    if task_id not in db:\n        raise HTTPException(status_code=404, detail=\"Task not found\")\n    db.pop(task_id)\n    return {\"message\": \"Task has been deleted\"}\n\nЭтот код создает простое приложение с API для управления задачами. Он использует модели Pydantic для валидации данных, а также async/await синтаксис для асинхронной обработки запросов. Код использует декораторы FastAPI для определения конечных точек API (маршрутов), а также для указания моделей данных, которые используются для запросов и ответов.\n\n\n\n\n\nРесурсы:\n\nОфициальная документация FastAPI\ncontent/tracks/python-101/400_frameworks/403_fastapi.ru.md",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/frameworks/flask",
            "title": "Flask",
            "description": "Python 101",
            "content": "\nFlask - это легковесный фреймворк для создания веб-приложений на языке Python. Он подходит как для небольших проектов, так и для крупных веб-приложений.\n\nFlask не имеет встроенной базы данных или абстракции уровня модели, поэтому вам нужно будет выбрать библиотеку, которая лучше всего подходит для вашего проекта.\n\n\n\npip install flask\npip install flask_sqlalchemy\n\nПример CRUD-операций с использованием Flask:\n\nfrom flask import Flask, request, jsonify\nfrom flask_sqlalchemy import SQLAlchemy\n\napp = Flask(name)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///example.db'\ndb = SQLAlchemy(app)\n\nclass Book(db.Model):\n    id = db.Column(db.Integer, primary_key=True)\n    title = db.Column(db.String(100))\n    author = db.Column(db.String(100))\n\n@app.route('/books', methods=['GET'])\ndef get_all_books():\n    books = Book.query.all()\n    result = [{'id': book.id, 'title': book.title, 'author': book.author} for book in books]\n    return jsonify(result)\n\n@app.route('/books/', methods=['GET'])\ndef get_book(book_id):\n    book = Book.query.get(book_id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    result = {'id': book.id, 'title': book.title, 'author': book.author}\n    return jsonify(result)\n\n@app.route('/books', methods=['POST'])\ndef create_book():\n    book = Book(title=request.json['title'], author=request.json['author'])\n    db.session.add(book)\n    db.session.commit()\n    result = {'id': book.id, 'title': book.title, 'author': book.author}\n    return jsonify(result), 201\n\n@app.route('/books/', methods=['PUT'])\ndef update_book(book_id):\n    book = Book.query.get(book_id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    book.title = request.json['title']\n    book.author = request.json['author']\n    db.session.commit()\n    result = {'id': book.id, 'title': book.title, 'author': book.author}\n    return jsonify(result)\n\n@app.route('/books/', methods=['DELETE'])\ndef delete_book(book_id):\n    book = Book.query.get(book_id)\n    if book is None:\n        return jsonify({'error': 'Book not found'}), 404\n    db.session.delete(book)\n    db.session.commit()\n    return '', 204\n\nДанный код использует Flask вместе с библиотекой SQLAlchemy для создания веб-приложения и взаимодействия с базой данных. Роуты приложения обрабатывают HTTP-запросы и возвращают соответствующий HTTP-ответ. В данном примере реализованы операции CRUD (Create, Read, Update, Delete) для модели Book.\n\n\nРесурсы:\n\nОфициальная документация Flask\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/frameworks/tornado",
            "title": "Tornado",
            "description": "Python 101",
            "content": "\nTornado - это еще один быстрый веб-фреймворк, который разработан для обработки больших объемов трафика в режиме реального времени.\n\nДля начала работы с Tornado нам нужно установить его, используя команду pip:\n\npip install tornado\n\nimport tornado.ioloop\nimport tornado.web\nimport tornado.escape\n\nclass MainHandler(tornado.web.RequestHandler):\n    def get(self):\n        items = [{'id': 1, 'name': 'Item 1'}, {'id': 2, 'name': 'Item 2'}]\n        self.write(tornado.escape.json_encode(items))\n\nclass ItemHandler(tornado.web.RequestHandler):\n    def get(self, id):\n        item = {'id': id, 'name': 'Item ' + id}\n        self.write(tornado.escape.json_encode(item))\n\n    def post(self, id):\n        item = {'id': id, 'name': self.get_argument('name')}\n        self.write(tornado.escape.json_encode(item))\n\n    def put(self, id):\n        item = {'id': id, 'name': self.get_argument('name')}\n        self.write(tornado.escape.json_encode(item))\n\n    def delete(self, id):\n        self.write('Item ' + id + ' deleted')\n\ndef make_app():\n    return tornado.web.Application([\n        (r'/', MainHandler),\n        (r'/item/(\\d+)', ItemHandler),\n    ])\n\nif name == 'main':\n    app = make_app()\n    app.listen(8888)\n    tornado.ioloop.IOLoop.current().start()\n\nВ этом примере мы создаем два класса-обработчика, один для главной страницы, другой для работы с конкретным элементом. Для тестирования мы создаем два элемента и возвращаем их в формате JSON при запросе к главной странице.\n\nКогда мы запрашиваем элемент, создается элемент соответствующий запрошенному и возвращается в формате JSON. Методы post, put и delete принимают данные из тела запроса и выполняют соответствующую операцию.\n\nЗапуск приложения осуществляется через командную строку:\n\npython tornado_app.py\n\nПосле запуска приложения, мы можем обращаться к нему через браузер по адресу http://localhost:8888/. При обращении к адресу http://localhost:8888/item/1, мы получим объект с идентификатором 1 в формате JSON.\n\nПри выполнении запроса post на тот же URL с параметрами, мы создадим новый элемент.\n\nПри запросе put мы обновим данные существующего элемента, а при выполнении delete - удалим элемент с указанным идентификатором.\n\n\nРесурсы:\n\nОфициальная документация Tornado\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/_index",
            "title": "II - Стандартные модули",
            "description": "Python 101",
            "content": "\nВ Python есть множество встроенных модулей, которые помогают ускорить и упростить написание программ. В этой главе мы рассмотрим некоторые из стандартных модулей Python и их возможности.\n\nМодуль logging позволяет вести журнал событий в приложении. С помощью этого модуля можно создавать различные уровни логирования и настраивать их вывод.\n\nМодуль sys предоставляет доступ к системным переменным и функциям Python. Например, с помощью этого модуля можно получить информацию о текущей версии Python или переданных параметрах командной строки.\n\nМодуль os предоставляет функции для работы с операционной системой, такие как создание и удаление файлов и директорий, запуск новых процессов и многое другое.\n\nМодуль email и smtplib используются для отправки электронной почты. Модуль email позволяет создавать электронные письма, а smtplib отправляет их.\n\nМодуль subprocess позволяет запускать новые процессы в операционной системе и взаимодействовать с ними.\n\nМодуль threading позволяет создавать и управлять потоками выполнения в Python.\n\nМодуль asyncio позволяет создавать асинхронный код, что может быть полезным для работы с сетевыми приложениями.\n\nМодуль datetime предоставляет классы для работы с датами и временем. С помощью этого модуля можно легко выполнять различные операции с датами и временем, такие как форматирование, расчет разницы между датами и многое другое.\n\nМодуль configparser позволяет работать с INI-файлами - это формат файлов конфигурации. С помощью этого модуля можно легко считывать и записывать параметры конфигурации.\n\nМодуль argparse в Python предоставляет удобный способ обрабатывать аргументы командной строки.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/argparse",
            "title": "Модуль argparse",
            "description": "Python 101",
            "content": "\nМодуль argparse позволяет легко парсить аргументы командной строки.\n\nЭто может быть полезно для создания сценариев командной строки, которые должны принимать аргументы от пользователя, например, при написании утилит командной строки.\n\nПример:\n\n#script.py\nimport argparse\n\nparser = argparse.ArgumentParser(description='Process some integers.')\nparser.add_argument('integers', metavar='N', type=int, nargs='+',\n                    help='an integer for the accumulator')\nparser.add_argument('--sum', dest='accumulate', action='store_const',\n                    const=sum, default=max,\n                    help='sum the integers (default: find the max)')\n\nargs = parser.parse_args()\nprint(args.accumulate(args.integers))\n\nВ этом примере мы создали парсер аргументов командной строки с помощью argparse, который принимает целочисленные значения и может вычислить их сумму или максимальное значение. При запуске скрипта мы можем указать значения, например:\n\npython script.py 1 2 3 4 --sum\n\nResources:\n\nargparse tutorial | python.org\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/asyncio",
            "title": "Модуль asyncio",
            "description": "Python 101",
            "content": "Асинхронное программирование — это концепция программирования, при применении которой запуск длительных операций происходит без ожидания их завершения и не блокирует дальнейшее выполнение программы.\nКорутина — это более общая форма подпрограмм. Подпрограммы имеют одну точку входа и одну точку выхода. А корутины поддерживают множество точек входа, выхода и возобновления их выполнения.\n\n\nPython модуль asyncio позволяет заниматься асинхронным программированием с применением конкурентного выполнения кода, основанного на корутинах.\n\nВот план использования модуля asyncio:\n\nimport asyncio\n\nОпределение асинхронной функции с помощью ключевого слова async.\nasync def my_coroutine():\ncode here\n\nСоздание цикла событий\nloop = asyncio.get_event_loop()\n\nЗапуск сопрограммы\nloop.run_until_complete(my_coroutine())\n\nобход асинхронного итератора\nasync for item in async_iterator:\n    print(item)\n\nМожно использовать функцию asyncio.gather() для выполнения нескольких сопрограмм параллельно:\n\nasync def coroutine1():\n    print(\"coroutine1 start\")\n    await asyncio.sleep(1)\n    print(\"coroutine1 end\")\n\nasync def coroutine2():\n    print(\"coroutine2 start\")\n    await asyncio.sleep(2)\n    print(\"coroutine2 end\")\n\nasync def main():\n    await asyncio.gather(coroutine1(), coroutine2())\n\nloop.run_until_complete(main())\n\nВ этом примере две сопрограммы coroutine1() и coroutine2() запускаются параллельно с помощью функции asyncio.gather(), которая возвращает результаты выполнения всех сопрограмм.\n\n\nРесурсы:\n\nруководство по модулю asyncio в Python | habr\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/configparser",
            "title": "Модуль configparser",
            "description": "Python 101",
            "content": "\nМодуль configparser позволяет работать с конфигурационными файлами в Python.\n\nДля использования модуля configparser нужно сначала импортировать его:\n\nimport configparser\n\nДля чтения конфигурационного файла используется метод configparser.ConfigParser() с методом read():\n\nconfig = configparser.ConfigParser()\nconfig.read('config.ini')\n\nДля записи в конфигурационный файл используется метод write():\n\nconfig.set('section', 'option', 'value')\nwith open('config.ini', 'w') as f:\n    config.write(f)\n\nПример работы с конфигурационным файлом:\n\nimport configparser\n\nСоздаем объект ConfigParser\nconfig = configparser.ConfigParser()\n\nЧитаем конфигурационный файл\nconfig.read('config.ini')\n\nПолучаем значение параметра из секции\ndb_name = config.get('database', 'db_name')\n\nМеняем значение параметра и записываем изменения в файл\nconfig.set('database', 'db_name', 'new_db_name')\nwith open('config.ini', 'w') as f:\n    config.write(f)\n\nКонфигурационный файл может иметь несколько секций, каждая из которых может иметь набор параметров со значениями. Например:\n\n[database]\ndb_name=my_db\ndb_user=user_name\ndb_password=secret_password\n\n[server]\nhost=127.0.0.1\nport=8080\n\nВ данном примере есть две секции: [database] и [server]. Каждая секция содержит набор параметров со значениями.\n\nМодуль configparser позволяет легко работать с этими параметрами, как с обычными переменными. Например, для получения значения параметра db_name из секции database нужно выполнить следующий код:\n\ndb_name = config.get('database', 'db_name')\n\nПараметры в файле могут быть определены без значения, только с именем параметра. В этом случае для получения значения параметра нужно использовать метод getboolean(), getint() или getfloat() в зависимости от типа значения параметра.\n\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/datetime_time",
            "title": "Модуль datetime/time",
            "description": "Python 101",
            "content": "\ndatetime\n\nМодуль datetime в Python предоставляет классы для работы с датами и временем. Он позволяет создавать объекты даты, времени и даты-времени, а также выполнять операции с этими объектами.\n\nКласс datetime является основным классом модуля datetime и представляет дату и время в формате \"ГГГГ-ММ-ДД ЧЧ:ММ:СС\". Класс date представляет только дату, а класс time - только время.\n\nФорматирование дат и времени может выполняться с помощью метода strftime, который позволяет создавать строку с заданным форматом даты и времени. Также существует метод strptime, который позволяет преобразовать строку в объект даты и времени.\n\nДля работы со временем и датами можно использовать методы класса datetime, такие как now для получения текущей даты и времени, date и time для получения объектов даты и времени соответственно, а также методы year, month, day, hour, minute, second для получения соответствующих значений.\n\nКласс timedelta позволяет выполнять арифметические операции над объектами дат и времени, такие как сложение и вычитание.\n\n\nimport datetime\n\nСоздание объекта datetime\nnow = datetime.datetime.now()\nprint(now)\n\nПолучение объекта date\ntoday = datetime.date.today()\nprint(today)\n\nПолучение объекта time\ncurrent_time = datetime.time(hour=12, minute=30, second=0)\nprint(current_time)\n\nФорматирование даты и времени\nformatted_date = now.strftime(\"%d-%m-%Y\")\nprint(formatted_date)\n\nПреобразование строки в объект datetime\ndate_string = \"2022-02-15 18:00:00\"\ndate_object = datetime.datetime.strptime(date_string, \"%Y-%m-%d %H:%M:%S\")\nprint(date_object)\n\nИспользование timedelta\none_day = datetime.timedelta(days=1)\nyesterday = today - one_day\nprint(yesterday)\n\nПеревод даты в строку и обратно\ndate_string = today.strftime(\"%Y-%m-%d\")\ndate_object = datetime.datetime.strptime(date_string, \"%Y-%m-%d\")\nprint(date_object)\n\ntime\n\nМодуль time в Python предоставляет доступ к системному времени и позволяет работать с временными значениями, такими как время в секундах, часах, минутах и т.д. Этот модуль также содержит функции для задержки выполнения программы, вычисления прошедшего времени и других операций, связанных со временем.\n\nВот некоторые из наиболее распространенных функций time:\n\ntime(): возвращает текущее время в секундах, начиная с начала эпохи Unix (1 января 1970 года 00:00:00 GMT).\nctime(): принимает время в секундах в качестве аргумента и возвращает строку с форматированным временем в удобочитаемом формате.\nsleep(): приостанавливает выполнение программы на заданное количество секунд.\ngmtime(): принимает время в секундах в качестве аргумента и возвращает объект структурированного времени, представленного в UTC (координированное всемирное время).\nlocaltime(): принимает время в секундах в качестве аргумента и возвращает объект структурированного времени, представленного в локальной временной зоне.\nstrftime(): преобразует объект структурированного времени в строку с заданным форматом.\n\n\nimport time\n\nПолучение текущего времени в секундах\ncurrent_time = time.time()\nprint(current_time)\n\nОтображение времени в удобочитаемом формате\nformatted_time = time.ctime(current_time)\nprint(formatted_time)\n\nПриостановка выполнения программы на 5 секунд\ntime.sleep(5)\n\nПолучение объекта структурированного времени\ngm_time = time.gmtime(current_time)\nprint(gm_time)\n\nПреобразование объекта структурированного времени в строку\nformatted_gm_time = time.strftime('%Y-%m-%d %H:%M:%S', gm_time)\nprint(formatted_gm_time)\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/logging",
            "title": "Модуль logging",
            "description": "Python 101",
            "content": "\nМодуль логирования logging является одним из стандартных модулей Python и предоставляет возможности для записи логов в приложении. Логирование используется для записи информации о работе приложения, которую можно использовать для отслеживания ошибок и диагностики проблем.\n\nВ модуле logging определены три основных компонента: логгеры (loggers), обработчики (handlers) и форматировщики (formatters). Логгеры представляют собой объекты, которые используются для записи сообщений лога. Обработчики определяют, куда будут записываться сообщения, а форматировщики определяют, как будут отформатированы эти сообщения.\n\nПример использования модуля logging:\n\nimport logging\n\nСоздание логгера\nlogger = logging.getLogger('example')\n\nУстановка уровня логирования\nlogger.setLevel(logging.INFO)\n\nСоздание обработчика\nhandler = logging.FileHandler('example.log')\n\nУстановка уровня логирования для обработчика\nhandler.setLevel(logging.INFO)\n\nСоздание форматировщика\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\nУстановка форматировщика для обработчика\nhandler.setFormatter(formatter)\n\nДобавление обработчика к логгеру\nlogger.addHandler(handler)\n\nЗапись сообщений лога\nlogger.debug('Debug message')\nlogger.info('Info message')\nlogger.warning('Warning message')\nlogger.error('Error message')\nlogger.critical('Critical message')\n\nЭтот пример создает логгер example, который записывает сообщения в файл example.log. Уровень логирования установлен на уровень INFO, что означает, что будут записаны сообщения с уровнем INFO и выше. Созданный обработчик определяет, что сообщения будут записываться в файл, а форматировщик определяет, как будут отформатированы сообщения.\n\nМетоды debug, info, warning, error и critical используются для записи сообщений лога разного уровня. В этом примере мы записываем сообщения всех уровней, поэтому в лог-файле будут отображены все эти сообщения.\n\nЭто только базовый пример использования модуля logging. В реальном приложении вы можете создать несколько логгеров с разными уровнями логирования и разными обработчиками для каждого из них, в зависимости от вашей конкретной задачи.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/os",
            "title": "Модуль os",
            "description": "Python 101",
            "content": "\nМодуль os предоставляет функции для работы с операционной системой. Этот модуль позволяет получить доступ к файловой системе, управлять процессами, получать информацию об окружении и другие.\n\nos.listdir - получение списка файлов и директорий в указанной директории:\nos.mkdir() - создание директории\nos.system() - выполнение команды в командной строке\nos.getenv()\nos.putenv()\nos.remove() - удаление файла\nos.rename()\nos.startfile()\nos.walk() - дает способ итерации по пути корневого уровня\npathlib.Path.walk() - похожий на os.walk(). (Добавлен в 3.12)\nos.environ: словарь, содержащий переменные окружения, доступные в текущем процессе. Можно использовать для получения значения переменной окружения или для установки ее значения.\nos.getcwd(): возвращает текущую рабочую директорию.\nos.chdir(path): изменяет текущую рабочую директорию на указанную.\nos.path.join(path1, path2, ...): объединяет несколько путей в один, используя правильный разделитель для операционной системы.\nos.path.exists(path): возвращает True, если файл или директория по указанному пути существует.\nos.path.isfile(path): возвращает True, если путь указывает на существующий файл.\nos.path.isdir(path): возвращает True, если путь указывает на существующую директорию.\nos.makedirs(path): создает директории (в том числе вложенные), если они не существуют.\nos.rmdir(path): удаляет директорию, если она пуста.\n\n\nimport os\n\nfiles = os.listdir(\".\")\nprint(f\"Files in current directory: {files}\") #['file1.txt', 'file2.txt']\n\nos.remove(\"file.txt\")\nos.system(\"ls -l\")\n\nПолучение значения переменной окружения\nhome_dir = os.environ['HOME']\n\nУстановка значения переменной окружения\nos.environ['MY_VAR'] = 'my_value'\n\nПолучение текущей рабочей директории\ncurrent_dir = os.getcwd()\n\nСмена рабочей директории\nos.chdir('/path/to/new/dir')\n\nОбъединение нескольких путей\nfull_path = os.path.join('/path/to', 'file.txt')\n\nПроверка наличия файла\nfile_exists = os.path.exists('/path/to/file.txt')\n\nПроверка наличия директории\ndir_exists = os.path.isdir('/path/to/dir')\n\nСоздание директории\nos.makedirs('/path/to/new/dir')\n\nУдаление директории\nos.rmdir('/path/to/dir')\n\nИтерация по каталогам\nfor root, dirs, files in os.walk(path):\n  print(root)\n  for _dir in dirs:\n      print(_dir)\n  for _file in files:\n      print(_file)\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/smtplib",
            "title": "Модуль email / smtplib",
            "description": "Python 101",
            "content": "\nМодуль smtplib в Python предоставляет возможность отправки электронных писем через Simple Mail Transfer Protocol (SMTP).\n\nОн предоставляет класс SMTP, который упрощает отправку электронной почты из Python-скрипта. Модуль smtplib позволяет отправлять электронные письма, как с аутентификацией, так и без, и можно отправлять как простые текстовые сообщения, так и письма с HTML-контентом.\n\nВот пример кода для отправки простого текстового сообщения:\n\nimport smtplib\n\nsmtp_server = 'smtp.yandex.ru'\nport = 587\nlogin = 'example@yandex.ru'\npassword = 'password'\nfrom_addr = 'example@yandex.ru'\nto_addr = 'example2@yandex.ru'\nmessage = 'Hello, world!'\n\nwith smtplib.SMTP(smtp_server, port) as server:\n    server.starttls()\n    server.login(login, password)\n    server.sendmail(from_addr, to_addr, message)\n\nВ этом примере мы создаем объект SMTP, указывая адрес сервера и номер порта. Затем мы используем starttls(), чтобы начать безопасное соединение и login(), чтобы авторизоваться на сервере. Затем мы отправляем электронное письмо с помощью метода sendmail().\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/subprocess",
            "title": "Модуль subprocess",
            "description": "Python 101",
            "content": "\nМодуль subprocess является одним из наиболее мощных и распространенных модулей Python для управления другими процессами в операционной системе.\n\nОсновная цель subprocess заключается в том, чтобы предоставить простой и удобный способ создания новых процессов, подключения к уже существующим процессам, их управления и взаимодействия с ними.\n\nОдним из основных классов в модуле subprocess является класс Popen, который представляет собой объект, связанный с запущенным в операционной системе процессом.\n\nНапример, чтобы запустить новый процесс с помощью Popen, мы можем использовать следующий код:\n\nimport subprocess\n\nprocess = subprocess.Popen(['ls', '-l'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\nstdout, stderr = process.communicate()\nprint(stdout.decode())\n\nВ этом примере мы создаем новый процесс, который выполняет команду ls -l в командной строке операционной системы.\n\nЗатем мы отправляем строку в стандартный ввод процесса, используя метод communicate(), и получаем результат его работы в переменной output.\n\nНаконец, мы выводим содержимое переменной output на экран.\n\nКроме того, модуль subprocess также предоставляет удобный способ проверки состояния завершения процессов с помощью метода poll() и ожидания их завершения с помощью метода wait().\n\nВ целом, модуль subprocess является очень полезным инструментом для управления процессами в операционной системе и взаимодействия с ними из Python.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/sys",
            "title": "Модуль sys",
            "description": "Python 101",
            "content": "\nМодуль sys предоставляет специфические для системы параметры и функции. Он содержит системную информацию и функции для взаимодействия со стандартными потоками ввода/вывода, аргументами командной строки и другими модулями Python.\n\nsys.argv - список аргументов командной строки, переданных в программу при ее запуске. Первым аргументом обычно является имя файла программы.\n\nsys.executable - путь к интерпретатору Python, который используется для запуска текущей программы.\n\nsys.exit([arg]) - завершает выполнение программы. Если задан аргумент, то он возвращается в качестве кода выхода.\n\nsys.modules - словарь, содержащий все загруженные модули Python, включая стандартные и сторонние модули.\n\nsys.path - список путей поиска модулей Python. Включает директории, содержащие стандартные модули, а также директории, перечисленные в переменной окружения PYTHONPATH.\n\nsys.platform - строка, содержащая название операционной системы, на которой запущен Python.\n\nsys.stdin, sys.stdout, sys.stderr - объекты для взаимодействия со стандартными потоками ввода/вывода.\n\nМы можем использовать sys.argv для получения доступа к аргументам командной строки:\n\nimport sys\n\nЗапуск: python my_program.py arg1 arg2\nprint(sys.argv)  # ['my_program.py', 'arg1', 'arg2']\n\nАтрибут sys.executable может быть полезен, если требуется запустить текущую программу с другим интерпретатором Python:\n\nimport sys\nimport subprocess\n\nif 'win' in sys.platform:\n    python_executable = 'python.exe'\nelse:\n    python_executable = 'python'\n\nsubprocess.call([python_executable, 'my_program.py'])\n\nsys.exit() используется для выхода из программы. Можно передать ей код возврата в качестве аргумента, который будет использоваться для определения статуса выхода:\n\nimport sys\n\nif len(sys.argv) < 2:\n    print('Please specify a file to read')\n    sys.exit(1)\n\nfilename = sys.argv[1]\n\nЧтение файла...\n\nМы можем использовать sys.modules для получения списка всех загруженных модулей:\n\nimport sys\n\nfor name, module in sys.modules.items():\n    print(name)\n\n\nКонстанты sys.stdin, sys.stdout и sys.stderr являются стандартными потоками ввода, вывода и ошибок соответственно.\n\nНапример, если мы хотим написать программу, которая запрашивает у пользователя ввод и выводит результат на экран, мы можем использовать sys.stdin и sys.stdout:\n\nimport sys\n\nname = input(\"What is your name? \")\nsys.stdout.write(f\"Hello, {name}!\\n\")\n\nЗдесь мы запрашиваем у пользователя ввод с помощью функции input() и выводим результат на экран с помощью sys.stdout.write().\n\nАналогично, мы можем перенаправить вывод в файл, например:\n\nimport sys\n\nwith open('output.txt', 'w') as f:\n    sys.stdout = f\n    print('Hello, world!')\n\nЗдесь мы перенаправляем стандартный вывод в файл \"output.txt\" с помощью операции присваивания sys.stdout = f. Далее, когда мы вызываем функцию print(), результат будет записан в файл вместо вывода на экран.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/standard_library/threading",
            "title": "Модуль потоков threading",
            "description": "Python 101",
            "content": "\n\nМодуль threading в Python предоставляет возможность создавать и управлять потоками выполнения. Потоки - это легковесные процессы, которые выполняются параллельно в пределах одного процесса, что позволяет лучше использовать ресурсы компьютера.\n\nДля создания нового потока необходимо создать объект Thread и передать в его конструктор функцию, которую вы хотите запустить в отдельном потоке. Затем вызовите метод start() у этого объекта, чтобы запустить поток. Если вы хотите дождаться завершения потока, вызовите метод join(), который блокирует текущий поток, пока поток, на который вы вызываете join(), не завершится.\n\nПример использования модуля threading:\n\nfrom time import sleep\nimport threading\n\ndef print_numbers():\n    for i in range(10):\n        sleep(1) # задержка печати для примера\n        print(i)\n\ndef print_letters():\n    for letter in ['a', 'b', 'c', 'd', 'e']:\n        print(letter)\n\nif name == 'main':\nt1 = threading.Thread(target=print_numbers)\nt2 = threading.Thread(target=print_letters)\n\nt1.start()\nt2.start()\nprint(\"Done!\")\nt1.join()\nt2.join()\n\nprint(\"Done!\")\n\nЗдесь мы создали две функции print_numbers() и print_letters(), каждая из которых печатает набор символов в консоль. Затем мы создали два потока, один для каждой из этих функций, и запустили их, вызвав метод start(). Затем мы дождались завершения каждого потока, вызвав метод join(), и напечатали сообщение \"Done!\".\n\nВ последнем примере кода мы увидим, что каждый поток будет печатать свою информацию в консоль, в произвольном порядке, так как потоки будут конкурировать за доступ к ресурсу (в данном случае, к выводу в консоль).\n\nРезультат может отличаться от запуска к запуску программы, так как порядок выполнения потоков не гарантирован и зависит от того, как ОС распределяет ресурсы между потоками.\n\nМодуль threading также предоставляет другие полезные классы, такие как Lock, Condition, Semaphore, которые помогают управлять доступом к ресурсам между несколькими потоками.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/python-101/top-questions/",
            "title": "Топ 55 вопросов по Python",
            "description": "55 вопросов по Python для Junior, Middle и Senior в 2023",
            "content": "В процессе заполнения\n\nСкачать | PDF обновление 2023/02/17\n\nJunior\n\n1. Что такое Python? Какие преимущества использования Python?\n\nPython - это высокоуровневый интерпретируемый язык программирования общего назначения. Будучи языком общего назначения, он может быть использован для создания практически любого типа приложений при наличии соответствующих инструментов/библиотек.\n\nКроме того, python поддерживает объекты, модули, потоки, обработку исключений и автоматическое управление памятью, что помогает моделировать реальные проблемы и создавать приложения для решения этих проблем.\n\nПреимущества использования Python:\n\nPython - это язык программирования общего назначения, который имеет простой, легко изучаемый синтаксис, подчеркивающий удобочитаемость и, следовательно, снижающий затраты на сопровождение программ. Более того, язык способен выполнять сценарии, является полностью открытым и поддерживает пакеты сторонних разработчиков, что способствует модульности и повторному использованию кода.\n\nЕго высокоуровневые структуры данных в сочетании с динамической типизацией и динамическим связыванием привлекают огромное сообщество разработчиков для быстрой разработки и развертывания приложений.\n\n2. Что такое динамически типизированный язык?\n\nПрежде чем понять, что такое динамически типизированный язык, мы должны узнать, что такое типизация. Типизация относится к проверке типов в языках программирования. В языке с сильной типизацией, таком как Python, \"1\" + 2 приведет к ошибке типа, поскольку эти языки не допускают \"приведения типов\" (неявного преобразования типов данных). С другой стороны, слабо типизированный язык, такой как JavaScript, просто выведет \"12\" в качестве результата.\n\nПроверка типов может быть выполнена на двух этапах:\n\nСтатический - типы данных проверяются перед выполнением.\nДинамический - типы данных проверяются во время выполнения.\n\nPython - интерпретируемый язык, каждый оператор выполняется построчно, поэтому проверка типов выполняется на лету, во время выполнения. Следовательно, Python является динамически типизированным языком.\n\n3. Что такое интерпретируемый язык?\n\nИнтерпретированный язык выполняет свои утверждения построчно. Такие языки, как Python, JavaScript, R, PHP и Ruby, являются яркими примерами интерпретируемых языков. Программы, написанные на интерпретируемом языке, выполняются непосредственно из исходного кода, без промежуточного этапа компиляции.\n\n4. Что такое PEP 8 и почему он важен?\n\nPEP расшифровывается как Python Enhancement Proposal. PEP - это официальный проектный документ, предоставляющий информацию сообществу Python или описывающий новую функцию для Python или его процессов.\n\nPEP 8 особенно важен, поскольку в нем документированы руководящие принципы стиля для кода Python. Очевидно, что вклад в сообщество разработчиков открытого кода Python требует от вас искреннего и строгого следования этим руководящим принципам стиля.\n\n5. Что такое область видимости в Python?\n\nКаждый объект в Python функционирует в пределах области видимости. Область видимости - это блок кода, в котором объект в Python остается актуальным. Пространства имен однозначно идентифицируют все объекты внутри программы.\n\nВ Python существует 3 области видимости:\n\nЛокальная\nГлобальная\nНелокальная\n\nОднако эти пространства имен также имеют область видимости, определенную для них, где вы можете использовать их объекты без префикса. Ниже приведено несколько примеров областей видимости, создаваемых во время выполнения кода в Python:\n\nЛокальная область видимости относится к локальным объектам, доступным в текущей функции.\n\nЛокальная область видимости - определенная внутри функции, метода или выражения. Переменные, определенные внутри этой области, недоступны за ее пределами.\n\nx = 10\n\ndef my_func(a, b):\n    print(x)\n    print(z)\n>>my_func(1, 2)\n\n10\nTraceback (most recent call last):\n    File \"\", line 1, in\n        my_func(1, 2)\n    File \"\", line 3, in my_func\n        print(z)\nNameError: name 'z' is not defined\n\nГлобальная область видимости относится к объектам, доступным во время выполнения кода с момента их создания.\n\nГлобальная область видимости - определенная вне функций, методов и выражений. Переменные, определенные в глобальной области видимости, доступны везде в коде.\n\nОбласть видимости на уровне модуля относится к глобальным объектам текущего модуля, доступным в программе.\n\nГлобальная область видимости относится ко всем встроенным именам, вызываемым в программе. Объекты в этой области видимости ищутся в последнюю очередь, чтобы найти имя, на которое ссылаются.\n\ndef my_func(a, b):\n    global x\n    print(x)\n    x = 5\n    print(x)\n\nif name == 'main':\n    x = 10\n    my_func(1, 2)\n    print(x)\n\n10\n5\n5\n\n🎾 Примечание: Объекты локальной области видимости могут быть синхронизированы с объектами глобальной области видимости с помощью таких ключевых слов, как global.\n\n\nВ Python 3 было добавлено новое ключевое слово под названием nonlocal. С его помощью мы можем добавлять переопределение области во внутреннюю область. Вы можете ознакомиться со всей необходимой на данный счет информацией в PEP 3104. Это наглядно демонстрируется в нескольких примерах. Один из самых простых – это создание функции, которая может увеличиваться:\n\ndef counter():\n    num = 0\n    def incrementer():\n        num += 1\n        return num\n    return incrementer\n\nЕсли вы попробуете запустить этот код, вы получите ошибку UnboundLocalError, так как переменная num ссылается прежде, чем она будет назначена в самой внутренней функции. Давайте добавим nonlocal в наш код:\n\ndef counter():\n    num = 0\n    def incrementer():\n        nonlocal num\n        num += 1\n        return num\n    return incrementer\n\nc = counter()\nprint(c) # .incrementer at 0x7f45caf44048>\n\nc() # 1\nc() # 2\nc() # 3\n\n6. Что такое списки и кортежи? В чем ключевое различие между ними?\n\nСписки и кортежи - это типы данных последовательности, которые могут хранить коллекцию объектов в Python. Объекты, хранящиеся в обеих последовательностях, могут иметь различные типы данных. Списки представлены квадратными скобками ['sara', 6, 0.19], а кортежи - круглыми ('ansh', 5, 0.97).\n\nНо в чем реальная разница между ними? Ключевое различие между ними заключается в том, что списки являются изменяемыми, а кортежи, напротив, неизменяемыми объектами. Это означает, что списки можно изменять, добавлять или нарезать на ходу, а кортежи остаются неизменными и не могут быть изменены никаким образом. Вы можете выполнить следующий пример, чтобы убедиться в разнице:\n\nmy_tuple = ('sara', 6, 5, 0.97)\nmy_list = ['sara', 6, 5, 0.97]\nprint(my_tuple[0])     # output => 'sara'\nprint(my_list[0])     # output => 'sara'\nmy_tuple[0] = 'ansh'    # modifying tuple => throws an error\nmy_list[0] = 'ansh'    # modifying list => list modified\nprint(my_tuple[0])     # output => 'sara'\nprint(my_list[0])     # output => 'ansh'\n\n7. Каковы общие встроенные типы данных в Python?\n\nВ Python существует несколько встроенных типов данных. Хотя Python не требует явного определения типов данных при объявлении переменных, ошибки могут возникнуть, если пренебречь знанием типов данных и их совместимости друг с другом. Python предоставляет функции type() и isinstance() для проверки типа этих переменных. Эти типы данных можно сгруппировать в следующие категории-\n\nТип None:\n\nКлючевое слово None представляет нулевые значения в Python. Операция булева равенства может быть выполнена с использованием этих объектов NoneType.\n\nNoneType Представляет значения NULL в Python.\n\nЧисловые типы:\n\nСуществует три различных числовых типа - целые числа (integers), числа с плавающей точкой (floating-point) и комплексные числа(complex numbers). Кроме того, булевы числа являются подтипом целых чисел.\n\nint** Хранит целочисленные литералы, включая шестнадцатеричные, восьмеричные и двоичные числа, как целые числа\nfloat** Хранит литералы, содержащие десятичные значения и/или знаки экспоненты, как числа с плавающей точкой\ncomplex** Хранит комплексные числа в виде (A + Bj) и имеет атрибуты: real и imag\nbool** Хранит булево значение (True или False).\n\nТипы последовательностей:\n\nСогласно Python Docs, существует три основных типа последовательностей - списки (lists), кортежи (tuples) и объекты диапазона (range objects). Типы последовательностей имеют операторы in и not in, определенные для обхода их элементов. Эти операторы имеют тот же приоритет, что и операции сравнения.\n\nlist** Неизменяемая последовательность, используемая для хранения коллекции элементов.\ntuple** Неизменяемая последовательность, используемая для хранения коллекции элементов.\nrange** Представляет собой неизменяемую последовательность чисел, генерируемую во время выполнения.\nstr** Неизменяемая последовательность кодовых точек Unicode для хранения текстовых данных.\n\nСтандартная библиотека также включает дополнительные типы для обработки:\n\nДвоичные данные\nТекстовые строки, такие как str.\n\nТип словарь (dict):\n\nОбъект отображения может отображать хэшируемые значения на произвольные объекты в Python. Объекты отображения являются изменяемыми, и в настоящее время существует только один стандартный тип отображения - dict.\n\ndict** Хранит список пар ключ: значение, разделенных запятыми.\n\nТипы множеств:\n\nВ настоящее время в Python есть два встроенных типа множеств - set и frozenset.\n\nТип set является изменяемым и поддерживает такие методы, как add() и remove().\n\nТип frozenset является неизменяемым и не может быть изменен после создания.\n\nset** Мутабельная неупорядоченная коллекция отдельных хэшируемых объектов.\nfrozenset** Неизменяемая коллекция отдельных хэшируемых объектов.\n\n\nset является изменяемым и поэтому не может быть использован в качестве ключа словаря. С другой стороны, frozenset является неизменяемым и, следовательно, хэшируемым, и может использоваться как ключ словаря или как элемент другого множества.\n\n\nМодули:\n\nModule - это дополнительный встроенный тип, поддерживаемый интерпретатором Python. Он поддерживает одну специальную операцию, т.е. доступ к атрибуту: mymod.myobj, где mymod - модуль, а myobj ссылается на имя, определенное в модуле.\n\nТаблица символов модуля находится в специальном атрибуте модуля dict, но прямое присвоение этому модулю невозможно и не рекомендуется.\n\nТипы Callable:\n\nCallable типы - это типы, к которым может быть применен вызов функции. Это могут быть определяемые пользователем функции, методы экземпляра, функции генератора и некоторые другие встроенные функции, методы и классы.\n\n8. Что такое pass в Python?\n\nКлючевое слово pass представляет собой нулевую операцию в Python. Обычно оно используется для заполнения пустых блоков кода, который может выполняться во время исполнения, но еще не написан. Без оператора pass в следующем коде мы можем столкнуться с некоторыми ошибками во время выполнения кода.\n\ndef myEmptyFunc():\ndo nothing\n   pass\nmyEmptyFunc()    # nothing happens\nWithout the pass keyword\nFile \"\", line 3\nIndentationError: expected an indented block\n\n9. Что такое модули и пакеты в Python?\n\nПакеты Python и модули Python - это два механизма, которые позволяют осуществлять модульное программирование в Python. Модулирование имеет несколько преимуществ:\n\nПростота: Работа над одним модулем помогает сосредоточиться на относительно небольшой части решаемой задачи. Это делает разработку более простой и менее подверженной ошибкам.\nУдобство обслуживания: Модули предназначены для обеспечения логических границ между различными проблемными областями. Если они написаны таким образом, что уменьшают взаимозависимость, то меньше вероятность того, что изменения в модуле могут повлиять на другие части программы.\nВозможность повторного использования: Функции, определенные в модуле, могут быть легко использованы повторно в других частях приложения.\nРазметка: Модули обычно определяют отдельное пространство имен, что помогает избежать путаницы между идентификаторами из других частей программы.\n\nМодули, в общем случае, это просто файлы Python с расширением .py, в которых может быть определен и реализован набор функций, классов или переменных. Они могут быть импортированы и инициализированы один раз с помощью оператора import. Если требуется частичная функциональность, импортируйте необходимые классы или функции с помощью оператора import: from foo import bar.\n\nПакеты позволяют иерархически структурировать пространство имен модуля с помощью точечной нотации. Как модули помогают избежать столкновений между именами глобальных переменных, так и пакеты помогают избежать столкновений между именами модулей.\n\nСоздать пакет очень просто, поскольку он использует присущую системе файловую структуру. Просто поместите модули в папку, и вот оно, имя папки как имя пакета. Для импорта модуля или его содержимого из этого пакета требуется, чтобы имя пакета было префиксом к имени модуля, соединенным точкой.\n\nПримечание: технически вы можете импортировать и пакет, но, увы, это не импортирует модули внутри пакета в локальное пространство имен.\n\n10. Что такое глобальные, защищенные и приватные атрибуты в Python?\n\nГлобальные переменные- это общедоступные переменные, которые определены в глобальной области видимости. Чтобы использовать переменную в глобальной области видимости внутри функции, мы используем ключевое слово global.\n\nЗащищенные атрибуты (Protected attributes) - это атрибуты, определенные с префиксом подчеркивания к их идентификатору, например, _sara. К ним все еще можно получить доступ и изменить их извне класса, в котором они определены, но ответственный разработчик должен воздержаться от этого.\n\nПриватные атрибуты (Private attributes) - это атрибуты с двойным подчеркиванием в префиксе к их идентификатору, например __ansh. Они не могут быть доступны или изменены извне напрямую, и при такой попытке будет выдана ошибка AttributeError.\n\n11. Как используется self в Python?\n\nself используется для представления экземпляра класса. С помощью этого ключевого слова вы можете получить доступ к атрибутам и методам класса в python.\n\nself связывает атрибуты с заданными аргументами. self используется в разных местах и часто считается ключевым словом. Но в отличие от C++, self не является ключевым словом в Python.\n\n12. Что такое init?\n\ninit - это метод-конструктор в Python, который автоматически вызывается для выделения памяти при создании нового объекта/экземпляра. Все классы имеют метод init, связанный с ними. Он помогает отличить методы и атрибуты класса от локальных переменных.\n\nclass definition\nclass Student:\n   def init(self, fname, lname, age, section):\n       self.firstname = fname\n       self.lastname = lname\n       self.age = age\n       self.section = section\ncreating a new object\nstu1 = Student(\"Sara\", \"Ansh\", 22, \"A2\")\n\n13. Что такое break, continue и pass в Python?\n\nОператор break немедленно завершает цикл, а управление переходит к оператору после тела цикла.\n\nОператор continue завершает текущую итерацию оператора, пропускает остальной код в текущей итерации, а управление переходит к следующей итерации цикла.\n\nКлючевое слово pass в Python обычно используется для заполнения пустых блоков и аналогично пустому утверждению, представленному точкой с запятой в таких языках, как Java, C++, Javascript и т.д.\n\n14. Что такое модульные тесты в Python?\n\nЮнит-тесты - это структура модульного тестирования в Python.\n\nЮнит-тестирование означает тестирование различных компонентов программного обеспечения по отдельности. Можете ли вы подумать о том, почему модульное тестирование важно? Представьте себе сценарий: вы создаете программное обеспечение, которое использует три компонента, а именно A, B и C. Теперь предположим, что в какой-то момент ваше программное обеспечение ломается. Как вы определите, какой компонент был ответственен за поломку программы? Может быть, это компонент A вышел из строя, который, в свою очередь, вышел из строя компонент B, что и привело к поломке программного обеспечения. Таких комбинаций может быть множество.\n\nВот почему необходимо должным образом протестировать каждый компонент, чтобы знать, какой компонент может быть ответственен за сбой программного обеспечения.\n\n15. Что такое docstring в Python?\n\ndocstring - это многострочная строка, используемая для документирования определенного участка кода.\n\nВ docstring должно быть описано, что делает функция или метод.\n\n16.  Что такое срез в Python?\n\nКак следует из названия, \"срез\" - это взятие частей.\n\nСинтаксис следующий [start : stop : step].\n\nstart - начальный индекс, с которого производится нарезка списка или кортежа\nstop - конечный индекс или место нарезки.\nstep - количество шагов для перехода.\n\nЗначение по умолчанию для start - 0, stop - количество элементов, step - 1.\n\nСрезы можно выполнять для строк, массивов, списков и кортежей.\n\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(numbers[1 : : 2])  #output : [2, 4, 6, 8, 10]\n\n17.  Объясните, как можно сделать Python Script исполняемым на Unix?\n\nФайл сценария должен начинаться с #!/usr/bin/env python\n\n18.  В чем разница между массивами и списками в Python?\n\nМассивы в python могут содержать элементы только одного типа данных, т.е. тип данных массива должен быть однородным. Это тонкая обертка вокруг массивов языка C, и они потребляют гораздо меньше памяти, чем списки.\n\nСписки в python могут содержать элементы разных типов данных, то есть тип данных списков может быть неоднородным. Их недостатком является потребление большого объема памяти.\n\nimport array\na = array.array('i', [1, 2, 3])\nfor i in a:\n    print(i, end=' ')    #OUTPUT: 1 2 3\na = array.array('i', [1, 2, 'string'])    #OUTPUT: TypeError: an integer is required (got type str)\n\n    a = [1, 2, 'string']\nfor i in a:\n   print(i, end=' ')    #OUTPUT: 1 2 string\n\nMiddle / Senior\n\n19. Как осуществляется управление памятью в Python?\n\nУправление памятью в Python осуществляется менеджером памяти Python. Память, выделяемая менеджером, представляет собой частное пространство кучи, предназначенное для Python. Все объекты Python хранятся в этой куче, и, будучи частной, она недоступна программисту. Тем не менее, Python предоставляет некоторые основные функции API для работы с частным пространством кучи.\n\nКроме того, Python имеет встроенную сборку мусора для утилизации неиспользуемой памяти для частного пространства кучи.\n\n20. Что такое пространства имен Python? Зачем они используются?\n\nПространство имен в Python гарантирует, что имена объектов в программе уникальны и могут использоваться без каких-либо конфликтов. Python реализует эти пространства имен в виде словарей, в которых \"имя как ключ\" сопоставлено с соответствующим \"объектом как значением\". Это позволяет нескольким пространствам имен использовать одно и то же имя и сопоставлять его с отдельным объектом. Ниже приведены несколько примеров пространств имен:\n\nЛокальное пространство имен включает локальные имена внутри функции. Пространство имен временно создается для вызова функции и очищается после возвращения функции.\n\nГлобальное пространство имен включает имена из различных импортированных пакетов/модулей, которые используются в текущем проекте. Это пространство имен создается при импорте пакета в скрипт и сохраняется до выполнения скрипта.\n\nВстроенное пространство имен включает встроенные функции ядра Python и встроенные имена для различных типов исключений.\n\nЖизненный цикл пространства имен зависит от области видимости объектов, с которыми они сопоставлены. Если область видимости объекта заканчивается, жизненный цикл этого пространства имен завершается. Следовательно, невозможно получить доступ к объектам внутреннего пространства имен из внешнего пространства имен.\n\n21. Что такое разрешение области видимости в Python?\n\nИногда объекты в одной области видимости имеют одинаковые имена, но функционируют по-разному. В таких случаях разрешение области видимости в Python происходит автоматически. Вот несколько примеров такого поведения:\n\nМодули Python 'math' и 'cmath' имеют множество функций, общих для обоих - log10(), acos(), exp() и т.д. Чтобы разрешить эту двусмысленность, необходимо снабдить их префиксом соответствующего модуля, например, math.exp() и cmath.exp().\n\nРассмотрим приведенный ниже код, объект temp был инициализирован на 10 глобально и затем на 20 при вызове функции. Однако вызов функции не изменил значение temp глобально. Здесь мы можем заметить, что Python проводит четкую границу между глобальными и локальными переменными, рассматривая их пространства имен как отдельные личности.\n\ntemp = 10   # global-scope variable\ndef func():\n     temp = 20   # local-scope variable\n     print(temp)\nprint(temp)   # output => 10\nfunc()    # output => 20\nprint(temp)   # output => 10\n\nЭто поведение может быть переопределено с помощью ключевого слова global внутри функции, как показано в следующем примере:\n\ntemp = 10   # global-scope variable\ndef func():\n     global temp\n     temp = 20   # local-scope variable\n     print(temp)\nprint(temp)   # output => 10\nfunc()    # output => 20\nprint(temp)   # output => 20\n\n22. Что такое декораторы в Python?\n\nДекораторы в Python - это, по сути, функции, которые добавляют функциональность к существующей функции в Python без изменения структуры самой функции. В Python они обозначаются @decorator_name и вызываются по принципу \"снизу вверх\". Например:\n\ndecorator function to convert to lowercase\ndef lowercase_decorator(function):\n   def wrapper():\n       func = function()\n       string_lowercase = func.lower()\n       return string_lowercase\n   return wrapper\n\ndecorator function to split words\ndef splitter_decorator(function):\n   def wrapper():\n       func = function()\n       string_split = func.split()\n       return string_split\n   return wrapper\n\n@splitter_decorator # this is executed next\n@lowercase_decorator # this is executed first\ndef hello():\n    return 'Hello World'\nhello()   # output => [ 'hello' , 'world' ]\n\nПрелесть декораторов заключается в том, что помимо добавления функциональности к выходу метода, они могут даже принимать аргументы для функций и дополнительно модифицировать эти аргументы перед передачей в саму функцию. Внутренняя вложенная функция, то есть функция-\"обертка\", играет здесь важную роль. Она реализуется для обеспечения инкапсуляции и, таким образом, скрывает себя от глобальной области видимости.\n\ndecorator function to capitalize names\ndef names_decorator(function):\n   def wrapper(arg1, arg2):\n       arg1 = arg1.capitalize()\n       arg2 = arg2.capitalize()\n       string_hello = function(arg1, arg2)\n       return string_hello\n   return wrapper\n\n@names_decorator\ndef say_hello(name1, name2):\n   return 'Hello ' + name1 + '! Hello ' + name2 + '!'\nsay_hello('sara', 'ansh')   # output => 'Hello Sara! Hello Ansh!'\n\n23. Что такое comprehensions Dict и List?\n\nPython comprehensions, как и декораторы, - это синтаксический сахар, который помогает строить измененные и отфильтрованные списки, словари или множества из заданного списка, словаря или множества. Использование понятий позволяет сэкономить много времени и сэкономить код, который мог бы быть значительно более многословным (содержать больше строк кода). Давайте рассмотрим несколько примеров, в которых понимания могут быть действительно полезны:\n\nСловарные (Dict) comprehension используют фигурные скобки и позволяют создавать новые словари на основе уже существующих.\n\nnew_dict = {key: value for key, value in old_dict.items() if value > 2}\n\nВыполнение математических операций над всем списком\n\nmy_list = [2, 3, 5, 7, 11]\nsquared_list = [x**2 for x in my_list]    # list comprehension\noutput => [4 , 9 , 25 , 49 , 121]\nsquared_dict = {x:x**2 for x in my_list}    # dict comprehension\noutput => {11: 121, 2: 4 , 3: 9 , 5: 25 , 7: 49}\n\nВыполнение операций условной фильтрации для всего списка\n\nmy_list = [2, 3, 5, 7, 11]\nsquared_list = [x**2 for x in my_list if x%2 != 0]    # list comprehension\noutput => [9 , 25 , 49 , 121]\nsquared_dict = {x:x**2 for x in my_list if x%2 != 0}    # dict comprehension\noutput => {11: 121, 3: 9 , 5: 25 , 7: 49}\n\nОбъединение нескольких списков в один\n\na = [1, 2, 3]\nb = [7, 8, 9]\n[(x + y) for (x,y) in zip(a,b)]  # parallel iterators\noutput => [8, 10, 12]\n[(x,y) for x in a for y in b]    # nested iterators\noutput => [(1, 7), (1, 8), (1, 9), (2, 7), (2, 8), (2, 9), (3, 7), (3, 8), (3, 9)]\n\nПреобразование многомерного массива в одномерный\n\nАналогичный подход вложенных итераторов (как описано выше) может быть применен для сглаживания многомерного списка или работы с его внутренними элементами.\n\nmy_list = [[10,20,30],[40,50,60],[70,80,90]]\nflattened = [x for temp in my_list for x in temp]\noutput => [10, 20, 30, 40, 50, 60, 70, 80, 90]\n\nПримечание: генератор списков имеет тот же эффект, что и метод map в других языках. Они используют математическую нотацию построителя множеств, а не функции map и filter в Python.\n\n24. Что такое лямбда в Python? Почему это используется?\n\nЛямбда - это анонимная функция в Python, которая может принимать любое количество аргументов, но может иметь только одно выражение. Обычно она используется в ситуациях, когда требуется анонимная функция на короткий промежуток времени. Лямбда-функции можно использовать одним из двух способов:\n\nПрисвоение лямбда-функций переменной:\n\nmul = lambda a, b : a * b\nprint(mul(2, 5))    # output => 10\n\nОбертывание лямбда-функций внутри другой функции:\n\ndef myWrapper(n):\n    return lambda a : a * n\nmulFive = myWrapper(5)\nprint(mulFive(2))    # output => 10\n\n25. Как скопировать объект в Python?\n\nВ Python оператор присваивания (=) не копирует объекты. Вместо этого он создает связь(ссылку) между существующим объектом и именем целевой переменной. Чтобы создать копии объекта в Python, необходимо использовать модуль copy. Существует два способа создания копий для данного объекта с помощью модуля copy.\n\nShallow Copy - это побитовая копия объекта. Созданный скопированный объект имеет точную копию значений в исходном объекте. Если одно из значений является ссылкой на другие объекты, копируются только адреса ссылок на них.\n\nГлубокое копирование рекурсивно копирует все значения от исходного объекта к целевому, т.е. дублирует даже объекты, на которые ссылается исходный объект.\n\nfrom copy import copy, deepcopy\nlist_1 = [1, 2, [3, 5], 4]\nshallow copy\nlist_2 = copy(list_1)\nlist_2[3] = 7\nlist_2[2].append(6)\nlist_2    # output => [1, 2, [3, 5, 6], 7]\nlist_1    # output => [1, 2, [3, 5, 6], 4]\ndeep copy\nlist_3 = deepcopy(list_1)\nlist_3[3] = 8\nlist_3[2].append(7)\nlist_3    # output => [1, 2, [3, 5, 6, 7], 8]\nlist_1    # output => [1, 2, [3, 5, 6], 4]\n\n26. В чем разница между xrange и range в Python?\n\nxrange() и range() довольно похожи по функциональности. Они оба генерируют последовательность целых чисел, с той лишь разницей, что range() возвращает список Python, тогда как xrange() возвращает объект xrange.\n\nВ отличие от range(), xrange() не генерирует статический список, а создает значение на ходу. Эта техника обычно используется с генератором объектного типа и называется \"yielding\".\n\nВыдача очень важна в приложениях, где память ограничена. Создание статического списка, как в range(), может привести к ошибке памяти в таких условиях, в то время как xrange() может справиться с этим оптимально, используя только достаточное количество памяти для генератора (значительно меньше по сравнению с другими).\n\nfor i in xrange(10):    # numbers from o to 9\n    print i       # output => 0 1 2 3 4 5 6 7 8 9\nfor i in xrange(1,10):    # numbers from 1 to 9\n    print i       # output => 1 2 3 4 5 6 7 8 9\nfor i in xrange(1, 10, 2):    # skip by two for next\n    print i       # output => 1 3 5 7 9\n\nПримечание: xrange была устаревшей начиная с Python 3.x. Теперь range делает то же самое, что делала xrange в Python 2.x, поскольку в Python 2.x было гораздо лучше использовать xrange(), чем оригинальную функцию range().\n\n27. Что такое pickling и unpickling?\n\nБиблиотека Python предлагает функцию сериализации из коробки. Сериализация объекта означает преобразование его в формат, который можно хранить, чтобы впоследствии можно было десериализовать его и получить исходный объект.\n\nPickling - это название процесса сериализации в Python. Любой объект в Python может быть сериализован в поток байтов и выгружен в память в виде файла. Процесс pickling компактен, но объекты pickle могут быть сжаты еще больше.  pickle отслеживает объекты, которые он сериализовал, и сериализация переносима между версиями.\n\nДля этого процесса используется функция pickle.dump().\n\nРаспаковка (Unpickling) - является полной противоположностью pickle. Он десериализует поток байтов для воссоздания объектов, хранящихся в файле, и загружает объект в память.\n\nДля этого используется функция pickle.load().\n\n28. Что такое генераторы в Python?\n\nГенераторы - это функции, которые возвращают итерируемую коллекцию элементов, по одному за раз, заданным образом. Генераторы, в общем случае, используются для создания итераторов с другим подходом. Они используют ключевое слово yield, а не return для возврата объекта генератора.\n\nПостроим генератор для чисел Фибоначчи:\n\ngenerate fibonacci numbers upto n\ndef fib(n):\n    p, q = 0, 1\n    while(p  0\nx.next()    # output => 1\nx.next()    # output => 1\nx.next()    # output => 2\nx.next()    # output => 3\nx.next()    # output => 5\nx.next()    # output => 8\nx.next()    # error\n\niterating using loop\nfor i in fib(10):\n    print(i)    # output => 0 1 1 2 3 5 8\n\n29. Что такое PYTHONPATH в Python?\n\nPYTHONPATH - это переменная окружения, которую можно установить, чтобы добавить дополнительные каталоги, в которых Python будет искать модули и пакеты.\n\nЭто особенно полезно при работе с библиотеками Python, которые вы не хотите устанавливать в глобальном месте по умолчанию.\n\n30. Как используются функции help() и dir()?\n\nФункция help() в Python используется для отображения документации по модулям, классам, функциям, ключевым словам и т.д. Если функции help() не передан ни один параметр, то на консоли запускается интерактивная справочная утилита.\n\nФункция dir() пытается вернуть правильный список атрибутов и методов объекта, к которому она обращается. Она ведет себя по-разному с разными объектами, поскольку стремится выдать наиболее релевантные данные, а не полную информацию.\n\nДля объектов Modules/Library он возвращает список всех атрибутов, содержащихся в данном модуле.\nДля объектов класса возвращает список всех допустимых атрибутов и базовых атрибутов.\nПри отсутствии аргументов возвращает список атрибутов в текущей области видимости.\n\n31. В чем разница между файлами .py и .pyc?\n\nФайлы .py содержат исходный код программы. В то время как файл .pyc содержит байткод программы. Мы получаем байткод после компиляции файла .py (исходного кода). Файлы .pyc создаются не для всех файлов, которые вы запускаете. Они создаются только для тех файлов, которые вы импортируете.\n\nПеред выполнением программы python интерпретатор python проверяет наличие скомпилированных файлов. Если файл присутствует, виртуальная машина выполняет его. Если файл не найден, он проверяет наличие файла .py. Если он найден, то компилирует его в файл .pyc, а затем виртуальная машина python выполняет его.\n\nНаличие файла .pyc экономит время компиляции.\n\n32. Как интерпретируется язык Python?\n\nPython как язык не интерпретируется и не компилируется. Интерпретация или компиляция - это свойство реализации. Python - это байткод (набор инструкций, читаемых интерпретатором), интерпретируемый в общем случае.\n\nИсходный код - это файл с расширением .py.\n\nPython компилирует исходный код в набор инструкций для виртуальной машины. Интерпретатор Python является реализацией этой виртуальной машины. Этот промежуточный формат называется \"байткод\".\n\nИсходный код .py сначала компилируется, чтобы получить .pyc, который является байткодом. Затем этот байткод может быть интерпретирован официальным CPython или JIT (Just in Time compiler) компилятором PyPy.\n\n33. Как в python аргументы передаются по значению или по ссылке?\n\nПередача по значению: Передается копия реального объекта. Изменение значения копии объекта не приведет к изменению значения исходного объекта.\n\nПередача по ссылке: Передается ссылка на реальный объект. Изменение значения нового объекта изменит значение исходного объекта.\n\nВ Python аргументы передаются по ссылке, т.е. передается ссылка на реальный объект.\n\ndef appendNumber(arr):\n    arr.append(4)\n\narr = [1, 2, 3].\nprint(arr) #Вывод: => [1, 2, 3]\nappendNumber(arr)\nprint(arr) #Вывод: => [1, 2, 3, 4]\n\n34. Что такое итераторы в Python?\n\nИтератор - это объект.\nОн запоминает свое состояние, т.е. где он находится во время итерации (см. код ниже, чтобы увидеть, как это делается).\nМетод iter() инициализирует итератор.\nУ него есть метод next(), который возвращает следующий элемент в итерации и указывает на следующий элемент. При достижении конца итерируемого объекта next() должен возвращать исключение StopIteration.\nОн также является самоитерируемым.\nИтераторы - это объекты, с помощью которых мы можем выполнять итерации над итерируемыми объектами, такими как списки, строки и т.д.\n\nclass ArrayList:\n   def init(self, number_list):\n       self.numbers = number_list\n   def iter(self):\n       self.pos = 0\n       возвращать себя\n   def next(self):\n       if(self.pos < len(self.numbers)):\n           self.pos += 1\n           return self.numbers[self.pos - 1]\n       else:\n           raise StopIteration\narray_obj = ArrayList([1, 2, 3])\nit = iter(array_obj)\nprint(next(it)) #вывод: 2\nprint(next(it)) #вывод: 3\nprint(next(it))\n#Throws Exception\n#Traceback (последний последний вызов):\n#...\n#StopIteration\n\n35. Объясните, как удалить файл в Python?\n\nИспользуйте команду os.remove(file_name)\n\nimport os\n\nos.remove(\"ChangedFile.csv\")\nprint(\"File Removed!\")\n\n36. Объясните функции split() и join() в Python?\n\nВы можете использовать функцию split() для разбиения строки на основе разделителя на список строк.\n\nС помощью функции join() можно объединить список строк на основе разделителя, чтобы получить одну строку.\n\nstring = \"Текст строки\".\nstring_list = string.split(' ') #разделителем является символ \"пробел\" или ' '\nprint(string_list) #вывод: ['This', 'is', 'a', 'string.'].\nprint(' '.join(string_list)) #вывод: Это строка.\n\n37. Что означают args и *kwargs?\n\n*args\n\n*args - это специальный синтаксис, используемый в определении функции для передачи аргументов переменной длины.\n\"*\" означает переменную длину, а \"args\" - это имя, используемое по соглашению. Вы можете использовать любое другое.\n\ndef multiply(a, b, *argv):\n   mul = a * b\n   for num in argv:\n       mul *= num\n   возвращать mul\nprint(multiply(1, 2, 3, 4, 5)) #вывод: 120\n\n**kwargs\n\n**kwargs - это специальный синтаксис, используемый в определении функции для передачи аргументов переменной длины с ключевыми словами.\n\nЗдесь также kwargs используется просто по соглашению. Вы можете использовать любое другое имя.\nАргумент с ключевым словом означает переменную, которая имеет имя при передаче в функцию.\nНа самом деле это словарь имен переменных и их значений.\n\ndef tellArguments(**kwargs):\n   for key, value in kwargs.items():\n       print(key + \": \" + value)\ntellArguments(arg1 = \"аргумент 1\", arg2 = \"аргумент 2\", arg3 = \"аргумент 3\")\n#вывод:\narg1: аргумент 1\narg2: аргумент 2\narg3: аргумент 3\n\n38. Что такое отрицательные индексы и зачем они используются?\n\nОтрицательные индексы - это индексы с конца списка, кортежа или строки.\n\narr = [1, 2, 3, 4, 5, 6]\n\narr[-1] означает последний элемент массива arr[]\n\n#получить последний элемент\nprint(arr[-1]) #вывод 6\n\n#получить второй последний элемент\nprint(arr[-2]) #вывод 5\n\nJunior/Middle+ / ООП\n\n39. Как создать класс в Python?\n\nЧтобы создать класс в python, используем ключевое слово class, как показано в примере ниже:\n\nclass Employee:\n    def init(self, emp_name):\n        self.emp_name = emp_name\n\nЧтобы инстанцировать или создать объект из класса, созданного выше, мы делаем следующее:\n\nemp_1 = Employee(\"Mr. Employee\").\n\nЧтобы получить доступ к атрибуту name, мы просто вызываем атрибут с помощью точки:\n\nprint(emp_1.emp_name)\nMr. Employee\n\nЧтобы создать методы внутри класса, мы включаем их в область видимости класса:\n\nclass Employee:\n   def init(self, emp_name):\n       self.emp_name = emp_name\n\n   def introduce(self):\n       print(\"Hello I am \" + self.emp_name)\n\nПараметр self в функциях init и introduce представляет собой ссылку на текущий экземпляр класса, которая используется для доступа к атрибутам и методам этого класса. Параметр self должен быть первым параметром любого метода, определенного внутри класса.\n\nДоступ к методу класса Employee можно получить:\n\nemp_1.introduce()\n\nОбщая программа будет выглядеть следующим образом:\n\nclass InterviewbitEmployee:\n   def init(self, emp_name):\n       self.emp_name = emp_name\n\n   def introduce(self):\n       print(\"Hello I am \" + self.emp_name)\n\ncreate an object of InterviewbitEmployee class\nemp_1 = InterviewbitEmployee(\"Mr Employee\")\nprint(emp_1.emp_name)    #print employee name\nemp_1.introduce()        #introduce the employee\n\n40. Как работает наследование в python?\n\nНаследование дает классу право доступа ко всем атрибутам и методам другого класса. Это способствует повторному использованию кода и помогает разработчику поддерживать приложения без лишнего кода. Класс, наследующий от другого класса, является дочерним классом или также называется производным классом. Класс, от которого дочерний класс получает свои члены, называется родительским классом или суперклассом.\n\nPython поддерживает различные виды наследования, а именно:\n\nОдиночное наследование\nМногоуровневое наследование\nМножественное наследование\nИерархическое наследование\n\nОдиночное наследование: Дочерний класс получает члены от одного родительского класса.\n\nОдиночное наследование python\n\nParent class\nclass ParentClass:\n    def par_func(self):\n         print(\"I am parent class function\")\n\nChild class\nclass ChildClass(ParentClass):\n    def child_func(self):\n         print(\"I am child class function\")\n\nDriver code\nobj1 = ChildClass()\nobj1.par_func()\nobj1.child_func()\n\nМногоуровневое наследование: Члены родительского класса A наследуются дочерним классом, который затем наследуется другим дочерним классом B. Характеристики базового и производного классов далее наследуются в новом производном классе C.\n\nЗдесь A является дедушкой класса C.\n\nМногоуровневое наследование python\n\nParent class\nclass A:\n   def init(self, a_name):\n       self.a_name = a_name\n\nIntermediate class\nclass B(A):\n   def init(self, b_name, a_name):\n       self.b_name = b_name\ninvoke constructor of class A\n       A.init(self, a_name)\n\nChild class\nclass C(B):\n   def init(self,c_name, b_name, a_name):\n       self.c_name = c_name\ninvoke constructor of class B\n       B.init(self, b_name, a_name)\n\n   def display_names(self):\n       print(\"A name : \", self.a_name)\n       print(\"B name : \", self.b_name)\n       print(\"C name : \", self.c_name)\n\nDriver code\nobj1 = C('child', 'intermediate', 'parent')\nprint(obj1.a_name)\nobj1.display_names()\n\nМножественное наследование: Это достигается, когда один дочерний класс получает свойста от более чем одного родительского класса. Все свойства родительских классов наследуются в дочернем классе.\n\nМножественное наследование python\n\nParent class1\nclass Parent1:\n   def parent1_func(self):\n       print(\"Hi I am first Parent\")\n\nParent class2\nclass Parent2:\n   def parent2_func(self):\n       print(\"Hi I am second Parent\")\n\nChild class\nclass Child(Parent1, Parent2):\n   def child_func(self):\n       self.parent1_func()\n       self.parent2_func()\n\nDriver's code\nobj1 = Child()\nobj1.child_func()\n\nИерархическое наследование: Когда от родительского класса происходит более одного дочернего класса.\n\nИерархическое наследование python\n\nBase class\nclass A:\n     def a_func(self):\n         print(\"I am from the parent class.\")\n\n1st Derived class\nclass B(A):\n     def b_func(self):\n         print(\"I am from the first child.\")\n\n2nd Derived class\nclass C(A):\n     def c_func(self):\n         print(\"I am from the second child.\")\n\nDriver's code\nobj1 = B()\nobj2 = C()\nobj1.a_func()\nobj1.b_func()    #child 1 method\nobj2.a_func()\nobj2.c_func()    #child 2 method\n\n41. Как получить доступ к членам родительского класса в дочернем классе?\n\nНиже перечислены способы, с помощью которых вы можете получить доступ к членам родительского класса в дочернем классе:\n\nС помощью имени родительского класса: Вы можете использовать имя родительского класса для доступа к атрибутам, как показано в примере ниже:\n\nclass Parent(object):\nConstructor\n   def init(self, name):\n       self.name = name\n\nclass Child(Parent):\nConstructor\n   def init(self, name, age):\n       Parent.name = name\n       self.age = age\n\n   def display(self):\n       print(Parent.name, self.age)\n\nDriver Code\nobj = Child(\"ParentName\", 6)\nobj.display()\n\nС помощью метода super(): Члены родительского класса могут быть доступны в дочернем классе с помощью ключевого слова super.\n\nclass Parent(object):\nConstructor\n   def init(self, name):\n       self.name = name\n\nclass Child(Parent):\nConstructor\n   def init(self, name, age):\n       '''\n       In Python 3.x, we can also use super().init(name)\n       '''\n       super(Child, self).init(name)\n       self.age = age\n\n   def display(self):\nNote that Parent.name cant be used\nhere since super() is used in the constructor\n      print(self.name, self.age)\n\nDriver Code\nobj = Child(\"Interviewbit\", 6)\nobj.display()\n\n42. Используются ли спецификаторы доступа в python?\n\nДа, в Python есть спецификаторы доступа, но они не являются строгими и не работают так же, как в других языках, таких как C++ или Java.\n\nВ Python есть три уровня спецификаторов доступа:\n\nPublic** - открытый доступ. Переменные и методы, объявленные без какого-либо спецификатора доступа, считаются общедоступными и могут быть использованы в любом месте программы.\nProtected** - защищенный доступ. Переменные и методы, которые начинаются с символа подчеркивания (_), считаются защищенными и должны использоваться только внутри класса и его потомков.\nPrivate** - закрытый доступ. Переменные и методы, которые начинаются с двух символов подчеркивания (__), считаются закрытыми и не могут быть использованы за пределами класса, даже его потомками.\n\nОднако в Python все переменные и методы на самом деле являются общедоступными и могут быть доступны вне класса, даже если они были объявлены с использованием защищенного или закрытого спецификатора доступа. Но общепринятым правилом считается использование подчеркиваний в начале имен переменных и методов, чтобы показать, что они не предназначены для использования вне класса.\n\nТакже существует соглашение, что имена методов и переменных, начинающихся с двух символов подчеркивания, должны использоваться только внутри класса.\n\nПример использования спецификаторов доступа в Python:\n\nclass Example:\n    def init(self):\n        self.public_variable = \"Public variable\" # публичная переменная\n        self._protected_variable = \"Protected variable\" # защищенная переменная\n        self.__private_variable = \"Private variable\" # приватная переменная\n\n    def public_method(self):\n        print(\"Public method\")\n\n    def _protected_method(self):\n        print(\"Protected method\")\n\n    def __private_method(self):\n        print(\"Private method\")\n\nexample = Example()\n\nДоступ к публичной переменной и методу\nprint(example.public_variable) # выведет \"Public variable\"\nexample.public_method() # выведет \"Public method\"\n\nДоступ к защищенной переменной и методу\nprint(example._protected_variable) # выведет \"Protected variable\"\nexample._protected_method() # выведет \"Protected method\"\n\nДоступ к приватной переменной и методу\nВозникнет ошибка AttributeError, потому что переменная и метод приватные\nprint(example.__private_variable)\nexample.__private_method()\n\nВ этом примере мы создали класс Example с тремя переменными и методами, которые имеют разные уровни спецификаторов доступа. Затем мы создали объект example и использовали различные спецификаторы доступа, чтобы получить доступ к его переменным и методам.\n\n43. Можно ли вызвать родительский класс без создания его экземпляра?\n\nДа, это возможно, если базовый класс инстанцируется другими дочерними классами или если базовый класс является статическим методом.\n\n44. Как в python создается пустой класс?\n\nПустой класс не имеет определенных свойств/методов, определенных внутри. Он создается с помощью ключевого слова pass (команда pass ничего не делает в python). Мы можем создавать объекты для этого класса вне класса.\n\nclass MyClass:\n    pass\n\n45. Проведите различие между модификаторами new и override.\n\nnew используется для создания новой реализации метода в классе-наследнике, которая заменяет реализацию метода в базовом классе. Это означает, что когда метод вызывается на объекте класса-наследника, будет использоваться новая реализация метода из класса-наследника, а не из базового класса.\n\noverride используется для переопределения реализации метода, унаследованного от базового класса. Это означает, что когда метод вызывается на объекте класса-наследника, будет использоваться новая реализация метода из класса-наследника, а не реализация метода из базового класса.\n\nЕсли в классе-наследнике не определен метод с тем же именем, что и метод в базовом классе, то метод базового класса будет унаследован.\n\nclass BaseClass:\n    def method(self):\n        print(\"BaseClass.method\")\n\nclass DerivedClass1(BaseClass):\n    def method(self):\n        print(\"DerivedClass1.method\")\n\nclass DerivedClass2(BaseClass):\n    def new_method(self):\n        print(\"DerivedClass2.new_method\")\n\nclass DerivedClass3(BaseClass):\n    def method(self):\n        super().method()\n        print(\"DerivedClass3.method\")\n\nbase_object = BaseClass()\nderived_object1 = DerivedClass1()\nderived_object2 = DerivedClass2()\nderived_object3 = DerivedClass3()\n\nbase_object.method() # выведет \"BaseClass.method\"\nderived_object1.method() # выведет \"DerivedClass1.method\"\nderived_object2.new_method() # выведет \"DerivedClass2.new_method\"\nderived_object3.method() # выведет \"BaseClass.method\" и \"DerivedClass3.method\"\n\n46. Как использовать декораторы для определения свойств (property) в Python?\n\nВ Python свойства (property) позволяют использовать методы геттера (getter) и сеттера (setter) для доступа к данным объекта, скрывая реализацию от пользователя.\n\nДля определения свойства (property) в Python используются декораторы @property, @property_name.setter и @property_name.deleter. Декоратор @property указывается перед методом геттера, который должен возвращать значение свойства. Декоратор @property_name.setter указывается перед методом сеттера, который должен устанавливать значение свойства. Декоратор @property_name.deleter указывается перед методом удаления, который должен удалить свойство.\n\nПример определения свойства (property) с помощью декораторов в Python:\n\nclass Rectangle:\n    def init(self, width, height):\n        self.width = width\n        self.height = height\n\n    @property\n    def area(self):\n        return self.width * self.height\n\n    @property\n    def perimeter(self):\n        return 2 * (self.width + self.height)\n\n    @property\n    def width(self):\n        return self._width\n\n    @width.setter\n    def width(self, value):\n        if value <= 0:\n            raise ValueError(\"Width must be positive.\")\n        self._width = value\n\n    @property\n    def height(self):\n        return self._height\n\n    @height.setter\n    def height(self, value):\n        if value <= 0:\n            raise ValueError(\"Height must be positive.\")\n        self._height = value\n\n    @property\n    def dimensions(self):\n        return (self.width, self.height)\n\n    @dimensions.setter\n    def dimensions(self, values):\n        self.width, self.height = values\n\n    def str(self):\n        return f\"Rectangle({self.width}, {self.height})\"\n\n\nВ этом примере мы создали класс Rectangle, который определяет прямоугольник с шириной и высотой. Мы определили свойства (property) area, perimeter, width, height и dimensions с помощью декораторов.\n\nСвойства area и perimeter используют методы геттера для вычисления площади и периметра.\n\nСвойства width и height используют методы геттера и сеттера для доступа к ширине и высоте.\n\nСвойство dimensions использует методы геттера и сеттера для доступа к ширине и высоте в виде кортежа.\n\nТеперь мы можем создать объект класса Rectangle и использовать свойства для доступа к данным:\n\nrect = Rectangle(3, 4)\nprint(rect.width) # выведет 3\nprint(rect.height) # выведет 4\nprint(rect.area) # выведет 12\nprint(rect.perimeter)\n\n47. Что такое метод init в python?\n\nВ Python метод init является конструктором класса, который вызывается при создании нового объекта класса. Он используется для инициализации свойств объекта и может принимать параметры, которые передаются при создании объекта.\n\nclass MyClass:\n    def init(self, name):\n        self.name = name\n\nmy_object = MyClass(\"John\")\nprint(my_object.name) # выведет \"John\"\n\nВ этом примере мы создали класс MyClass с методом init, который инициализирует свойство name объекта класса. При создании объекта my_object мы передали ему параметр \"John\", который был использован для инициализации свойства name.\n\n48. Как проверить, является ли класс дочерним по отношению к другому классу?\n\nВ Python можно проверить, является ли класс дочерним по отношению к другому классу с помощью функции issubclass. Функция issubclass принимает два аргумента: класс-потомок и класс-родитель, и возвращает True, если класс-потомок является подклассом класса-родителя, и False в противном случае.\n\nВот пример использования функции issubclass в Python:\n\nclass BaseClass:\n    pass\n\nclass DerivedClass(BaseClass):\n    pass\n\nprint(issubclass(DerivedClass, BaseClass)) # выведет True\nprint(issubclass(BaseClass, DerivedClass)) # выведет False\n\nВ этом примере мы создали два класса, BaseClass и DerivedClass, где DerivedClass наследует BaseClass. Мы затем использовали функцию issubclass, чтобы проверить, является ли DerivedClass дочерним по отношению к BaseClass, и вывели результат на экран.\n\nВ этом примере функция issubclass(DerivedClass, BaseClass) возвращает True, потому что DerivedClass является дочерним по отношению к BaseClass.\n\nА функция issubclass(BaseClass, DerivedClass) возвращает False, потому что BaseClass не является дочерним по отношению к DerivedClass.\n\nБиблиотеки\n\n49. Различия между пакетом и модулем в python.\n\nРазличия между пакетами и модулями в Python: модуль - это файл с расширением .py, который содержит определение функций, классов и других объектов, которые могут быть использованы в других модулях. Пакет - это каталог, который содержит один или несколько файлов модулей и может содержать другие подкаталоги.\n\n50. Каковы некоторые из наиболее часто используемых встроенных модулей в Python?\n\nНекоторые из наиболее часто используемых встроенных модулей в Python включают os, sys, math, random, datetime, re, json, csv, urllib, socket и многие другие.\n\n51. Что такое лямбда-функции?\n\nЛямбда-функции в Python - это функции, которые определяются без использования ключевого слова def. Они используются для определения функции в одной строке кода.\n\n52. Как можно генерировать случайные числа?\nВ Python можно генерировать случайные числа с помощью модуля random. Например, для генерации случайного числа в диапазоне от 1 до 10 можно использовать функцию random.randint(1, 10).\n\n53. Можете ли вы проверить, все ли символы в заданной строке являются буквенно-цифровыми?\nДля проверки, являются ли все символы в заданной строке буквенно-цифровыми, можно использовать метод isalnum(). Например, \"Abc123\".isalnum() вернет True, а \"Abc 123\".isalnum() вернет False.\n\n54. Дайте определение понятию GIL.\nGIL (Global Interpreter Lock) - это механизм блокировки интерпретатора Python, который ограничивает выполнение только одного потока Python в любой момент времени. Это ограничение делает невозможным многопоточное выполнение Python-кода на нескольких ядрах процессора.\n\n55. Существуют ли инструменты для выявления ошибок и выполнения статического анализа в python?\nВ Python существует множество инструментов для идентификации ошибок и проведения статического анализа кода, таких как pylint, pyflakes, pycodestyle и другие.\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/_index",
            "title": "Карманная книга по WebRTC",
            "description": "Карманная книга по WebRTC",
            "content": "\nСоздание нового приложения на базе WebRTC-технологий может стать непосильной задачей, если вы не знакомы с API. В этом разделе мы покажем, как начать работать с различными API в стандарте WebRTC, на большом количестве примеров и фрагментов кода, решающих эти задачи.\n\nWebRTC API\n\nСтандарт WebRTC работает с двумя различными технологиями: мультимедиа-устройства и P2P-соединение.\nМультимедиа-устройства включают в себя не только камеры и микрофоны, но также и «устройства» захвата экрана. Для камер и микрофонов мы используем navigator.mediaDevices.getUserMedia() для захвата MediaStreams. Для записи же мы используем navigator.mediaDevices.getDisplayMedia().\n\nP2P соединение настраивается через интерфейс RTCPeerConnection. Это ключевой пункт для установления и управления соединением между двумя узлами в WebRTC.\n\nРесурсы:\nhttps://webrtc.org/getting-started/overview\nhttps://codelabs.developers.google.com/codelabs/webrtc-web\n\n\n\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/data-channels",
            "title": "Каналы данных",
            "description": "Карманная книга по WebRTC",
            "content": "Стандарт WebRTC также охватывает API для отправки произвольных данных через RTCPeerConnection. Это происходит через запрос createDataChannel() для объекта RTCPeerConnection, который возвращает объект RTCDataChannel.\n\nconst peerConnection = new RTCPeerConnection(configuration);\nconst dataChannel = peerConnection.createDataChannel();\n\nУдаленный узел может получать каналы данных через отслеживание события datachannel в объекте RTCPeerConnection. Полученное событие имеет тип RTCDataChannelEvent и содержит свойство channel, которое представляет RTCDataChannel между двумя узлами.\n\nconst peerConnection = new RTCPeerConnection(configuration);\npeerConnection.addEventListener('datachannel', event => {\n    const dataChannel = event.channel;\n});\n\nСобытия Open и Close\n\nПрежде чем канал данных можно будет использовать для отправки данных, клиент должен дождаться его открытия. Это происходит через прослушивание события open. Точно так же существует событие close, когда одна из сторон закрывает канал.\n\nconst messageBox = document.querySelector('#messageBox');\nconst sendButton = document.querySelector('#sendButton');\nconst peerConnection = new RTCPeerConnection(configuration);\nconst dataChannel = peerConnection.createDataChannel();\n\n// Enable textarea and button when opened\ndataChannel.addEventListener('open', event => {\n    messageBox.disabled = false;\n    messageBox.focus();\n    sendButton.disabled = false;\n});\n\n// Disable input when closed\ndataChannel.addEventListener('close', event => {\n    messageBox.disabled = false;\n    sendButton.disabled = false;\n});\n\nСообщения\n\nОтправка сообщения в RTCDataChannel выполняется через вызов функции send() с данными, которые мы хотим отправить. Параметр data для этой функции может быть типа String, Blob, ArrayBuffer или ArrayBufferView.\n\nconst messageBox = document.querySelector('#messageBox');\nconst sendButton = document.querySelector('#sendButton');\n\n// Send a simple text message when we click the button\nsendButton.addEventListener('click', event => {\n    const message = messageBox.textContent;\n    dataChannel.send(message);\n})\n\nУдаленный узел будет получать сообщения, отправленные на RTCDataChannel, через отслеживание события message.\n\nconst incomingMessages = document.querySelector('#incomingMessages');\n\nconst peerConnection = new RTCPeerConnection(configuration);\nconst dataChannel = peerConnection.createDataChannel();\n\n// Append new messages to the box of incoming messages\ndataChannel.addEventListener('message', event => {\n    const message = event.data;\n    incomingMessages.textContent += message + '\\n';\n});\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/media-capture-and-constraints",
            "title": "Захват мультимедиа и ограничения",
            "description": "Карманная книга по WebRTC",
            "content": "\nЗахват мультимедиа и ограничения\n\nМультимедиа-часть WebRTC показывает, как получить доступ к оборудованию, способному записывать видео и аудио (например, камеры и микрофоны), а также как работают медиа-потоки. И помимо этого – средства отображения, которые позволяют делать захват экрана.\n\nМультимедиа-устройства\n\nВсе камеры и микрофоны, поддерживаемые браузером, доступны и управляются через объект navigator.mediaDevices. Приложения могут получать текущий список подсоединенных устройств и отслеживать изменения, т.к. многие камеры и микрофоны подсоединены через USB, и могут подключаться/отключаться в течение работы приложения. Поскольку статус мультимедиа-устройства может меняться в любой момент времени, рекомендуем, чтоб приложения регистрировали все изменения в статусе устройства для правильной обработки статусов изменений.\n\nОграничения\n\nПри получении доступа к мультимедиа-устройствам, хорошо бы обеспечить настолько подробные ограничения, насколько это возможно. И хотя можно открыть камеру и микрофон по умолчанию с простым ограничением, это может привести к тому, что медиапоток будет далеко не самым оптимальным для приложения.\n\nКонкретные ограничения определяются в объекте MediaTrackConstraint (одно для аудио, одно для видео). Атрибуты в этом объекте типа ConstraintLong, ConstraintBoolean, ConstraintDouble или ConstraintDOMString. Данные могут быть как конкретным значением (например, число, Boolean или String), диапазоном (LongRange или DoubleRange с минимальным и максимальным значением) или объектом c ideal или exact определением. Для конкретных значений браузер будет пытаться выбрать что-то наиболее близкое. Для диапазонных будет использоваться лучшее значение из диапазона. Для exact – будет передаваться только тот медиа-поток, который точно соответствует заданным ограничениям.\n\nNEAR\n// Camera with a resolution as close to 640x480 as possible\n{\n    \"video\": {\n        \"width\": 640,\n        \"height\": 480\n    }\n}\n\nRANGE\n// Camera with a resolution in the range 640x480 to 1024x768\n{\n    \"video\": {\n        \"width\": {\n            \"min\": 640,\n            \"max\": 1024\n        },\n        \"height\": {\n            \"min\": 480,\n            \"max\": 768\n        }\n    }\n}\n\nEXACT\n// Camera with the exact resolution of 1024x768\n{\n    \"video\": {\n        \"width\": {\n            \"exact\": 1024\n        },\n        \"height\": {\n            \"exact\": 768\n        }\n    }\n}\n\nЧтобы определить актуальную конфигурацию конкретной дорожки медиа-потока, мы можем воспользоваться запросом MediaStreamTrack.getSettings(), который возвращает набор настроек MediaTrackSettings, используемых в данные момент.\n\nТакже можно обновить ограничения дорожки с мультимедиа-устройства, которое открываем через applyConstraints(). Это позволяет приложению перенастроить устройство без прерывания текущего потока.\n\nЗахват экрана\n\nПриложение, которое потенциально может выполнять захват и запись экрана, должно использовать Display Media API. Функция getDisplayMedia() (которая является частью navigator.mediaDevices), аналогична getUserMedia() и используется, чтобы открыть содержимое дисплея (или его части, например, окна). Возвращенный MediaStream работает также, как при использовании getUserMedia().\n\nОграничения для getDisplayMedia() отличаются от ограничений, используемых для обычных входящих видео- и аудио-потоков.\n\n{\n    video: {\n        cursor: ‘always’ | ‘motion’ | ‘never’,\n        displaySurface: ‘application’ | ‘browser’ | ‘monitor’ | ‘window’\n    }\n}\n\nФрагмент кода выше показывает, как работают специальные ограничения для записи экрана. Обратите внимание, что они могут не поддерживаться некоторыми браузерами, поддерживающими отображение мультимедиа.\n\nПотоки и дорожки\n\nMediaStream представляет собой поток медиаконтента, который состоит из аудио- и видео- дорожек (MediaStreamTrack). Можно достать все дорожки из MediaStream, вызвав команду MediaStream.getTracks(), которая возвращает массив объектов из MediaStreamTrack.\n\nMediaStreamTrack\n\nMediaStreamTrack обладает свойством kind (audio или video, указывающий тип мультимедиа, который он воспроизводит). Каждую дорожку можно выключить, переключив ее свойство enabled. У дорожки есть логическое свойство remote, которое показывает, является ли она источником RTCPeerConnection и идет ли она от удаленного узла.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/media-devices",
            "title": "Мультимедиа-устройства",
            "description": "Карманная книга по WebRTC",
            "content": "\nМультимедиа-устройства\n\nНачало работы с мультимедийными устройствами\nПри web-разработке WebRTC-стандарт предоставляет API для доступа к камерам и микрофонам, подключенным к компьютеру или смартфону. Эти устройства обычно называются мультимедийными устройствами, и к ним можно получить доступ с помощью Java-скрипта через объект navigator.mediaDevices, который реализует интерфейс MediaDevices. С помощью этого объекта мы можем просмотреть все подключенные устройства, отслеживать изменения статуса устройства (когда устройство подключается или отключается) и открывать устройство для извлечения мультимедийного потока (см. ниже).\nЧаще всего для этого используют функцию getUserMedia(), которая возвращает промис, который будет преобразован в MediaStream для соответствующих мультимедийных устройств. Эта функция принимает один объект MediaStreamConstraints, который определяет имеющиеся требования. Например, чтобы просто открыть микрофон и камеру по умолчанию, мы должны сделать следующее:\n\nЧерез промисы:\nconst constraints = {\n    'video': true,\n    'audio': true\n}\nnavigator.mediaDevices.getUserMedia(constraints)\n    .then(stream => {\n        console.log('Got MediaStream:', stream);\n    })\n    .catch(error => {\n        console.error('Error accessing media devices.', error);\n    });\n\nЧерез async/await\nconst openMediaDevices = async (constraints) => {\n    return await navigator.mediaDevices.getUserMedia(constraints);\n}\n\ntry {\n    const stream = openMediaDevices({'video':true,'audio':true});\n    console.log('Got MediaStream:', stream);\n} catch(error) {\n    console.error('Error accessing media devices.', error);\n}\n\nОбращение к getUserMedia() запускает запрос на разрешение. Если пользователь одобряет запрос, промис разрешает MediaStream, содержащий одну видео и одну аудио дорожку. Если запрос отклонен, появляется PermissionDeniedError. Если же нет подключенных устройств, появляется NotFoundError.\nПолный список API для интерфейса MediaDevices доступен по ссылке https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices\n\nОбращение к мультимедиа-устройствам\n\nВ более сложных приложениях, мы скорее всего захотим проверить все подключенные камеры и микрофоны и дать соответствующий отчет пользователю. Это можно сделать через запрос функции enumerateDevices(). Она возвращает промис, который преобразуется в массив MediaDevicesInfo, описывающий каждое известное мультимедиа-устройство. Через него мы можем предоставить пользовательский интерфейс пользователю, который позволит выбрать те или иные устройства. Каждый список MediaDevicesInfo содержит свойства, которые называются kind с значениями audioinput, audiooutput или videoinput, отражая, какой это тип мультимедиа-устройства.\n\nЧерез промисы\n\nfunction getConnectedDevices(type, callback) {\n    navigator.mediaDevices.enumerateDevices()\n        .then(devices => {\n            const filtered = devices.filter(device => device.kind === type);\n            callback(filtered);\n        });\n}\n\ngetConnectedDevices('videoinput', cameras => console.log('Cameras found', cameras));\n\nчерез async/await\nasync function getConnectedDevices(type) {\n    const devices = await navigator.mediaDevices.enumerateDevices();\n    return devices.filter(device => device.kind === type)\n}\n\nconst videoCameras = getConnectedDevices('videoinput');\nconsole.log('Cameras found:', videoCameras);\n\nОтслеживание изменений в статусах устройств\n\nБольшинство компьютеров поддерживают подключение различных устройств прямо во время работы. Это может быть веб-камера, подключенная через USB, Bluetooth-гарнитура или внешние динамики. Чтобы должным образом поддерживать все это, веб-приложение должно отслеживать изменения в статусах мультимедиа-устройств. Это можно сделать, добавив «отслеживатель» в navigator.mediaDevices для события devicechange.\n\n// Updates the select element with the provided set of cameras\nfunction updateCameraList(cameras) {\n    const listElement = document.querySelector(‘select#availableCameras’);\n    listElement.innerHTML = ‘’;\n    cameras.map(camera => {\n        const cameraOption = document.createElement(‘option’);\n        cameraOption.label = camera.label;\n        cameraOption.value = camera.deviceId;\n    }).forEach(cameraOption => listElement.add(cameraOption));\n}\n\n// Fetch an array of devices of a certain type\nasync function getConnectedDevices(type) {\n    const devices = await navigator.mediaDevices.enumerateDevices();\n    return devices.filter(device => device.kind === type)\n}\n\n// Get the initial set of cameras connected\nconst videoCameras = getConnectedDevices(‘videoinput’);\nupdateCameraList(videoCameras);\n\n// Listen for changes to media devices and update the list accordingly\nnavigator.mediaDevices.addEventListener(‘devicechange’, event => {\n    const newCameraList = getConnectedDevices(‘video’);\n    updateCameraList(newCameraList);\n});\n\nОграничения для мультимедиа\n\nОбъект ограничений, осуществляющий интерфейс MediaStreamConstraints и который мы отправляем в качестве параметра в getUserMedia(), позволяет нам открывать мультимедиа-устройство, которое отвечает определенным требованиям. Эти требования могут быть как очень расплывчатыми (аудио и/или видео), так и очень специфичными (минимальное разрешение камеры или точный ID устройства). Рекомендуем, чтобы приложения, использующие getUserMedia() API, сначала проверяли существующие устройства, а затем определяли ограничения, которые соответствуют точному устройству через deviceID-ограничение. Устройства, по возможности, будут настроены в соответствии с ограничениями. Мы можем включить эхоподавление на микрофоне, установить определенную или минимальную ширину и высоту видео с камеры.\n\nasync function getConnectedDevices(type) {\n    const devices = await navigator.mediaDevices.enumerateDevices();\n    return devices.filter(device => device.kind === type)\n}\n\n// Open camera with at least minWidth and minHeight capabilities\nasync function openCamera(cameraId, minWidth, minHeight) {\n    const constraints = {\n        'audio': {'echoCancellation': true},\n        'video': {\n            'deviceId': cameraId,\n            'width': {'min': minWidth},\n            'height': {'min': minHeight}\n            }\n        }\n\n    return await navigator.mediaDevices.getUserMedia(constraints);\n}\n\nconst cameras = getConnectedDevices('videoinput');\nif (cameras && cameras.length > 0) {\n    // Open first available video camera with a resolution of 1280x720 pixels\n    const stream = openCamera(cameras[0].deviceId, 1280, 720);\n}\n\nПолную документацию для интерфейса MediaStreamConstraints можно найти по ссылке: https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamConstraints\n\nЛокальное воспроизведение\nКак только мультимедиа-устройство открыто и есть доступный MediaStream, мы можем назначить его для его видео- или аудио-элемента локальное воспроизведение потока.\n\nasync function playVideoFromCamera() {\n    try {\n        const constraints = {'video': true, 'audio': true};\n        const stream = await navigator.mediaDevices.getUserMedia(constraints);\n        const videoElement = document.querySelector('video#localVideo');\n        videoElement.srcObject = stream;\n    } catch(error) {\n        console.error('Error opening video camera.', error);\n    }\n}\n\nОбычно код HTML, необходимый для типичного видео-элемента с getUserMedia(), имеет атрибуты autoplay и playsinline. Атрибут autoplay запускает воспроизведение новых потоков, связанных с элементом, автоматически. Атрибут playsinline позволяет проигрывать встроенное видео вместо видео на весь экран, в некоторых мобильных браузерах. Также рекомендуем использовать controls = “false” для прямых эфиров, если у пользователя нет необходимости ставить их на паузу.\n\n\nLocal video playback\n\n\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/peer-connections",
            "title": "Одноранговые соединения",
            "description": "Карманная книга по WebRTC",
            "content": "\nНачало работы с одноранговыми соединениями\n\nОдноранговые соединения – часть спецификации WebRTC, которая занимается связью двух приложений на различных компьютерах для коммуникации через P2P-протокол. Коммуникация между узлами может быть видео-, аудио- или произвольными двоичными данными (для клиентов, поддерживающих RTCDataChannel API). Чтобы выяснить, как два узла могут быть соединены, оба клиента должны предоставить конфигурацию ICE-Server. Это или STUN, или TURN-сервер, и их роль – обеспечить ICE-кандидатов для каждого клиента, который затем передается на удаленный узел. Эта «передача» ICE-кандидатов обычно называется «сигналинг».\n\nСигналинг\n\nСпецификации WebRTC включают API для коммуникации с ICE-сервером (ICE =Internet Connectivity Establishment, установление интерактивного подключения), но компонент сигналинга не является частью этого сервера. Сигналинг необходим, чтобы два узла могли использовать один и тот же способ подключения. Обычно это можно решить через обычный Web API на базе HTTP (то есть службу REST или другой механизм RPC), где веб-приложения могут передавать необходимую информацию до того, как будет установлено соединение.\nСледующий фрагмент кода показывает, как эту придуманную службу сигналинга можно использовать для отправки и получения асинхронных сообщений. Мы будем использовать по необходимости этот прием в оставшихся примерах в этом гайде.\n\n// Set up an asynchronous communication channel that will be\n// used during the peer connection setup\nconst signalingChannel = new SignalingChannel(remoteClientId);\nsignalingChannel.addEventListener('message', message => {\n    // New message from remote client received\n});\n\n// Send an asynchronous message to the remote client\nsignalingChannel.send('Hello!');\n\nСигналинг может быть реализован разными способами, и спецификация WebRTC не отдает предпочтений какому-то определенному варианту.\n\nИнициирование одноранговых соединений\nКаждое одноранговое соединение управляется объектом RTCPeerConnection. Конструктор для этого класса берет в качестве параметра одиночный объект RTCConfiguration. Этот объект определяет, как одноранговое соединение устанавливается, и какую информацию должен содержать об используемых ICE-серверах.\n\nПосле того, как RTCPeerConnection установлено, мы должны задать SDP-запрос/ответ, в зависимости от того, являемся мы вызывающим или принимающим узлом. После того, как SDP-запрос/ответ создан, он должен быть отправлен на удаленный узел через другой канал. Передача SDP-объектов на удаленные узлы называется сигналингом и не рассматривается в WebRTC спецификации.\n\nДля установки однорангового соединения с вызывающей стороны, мы создаем объект RTCPeerConnection, и затем вызываем createOffer() для создания объекта RTCSessionDescription. Описание этого сеанса устанавливается как локальное описание с использованием setLocalDescription(), и затем отправляется через наш сигналинг-канал получающей стороне. Мы также устанавливаем «прослушиватель» для нашего сигналинг-канала, чтобы знать, когда получающей стороной будет получен ответ на описание нашего запрошенного сеанса.\n\nasync function makeCall() {\n    const configuration = {'iceServers': [{'urls': 'stun:stun.l.google.com:19302'}]}\n    const peerConnection = new RTCPeerConnection(configuration);\n    signalingChannel.addEventListener('message', async message => {\n        if (message.answer) {\n            const remoteDesc = new RTCSessionDescription(message.answer);\n            await peerConnection.setRemoteDescription(remoteDesc);\n        }\n    });\n    const offer = await peerConnection.createOffer();\n    await peerConnection.setLocalDescription(offer);\n    signalingChannel.send({'offer': offer});\n}\n\nНа получающей стороне мы ждем входящий запрос до того, как мы создали пример RTCPeerConnection. После этого мы устанавливаем полученный запрос, используя setRemoteDescription().\n\nДалее, мы делаем запрос createAnswer() для создания ответа на полученный запрос. Этот ответ устанавливается как локальное описание через использование setLocalDescription() и затем отправляется набирающей стороне через наш сигналинг-сервер.\n\nconst peerConnection = new RTCPeerConnection(configuration);\nsignalingChannel.addEventListener('message', async message => {\n    if (message.offer) {\n        peerConnection.setRemoteDescription(new RTCSessionDescription(message.offer));\n        const answer = await peerConnection.createAnswer();\n        await peerConnection.setLocalDescription(answer);\n        signalingChannel.send({'answer': answer});\n    }\n});\n\nКак только два узла установили описания и локального, и удаленного сеансов, становятся доступны возможности удаленного узла. Это еще не означает, что соединение между узлами готово. Для работы необходимо собрать ICE-кандидатов на каждом узле и передать (по сигналинг-каналу) другому узлу.\n\nICE-кандидаты\n\nДо того, как два узла смогут коммуницировать через WebRTC, им необходимо обменяться информацией о подключении. Так как условия сети могут отличаться в зависимости от ряда факторов, для обнаружения возможных кандидатов на соединение с узлом обычно используется внешний сервис.\nЭтот сервис называется ICE и использует серверы STUN или TURN. STUN – это аббревиатура от Session Traversal for NAT, и обычно косвенно используется в большинстве WebRTC приложениях.\n\nTURN (Traversal Using Relay NAT) более продвинутое решение, которое включает в себя протоколы STUN, и большинство коммерческих служб WebRTC используют TURN сервер для установки соединения между узлами.\n\nAPI WebRTC напрямую поддерживает как STUN, так и TURN, и объединяется под более полным термином ICE (Internet Connectivity Establishment  - «Установление подключения к Интернету»). При установке WebRTC-соединения мы обычно предоставляем один или несколько ICE-серверов в конфигурации для объекта RTCPeerConnection.\n\nTrickle ICE\nПосле создания объекта RTCPeerConnection, исходный фреймворк использует предоставленные ICE-серверы для сбора кандидатов на установление соединения (кандидатов ICE).\n\nСобытие icegatheringstatechange на RTCPeerConnection  передает информацию о том, в каком состоянии находится ICE-сбор (new, gathering или complete).\nНесмотря на то, что для узла возможно просто дождаться, пока ICE-сбор будет завершен, обычно гораздо эффективнее использовать метод «trickle ice» и передавать каждого вновь обнаруженного ICE-кандидата удаленному узлу. Это значительно сократит время настройки однорангового соединения и позволит начать видео-звонок с меньшими задержками.\n\nДля сбора ICE-кандидатов, просто добавьте «прослушиватель» в событие icecandidate. Объект RTCPeerConnectionIceEvent, созданный этим «прослушивателем», будет содержать свойство candidate, представляющее нового кандидата, которого нужно отправить удаленному узлу (см. Сигналинг)\n\n// Listen for local ICE candidates on the local RTCPeerConnection\npeerConnection.addEventListener(‘icecandidate’, event => {\n    if (event.candidate) {\n        signalingChannel.send({‘new-ice-candidate’: event.candidate});\n    }\n});\n\n// Listen for remote ICE candidates and add them to the local RTCPeerConnection\nsignalingChannel.addEventListener(‘message’, async message => {\n    if (message.iceCandidate) {\n        try {\n            await peerConnection.addIceCandidate(message.iceCandidate);\n        } catch € {\n            console.error(‘Error adding received ice candidate’, e);\n        }\n    }\n});\n\nСоединение установлено\n\nПосле того, как ICE-кандидаты получены, нужно дождаться, пока состояние нашего однорангового соединения изменится на подключенное состояние. Чтобы отследить это, добавим «прослушиватель» в наш RTCPeerConnection, где можно просматривать изменения события connectionstatechange.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/_index",
            "title": "Практика",
            "description": "Карманная книга по WebRTC",
            "content": "\nСоединение в режиме реального времени с использованием WebRTC\n\nWebRTC – это проект с открытым исходным кодом, позволяющий передавать аудио, видео и данные в режиме реального времени в браузере или через приложения.\nWebRTC имеет несколько Java-скрипт API – нажмите на ссылки, чтоб посмотреть примеры\ngetUserMedia(): захват аудио и видео\nMediaRecorder: запись аудио и видео\nRTCPeerConnection: потоковая передача аудио и видео между пользователями\nRTCDataChannel: потоковая передача данных между пользователями.\n\nГде я могу использовать WebRTC?\n\nВ браузерах Firefox, Opera и Chrome на компьютере и Андроиде. Также WebRTC работает в приложениях iOS и Android.\n\nЧто такое «сигналинг»?\n\nWebRTC использует RTCPeerConnection для обмена потоковыми данными между браузерами, но ему необходим механизм для координации обмена данными и отправки контрольных сообщений. Этот процесс называется «сигналингом». Методы сигналинга и протоколы передачи не определены в WebRTC. Поэтому в коде придется использовать Socket.IO для обмена сообщениями, но существует много других альтернатив (https://github.com/muaz-khan/WebRTC-Experiment/blob/master/Signaling.md)\n\nЧто такое STUN и TURN?\n\nWebRTC разработан для работы в P2P, поэтому пользователи могут подключаться по самому прямому возможному маршруту. Однако WebRTC создан для работы в реальных сетях: клиентским приложениям необходимо проходить через шлюзы NAT (http://en.wikipedia.org/wiki/NAT_traversal) и брандмауэры, а P2P-сеть нуждается в резервном варианте на случай сбоя прямого соединения. В рамках этого процесса API WebRTC используют STUN-серверы для получения IP-адреса вашего компьютера и TURN-серверы для работы в качестве серверов ретрансляции в случае сбоя P2P-соединения. (WebRTC в реальном мире объясняет более подробно - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/)\n\nБезопасен ли WebRTC?\n\nШифрование является обязательным для всех компонентов WebRTC, а его Javasсript API могут использоваться только из безопасных источников (HTTPS или localhost). Механизмы сигналинга не определены стандартами WebRTC, поэтому важно убедиться, что вы используете безопасные протоколы.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-get-code",
            "title": "Загрузка кода",
            "description": "Карманная книга по WebRTC",
            "content": "\nЗагрузка кода\n\nЕсли вы знакомы с сайтом git, вы можете скачать код для данной codelab с GitHub, клонировав его:\ngit clone https://github.com/googlecodelabs/webrtc-web\n\nМожно также нажать на ссылку ниже для загрузки zip-файла кода:\nhttps://github.com/googlecodelabs/webrtc-web/archive/master.zip\n\nОткройте загруженный zip-файл. Разархивируйте папку проекта (adaptive-web-media), в которой по одной папке на каждый шаг этой codelab, и есть все необходимые вам ресурсы.\nВы будете выполнять все действия в папке work.\n\nПапки step-nn содержат финальную версию для каждого шага этой codelab. Они там для справки.\n\nУстановите и проверьте веб-сервер\n\nНесмотря на то, что вы можете использовать и свой собственный веб-сервер, эта codelab подразумевает работу с веб-сервером Chrome. Если у вас он еще не установлен, вы можете инсталлировать его из Chrome Web Store https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb?hl=en\n\nПосле установки приложения Web Server для Chrome, нажмите на ярлык Chrome Apps на панели закладок, на странице новой вкладки или в панели запуска приложений:\n\nНажмите на значок Web Server\n\nДалее вы увидите это диалоговое окно, которое позволит настроить локальный веб-сервер:\n\nНажмите на кнопку «Choose Folder», и выберите папку work, которую вы только что создали. Это позволит вам просматривать текущую работу в Chrome по ссылке URL, подчеркнутой в диалоговом окне в разделе Web Server URL(s).\nНиже, в Options, поставьте флажок в Automatically show index.html, как показано ниже:\n\nЗатем остановите и перезапустите сервер, сдвинув флажок с надписью «Web Server: STARTED» влево, а затем снова вправо.\n\nТеперь посетите свой рабочий сайт в браузере, кликнув на выделенный Web Server URL. Вы должны увидеть подобную страницу, которая соответствует пути work/index.html:\n\nОчевидно, что данное приложение пока еще не делает ничего интересного – пока это просто минимальный скелет, который нужен для того, чтоб убедиться, что веб-сервер работает, как надо. На следующих этапах мы добавим функциональности в этом приложение.\nС этого момента все тестирование и проверка должны выполняться с использованием этой настройки веб-сервера. Обычно достаточно просто обновить вкладку тестового браузера.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-overview",
            "title": "Обзор",
            "description": "Карманная книга по WebRTC",
            "content": "\nСоздайте приложение для получения видео и снимков с веб-камеры, с возможностью делиться ими в P2P через WebRTC. В ходе codelab вы узнаете, как использовать основные API WebRTC и настроить сервер обмена сообщениями через Node.js.\n\nЧему вы научитесь\n\nполучать видео с вашей веб-камеры\nпотоковое видео через RTCPeerConnection\nпотоковая передача данных через RTCDataChannel\nнастраивать сигналинг для обмена сообщениями\nкомбинировать одноранговое соединение и сигналинг\nделать фото и передавать его через канал данных\n\nЧто понадобится\n\nChrome версии 47 и выше\nвеб-сервер для Chrome https://chrome.google.com/webstore/detail/web-server-for-chrome/ofhbbkphhbklhfoeikjpcbhemlocgigb , и ваш собственный веб-сервер по выбору\nпример кода\nтекстовый редактор\nбазовые знания HTML, CSS и Javaskript",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-peer-signaling-combine",
            "title": "Соединение однорангового соединения и сигналинга",
            "description": "Карманная книга по WebRTC",
            "content": "\nЧему вы научитесь\n\nНа этом шаге вы узнаете, как:\nЗапускать службу сигнализации WebRTC с помощью Socket.IO на Node.js\nИспользовать эту службу для обмена метаданными WebRTC между узлами.\nПолная версия этого шага находится в папке step-05.\n\nПоменяйте HTML и JavaScript\n\nЗамените содержимое index.html следующим:\n\n\n\n\n\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n\nЗамените js/main.js содержимым из step-05/js/main.js.\n\nЗапустите Node.js сервер\n\nЕсли вы не отслеживаете эту codelab из своей папки work, вам может потребоваться установить зависимости для папки step-05 или вашей текущей рабочей папки. Выполните следующую команду из своей рабочей папки:\nnpm install\n\nПосле установки, если ваш Node.js сервер не запущен, запустите его, вызвав следующую команду в папке work:\nnode index.js\n\nУбедитесь, что вы используете версию index.js из предыдущего шага, который реализует Socket.IO. Для получения дополнительной информации о Node и Socket.IO, посмотрите раздел \"Set up a signaling service to exchange messages\".\nВ вашем браузере откройте localhost:8080.\n\nСнова откройте localhost: 8080 в новой вкладке или окне. Один видеоэлемент будет отображать локальный поток из getUserMedia(), а другой будет показывать \"удаленное\" видео, передаваемое через RTCPeerConnection.\nВам необходимо перезапускать Node.js сервер каждый раз, когда вы закрываете клиентскую вкладку или окно.\nПосмотрите логи в консоли браузера.\n\nБонусные задания\n\nЭто приложение поддерживает только видеочат один на один. Как вы можете изменить дизайн, чтобы несколько человек могли посещать одну и ту же комнату видеочата?\nВ примере строго задано имя комнаты foo. Каков наилучший способ включить другие имена комнат?\nКак пользователям обмениваться названием комнаты? Попробуйте создать альтернативу для обмена именами комнат.\nКак вы могли бы изменить приложение\n\nЧто вы узнали\n\nНа этом шаге вы узнали, как:\nЗапускать сигналинг-службу WebRTC с помощью Socket.IO через Node.js .\nИспользовать эту службу для обмена метаданными WebRTC между узлами.\nПолная версия этого шага находится в папке step-05.\n\nСоветы\n\nСтатистика WebRTC и данные отладки доступны в chrome:// webrtc-internals.\ntest.webrtc.org может использоваться для проверки ваших локальных настроек и тестирования камеры и микрофона.\nЕсли у вас возникли странные проблемы с кэшированием, попробуйте следующее:\nВыполните принудительную перезагрузку обновление, удерживая нажатой клавишу ctrl и нажав кнопку Reload\nПерезапустите браузер\nЗапустите npm cache clean из командной строки.\n\nДалее\n\nУзнайте, как делать снимки, получать изображения и делиться ими между удаленными узлами.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-results",
            "title": "Выводы",
            "description": "Карманная книга по WebRTC",
            "content": "\nВы создали приложение для потоковой передачи видео в реальном времени и обмена данными!\n\nЧто вы узнали\n\nВ этой codelab вы узнали, как:\nПолучать видео с вашей веб-камеры.\nстримить видео с помощью RTCPeerConnection.\nСтримить данные с помощью RTCDataChannel.\nНастраивать сигналинг-службу для обмена сообщениями.\nКомбинировать одноранговое соединение и сигналинг.\nСделать снимок и поделиться им через канал передачи данных.\n\nСледующие шаги\n\nПосмотрите на код и архитектуру канонического приложения AppRTC для чата WebRTC – приложение (https://appr.tc/), код (https://github.com/webrtc/apprtc)\nПопробуйте реальные примеры (http://webrtc.github.io/samples) из github.com/webrtc/samples.\n\nУзнать больше\n\nРяд ресурсов для начала работы с WebRTC доступен на https://webrtc.org/\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-RTCDataChannel-exchange-data",
            "title": "Использование RTCDataChannel для обмена данными",
            "description": "Карманная книга по WebRTC",
            "content": "\nЧему вы научитесь\nкак обмениваться данными между узлами WebRTC\n\nПолная версия этого шага находится в папке step-03.\n\nОбновите свой HTML\n\nНа этом шаге вы будете использовать WebRTC каналы данных для отправки текста между двумя textarea элементами на одной странице. Это опять не сильно применимо на практике, но зато демонстрирует, как WebRTC можно использовать для обмена данными, а также для потоковых видео.\n\nУдалите элементы video и button из index.html и замените их следующим HTML-кодом:\n\n\n\n\n\n  Start\n  Send\n  Stop\n\n\nОдна текстовая область будет предназначена для ввода текста, другая будет отображать текст в потоковом режиме между узлами.\nТеперь index.html должен выглядеть так:\n\n\n\n\n\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n    Start\n    Send\n    Stop\n\n\nОбновите свой JavaScript\n\nЗамените main.js содержимым из step-03/js/main.js.\nКак и на предыдущем шаге, делать копи-паст на больших кусках кода – не идеальный вариант развития событий в codelab (как и с RTCPeerConnection). Но альтернатив у нас нет.\n\nПротестируйте потоковые данные между узлами: откройте index.html, нажмите Start для установки соединения между узлами, введите какой-то текст в textarea слева, затем нажмите на Send, чтобы передать текст через каналы данных WebRTC.\n\nКак это работает\nЭтот код использует RTCPeerConnection и RTCDataChannel для обмена текстовыми сообщениями\nБольшая часть кода на этом шаге такая же, как и в примере RTCPeerConnection.\nФункции sendData() и createConnection() содержат большую часть нового кода:\n\nfunction createConnection() {\n  dataChannelSend.placeholder = '';\n  var servers = null;\n  pcConstraint = null;\n  dataConstraint = null;\n  trace('Using SCTP based data channels');\n  // For SCTP, reliable and ordered delivery is true by default.\n  // Add localConnection to global scope to make it visible\n  // from the browser console.\n  window.localConnection = localConnection =\n      new RTCPeerConnection(servers, pcConstraint);\n  trace('Created local peer connection object localConnection');\n\n  sendChannel = localConnection.createDataChannel('sendDataChannel',\n      dataConstraint);\n  trace('Created send data channel');\n\n  localConnection.onicecandidate = iceCallback1;\n  sendChannel.onopen = onSendChannelStateChange;\n  sendChannel.onclose = onSendChannelStateChange;\n\n  // Add remoteConnection to global scope to make it visible\n  // from the browser console.\n  window.remoteConnection = remoteConnection =\n      new RTCPeerConnection(servers, pcConstraint);\n  trace('Created remote peer connection object remoteConnection');\n\n  remoteConnection.onicecandidate = iceCallback2;\n  remoteConnection.ondatachannel = receiveChannelCallback;\n\n  localConnection.createOffer().then(\n    gotDescription1,\n    onCreateSessionDescriptionError\n  );\n  startButton.disabled = true;\n  closeButton.disabled = false;\n}\n\nfunction sendData() {\n  var data = dataChannelSend.value;\n  sendChannel.send(data);\n  trace('Sent Data: ' + data);\n}\n\nСинтаксис в RTCDataChannel намеренно похож на WebSocket, с методом send() событием message.\n\nОбратите внимание на использование dataConstraint. Каналы передачи данных могут быть настроены для обеспечения различных типов обмена данными — например, отправляемые данные могут быть в приоритете над над производительностью. Более подробную информацию о возможностях можно найти на https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createDataChannel .\nТри типа ограничений\nЭто сбивает с толку!\n\nРазличные типы параметров настройки вызовов WebRTC часто называются «ограничениями».\n\nУзнайте больше об ограничениях и возможностях:\n\nRTCPeerConnection https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/RTCPeerConnection\nRTCDataChannel https://developer.mozilla.org/en-US/docs/Web/API/RTCPeerConnection/createDataChannel\ngetUserMedia() https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia\n\nБонусные задания\n1)\tс SCTP-протоколом, используемым каналами передачи данных WebRTC, надежная и упорядоченная доставка данных включена по умолчанию. Когда может понадобиться RTCDataChannel для обеспечения надежной доставки даных, а когда производительность может быть важнее – даже если это означает потерю каких-то данных?\n2)\tИспользуйте CSS для улучшения макета страницы и добавьте атрибут placeholder в текстовую область dataChannelReceive.\n3)\tПротестируйте страницу на мобильном устройстве.\n\nЧто вы узнали?\nНа этом шаге вы узнали, как\nустанавливать соединение между двумя узлами WebRTC\nобмениваться текстовыми данными между узлами\n\nПолная версия этого шага находится в папке step-03.\n\nУзнайте больше\n- Каналы передачи данных WebRTC (написано пару лет назад, но все еще стоит прочитать) - http://www.html5rocks.com/en/tutorials/webrtc/datachannels/\nПочему SCTP был выбран для канала передачи данных WebRTC? - https://bloggeek.me/sctp-data-channel/\n\nСледующий шаг\nВы узнали, как обмениваться данными между узлами на одной и той же странице, но как вы собираетесь это делать между разными устройствами? Сначала вам необходимо настроить сигналинг-канал для обмена сообщениями метаданных. Как – узнайте на следующем шаге!",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-setup-signaling-service",
            "title": "Настройка службы сигналинга для обмена сообщениями",
            "description": "Карманная книга по WebRTC",
            "content": "\nЧему вы научитесь\n\nНа этом шаге вы узнаете, как:\nИспользовать npm для установки взаимосвязей, как указано в package.json\nЗапускать сервер Node.js и использовать node-static для обслуживания статических файлов.\nНастраивать службу обмена сообщениями на Node.js через Socket.IO .\nИспользовать это для создания ‘комнат\" и обмена сообщениями.\nПолная версия этого шага находится в папке step-04.\n\nКонцепции\n\nЧтобы установить и поддерживать вызов WebRTC, клиенты WebRTC (узлы) должны обмениваться метаданными:\nИнформация о кандидате (сети).\nсообщения offer и answer, содержащие информацию о медиа, например, о разрешении и кодеках.\nДругими словами, обмен метаданными требуется до P2P потоковой передачи аудио, видео или данных. Этот процесс называется сигналингом.\nНа предыдущих этапах объекты RTCPeerConnection отправителя и получателя находились на одной странице, поэтому \"сигналинг\" - это просто вопрос передачи метаданных между объектами.\n\nВ реальном приложении отправитель и получатель RTCPeerConnections запущены на веб-страницах на разных устройствах, и вам нужен способ для обмена метаданными.\n\nДля этого используется signaling-server: сервер, который может передавать сообщения между клиентами WebRTC (узлами). Фактически сообщения представляют собой обычный текст: строковые объекты JavaScript.\n\nОбязательное условие: установить Node.js\n\nДля выполнения следующих шагов этой codelab (папки step-04 до step-06) вам необходимо запустить сервер на локальном хосте с помощью Node.js .\nВы можете скачать и установить Node.js по этой ссылке (https://nodejs.org/en/download/) или через предпочтительный для вас менеджер пакетов (https://nodejs.org/en/download/package-manager/).\nПосле установки вы сможете импортировать зависимости, необходимые для следующих шагов (запуск npm install), а также запустить небольшой локальный сервер для выполнения codelab (запуск node index.js). Эти команды будут указаны позже, когда они потребуются.\n\nО приложении\n\nWebRTC использует клиентский JavaScript API, но для использования в реальных приложениях также требуется сигналинг-сервер (обмена сообщениями), а также серверы STUN и TURN. Вы можете узнать больше здесь - https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/.\nНа этом шаге вы создадите простой Node.js сигналинг-сервер, использующий Socket.IO Node js модуль и библиотеку JavaScript для обмена сообщениями. Опыт работы с Node.js и Socket.IO будет полезным, но не решающим; компоненты обмена сообщениями очень просты.\nВыбор правильного сигналинг-сервера\nВ этой кодовой лаборатории используется Socket.IO для сигналинг-сервера.\nДизайн Socket.IO упрощает создание службы для обмена сообщениями. и Socket.IO подходит для изучения сигналинга WebRTC благодаря встроенной концепции ‘комнат\".\nОднако для производственного сервиса есть альтернативы получше. Смотрите, как выбрать сигналинг-протокол для вашего следующего проекта WebRTC - https://bloggeek.me/siganling-protocol-webrtc/\n\nВ этом примере сервер (Node.js приложение) реализовано в index.js, и клиент, который работает на нем (веб-приложение), реализован в index.html.\nNode.js приложение на этом этапе имеет две задачи.\nВо-первых, он действует как ретранслятор сообщений:\n\nsocket.on('message', function (message) {\n  log('Got message: ', message);\n  socket.broadcast.emit('message', message);\n});\n\nВо-вторых, он управляет «комнатами» видеочата WebRTC:\n\nif (numClients === 0) {\n  socket.join(room);\n  socket.emit('created', room, socket.id);\n} else if (numClients === 1) {\n  socket.join(room);\n  socket.emit('joined', room, socket.id);\n  io.sockets.in(room).emit('ready');\n} else { // max two clients\n  socket.emit('full', room);\n}\n\nНаше простое приложение WebRTC позволит максимум двум узлам совместно использовать комнату.\n\nHTML и JavaScript\n\nОбновите index.html. Теперь страница должна выглядеть примерно так:\n\n\n\n\n\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n\nНа этом шаге вы ничего не увидите на странице: все протоколирование выполняется в консоли браузера. (Чтобы просмотреть консоль в Chrome, нажмите Ctrl-Shift-J или Command-Option-J, если работаете на Mac.)\nЗаменить js/main.js следующим файлом:\n\n'use strict';\n\nvar isInitiator;\n\nwindow.room = prompt(\"Enter room name:\");\n\nvar socket = io.connect();\n\nif (room !== \"\") {\n  console.log('Message from client: Asking to join room ' + room);\n  socket.emit('create or join', room);\n}\n\nsocket.on('created', function(room, clientId) {\n  isInitiator = true;\n});\n\nsocket.on('full', function(room) {\n  console.log('Message from client: Room ' + room + ' is full :^(');\n});\n\nsocket.on('ipaddr', function(ipaddr) {\n  console.log('Message from client: Server IP address is ' + ipaddr);\n});\n\nsocket.on('joined', function(room, clientId) {\n  isInitiator = false;\n});\n\nsocket.on('log', function(array) {\n  console.log.apply(console, array);\n});\n\nНастройте Socket.IO для запуска Node.js\nВ HTML-файле вы, возможно, видели, что используете Socket.IO файл:\n\n\n\nНа верхнем уровне вашей папки work создайте файл с именем package.json со следующим содержимым:\n\n{\n  \"name\": \"webrtc-codelab\",\n  \"version\": \"0.0.1\",\n  \"description\": \"WebRTC codelab\",\n  \"dependencies\": {\n    \"node-static\": \"^0.7.10\",\n    \"socket.io\": \"^1.2.0\"\n  }\n}\n\nЭто манифест приложения, который сообщает диспетчеру пакетов узлов (npm), какие зависимости проекта следует установить.\n\nЧтобы установить зависимости (например, /socket.io/socket.io.js), выполните следующие действия из терминала командной строки в вашей папке work:\nnpm install\n\nВы должны увидеть журнал установки, который заканчивается примерно так:\n\nКак вы видите, npm установил зависимости, определенные в package.json.\n\nСоздайте новый файл index.js на верхнем уровне вашей папки work (не в папке js) и добавьте следующий код:\n\n'use strict';\n\nvar os = require('os');\nvar nodeStatic = require('node-static');\nvar http = require('http');\nvar socketIO = require('socket.io');\n\nvar fileServer = new(nodeStatic.Server)();\nvar app = http.createServer(function(req, res) {\n  fileServer.serve(req, res);\n}).listen(8080);\n\nvar io = socketIO.listen(app);\nio.sockets.on('connection', function(socket) {\n\n  // convenience function to log server messages on the client\n  function log() {\n    var array = ['Message from server:'];\n    array.push.apply(array, arguments);\n    socket.emit('log', array);\n  }\n\n  socket.on('message', function(message) {\n    log('Client said: ', message);\n    // for a real app, would be room-only (not broadcast)\n    socket.broadcast.emit('message', message);\n  });\n\n  socket.on('create or join', function(room) {\n    log('Received request to create or join room ' + room);\n\n    var clientsInRoom = io.sockets.adapter.rooms[room];\n    var numClients = clientsInRoom ? Object.keys(clientsInRoom.sockets).length : 0;\n\n    log('Room ' + room + ' now has ' + numClients + ' client(s)');\n\n    if (numClients === 0) {\n      socket.join(room);\n      log('Client ID ' + socket.id + ' created room ' + room);\n      socket.emit('created', room, socket.id);\n\n    } else if (numClients === 1) {\n      log('Client ID ' + socket.id + ' joined room ' + room);\n      io.sockets.in(room).emit('join', room);\n      socket.join(room);\n      socket.emit('joined', room, socket.id);\n      io.sockets.in(room).emit('ready');\n    } else { // max two clients\n      socket.emit('full', room);\n    }\n  });\n\n  socket.on('ipaddr', function() {\n    var ifaces = os.networkInterfaces();\n    for (var dev in ifaces) {\n      ifaces[dev].forEach(function(details) {\n        if (details.family === 'IPv4' && details.address !== '127.0.0.1') {\n          socket.emit('ipaddr', details.address);\n        }\n      });\n    }\n  });\n\n});\n\nИз терминала командной строки выполните следующую команду в папке work:\nnode index.js\n\nВ браузере откройте localhost:8080.\n\nКаждый раз, когда вы открываете этот URL-адрес, вам будет предложено ввести название комнаты. Чтобы присоединиться к одной и той же комнате, каждый раз выбирайте одно и то же имя комнаты, например, «foo».\n\nОткройте новую вкладку и снова откройте localhost: 8080. Выберите то же самое название комнаты.\n\nОткройте localhost:8080 в третьей вкладке или окне. Выберите то же название комнаты еще раз.\n\nПроверьте консоль на каждой из вкладок: вы должны увидеть логи из JavaScript выше.\n\nБонусные задания\n\nКакие альтернативные механизмы обмена сообщениями могут быть возможны? С какими проблемами вы можете столкнуться при использовании «чистого» WebSocket?\nКакие проблемы могут быть связаны с масштабированием этого приложения? Можете ли вы разработать метод для тестирования тысяч или миллионов одновременных запросов на номер?\nЭто приложение использует запрос JavaScript для получения названия комнаты. Разработайте способ получения названия комнаты из URL. Например, localhost:8080/foo будет указывать имя комнаты foo.\n\nЧто вы узнали\n\nНа этом шаге вы узнали, как:\nИспользовать npm для установки зависимостей проекта, как указано в package.json\nЗапускать Node.js сервер для обмена системных файлов.\nНастраивать службу обмена сообщениями на Node.js через socket.io .\nИспользовать это для создания ‘комнат\" и обмена сообщениями.\nПолная версия этого шага находится в папке step-04.\n\nУзнайте больше\n\nПример socket.io chat - https://github.com/rauchg/chat-example\nWebRTC в реальном мире: STUN, TURN и сигналинг - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/\nТермин \"signaling\" в WebRTC - https://www.webrtc-experiment.com/docs/WebRTC-Signaling-Concepts.html\n\nСледующий шаг\nУзнайте, как исполь\nзовать сигналинг, чтобы позволить двум пользователям установить одноранговое соединение.\n",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-stream-to-cam",
            "title": "Потоковое видео с веб-камеры",
            "description": "Карманная книга по WebRTC",
            "content": "\nЧему вы научитесь:\n\nНа этом шаге вы узнаете, как\nполучить видеопоток с вашей веб-камеры\nуправлять воспроизведением потока\nиспользовать CSS и SVG для обработки видео\nПолная версия этого шага находится в папке step-01.\n\nНемного HTML\n\nДобавьте элемент video и элемент script в index.html в папку work.\n\n\n\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n\nИ немного JavaScript\n\nДобавьте следующее в main.js в вашей папке js:\n\n'use strict';\n\n// On this codelab, you will be streaming only video (video: true).\nconst mediaStreamConstraints = {\n  video: true,\n};\n\n// Video element where stream will be placed.\nconst localVideo = document.querySelector('video');\n\n// Local stream that will be reproduced on the video.\nlet localStream;\n\n// Handles success by adding the MediaStream to the video element.\nfunction gotLocalMediaStream(mediaStream) {\n  localStream = mediaStream;\n  localVideo.srcObject = mediaStream;\n}\n\n// Handles error by logging a message to the console with the error message.\nfunction handleLocalMediaStreamError(error) {\n  console.log('navigator.getUserMedia error: ', error);\n}\n\n// Initializes media stream.\nnavigator.mediaDevices.getUserMedia(mediaStreamConstraints)\n  .then(gotLocalMediaStream).catch(handleLocalMediaStreamError);\nВсе приведенные здесь примеры JavaScript используют ‘use strict’, для избежания частых ошибок в кодировании.\nУзнайте больше, что это означает в http://ejohn.org/blog/ecmascript-5-strict-mode-json-and-more/\n\nПопробуйте\n\nОткройте index.html в вашем браузере и вы должны увидеть что-то подобное (с видом из вашей камеры, конечно!):\n\nКак это работает\n\nСледуя запросу getUserMedia(), браузер запрашивает у пользователя разрешение на доступ к своей камере (если это впервые, когда запрашивается доступ к камере для текущего источника). В случае успеха возвращается MediaStream, который может быть использован элементов мультимедиа через атрибут srcObject:\n\nnavigator.mediaDevices.getUserMedia(mediaStreamConstraints)\n  .then(gotLocalMediaStream).catch(handleLocalMediaStreamError);\n\nfunction gotLocalMediaStream(mediaStream) {\n  localVideo.srcObject = mediaStream;\n}\n\nАргумент constraints позволяет указать, какой тип мультимедиа получать. В этом примере используется только видео, т.к. звук по умолчанию отключен:\n\nconst mediaStreamConstraints = {\n  video: true,\n};\n\nВы можете использовать ограничения для дополнительных требований, таких как разрешение видео:\n\nconst hdConstraints = {\n  video: {\n    width: {\n      min: 1280\n    },\n    height: {\n      min: 720\n    }\n  }\n}\n\nСпецификация MediaTrackConstraints перечисляет все возможные типы ограничений, хотя не все параметры поддерживаются во всех браузерах. Если запрошенное разрешение не поддерживается выбранной в данный момент камерой, getUserMedia() будет отклонен с ошибкой OverconstrainedError и пользователю даже не предложат предоставить разрешение на доступ к своей камере.\nДемо-версию, демонстрирующую, как использовать ограничения для запроса различных разрешений, можно посмотреть по ссылке https://simpl.info/getusermedia/constraints/, а демо-версию с использованием ограничений для выбора камеры и микрофона – по этой ссылке https://simpl.info/getusermedia/sources/.\n\nЕсли getUserMedia() сработал успешно, в качестве источника элемента video устанавливается видеопоток с веб-камеры:\n\nfunction gotLocalMediaStream(mediaStream) {\n  localVideo.srcObject = mediaStream;\n}\n\nБонусные задания\n\nПереданный getUserMedia() объект localStream находится в глобальной области видимости, поэтому вы можете проверить его через консоль браузера: откройте консоль в Chrome, введите stream и нажмите Return (для просмотра консоли в Chrome, нажмите Ctrl+Shift+J, или command+Option+J, если вы работаете на Mac).\nчто возвращает localStream.getVideoTracks()?\nпопробуйте сделать запрос localStream.getVideoTracks()[0].stop()\nПосмотрите на объект constraints: что произойдет, когда вы меняете его на {audio: true, video: true)?\nКакой размер у элемента video? Как можно получить естественный размер из JavaScript, в отличие от размера экрана? Используйте Chrome Dev Tools для проверки\nПопробуйте добавить CSS фильтры в элемент video. Например:\n\nvideo {\n  filter: blur(4px) invert(1) opacity(0.5);\n}\n\nПопробуйте добавить SVG-фильтры. Например:\n\nvideo {\n   filter: hue-rotate(180deg) saturate(200%);\n }\n\nЧто вы узнали\n\nНа этом шаге вы узнали, как\nполучать видео с вашей веб-камеры\nустанавливать ограничения для мультимедиа\nкак навести хаос в элементе video\n\nПолная версия этого шага находится в папке step-01.\n\nСоветы\n\nне забывайте про атрибут autoplay в элемент video. Без него вы будете видеть только один кадр!\nесть гораздо больше ограничений для getUserMedia(). Посмотрите их по ссылке https://webrtc.github.io/samples/src/content/peerconnection/constraints/. Как видите, есть много интересных примеров c WebRTC на сайте.\n\nЛучшая практика\n\nубедитесь, что ваш элемент video не переполняет его контейнер. Мы добавили width и max-width для установки соответствующего размера и максимального размера видео. Браузер будет рассчитывать высоту автоматически.\n\nvideo {\n  max-width: 100%;\n  width: 320px;\n}\n\nСледующий шаг\n\nВы получили видео, но как его транслировать? Узнайте на следующем шаге!",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-stream-with-RTCPeerConnection",
            "title": "Потоковое видео с помощью RTCPeerConnection",
            "description": "Карманная книга по WebRTC",
            "content": "\nЧему вы научитесь\n\nНа этом шаге вы узнаете, как:\nАбстрагироваться от различий браузера с помощью оболочки WebRTC, adapter.js.\nИспользовать RTCPeerConnection API для потоковой передачи видео.\nУправлять захватом и потоковой передачей мультимедиа.\n\nПолная версия этого шага находится в папке step-2.\n\nЧто такое RTCPeerConnection?\n\nRTCPeerConnection - это API для выполнения WebRTC-запросов для потоковой передачи видео и аудио и обмена данными.\n\nВ этом примере устанавливается соединение между двумя объектами RTCPeerConnection (известными как узлы) на одной и той же странице.\n\nНе очень практично, но зато полезно для понимания того, как работает RTCPeerConnection.\n\nДобавление элементов video и кнопок управления\n\nВ index.html замените один видеоэлемент двумя видеоэлементами и тремя кнопками:\n\n\n\n\n\n\n  Start\n  Call\n  Hang Up\n\n\nОдин видеоэлемент будет отображать поток из getUserMedia(), а другой будет показывать это же видео, но передаваемое через RTCPeerConnection (в реальном приложении один видеоэлемент будет отображать локальный поток, а другой – удаленный поток).\n\nДобавьте adapter.js\nДобавьте ссылку на текущую версию adapter.js  выше ссылки на main.js:\nadapter.js - это оболочка для изоляции приложений от изменений спецификаций и различий в префиксах. (Хотя на самом деле стандарты и протоколы, используемые для реализации WebRTC, очень стабильны, и существует всего несколько имен с префиксами.)\nНа этом этапе мы используем самую последнюю версию adapter.js, что хорошо для codelab, но не всегда хорошо для приложений. Здесь https://github.com/webrtc/adapter мы объясняем, как сделать так, чтоб у вашего приложения всегда был доступ к самой последней версии.\nДля получения полной информации о взаимодействии сWebRTC, переходи по ссылке https://webrtc.github.io/webrtc-org/web-apis/interop/\n\nТеперь index.html должен выглядеть так:\n\n\n\n\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n    Start\n    Call\n    Hang Up\n\n\nУстановите код RTCPeerConnection\n\nЗамените main.js в папке step-02.\nДелать копи-паст в больших кусках кода в codelab – это так себе вариант, конечно. Но чтобы получить и запустить RTCPeerConnection, у нас нет других альтернатив, как провести вас через весь этот путь.\nВам нужно научиться, как код работает в каждый момент.\n\nСделайте звонок\nОткройте index.html, нажмите кнопку Start, чтоб получить видео с вашей веб-камеры, и затем нажмите Call, чтобы установить одноранговое соединение. Вы должны увидеть одно и то же видео (с вашей веб-камеры) в обоих видео-элементах. Посмотрите консоль браузера, чтоб увидеть логи WebRTC.\n\nКак это работает\n\nВ этом шаге будет много всего…\nЕсли вы хотите пропустить объяснение ниже - ок.\nВы все еще можете продолжить работу с codelab!\n\n\nWebRTC использует API RTCPeerConnection для настройки соединения для потоковой передачи видео между клиентами WebRTC, известными как узлы.\nВ этом примере два объекта RTCPeerConnection находятся на одной странице: pc1 и pc2. Это мало используется на практике, но зато хорошо демонстрирует, как работают API.\nНастройка вызова между WebRTC-узлами включает в себя три задачи:\nСоздать RTCPeerConnection для каждого конца вызова и на каждом конце добавить локальный поток из getUserMedia().\nПолучать и делиться сетевой информацией: потенциальные конечные точки подключения известны как ICE-кандидаты.\nПолучать и делиться локальными и удаленными описаниями: метаданные о локальными мультимедиа в формате SDP.\n\nПредставьте, что Алиса и Боб хотят использовать RTCPeerConnection для настройки видеочата.\nСначала Алиса и Боб обмениваются информацией о сети. Выражение \"finding candidates\" относится к процессу поиска сетевых интерфейсов и портов с использованием ICE-фреймворк.\nАлиса создает объект RTCPeerConnection с обработчиком onicecandidate (addEventListener('icecandidate')). Это соответствует следующему коду из main.js\n\nlet localPeerConnection;\n\nlocalPeerConnection = new RTCPeerConnection(servers);\nlocalPeerConnection.addEventListener('icecandidate', handleConnection);\nlocalPeerConnection.addEventListener(\n    'iceconnectionstatechange', handleConnectionChange);\nАргумент servers для RTCPeerConnection в этом примере не используется.\nЗдесь вы можете указать STUN и TURN серверы.\nWebRTC разработан для работы с P2P, поэтому пользователи могут подключаться по самому прямому возможному маршруту. Однако WebRTC создан для работы в реальных сетях: клиентским приложениям необходимо проходить через шлюзы NAT (http://en.wikipedia.org/wiki/NAT_traversal) и брандмауэры, а P2P сеть нуждается в резервном варианте на случай сбоя прямого соединения.\nВ рамках этого процесса, API WebRTC используют STUN-серверы для получения IP-адреса вашего компьютера и TURN-серверы для ретрансляции в случае сбоя P2P связи. Подробнее об этом - http://www.html5rocks.com/en/tutorials/webrtc/infrastructure/\n\nАлиса вызывает getUserMedia() и добавляет переданные поток:\n\nnavigator.mediaDevices.getUserMedia(mediaStreamConstraints).\n  then(gotLocalMediaStream).\n  catch(handleLocalMediaStreamError);\n\nfunction gotLocalMediaStream(mediaStream) {\n  localVideo.srcObject = mediaStream;\n  localStream = mediaStream;\n  trace('Received local stream.');\n  callButton.disabled = false;  // Enable call button.\n}\n\nlocalPeerConnection.addStream(localStream);\ntrace('Added local stream to localPeerConnection.');\nОбработчик onicecandidate из шага 1 вызывается, когда становятся доступными сетевые кандидаты.\nАлиса отправляет Бобу данные кандидата. В реальном приложении этот процесс (известный как сигналинг) осуществляется через службу обмена сообщениями – вы узнаете, как это сделать, позднее. Конечно, на этом этапе два объекта RTCPeerConnection находятся на одной странице и могут взаимодействовать напрямую без необходимости во внешних сообщениях.\nКогда Боб получает сообщение о кандидате от Алисы, он вызывает addIceCandidate(), чтобы добавить кандидата в описание удаленного узла:\n\nfunction handleConnection(event) {\n  const peerConnection = event.target;\n  const iceCandidate = event.candidate;\n\n  if (iceCandidate) {\n    const newIceCandidate = new RTCIceCandidate(iceCandidate);\n    const otherPeer = getOtherPeer(peerConnection);\n\n    otherPeer.addIceCandidate(newIceCandidate)\n      .then(() => {\n        handleConnectionSuccess(peerConnection);\n      }).catch((error) => {\n        handleConnectionFailure(peerConnection, error);\n      });\n\n    trace(${getPeerName(peerConnection)} ICE candidate:\\n +\n          ${event.candidate.candidate}.);\n  }\n}\n\nУзлам WebRTC также необходимо узнавать и обмениваться информацией о локальных и удаленных аудио- и видеоматериалах, такими как разрешение и возможности кодеков, и обмениваться ими. Сигналинг для обмена информацией о конфигурации мультимедиа осуществляется путем обмена большими двоичными объектами метаданных, известными как offer и answer, с использованием формата Session Description Protocol, известного как SDP (http://en.wikipedia.org/wiki/Session_Description_Protocol):\nАлиса запускает метод RTCPeerConnectioncreateOffer(). Возвращенный промис обеспечивает RTCSessionDescription: Alice’s local session description:\n\ntrace('localPeerConnection createOffer start.');\nlocalPeerConnection.createOffer(offerOptions)\n  .then(createdOffer).catch(setSessionDescriptionError);\n\nВ случае успеха Алиса устанавливает локальное описание, используя setLocalDescription(), а затем отправляет это описание сеанса Бобу через сигналинг-канал.\nБоб принимает описание, отправленное ему Алисой, в качестве удаленного описания, используя setRemoteDescription().\nБоб запускает метод RTCPeerConnection createAnswer(), передавая ему удаленное описание, которое он получил от Алисы, чтобы можно было создать локальный сеанс, совместимый с ее сеансом. Промис createAnswer() передает описание RTCSessionDescription: Боб устанавливает это как локальное описание и отправляет его Алисе.\nКогда Алиса получает описание сеанса Боба, она устанавливает его в качестве удаленного описания с помощью setRemoteDescription().\n\n// Logs offer creation and sets peer connection session descriptions.\nfunction createdOffer(description) {\n  trace(Offer from localPeerConnection:\\n${description.sdp});\n\n  trace('localPeerConnection setLocalDescription start.');\n  localPeerConnection.setLocalDescription(description)\n    .then(() => {\n      setLocalDescriptionSuccess(localPeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('remotePeerConnection setRemoteDescription start.');\n  remotePeerConnection.setRemoteDescription(description)\n    .then(() => {\n      setRemoteDescriptionSuccess(remotePeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('remotePeerConnection createAnswer start.');\n  remotePeerConnection.createAnswer()\n    .then(createdAnswer)\n    .catch(setSessionDescriptionError);\n}\n\n// Logs answer to offer creation and sets peer connection session descriptions.\nfunction createdAnswer(description) {\n  trace(Answer from remotePeerConnection:\\n${description.sdp}.);\n\n  trace('remotePeerConnection setLocalDescription start.');\n  remotePeerConnection.setLocalDescription(description)\n    .then(() => {\n      setLocalDescriptionSuccess(remotePeerConnection);\n    }).catch(setSessionDescriptionError);\n\n  trace('localPeerConnection setRemoteDescription start.');\n  localPeerConnection.setRemoteDescription(description)\n    .then(() => {\n      setRemoteDescriptionSuccess(localPeerConnection);\n    }).catch(setSessionDescriptionError);\n}\n\nПинг!\n\nБонусные задания\n\nПосмотрите chrome://webrtc-internals. Там отражены статы WebRTC и отлаженные данные (Полный список ссылок в Chrome – chrome://about).\nСделайте разметку страницы через CSS:\nРасположите видео друг за другом\nСделайте кнопки такой же ширины, но с большим размером текста\nУбедитесь, что макет работает на мобильных устройствах\nВ консоли Chrome Dev Tools посмотрите localStream, localPeerConnection и remotePeerConnection.\nИз консоли, посмотрите на localPeerConnecionpc1.localDescription. Как выглядит формат SDP?\n\nЧто вы узнали?\n\nНа этом шаге вы узнали, как\nуйти от различий в браузерах через WebRTC оболочку adapter.js\nиспользовать RTCPeerConncetion API для потоковой передачи видео\nконтролировать захват медиа и потоковую передачу данных\nделиться мультимедиа и сетевой информацией между узлами, чтоб разрешить вызов WebRTC.\nПолная версия этого шага находится в папке step-2.\n\nСоветы\n\nна этом шаге вам нужно столько всего освоить! Чтобы найти другие ресурсы, объясняющие более детально RTCPeerConnection, загляните на webrtc.org. Эта страница включает решения для JavaScript фреймворков – если вы хотите использовать WebRTC, но не хотите конфликтовать с API.\nУзнайте больше про оболочку adapter.js из https://github.com/webrtc/adapter\nХотите посмотреть, как выглядит лучшее в мире приложение для видеочата? Посмотрите на AppRTC, каноническое приложение для звонков WebRTC: приложение (https://appr.tc/) и код (https://github.com/webrtc/apprtc) . Время настройки вызова составляет менее 500 мс!\n\nЛучшая практика\n\nДля обеспечения надежности вашего кода в будущем используйте новые API-интерфейсы на основе промисов и включите совместимость с браузерами, которые их не поддерживают, используя adapter.js\n\nСледующий шаг\n\nЭтот шаг показывает, как использовать WebRTC для передачи видео между узлами – но эта codelab в том числе и о данных!\nВ следующем шаге выясним, как передавать произвольные данные с помощью RTCDataChannel.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/practice/practice-take-photo",
            "title": "Сделайте фото и отправьте его через канал данных",
            "description": "Карманная книга по WebRTC",
            "content": "\n\nЧему вы научитесь\n\nНа этом шаге вы узнаете, как:\nДелать снимок и получать из него данные, используя элемент canvas.\nОбмениваться изображениями с удаленным пользователем.\nПолная версия этого шага находится в папке step-06.\n\nКак это работает\n\nРанее вы узнали, как обмениваться текстовыми сообщениями с помощью RTCDataChannel.\n\nЭтот шаг позволяет обмениваться целыми файлами: в этом примере - фотографиями, снятыми с помощью getUserMedia().\n\nОсновные части этого шага заключаются в следующем:\n\nУстановите канал передачи данных. Обратите внимание, что на этом шаге вы не добавляете никаких медиапотоков к одноранговому соединению.\nЗахватите видеопоток пользователя с веб-камеры с помощью getUserMedia():\n\nvar video = document.getElementById('video');\n\nfunction grabWebCamVideo() {\n  console.log('Getting user media (video) ...');\n  navigator.mediaDevices.getUserMedia({\n    video: true\n  })\n  .then(gotStream)\n  .catch(function(e) {\n    alert('getUserMedia() error: ' + e.name);\n  });\n}\nКогда пользователь нажимает кнопку Snap, получает снимок (видеокадр) из видеопотока и отображает его в элементе canvas:\n\nvar photo = document.getElementById('photo');\nvar photoContext = photo.getContext('2d');\n\nfunction snapPhoto() {\n  photoContext.drawImage(video, 0, 0, photo.width, photo.height);\n  show(photo, sendBtn);\n}\nКогда пользователь нажимает кнопку Send, преобразуйте изображение в байты и отправьте их по каналу передачи данных:\n\nfunction sendPhoto() {\n  // Split data channel message in chunks of this byte length.\n  var CHUNK_LEN = 64000;\n  var img = photoContext.getImageData(0, 0, photoContextW, photoContextH),\n    len = img.data.byteLength,\n    n = len / CHUNK_LEN | 0;\n\n  console.log('Sending a total of ' + len + ' byte(s)');\n  dataChannel.send(len);\n\n  // split the photo and send in chunks of about 64KB\n  for (var i = 0; i\n\n  Realtime communication with WebRTC\n\n\n  Realtime communication with WebRTC\n\n    Room URL: ...\n\n    Snap then Send\n     or\n    Snap &amp; Send\n\n    Incoming photos\n\n\nЕсли вы не отслеживаете эту codelab из своей папки work, вам может потребоваться установить зависимости для папки step-06 или вашей текущей рабочей папки. Просто запустите следующую команду из своей рабочей папки:\n\nnpm install\n\nПосле установки, если ваш Node.js сервер не запущен, запустите его, вызвав следующую команду из вашей папки work:\nnode index.js\n\nУбедитесь, что вы используете версию index.js, который реализует Socket.IO, и не забудьте перезапустить ваш сервер Node.js, если вы собираетесь что-то менять. Для большей информации на Node и Socket.IO, загляните в раздел «Set up a signaling service to exchange messages».\n\nПри необходимости нажмите на кнопку Allow, чтобы разрешить приложению использовать вашу веб-камеру.\n\nПриложение создаст случайный ID комнаты, и добавьте этот ID в URL. Откройте URL из адресной стройки в новой вкладке или окне браузера.\n\nНажмите кнопку Snap&Send и затем посмотрите входящую область в другой вкладке внизу страницы. Приложение переносит фотографии между вкладками.\n\nВы должны увидеть что-то типа этого:\n\nБонусные задания:\n\nКак вы можете изменить код, чтобы сделать возможным совместное использование файлов любого типа?\n\nУзнайте больше\n\nThe MediaStream Image Capture API (https://www.chromestatus.com/features/4843864737185792): API для фотосъемки и управления камерами — скоро появится в браузере!\nAPI MediaRecorder для записи аудио и видео: демо-примеры (https://webrtc.github.io/samples/src/content/getusermedia/record/) и документация (https://www.chromestatus.com/features/5929649028726784)\n\nЧто вы узнали\n\nКак делать фото и получать из нее данные с помощью элемента canvas.\nКак обмениваться этими данными с удаленным пользователем.\n\nПолная версия этого шага находится в папке step-06.",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/remote-streams",
            "title": "Удаленные потоки",
            "description": "Карманная книга по WebRTC",
            "content": "\nНачало работы с удаленными потоками\n\nКак только RTCPeerConnection подключился к удаленному узлу, между ними можно передавать аудио- и видео-потоки. Это точка, в которой мы подключаем поток, полученный от getUserMedia(), к RTCPeerConnection. Медиаопоток состоит как минимум из одной дорожки мультимедиа, и они по отдельности добавляются в RTCPeerConnection, когда мы хотим передать данные удаленному узлу.\n\nconst localStream = await getUserMedia({vide: true, audio: true});\nconst peerConnection = new RTCPeerConnection(iceConfig);\nlocalStream.getTracks().forEach(track => {\n    peerConnection.addTrack(track, localStream);\n});\n\nДорожки можно добавлять в RTCPeerConnection до подключения к удаленному узлу, поэтому имеет смысл выполнить эту настройку как можно раньше, а не ждать завершения соединения.\n\nДобавление удаленных дорожек\n\nДля получения удаленных дорожек, которые были добавлены другим узлом, мы регистрируем «прослушиватель» на локальном RTCPeerConnection, отслеживая изменения в событии track. RTCTrackEvent содержит массив объектов MediaStream, которые имеют те же значения MediaStream.id, что и соответствующие локальные потоки узла. В нашем примере каждая дорожка связана только с одним потоком.\n\nОбратите внимание, что, хотя ID из MediaStream совпадают на обеих сторонах однорангового соединения, в общем случае это не работает для ID MediaStreamTrack.\n\nconst remoteVideo = document.querySelector('#remoteVideo');\n\npeerConnection.addEventListener('track', async (event) => {\n    const [remoteStream] = event.streams;\n    remoteVideo.srcObject = remoteStream;\n});\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/testing",
            "title": "Тестирование приложений WebRTC",
            "description": "Карманная книга по WebRTC",
            "content": "\n\nПри написании автоматических тестов для приложений WebRTC, существуют полезные конфигурации, которые можно включить для браузеров, и которые упростят разработку и тестирование.\n\nChrome\n\nПри запуске автоматических тестов в Chrome полезны следующие функции:\nallow-file-access-from-files — дает API-доступ для file://URLs\ndisable-translate — отключает всплывающие окна\nuse-fake-ui-for-media-stream — Представляет поддельные медиапотоки. Полезно при работе на CI-серверах.\nuse-file-for-fake-audio-capture= — дает возможность использовать файл при захвате звука.\nuse-file-for-fake-video-capture= — дает возможность использовать файл при захвате видео.\nheadless - Запустить в автономном режиме. Полезно при работе на CI-серверах.\nmute-audio - Отключить аудио.\n\nFirefox\n\nПри запуске автоматических тестов в Firefox, необходимо указать набор ключей предпочтений, которые будут использоваться в запущенном соединении. Ниже приведена конфигурация, используемая для автоматических тестов образцов WebRTC:\n\n\"prefs\": {\n    \"browser.cache.disk.enable\": false,\n    \"browser.cache.disk.capacity\": 0,\n    \"browser.cache.disk.smart_size.enabled\": false,\n    \"browser.cache.disk.smart_size.first_run\": false,\n    \"browser.sessionstore.resume_from_crash\": false,\n    \"browser.startup.page\": 0,\n    \"media.navigator.streams.fake\": true,\n    \"media.navigator.permission.disabled\": true,\n    \"device.storage.enabled\": false,\n    \"media.gstreamer.enabled\": false,\n    \"browser.startup.homepage\": \"about:blank\",\n    \"browser.startup.firstrunSkipsHomepage\": false,\n    \"extensions.update.enabled\": false,\n    \"app.update.enabled\": false,\n    \"network.http.use-cache\": false,\n    \"browser.shell.checkDefaultBrowser\": false\n}\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/turn-server",
            "title": "TURN сервер",
            "description": "Карманная книга по WebRTC",
            "content": "\nДля работы большинства приложений WebRTC необходим сервер для ретрансляции трафика между узлами, поскольку прямой сокет часто невозможен между клиентами (если только они не находятся в одной локальной сети). Обычный способ решить эту проблему — использовать TURN-сервер (Traversal Using Relay NAT), который представляет собой протокол ретрансляции сетевого трафика.\n\nВ настоящее время существует несколько вариантов TURN-серверов, доступных в Интернете, как в виде самостоятельных приложений (например, проект COTURN с открытым исходным кодом), так и в виде облачных сервисов.\n\nЕсли у вас есть доступный онлайн TURN-сервер, то все, что вам нужно - это правильная RTCConfiguration для вашего клиентского приложения. Следующий фрагмент кода иллюстрирует пример конфигурации для RTCPeerConnection, где TURN-сервер hostname my-turn-server.mycompany.com работает на порту 19403.\n\nОбъект конфигурации также поддерживает свойства username и credentials для защиты доступа к серверу. Они необходимы при подключении к TURN-серверу.\n\nconst iceConfiguration = {\n    iceServers: [\n        {\n            urls: 'turn:my-turn-server.mycompany.com:19403',\n            username: 'optional-username',\n            credentials: 'auth-token'\n        }\n    ]\n}\n\nconst peerConnection = new RTCPeerConnection(iceConfiguration);\n`",
            "tags": [],
            "lang": "ru"
        },
        {
            "uri": "/tracks/webrtc/unified-plan-transition-guide",
            "title": "Формат SDP унифицированного плана – план перехода",
            "description": "Карманная книга по WebRTC",
            "content": "\n\nGoogle планирует перевести реализацию WebRTC в Chrome с текущего SDP-формата (называемого «Plan B») на формат соответствующих стандартов («Unified Plan», draft-ietf-rtcweb-jsep) в течение следующих нескольких кварталов.\nПлан включает 5 этапов и одну временную функцию API.\n\nКто будет затронут?\nЛюдям, которые используют несколько аудиодорожек или несколько видеодорожек в одном PeerConnection, придется протестировать свой продукт в рамках Унифицированного Плана и, соответственно, адаптироваться. В случае, когда вызов инициируется с конечной точки не из Chrome, и на него отвечают в Chrome, форма запросов может измениться.\n\nЛюдям, выполняющим детальный анализ SDP и заботящимся о msid атрибутах, придется убедиться, что их код синтаксического анализа поддерживает новый формат (a=msid). Подробная информация о том, потребуются ли изменения и как должны измениться приложения, будет зависеть от приложения. Мы думаем, что почти все приложения, которые используют только одну аудио- и одну видеодорожку для каждого RTCPeerConnection, - их эти изменения не коснутся.\n\nФункция API\nМы добавляем новую функцию в RTCConfiguration RTCPeerConnection:\n\nenum SdpSemantics {\n  \"plan-b\",\n  \"unified-plan\"\n};\n\n\npartial dictionary RTCConfiguration {\n   SdpSemantics sdpSemantics;\n}\n\nRTCConfiguration может быть передана конструктору из RTCPeerConnection, и все запросы и ответы будут в формате Унифицированного Плана. Запросы в setLocalDescription и setRemoteDescription также будут ожидать, что SDP будет в формате Унифицированного Плана; если он в устаревшем формате Chrome, то все, кроме первой звуковой дорожки и первой видеодорожки, будут игнорироваться.\n\nТакже есть флаг командной строки (–enable-features=RTCUnifiedPlanByDefault в версии Chrome M71 и выше, –enable-blink-features=RTCUnifiedPlanByDefault в более ранних версиях), который позволяет установить для этого флага значение по умолчанию в «Unified-plan».\n\nЭтапы\n\nЭтап 1. Внедрение Унифицированного Плана\nНа этом этапе Унифицированный План разрабатывался под флагом экспериментов, доступным с версии M65. До этапа 2 разумнее всего было тестировать Chrome Canary, используя «–enable-blink-features=RTCUnifiedPlan».\n\nЭтап 2. Сделать функцию API общедоступной\nПредставлено в версии M69 (бета-август 2018 г., стабильная версия в сентябре 2018 г.)\n\nНа этом этапе значением по умолчанию флага sdpSemantics было «plan-b». На этапе 2 люди, у которых были реализации, зависящие от формата SDP, должны были протестировать, работают ли их приложения при использовании Унифицированного Плана. Для приложений, поддерживающих Firefox, это очень простое упражнение: просто делайте то же, что делали до этого в Firefox.\nЗначение по умолчанию флага sdpSemantics можно изменить в «chrome://flags»; найдите функцию «WebRTC: Use Unified Plan SDP Semantics by default».\n\nЭтап 3 Переключите значение по умолчанию\nДатой перехода была версия M72 (бета-декабрь 2018 г., стабильная версия — январь 2019 г.).\nНа этом этапе было изменено значение флага sdpSemantics по умолчанию на «unified plan». Приложения, которые обнаружили, что стали работать медленнее, переустановили флаг sdpSemantics в «plan-b», чтобы вернуться к предыдущему поведению.\n\nЭтап 4: бросьте «План Б»\nНа этом этапе установка флага sdpSemantics в значение «plan-b» приводит к возникновению исключения. Это было сделано при переходе от версии Canary к M93. Что касается M96, исключение работает как в Canary, так и на Beta. План состоит в том, чтобы добавить его и в стабильную версию. Мы следим за использованием Plan B.\nНа этом этапе доступна пробная версия, которая позволяет использовать план Б без создания исключений. Эта пробная версия перестала работать 29 декабря 2021 г.\n\nЭтап 5: Уберите «План Б»\nПосле окончания пробного периода Plan B будет удален из Chrome. На этом этапе флаг sdpSemantics будет удален. Попытка установить его на «plan-b» не вызовет исключение, и перестанет работать.\n",
            "tags": [],
            "lang": "ru"
        }
    ],
    "en": [
        {
            "uri": "/apps/_index",
            "title": "Apps",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/apps/brewmate/",
            "title": "BrewMate",
            "description": "Homebrew GUI Apps Manager",
            "content": "\n\n\n\nBrewMate is a macOS GUI application that makes it easy to search for, install, and uninstall Homebrew casks. You can also see the top downloaded casks for the last month.\n\nInstall\n\nDownload the latest DMG file from the releases page or from sourceforge.net\nDouble-click the DMG file to open it.\nDrag the BrewMate app to your Applications folder.\nLaunch BrewMate from your Applications folder.\n\nor\nbrew install romankurnovskii/cask/brewmate --cask\n\nor\nbrew tap romankurnovskii/cask\nbrew update\nbrew install brewmate --cask\n\nFAQ\n\nIs this app free?\nYes, the app is free to download and use.\n\nWhat operating systems does this app support?\nThis app is designed for macOS, and it supports macOS 10.15 (Catalina) and newer versions.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/apps/cloud-exam-quizz/",
            "title": "Cloud exam Quizz",
            "description": "Check if you are ready to pass Cloud exam",
            "content": "\nGoal: Check if you are ready to pass the Cloud exam\nThe application calculates progress after each answered question.\nAbility to answer at least one question and get a comment at the same time. No need to pass all questions before.\nIt is convenient to spend 20 min a day\nWorks from web/tablet/mobile\n\nLink: https://www.cloud-exam-prepare.com\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/apps/npm/cognito-token-observer/",
            "title": "cognito-token-observer",
            "description": "Monitors date expiration of access and id tokens provided by Amazon Cognito. Refreshes when expired.",
            "content": "",
            "tags": [
                "npm"
            ],
            "lang": "en"
        },
        {
            "uri": "/apps/npm/hugo-lunr-ml/",
            "title": "hugo-lunr-ml",
            "description": "Create lunr index file for multilingual hugo static site",
            "content": "",
            "tags": [
                "npm"
            ],
            "lang": "en"
        },
        {
            "uri": "/authors/roman-kurnovskii/_index",
            "title": "Roman Kurnovskii",
            "content": "\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/homepage/about",
            "title": "Roman Kurnovskii",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/homepage/education",
            "title": "Education",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/homepage/experience",
            "title": "Experience",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/homepage/",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/homepage/pages",
            "title": "Posts",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/p/links",
            "title": "Links",
            "description": null,
            "content": "\nAI\n\nThe curated list of AI tools for marketing\nDiscover the newest AIs for any given task\nCreate 1,000 AI art images a day for free\nopenai examples\nopenai answers from file\nAI-powered video summaries\nChrome extension\n\nSoft/Architecture\n\nBlock Diagram Maker\nTheme for Docs - MkDocs\n\nMedia\n\nFast and simple way to visualize your story - free 1 Storyboard and 10 Frames\nVideo from Screenshot\n\nOther\n\nCreate and sell your own personalized books and journals\nBooks nice covers\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/p/supportme",
            "title": "Support me",
            "description": null,
            "content": "\nhttps://www.buymeacoffee.com/romankurnovskii\nhttps://rom.gumroad.com/\nhttps://www.patreon.com/user?u=79828420\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/_index",
            "title": "Notes",
            "content": "\nList style view",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/archive/",
            "title": "Posts Archive",
            "content": "\nDocs EN | RU\nPosts EN | RU\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/bash-snippets",
            "title": "Bash code snippets",
            "description": "This bash code snippet provides a useful solution for renaming files in the current directory based on a specific pattern. The script loops through all files in the directory and checks if the filename contains the specified pattern (in this case, the \"№\" symbol). If it does, the script removes everything before and including the symbol and renames the file with the new name. The script then prints a message for each renamed file. This code snippet can be a useful time-saver for those who need to rename many files based on a certain pattern.",
            "content": "\n\nRename all files by pattern in current directory\n\nfiles=(*)\n\nLoop through all files\nfor file in \"${files[@]}\"\ndo\nCheck if the file name contains the \"№\" symbol\n  if [[ $file == \"№\" ]]; then\nRemove everything before and including the \"№\" symbol\n    new_file=${file##*\"№\"}\n    mv \"$file\" \"$new_file\"\n    echo \"Renamed $file to $new_file\"\n  fi\ndone\n",
            "tags": [
                "Linux",
                "Bash"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/cheat-sheet-command-tar/",
            "title": "Tar command Cheat Sheet",
            "description": "tar command Cheat Sheet",
            "content": "\nCommon options\n\n    z\tcompress with gzip\n    c\tcreate an archive\n    u\tappend files which are newer than\n        the corresponding copy ibn the archive\n    f\tfilename of the archive\n    v\tverbose, display what is inflated or deflated\n    a\tunlike of z, determine compression based on file extension\n\nCreate tar named archive.tar containing directory\n\ntar cf archive.tar /path/files\n\nConcatenate files into a single tar\n\ntar -cf archive.tar /path/files\nExtract the contents from archive.tar\n\ntar xf archive.tar\n\nCreate a gzip compressed tar file name archive.tar.gz\n\ntar czf archive.tar.gz /path/files\n\nExtract a gzip compressed tar file\n\ntar xzf archive.tar.gz\n\nCreate a tar file with bzip2 compression\n\ntar cjf archive.tar.bz2 /path/files\n\nExtract a bzip2 compressed tar file\n\ntar xjf archive.tar.bz2\n\nList content of tar file\n\ntar -tvf archive.tar\n`",
            "tags": [
                "Linux",
                "CLI",
                "tar",
                "cheatsheet"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/cloud-exam-quizz/amplify-custom-domain",
            "title": "AWS Amplify - Set custom domain",
            "description": "Set custom domain on AWS Amplify",
            "content": "\nYou can use any custom domain with Amplify and no need register it with AWS Route53.\n\nI am adding domain at the setup app stage. Another way is from console.\n\nClick Domain management.\n\namplify-add-custom-domain\n\nor\n\namplify-add-custom-domain\n\nAdd domain\nWrite domain name -> Configure domain -> Save\n\namplify-add-custom-domain\n\nNest starts SSL configuration process. Amplify provides with DNS data that you need to write in the domain register account.\n\namplify-add-custom-domain\n\nOnce SSL creation starts you can get domain data\n\nAction -> View DNS records\n\namplify-add-custom-domain\n\nCopy provided data (DNS records) and then set it in the domain registrar panel.\n\namplify-add-custom-domain\n\nGo to domain registrar\nSet dns servers to default\n\namplify-add-custom-domain\n\nIn my case panel looks like this:\n\namplify-add-custom-domain\namplify-add-custom-domain\n\nSave\nGo to amplify and check for updates. Amplify checks DNS server and if everything is correct (CNAME set) it will proceed to the next step.\n\namplify-add-custom-domain\n\nSSL configuration passed, waiting up to 30 min for domain activation\n\nOnce done we can check url: https://cloud-exam-prepare.com\n\namplify-add-custom-domain\n\nCheck url: cloud-exam-prepare.com\n\nResources:\n\nhttps://docs.aws.amazon.com/amplify/latest/userguide/to-add-a-custom-domain-managed-by-a-third-party-dns-provider.html\n",
            "tags": [
                "AWS",
                "AWS Amplify"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/cloud-exam-quizz/amplify-setup-project",
            "title": "AWS Amplify - project setup with Github",
            "description": "AWS Amplify - Initial setup with Github",
            "content": "\nPreface\n\nFor Amplify project I use eu-west region\ngithub repo has to be ready private or public\n\nNew project\n\ngoto https://eu-west-1.console.aws.amazon.com/amplify/home?region=eu-west-1#/\n\nNew app → Host web app → Github\n    Add access to github repo\ngithub-setup-access\n    Select repository\ngithub-setup-access\n\nCome back to Amplify and try again to choose repo\ngithub-setup-access\n\nClick Next\n\nUpdate amplify.yml for node.js project\n\nversion: 1\nfrontend:\n  phases:\n    preBuild:\n      commands:\n        yarn install\n    build:\n      commands:\n        yarn run build\n  artifacts:\n    baseDirectory: build\n    files:\n      '*/'\n  cache:\n    paths:\n      node_modules/*/\n\namplify-yml\n\nNext → Save and deploy\n\nAmplify starts to build project and generates project url.\n\namplify-build-process\n\nOnce build done you can open project.\n\namplify-success-url",
            "tags": [
                "AWS",
                "AWS Amplify",
                "Github"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/code-style",
            "title": "Code style notes",
            "description": "Code style notes",
            "content": "\nRelease notes example\n\nChanged\nfeat(exports): export mergeConfig #5151\n\nFixed\n  fix(CancelledError): include config #4922\n  fix(general): removing multiple/trailing/leading whitespace #5022\n  fix(headers): decompression for responses without Content-Length header #5306\n  fix(webWorker): exception to sending form data in web worker #5139\n\nRefactors\n  refactor(types): AxiosProgressEvent.event type to any #5308\n  refactor(types): add missing types for static AxiosError.from method #4956\n\nChores\n  chore(docs): remove README link to non-existent upgrade guide #5307\n  chore(docs): typo in issue template name #5159\n\nCode format\n\nPython style\nJavaScript style",
            "tags": [
                "Code"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/diploma/",
            "title": "IT courses 2020",
            "description": "Certified IT knowledge for the year 2020",
            "content": "\nInterim metrics still in process\n\nFor 2020:\n\nTime spent studying/practicing: ~5500 hours\n\n\n",
            "tags": [
                "study",
                "it",
                "emigration"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/docker-commands/",
            "title": "Top Docker Commands",
            "description": "Most Popular Docker Commands",
            "content": "\nmost popular\n\n    docker images                       ##  lists the images\n    docker pull imagename               ##  Pull an image or a repository from a registry\n    docker ps -a                        ##  See a list of all containers, even the ones not running\n    docker build -t imagename .         ##  Create image using this directory's Dockerfile\n    docker run -p 4000:80 imagename     ##  Run \"imagename\" mapping port 4000 to 80\n    docker rmi                          ##  removes the image\n    docker rm                           ##  removes the container\n    docker stop                         ##  stops the container\n    docker volume ls                    ##  lists the volumes\n    docker kill                         ##  kills the container\n    docker logs                         ##  see logs\n    docker inspect                      ##  shows all the info of a container\n\ndocker\n\n    docker cp                                   ##  Copy files/folders between a container and the local filesystem\n    docker pull imagename                       ##  Pull an image or a repository from a registry\n    docker build -t imagename .                 ##  Create image using this directory's Dockerfile\n    docker run -p 4000:80 imagename             ##  Run \"imagename\" mapping port 4000 to 80\n    docker run -d -p 4000:80 imagename          ##  Same thing, but in detached mode\n    docker exec -it [container-id] bash         ##  Enter a running container\n    docker ps                                   ##  See a list of all running containers\n    docker stop                           ##  Gracefully stop the specified container\n    docker ps -a                                ##  See a list of all containers, even the ones not running\n    docker kill                           ##  Force shutdown of the specified container\n    docker rm                             ##  Remove the specified container from this machine\n    docker rm -f                          ##  Remove force specified container from this machine\n    docker rm $(docker ps -a -q)                ##  Remove all containers from this machine\n    docker images -a                            ##  Show all images on this machine\n    docker rmi                       ##  Remove the specified image from this machine\n    docker rmi $(docker images -q)              ##  Remove all images from this machine\n    docker top                    ##  Display the running processes of a container\n    docker logs  -f               ##  Live tail a container's logs\n    docker login                                ##  Log in this CLI session using your Docker credentials\n    docker tag  username/repository:tag  ##  Tag  for upload to registry\n    docker push username/repository:tag         ##  Upload tagged image to registry\n    docker run username/repository:tag          ##  Run image from a registry\n    docker system prune                         ##  Remove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes. (Docker 17.06.1-ce and superior)\n    docker system prune -a                      ##  Remove all unused containers, networks, images not just dangling ones (Docker 17.06.1-ce and superior)\n    docker volume prune                         ##  Remove all unused local volumes\n    docker network prune                        ##  Remove all unused networks\n\ndocker compose\n\n    docker-compose up                               # Create and start containers\n    docker-compose up -d                            # Create and start containers in detached mode\n    docker-compose down                             # Stop and remove containers, networks, images, and volumes\n    docker-compose logs                             # View output from containers\n    docker-compose restart                          # Restart all service\n    docker-compose pull                             # Pull all image service\n    docker-compose build                            # Build all image service\n    docker-compose config                           # Validate and view the Compose file\n    docker-compose scale =   # Scale special service(s)\n    docker-compose top                              # Display the running processes\n    docker-compose run -rm -p 2022:22 web bash      # Start web service and runs bash as its command, remove old container.\n\ndocker services\n\n    docker service create      # Create new service\n    docker service inspect --pretty       # Display detailed information Service(s)\n    docker service ls                                   # List Services\n    docker service ps                                   # List the tasks of Services\n    docker service scale =       # Scale special service(s)\n    docker service update        # Update Service options\n\ndocker stack\n\n    docker stack ls                                 # List all running applications on this Docker host\n    docker stack deploy -c    # Run the specified Compose file\n    docker stack services                  # List the services associated with an app\n    docker stack ps                        # List the running containers associated with an app\n    docker stack rm                        # Tear down an application\n\ndocker machine\n\n    docker-machine create --driver virtualbox myvm1                           # Create a VM (Mac, Win7, Linux)\n    docker-machine create -d hyperv --hyperv-virtual-switch \"myswitch\" myvm1  # Win10\n    docker-machine env myvm1                                                  # View basic information about your node\n    docker-machine ssh myvm1 \"docker node ls\"                                 # List the nodes in your swarm\n    docker-machine ssh myvm1 \"docker node inspect \"                  # Inspect a node\n    docker-machine ssh myvm1 \"docker swarm join-token -q worker\"              # View join token\n    docker-machine ssh myvm1                                                  # Open an SSH session with the VM; type \"exit\" to end\n    docker-machine ssh myvm2 \"docker swarm leave\"                             # Make the worker leave the swarm\n    docker-machine ssh myvm1 \"docker swarm leave -f\"                          # Make master leave, kill swarm\n    docker-machine start myvm1                                                # Start a VM that is currently not running\n    docker-machine stop $(docker-machine ls -q)                               # Stop all running VMs\n    docker-machine rm $(docker-machine ls -q)                                 # Delete all VMs and their disk images\n    docker-machine scp docker-compose.yml myvm1:~                             # Copy file to node's home dir\n    docker-machine ssh myvm1 \"docker stack deploy -c  \"            # Deploy an app\n\nOptions for popular commands\ndocker build\n\nDocs\nBuild an image from a Dockerfile.\n\ndocker build [DOCKERFILE PATH]\n\nExample\n\nBuild an image tagged my-org/my-image where the Dockerfile can be found at\n/tmp/Dockerfile.\n\ndocker build -t my-org:my-image -f /tmp/Dockerfile\n\n--file -f Path where to find the Dockerfile\n--force-rm Always remove intermediate containers\n--no-cache Do not use cache when building the image\n--rm Remove intermediate containers after a successful build (this is\ntrue) by default\n--tag -t Name and optionally a tag in the ‘name:tag’ format\n\ndocker run\nDocs\n\nCreates and starts a container in one operation. Could be used to execute a\nsingle command as well as start a long-running container.\n\nExample\n\ndocker run -it ubuntu:latest /bin/bash\n\nThis will start a ubuntu container with the entrypoint /bin/bash. Note that\nif you do not have the ubuntu image downloaded it will download it before\nrunning it.\n\n\n-it This will not make the container you started shut down immediately, as\nit will create a pseudo-TTY session (-t) and keep STDIN open (-i)\n--rm Automatically remove the container when it exit. Otherwise it will be\nstored and visible running docker ps -a.\n--detach -d Run container in background and print container ID\n--volume -v Bind mount a volume. Useful for accessing folders on your local\ndisk inside your docker container, like configuration files or storage that\nshould be persisted (database, logs etc.).\n\ndocker exec\n\nDocs\n\nExecute a command inside a running container.\n\ndocker exec [CONTAINER ID]\n\nExample\n\ndocker exec [CONTAINER ID] touch /tmp/exec_works\n\n--detach -d Detached mode: run command in the background\n-it This will not make the container you started shut down immediately, as\nit will create a pseudo-TTY session (-t) and keep STDIN open (-i)\n\ndocker images\n\nDocs\n\nList all downloaded/created images.\n\ndocker images\n\n-q Only show numeric IDs\n\ndocker inspect\nDocs\n\nShows all the info of a container.\n\ndocker inspect [CONTAINER ID]\n\ndocker logs\nDocs\n\nGets logs from container.\n\ndocker logs [CONTAINER ID]\n\n--details Log extra details\n--follow -f Follow log output. Do not stop when end of file is reached, but\nrather wait for additional data to be appended to the input.\n--timestamps -t Show timestamps\n\ndocker ps\nDocs\n\nShows information about all running containers.\n\ndocker ps\n\n--all -a Show all containers (default shows just running)\n--filter -f Filter output based on conditions provided, docker ps -f=\"name=\"example\"\n--quiet -q Only display numeric IDs\n\ndocker rmi\nDocs\n\nRemove one or more images.\n\ndocker rmi [IMAGE ID]\n\n--force -f Force removal of the image\n\nSnippets\n\nA collection of useful tips and tricks for Docker.\n\nDelete all containers\n\nNOTE: This will remove ALL your containers.\n\ndocker container prune\n\nOR, if you're using an older docker client:\n\ndocker rm $(docker ps -a -q)\n\nDelete all untagged containers\n\ndocker image prune\n\nOR, if you're using an older docker client:\n\ndocker rmi $(docker images | grep '^' | awk '{print $3}')\n\nRemove all docker images with none tag\n\ndocker rmi --force $(docker images --filter \"dangling=true\" -q)\n\nSee all space Docker take up\n\ndocker system df\n\nGet IP address of running container\n\ndocker inspect [CONTAINER ID] | grep -wm1 IPAddress | cut -d '\"' -f 4\n\nKill all running containers\n\ndocker kill $(docker ps -q)\n\nResources\n\ndocs.docker.com\ndocker-cheat-sheet\ndocker-cheat-sheet\nhttps://sourabhbajaj.com/mac-setup/Docker/\n",
            "tags": [
                "docker",
                "cheatsheet"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/git-snippets",
            "title": "Git snippets",
            "description": "Git snippets",
            "content": "\nGithub Actions\n\nSubmodules Sync\n\nname: 'Submodules Sync'\n\non:\n  schedule:\n    cron: \"0 * * * *\"\n\njobs:\n  sync:\n    runs-on: ubuntu-latest\n\n    steps:\nChecks-out your repository under $GITHUB_WORKSPACE, so your job can access it\n      uses: actions/checkout@v2\n        with:\n          submodules: true\n\n      name: Pull & update submodules recursively\n        run: |\n          git pull --recurse-submodules\n          git submodule update --remote --recursive\n\n      name: Commit & push changes\n        run: |\n          git config --global user.name 'Git bot'\n          git config --global user.email 'bot@noreply.github.com'\n          git commit -am \"Auto updated submodule references\" && git push || echo \"No changes to commit\"\n\nFree space in git repo\n\nDownload BFG\n\nRemove history files bigger than 100Kb:\n\n    cd repo\n    java -jar bfg-1.14.0.jar --strip-blobs-bigger-than 100K .\n    git reflog expire --expire=now --all && git gc --prune=now --aggressive\n\nRemoving an entire commit:\n\nReplace \"SHA\" with the reference you want to get rid of. The \"^\" in that command is literal.\n\n    git rebase -p --onto SHA^ SHA\n\nWe want to remove commits 2 & 4 from the repo. (Higher the the number newer the commit; 0 is the oldest commit and 4 is the latest commit)\n\n    commit 0 : b3d92c5\n    commit 1 : 2c6a45b\n    commit 2 :\n    commit 3 : 77b9b82\n    commit 4 :\n\nNote: You need to have admin rights over the repo since you are using --hard and -f.\n\ngit checkout b3d92c5 Checkout the last usable commit.\ngit checkout -b repair Create a new branch to work on.\ngit cherry-pick 77b9b82 Run through commit 3.\ngit cherry-pick 2c6a45b Run through commit 1.\ngit checkout master Checkout master.\ngit reset --hard b3d92c5 Reset master to last usable commit.\ngit merge repair Merge our new branch onto master.\ngit push -f origin master Push master to the remote repo.\n\n\nIf didn't publish changes, to remove the latest commit, do:\n\n    git rebase -i HEAD~\n    git rebase -i ~1\n    git reset --hard HEAD^\n    git reset --hard commitId\n    git rebase -i HEAD~5\n\nIf already published to-be-deleted commit:\n\ngit revert HEAD\n\nCleanups:\n\n    git stash clear\n    git reflog expire --expire-unreachable=now --all\n    git fsck --full\n    git fsck --unreachable\t\t# Will show you the list of what will be deleted\n    git gc --prune=now\t\t\t# Cleanup unnecessary files and optimize the local repository\n\nCommon git commands\n\n    git rev-list --all --count # count commits\n    git clean -fd # To remove all untracked (non-git) files and folders!\n\nResources\n\nhttps://sethrobertson.github.io/GitFixUm/fixup.html\nhttps://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-clone.html\nhttps://passingcuriosity.com/2017/truncating-git-history/\nhttps://www.npmjs.com/package/clear-git-branch?activeTab=explore",
            "tags": [
                "git"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/how-to-upload-app-to-sourceforge/",
            "title": "How to upload an opensource application to SourceForge",
            "description": "Step-by-Step Guide how to upload and manage your open-source application on SourceForge with this detailed, step-by-step guide, ensuring maximum visibility and accessibility for your project.",
            "content": "\nStep 1: Create a SourceForge account\n\nGo to the SourceForge website at https://sourceforge.net/\nClick on the \"Join\" button in the top-right corner.\nFill in the required fields, such as username, email, and password, then click \"Register\"\nYou'll receive a confirmation email from SourceForge. Click on the link provided to confirm your account.\n\nCreate a SourceForge account\n\nStep 2: Start a new project\n\nLog in to your SourceForge account.\nClick on the \"Create\" button in the top-right corner of the page.\nSelect \"Create Your Project Now\" from the dropdown menu.\n\nStart a new project\n\nStep 3: Configure your project\n\nEnter a unique name for your project in the \"Project Name\" field. This name will also serve as your project's URL.\n\nConfigure your project\n\nProvide a Phone number if required. You will get a pin for verification.\nNext you will see a quick tour that could help you to fulfill all the required fields.\n\nStart a new project\n\nProvide a brief description of your project in the \"Short Summary\" field.\nUpload Project Logo\nClick \"Save\".\n\n\nFill other fields from left sidebar menu.\n\nStart a new project\n\nChoose an appropriate \"License\" for your open-source project from the dropdown menu. If you're unsure which license to choose, you can refer to the Open Source Initiative's list of approved licenses (https://opensource.org/licenses).\nSelect the \"Programming Language\" and \"Operating System\" that your project is built for.\nAdd any relevant \"Tags\" to help users find your project.\nClick the \"Create\" button at the bottom of the form to create your project.\n\nStep 4: Configure your project's Source Control Management (SCM)\n\nSourceForge supports several SCM options, including Git, Mercurial, and Subversion. Choose the one that best suits your needs.\n\nGo to your project's main page.\nClick on the \"Buttons & Badges\" link in the left side bar.\nStart a new project\nClick \"GitHub Integration\".\nEnter your GitHub username/repo.\nClick \"Set up\" or choose \"Set up integration manually\".\nStart a new project\n\nStep 5: Set up release files (optional)\n\nIf you want to provide compiled binaries or other release files for users to download, follow these steps:\n\nGo to your project's main page.\nClick on the \"Files\" tab in the top navigation bar.\nClick on the \"Add Folder\" button to create a new folder for your release files (e.g., \"v1.0\").\nClick on the newly created folder and then click the \"Upload\" button.\nSelect the release files you want to upload, and click \"Open\" to start the upload process.\nStart a new project\n\nSummary\n\nNow we have a project page on SourceForge.\n\nHere is a button for download:\nDownload BrewMate\n\nProject url: https://sourceforge.net/projects/brewmate/\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/howto-create-deepclone-js/",
            "title": "How to create a deep clone of an object in JavaScript",
            "description": "How to create a deep clone of an object in JavaScript",
            "content": "\nWe can use recursion.\nUse Object.assign() and an empty object ({}) to create a shallow clone of the original.\nUse Object.keys() and Array.prototype.forEach() to determine which key-value pairs need to be deep cloned.\n\nconst deepClone = obj => {\n  let clone = Object.assign({}, obj);\n  Object.keys(clone).forEach(\n    key => (clone[key] = typeof obj[key] === 'object' ? deepClone(obj[key]) : obj[key])\n  );\n  return Array.isArray(obj) && obj.length\n    ? (clone.length = obj.length) && Array.from(clone)\n    : Array.isArray(obj)\n      ? Array.from(obj)\n      : clone;\n};\n\nconst a = { foo: 'bar', obj: { a: 1, b: 2 } };\nconst b = deepClone(a); // a !== b, a.obj !== b.obj\n`",
            "tags": [
                "JavaScript"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/howto-install-rhel-9-free/",
            "title": "How to Download and Install Linux RHEL 9 for Free",
            "description": "How to Download and Install Linux RHEL 9 for Free",
            "content": "\n\nRed Hat Enterprise Linux 9 (RHEL 9), codenamed Plow, has gone public (GA). Red Hat announced it on May 18, 2022. It replaced the beta version, which had been in existence since November 3, 2021.\n\nRHEL 9 is the first few releases in the Red Hat family. It is the first major release since IBM acquired Red Hat in July 2019, and the first major release since abandoning the CentOS project in favor of CentOS Stream, which is now RHEL's predecessor.\n\nRHEL 9 is the latest major version of RHEL and comes with a 5.14 kernel, lots of new software packages and a host of improvements. It emphasizes security, stability, flexibility and reliability.\n\nDescription.\nRHEL 9 ships with new versions of software including Python 3.9. Node.JS 16, GCC 11, Perl 5.32, Ruby 3.0, PHP 8.0, and many more.\n\nPreparing for installation\n\nRegistration on the Red Hat portal\n\nRed Hat Developer Subscription is a free Red Hat Developer Program offer designed for individual developers who want to take full advantage of Red Hat Enterprise Linux.\n\nIt gives developers access to all versions of Red Hat Enterprise Linux, as well as other Red Hat products such as add-ons, software updates and security bugs.\n\nFirst of all, make sure you have an active Red Hat account. If you don't already have an account, go to the Red Hat Customer Portal, click on \"Register\" and fill out your information to create a Red Hat account.\n\nDownloading the installation image\nAfter creating a Red Hat account, you can start downloading RHEL 9. To download Red Hat Enterprise Linux 9 absolutely free, go to Red Hat Developer Portal  and log in with your account credentials.\n\n\nThen go to the download RHEL 9 page and click on the download button shown below.\n\nI'm using a MacBook M1, so I download the RHEL 9 image for the M1 processor aarch64\n\nVirtual machine\nI use the free UTM virtual machine as a virtual machine to install RHEL 9. You can install using Homebrew by running the command brew install --cask utm.\n\nInstalling Red Hat Enterprise Linux 9\n\nSetting up the UTM virtual machine\nIn UTM click Create a New Virtual Machine -> Virtualize\n\n\nChoose the downloaded RHEL 9 image and click Continue.\n\nMain Setup Menu\n\n\nThe marked fields need to be filled in\n\nCreate Root Password\n\nUser Creation. Create the user you want to log in with.\n\n\n\nConnect to Red Hat. Here we will use the account created above.\n\nHere you will enter your account data and click Register.\n\nPress Done\n\nUnder Installation Destination choose your default drive.\n\nWe can now continue with the installation. A Begin installation button will appear on the main screen\n\nAfter installation is complete, we will have to reboot the system.\n\nSometimes rebooting will unload the installation image again. It's necessary to either disable the disk in the installer setup or reboot the UTM.\n\nRunning Red Hat Enterprise Linux 9\n\n\n\nEnter your password and see the RHEL 9 desktop\n\n\nTo access the applications, click the Activities button in the upper left corner\n\nConfiguring Red Hat Enterprise Linux 9\n\nChecking the ROOT user\nIn a Linux system users belong to different groups which have certain rights. If during the installation process we did not check the checkbox to make the user an administrator, by default he will not be able to install some system programs.\n\nExit and log in as root (the same user we created earlier on the main screen). Press Log out\n\n\nNow log in as root. The user may not be listed. Press Not listed and enter the account data.\n\nOpen terminal and check\n\nConfiguring system settings\n\nButton to minimize the application\nThe first thing that seems unusual about using the GUI is that there are no buttons to minimize windows\n\n\nInstall the necessary package\n\nyum install gnome-tweaks -y\n\n\n\nAfter installation, the Tweaks application will appear. Find it by searching.\n\n\n\nThere are many other tweaks in the app as well. We will show you the minimize buttons for the applications.\n\nLet's go to Windows titlebars and set the Maximize, Minimize options\n\nUser access to install applications\n\nTo avoid constantly switching to a root user to install applications, we can give the normal user access to install applications.\nWe will continue to do this as root.\nOpen /etc/sudoers and add the user\n\nsudo vi /etc/sudoers\n\nAdd user data to the end of the file. My user name: rhel-user\n\nrhel-user ALL= NOPASSWD: /usr/sbin/synaptic, /usr/bin/software-center, /usr/bin/apt-get, /usr/bin/dnf\n\n\n\n\nLet's install Visual Studio Code as a normal user\nInstallation consists of the following steps:\nadding the desired repository. Rights to add the repository (changing the files in the directory is still only for root user)\nDownloading and installing.\n\nFirst step is done as root user\nGo to https://code.visualstudio.com/docs/setup/linux\n\nCopy the code and run it in the terminal\n\nsudo rpm --import https://packages.microsoft.com/keys/microsoft.asc\nsudo sh -c 'echo -e \"[code]\\nname=Visual Studio Code\\nbaseurl=https://packages.microsoft.com/yumrepos/vscode\\nenabled=1\\ngpgcheck=1\\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc\" > /etc/yum.repos.d/vscode.repo'\n\nSwitch to user rhel-user. This can also be done in the terminal.\nUpdating the repositories\nInstall VSCode\n\nsu rhel-user\ndnf check-update\n\nsudo dnf install code\n\nReferences\nhttps://developers.redhat.com/products/rhel/getting-started\nhttps://www.redhat.com/sysadmin/install-linux-rhel-9",
            "tags": [
                "linux",
                "rhel"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/howto-rename-files-in-python/",
            "title": "How to rename files in Python",
            "description": "How to rename files in Python",
            "content": "\nLearn different ways to rename files in Python using the os and pathlib modules.\n\nos.rename\n\nRename files with os\n\nYou can use\n\nos.rename(old_name, new_name)\n\nFor example we can combine it with os.path.splitext() to get the base name and file extension, and then combine it to a new name:\n\nimport os\nfor file in os.listdir():\n    name, ext = os.path.splitext(file)\n    new_name = f\"{name}_new{ext}\"\n    os.rename(file, new_name)\n\npathlib\n\nRename files with pathlib\n\nThe same could be achieved with the pathlib module and\n\nPath.rename(new_name)\n\nWith a Path object we can access .stem and .suffix:\n\nfrom pathlib import Path\nfor file in os.listdir():\n    f = Path(file)\n    new_name = f\"{f.stem}_new{f.suffix}\"\n    f.rename(new_name)\n\nshutil.move\n\nThe shutil module offers a number of high-level operations on files and collections of files. In particular, functions are provided which support file copying and removal. For operations on individual files, see also the os module.\n\nimport shutil\n\nold_source = '/Users/r/Desktop/old_source.txt'\nnew_source = '/Users/r/Desktop/new_source.txt'\n\nnewFileName = shutil.move(old_source, new_source)\n\nprint(\"New file:\", newFileName)\nNew file: /Users/r/Desktop/new_source.txt\n`",
            "tags": [
                "Python"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-add-copy-button-on-highlight-block",
            "title": "How to add copy code button on HUGO highligh code block",
            "description": "Learn how to quickly add a copy button to code highlight blocks in Hugo to make it easier for users to share code snippets on your site.",
            "content": "\n\nfunction addCopyButtonToCodeBlocks() {\n    // Get all code blocks with a class of \"language-*\"\n    const codeBlocks = document.querySelectorAll('code[class^=\"language-\"]');\n\n    // For each code block, add a copy button inside the block\n    codeBlocks.forEach(codeBlock => {\n        // Create the copy button element\n        const copyButton = document.createElement('button');\n        copyButton.classList.add('copy-code-button');\n        copyButton.innerHTML = '';\n\n        // Add a click event listener to the copy button\n        copyButton.addEventListener('click', () => {\n            // Copy the code inside the code block to the clipboard\n            const codeToCopy = codeBlock.innerText;\n            navigator.clipboard.writeText(codeToCopy);\n\n            // Update the copy button text to indicate that the code has been copied\n            copyButton.innerHTML = '';\n            setTimeout(() => {\n                copyButton.innerHTML = '';\n            }, 1500);\n        });\n\n        // Add the copy button to the code block\n        codeBlock.parentNode.insertBefore(copyButton, codeBlock);\n    });\n}\n\n\n",
            "tags": [
                "hugo",
                "javascript"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-add-image-zoomin/",
            "title": "Hugo resize a picture on click",
            "description": "Script will zoom in on a picture on click in Hugo",
            "content": "\nIntroduction\n\nHugo by default uses parsing of markdown files. This means that we get the html code as it is written in markdown.\n\nIn order to understand which images we can enhance, we add a separate tag/key/id to those images\n\nTools\n\nTo implement the functionality, we need to:\nWrite/connect a script/handler that will perform the zoomin effect on the images we need\nAdd the necessary metadata to the images, so the script can find them\n\nzoomin script\n\nTo add the ability to zoom on click, we will use the medium-zoom package.\n\nThis package provides this functionality in a non-loaded, handy style.\n\n\nDemo\n\nScript logic\n\nThe script finds images with id and so understands to apply the zoomin property to those images\n\nPossible id:\n\nzoom-default\nzoom-margin\nzoom-background\nzoom-scrollOffset\nzoom-trigger\nzoom-detach\nzoom-center\n\nConnecting the scripts\n\nIn order for the script to work, we need to connect the logic as well as the handler.\n\nHugo has a static folder in the root of the project, which can be used to store static files (styles, scripts) and used to connect them to the site. If there is no such folder, you can create one.\n\nIn the static folder create a folder zoom-image and add two scripts to it\n\nstatic/js/zoom-image/index.js\n\nconst zoomDefault = mediumZoom('#zoom-default')\nconst zoomMargin = mediumZoom('#zoom-margin', { margin: 48 })\nconst zoomBackground = mediumZoom('#zoom-background', { background: '#212530' })\nconst zoomScrollOffset = mediumZoom('#zoom-scrollOffset', {\n    scrollOffset: 0,\n    background: 'rgba(25, 18, 25, .9)',\n})\n\n// Trigger the zoom when the button is clicked\nconst zoomToTrigger = mediumZoom('#zoom-trigger')\nconst button = document.querySelector('#button-trigger')\nbutton.addEventListener('click', () => zoomToTrigger.open())\n\n// Detach the zoom after having been zoomed once\nconst zoomToDetach = mediumZoom('#zoom-detach')\nzoomToDetach.on('closed', () => zoomToDetach.detach())\n\n// Observe zooms to write the history\nconst observedZooms = [\n    zoomDefault,\n    zoomMargin,\n    zoomBackground,\n    zoomScrollOffset,\n    zoomToTrigger,\n    zoomToDetach,\n]\n\n// Log all interactions in the history\nconst history = document.querySelector('#history')\n\nobservedZooms.forEach(zoom => {\n    zoom.on('open', event => {\n        const time = new Date().toLocaleTimeString()\n        history.innerHTML += `Image \"${event.target.alt\n            }\" was zoomed at ${time}`\n    })\n\n    zoom.on('detach', event => {\n        const time = new Date().toLocaleTimeString()\n        history.innerHTML += `Image \"${event.target.alt\n            }\" was detached at ${time}`\n    })\n})\n\nstatic/js/zoom-image/placeholders.js\n\n// Show placeholders for paragraphs\nconst paragraphs = [].slice.call(document.querySelectorAll('p.placeholder'))\n\nparagraphs.forEach(paragraph => {\n  // eslint-disable-next-line no-param-reassign\n  paragraph.innerHTML = paragraph.textContent\n    .split(' ')\n    .filter(text => text.length > 4)\n    .map(text => ${text})\n    .join(' ')\n})\n\nCDN script\n\nYou can download the script, or you can upload it\n\nScript Link\n\nAdding to template\n\nIn order for these scripts to work in the website template, they must be connected.\n\nI use for this the template baseof.html. I simply add links to the scripts in body of the template.\n\nbaseof.html\n\n    ...\n\n\nimage ID\n\nHugo allows you to change the parsing behavior of markdown files with hooks. You can read more about render-hooks at website.\n\nIn the *layouts folder.\n\nLet's add the file render-image.html to the following path layouts -> _default -> _markup\n\n\nfile code:\n\n\nWe only added id=\"zoom-default\" to the default code\n\nResult\n\nYour browser does not support the video tag.\n\nProcess\n\n{{}}",
            "tags": [
                "hugo"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-add-search-lunr-popup/",
            "title": "Add search to Hugo multilingual static site with Lunr",
            "description": "Make your multilingual Hugo static site searchable with a client-side search index",
            "content": "\nInitial\n\nI had the need to implement search functionality on my site. Content on is in different languages.\n\nThe goal is to impelemnt search for all pages and separate search results for each and every language.\n\nHow it works\n\nHugo generates the search index. In this case it means that we get json file with every static page on the site.\n\nTo make search works we need to create index. lunr.js takes care of it.\n\nClient send query -> our script \"tries to find\" in the index\n\nRender the results\n\nThis is how the logic looks like:\n\nImplementation\n\n\nCreate search form\nCreate popup modal where will render search results\nConnect Lunr.js script\nGenerate pages data\nConnect search/result forms with lunr.js search\n\nTL;DR\n\nFiles to change/create:\n\n\n\n1.  /layouts/partials/header.html\n\n\n2. /layouts/partials/components/search-list-popup.html\n\n        Search results\n\n\n3. /layouts/partials/footer.html\n\n    ...\n  {{ $languageMode := .Site.Language }}\n\n\n  {{ partial \"components/search-list-popup.html\" . }}\n  ...\n\n\n4. /layouts/_default/index.json\n\n[\n    {{- range $index, $page := .Site.RegularPages.ByTitle -}}\n      {{- if gt $index 0 -}} , {{- end -}}\n      {{- $entry := dict \"uri\" $page.RelPermalink \"title\" $page.Title -}}\n      {{- $entry = merge $entry (dict \"description\" .Description) -}}\n      {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}}\n      {{- $entry | jsonify -}}\n    {{- end -}}\n]\n\n\n\n\n5. config.yaml\n\nconfig.yaml\nneed for search popup service / creates search.json index fo lunr.js\n\noutputFormats:\n  SearchIndex:\n    baseName: search\n    mediaType: application/json\n\noutputs:\n  home:\n    HTML\n    RSS\n    SearchIndex\n\n\n\n\n6. static/js/search.js\n\nconst languageMode = window.document.currentScript.getAttribute('languageMode');\nconst MAX_SEARCH_RESULTS = 10\n\nlet searchIndex = {}\nlet pagesStore = {}\n\n// Need to create ONLY once , maybe before push | during build\nconst createIndex = (documents) => {\n    searchIndex = lunr(function () {\n        this.field(\"title\");\n        this.field(\"content\");\n        this.field(\"description\");\n        this.field(\"uri\");\n\n        this.ref('uri')\n\n        documents.forEach(function (doc) {\n            pagesStore[doc['uri']] = doc['title']\n            this.add(doc)\n        }, this)\n    })\n\n}\n\nconst loadIndexData = () => {\n    const url = /${languageMode}/search.json;\n\n    var xmlhttp = new XMLHttpRequest();\n    xmlhttp.onreadystatechange = function () {\n        if (this.readyState == 4 && this.status == 200) {\n            const pages_content = JSON.parse(this.responseText);\n            createIndex(pages_content)\n        }\n    };\n\n    xmlhttp.open(\"GET\", url, true);\n    xmlhttp.send();\n}\n\nconst search = (text) => {\n    let result = searchIndex.search(text)\n    return result\n}\n\nconst hideSearchResults = (event, divBlock) => {\n    event.preventDefault()\n    if (!divBlock.contains(event.target)) {\n        divBlock.style.display = 'none';\n        divBlock.setAttribute('class', 'hidden')\n    }\n}\n\n// TODO refactor\nconst renderSearchResults = (results) => {\n    const searchResultsViewBlock = document.getElementById('search-result')\n\n    // hide on move mouse from results block\n    document.addEventListener('mouseup', (e) => hideSearchResults(e, searchResultsViewBlock));\n\n    const searchResultsDiv = document.getElementById('search-results')\n    searchResultsDiv.innerHTML = ''\n\n    searchResultsViewBlock.style.display = 'initial';\n    searchResultsViewBlock.removeAttribute('hidden')\n\n\n    const resultsBlock = document.createElement('ul')\n\n    for (let post of results) {\n        const url = post['ref']\n        const title = pagesStore[url]\n\n        let commentBlock = document.createElement('li')\n\n        let link = document.createElement('a',)\n        let linkText = document.createTextNode(title);\n        link.appendChild(linkText)\n        link.href = url\n\n        commentBlock.appendChild(link)\n        resultsBlock.appendChild(commentBlock)\n    }\n\n    searchResultsDiv.appendChild(resultsBlock)\n\n}\n\n\nconst searchFormObserver = () => {\n    var form = document.getElementById(\"search\");\n    var input = document.getElementById(\"search-input\");\n\n    form.addEventListener(\"submit\", function (event) {\n        event.preventDefault();\n        var term = input.value.trim();\n        if (!term) {\n            return\n        }\n\n        const search_results = search(term, languageMode);\n        renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS))\n\n    }, false);\n}\n\n// create indexes\nloadIndexData()\n\nsearchFormObserver()\n\nSearch form\n\nI am going to add search form to the header part. For thios purpose edit header.html file in the path /layouts/partials/header.html\n\nSet form id: search. By this id script can find this form\n\nMinimal form for work:\n\n\nI use Tailwind, so this is how my form looks like:\n\n        Search\n\n\nModal with results\n\nBy default this modal window is hidden. So don't need to add this to any page. But need to add somewhere.\n\n1. Create .html component\n\npath: /layouts/partials/components/search-list-popup.html\n\nFor modal block to show or hide I use id: search-result\n\nFor block with search results id is: search-results\n\nContent:\n\n        Search results\n\n\n2. Add component to the site\n\nAdd this component to the footer. File path: /layouts/partials/footer.html\n\n...\n    {{ partial \"components/search-list-popup.html\" . }}\n...\n\nConnect Lunr.js\n\nAdd link to this script to the footer template too\n\nPart of the footer template:\n\n...\n\n    {{ partial \"components/search-list-popup.html\" . }}\n...\n\nGenerate pages data\n\nHugo can generate the search index the same way it generates RSS feeds for example, it’s just another output format.\n\n1. Generate script\n\nThis generator is for multilingual site\n\nCreates json in each language catalog in format:\n\n[{\"title\":\"title01\",...}]\n\nFepends on fileds inckluded in the layout /layouts/_default/index.json\n\nCreate file /layouts/_default/index.json\n\n[\n    {{- range $index, $page := .Site.RegularPages.ByTitle -}}\n        {{- if $page.IsTranslated -}}\n          {{ if gt (index $page.Translations 0).WordCount 0 }}\n              {{ range .Translations }}\n                {{- if gt $translatedCount 0 -}} , {{- end -}}\n                {{- $entry := dict \"uri\" .RelPermalink \"title\" .Title -}}\n                {{- $entry = merge $entry (dict \"description\" .Description) -}}\n                {{- $entry = merge $entry (dict \"content\" (.Plain | htmlUnescape)) -}}\n                {{- $entry | jsonify -}}\n                {{ $translatedCount = add $translatedCount 1 }}\n              {{ end}}\n          {{ end }}\n        {{- end -}}\n    {{- end -}}\n  ]\n\nCreates search.json file with page indexes in /public/search.json\n\n2. Set index file path\n\nUpdate config.yaml file:\n\nconfig.yaml\nneed for search popup service / creates search.json index fo lunr.js\n\noutputFormats:\n  SearchIndex:\n    baseName: search\n    mediaType: application/json\n\noutputs:\n  home:\n    HTML\n    RSS\n    SearchIndex\n\nConnect search/result forms with lunr.js search\n\n\nCreate file in the path: static/js/search.js\n\nconst languageMode = window.document.currentScript.getAttribute('languageMode');\nconst MAX_SEARCH_RESULTS = 10\n\nlet searchIndex = {}\nlet pagesStore = {}\n\n// Need to create ONLY once , maybe before push | during build\nconst createIndex = (documents) => {\n    searchIndex = lunr(function () {\n        this.field(\"title\");\n        this.field(\"content\");\n        this.field(\"description\");\n        this.field(\"uri\");\n\n        this.ref('uri')\n\n        documents.forEach(function (doc) {\n            pagesStore[doc['uri']] = doc['title']\n            this.add(doc)\n        }, this)\n    })\n\n}\n\nconst loadIndexData = () => {\n    const url = /${languageMode}/search.json;\n\n    var xmlhttp = new XMLHttpRequest();\n    xmlhttp.onreadystatechange = function () {\n        if (this.readyState == 4 && this.status == 200) {\n            const pages_content = JSON.parse(this.responseText);\n            createIndex(pages_content)\n        }\n    };\n\n    xmlhttp.open(\"GET\", url, true);\n    xmlhttp.send();\n}\n\nconst search = (text) => {\n    let result = searchIndex.search(text)\n    return result\n}\n\nconst hideSearchResults = (event, divBlock) => {\n    event.preventDefault()\n    if (!divBlock.contains(event.target)) {\n        divBlock.style.display = 'none';\n        divBlock.setAttribute('class', 'hidden')\n    }\n}\n\n// TODO refactor\nconst renderSearchResults = (results) => {\n    const searchResultsViewBlock = document.getElementById('search-result')\n\n    // hide on move mouse from results block\n    document.addEventListener('mouseup', (e) => hideSearchResults(e, searchResultsViewBlock));\n\n    const searchResultsDiv = document.getElementById('search-results')\n    searchResultsDiv.innerHTML = ''\n\n    searchResultsViewBlock.style.display = 'initial';\n    searchResultsViewBlock.removeAttribute('hidden')\n\n\n    const resultsBlock = document.createElement('ul')\n\n    for (let post of results) {\n        const url = post['ref']\n        const title = pagesStore[url]\n\n        let commentBlock = document.createElement('li')\n\n        let link = document.createElement('a',)\n        let linkText = document.createTextNode(title);\n        link.appendChild(linkText)\n        link.href = url\n\n        commentBlock.appendChild(link)\n        resultsBlock.appendChild(commentBlock)\n    }\n\n    searchResultsDiv.appendChild(resultsBlock)\n\n}\n\n\nconst searchFormObserver = () => {\n    var form = document.getElementById(\"search\");\n    var input = document.getElementById(\"search-input\");\n\n    form.addEventListener(\"submit\", function (event) {\n        event.preventDefault();\n        var term = input.value.trim();\n        if (!term) {\n            return\n        }\n\n        const search_results = search(term, languageMode);\n        renderSearchResults(search_results.slice(0, MAX_SEARCH_RESULTS))\n\n    }, false);\n}\n\n// create indexes\nloadIndexData()\n\nsearchFormObserver()\n\n\n\nNext need to add this file to the site: /layouts/partials/footer.html\n\nNow footer looks like this:\n\n...\n{{ $languageMode := .Site.Language }}\n\n\n\n{{ partial \"components/search-list-popup.html\" . }}\n...\n\n\n",
            "tags": [
                "hugo",
                "lunr",
                "javascript"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-shortcode-examples/_index",
            "title": "Hugo shortcode examples",
            "description": null,
            "content": "\nSource code of examples can be found in the repo\n\n\nImage - insert resizable image in post",
            "tags": [
                "Hugo"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-shortcode-examples/chart",
            "title": "chart",
            "description": "Hugo chart.js shortcode",
            "content": "\nDisplay Chart.js diagrams/blocks\n\nSources\n\n\n{{}}\n{\n    type: 'bar',\n    data: {\n        labels: ['Red', 'Blue', 'Yellow', 'Green', 'Purple', 'Orange'],\n        datasets: [{\n            label: 'Bar Chart',\n            data: [12, 19, 18, 16, 13, 14],\n            backgroundColor: [\n                'rgba(255, 99, 132, 0.2)',\n                'rgba(54, 162, 235, 0.2)',\n                'rgba(255, 206, 86, 0.2)',\n                'rgba(75, 192, 192, 0.2)',\n                'rgba(153, 102, 255, 0.2)',\n                'rgba(255, 159, 64, 0.2)'\n            ],\n            borderColor: [\n                'rgba(255, 99, 132, 1)',\n                'rgba(54, 162, 235, 1)',\n                'rgba(255, 206, 86, 1)',\n                'rgba(75, 192, 192, 1)',\n                'rgba(153, 102, 255, 1)',\n                'rgba(255, 159, 64, 1)'\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        scales: {\n            yAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        }\n    }\n}\n{{}}\n\n\n\n\n{{}}\n{\n    type: 'line',\n    data: {\n        labels: ['Red', 'Blue', 'Yellow', 'Green', 'Purple', 'Orange'],\n        datasets: [{\n            label: 'Line Chart',\n            data: [1, 2, 3231, 4324, 3, 331],\n            backgroundColor: [\n                'rgba(255, 99, 132, 0.2)',\n                'rgba(54, 162, 235, 0.2)',\n                'rgba(255, 206, 86, 0.2)',\n                'rgba(75, 192, 192, 0.2)',\n                'rgba(153, 102, 255, 0.2)',\n                'rgba(255, 159, 64, 0.2)'\n            ],\n            borderColor: [\n                'rgba(255, 99, 132, 1)',\n                'rgba(54, 162, 235, 1)',\n                'rgba(255, 206, 86, 1)',\n                'rgba(75, 192, 192, 1)',\n                'rgba(153, 102, 255, 1)',\n                'rgba(255, 159, 64, 1)'\n            ],\n            borderWidth: 1\n        }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        scales: {\n            yAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        }\n    }\n}\n{{}}\n\n\n\n{{}}\n{\n    type: 'pie',\n    data: {\n        labels: [1, 2, 3, 4],\n        datasets: [{\n            data: [1, 2, 3, 4],\n            backgroundColor: [\n                'rgba(255, 99, 132, 0.2)',\n                'rgba(255, 206, 86, 0.2)',\n                'rgba(75, 192, 192, 0.2)',\n                'rgba(255, 159, 64, 0.2)'\n            ]\n            }]\n    },\n    options: {\n        maintainAspectRatio: false,\n        scales: {\n            yAxes: [{\n                ticks: {\n                    beginAtZero: true\n                }\n            }]\n        }\n    }\n}\n{{}}\n",
            "tags": [
                "Hugo"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/hugo-shortcode-examples/img",
            "title": "img",
            "description": "Insert resizable image in post",
            "content": "\n\n{{}}\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\n{{}}\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\n\n{{}}\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\n\n{{}}\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n\n\n\n{{}}\n\nLorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n",
            "tags": [
                "Hugo"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/interactivebrokers-deposit/",
            "title": "Deposit Interactive Brokers from Israel Discount bank",
            "description": "Deposit Interactive Brokers from Israel Discount bank",
            "content": "\nWeb\n\nCreate IB notification\n\nLogin to https://www.interactivebrokers.co.uk/portal/#/\nClick Deposit\n\n\n\nClick Use a new deposit method if no one exist\n\n\n\nBank Wire -> Get instructions\n\n\n\nAccount Number: Bank account number\n\n\n\nNext you get Bank Wire Instructions\n\n\n\nThese data you need to make a payment from Discount bank\n\nSend money from Discount bank\n\nLogin start.telebank.co.il\n\n\n\nClick: ביצוע העברה\n\nFill the form\n\nClick המשך and proceed\n\n\n",
            "tags": [
                "interactivebrokers",
                "invest"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/js-snippets",
            "title": "JavaScript code snippets",
            "description": "JavaScript code snippets",
            "content": "\nWeb / Browser\n\nget base URL\n\nconst getBaseURL = url => url.replace(/[?#].*$/, '');\n\ngetBaseURL('http://url.com/page?name=Adam&surname=Smith');\n// 'http://url.com/page'\n\nconst url = new URL(\"https://example.com/login?user=someguy&page=news\");\n\nurl.origin  // \"https://example.com\"\nurl.host  // \"example.com\"\nurl.protocol  // \"https:\"\nurl.pathname  // \"/login\"\nurl.searchParams.get('user')  // \"someuser\"\n\nget URL parameters as object\n\nconst getURLParameters = url =>\n  (url.match(/(+)(=(*))/g) || []).reduce(\n    (a, v) => (\n      (a[v.slice(0, v.indexOf('='))] = v.slice(v.indexOf('=') + 1)), a\n    ),\n    {}\n  );\n\ngetURLParameters('google.com'); // {}\ngetURLParameters('http://url.com/page?name=Adam&surname=Smith');\n// {name: 'Adam', surname: 'Smith'}\n\n// One line\nObject.fromEntries('http://url.com/page?name=Adam&surname=Smith'.split('?')[1].split('&').map(x=>x.split('=')))\n\nif DOC element contains another element\n\nconst elementContains = (parent, child) =>\n  parent !== child && parent.contains(child);\n\nelementContains(\n  document.querySelector('head'),\n  document.querySelector('title')\n);\n// true\nelementContains(document.querySelector('body'), document.querySelector('body'));\n// false\n\nDate\n\nconst {locale, timeZone} = Intl.DateTimeFormat().resolvedOptions();\n\nis Date valid\n\nconst isDateValid = (...val) => !Number.isNaN(new Date(...val).valueOf());\n\nisDateValid('December 17, 1995 03:24:00'); // true\nisDateValid('1995-12-17T03:24:00'); // true\nisDateValid('1995-12-17 T03:24:00'); // false\nisDateValid('Duck'); // false\nisDateValid(2023, 01, 22); // true\nisDateValid(1995, 11, 17, 'Duck'); // false\nisDateValid({}); // false\n\nUNIX timestamp from Date\n\nconst getTimestamp = (date = new Date()) => Math.floor(date.getTime() / 1000);\n\ngetTimestamp(); // 1602162242\n\nCompare dates / sort\n\nexport function compareDates(date1, date2) {\n    if (!date1) {\n        return 1; // move date1 to the end of the array\n    }\n    if (!date2) {\n        return -1; // move date2 to the end of the array\n    }\n\n    // Convert the date strings to Date objects\n    const d1 = new Date(date1);\n    const d2 = new Date(date2);\n\n    // Compare the dates\n    if (d1.getTime() === d2.getTime()) {\n        return 0; // dates are equal\n    }\n    if (d1\n        compareDates(itemA.date, itemB.date)\n    );\n\nLogin\n\nSecure Your Node.js App with JSON Web Tokens\n\nclient.ts\n\n// client.ts\nimport axios, { AxiosInstance } from 'axios';\n\nexport class Client {\n  private _client: AxiosInstance;\n\n  constructor(accessToken?: string, url?: string) {\n    const apiUrl = this.selectApiTarget();\n    let headers = {};\n    if (accessToken !== undefined) {\n      headers = {\n        'Authorization': Bearer ${accessToken}\n      };\n    }\n    this._client = axios.create({\n      baseURL: url || apiUrl,\n      headers: headers,\n    });\n  }\n\n  private selectApiTarget(): string {\n    let backendUrl = config.backend.url;\n    if (window.location.host.includes(\"node.sharedtodos.com\")) {\n      backendUrl = config.backend.url.slice().replace(\"api.sharedtodos.com\", \"node-api.sharedtodos.com\");\n    }\n    return ${backendUrl}/api/v1/;\n  }\n\n  async getLoggedInUser(): Promise {\n    return await this._client.get('/user/me').then((response) => response.data);\n  }\n\n  async forgetLoggedInUser(): Promise {\n    return await this._client.delete('/user/me').then((response) => response.data);\n  }\n\n  async getTasks(listId: number): Promise {\n    return await this._client.get(boards/${listId}/tasks).then((response) => response.data);\n  }\n\n  async deleteTask(listId: number, taskId: number) {\n    return await this._client.delete(boards/${listId}/tasks/${taskId}).then((response) => response.data);\n  }\n\n  async createTask(listId: number, title: string, description: string) {\n    const task: Task = {\n      title: title,\n      description: description,\n    };\n    return await this._client.post(boards/${listId}/tasks, task);\n  }\n  async updateTask(listId: number, taskId: string, task: Task) {\n    return await this._client.put(boards/${listId}/tasks/${taskId}, task);\n  }\n\n  async login(email: string): Promise {\n    let data = new FormData();\n    data.append('user_email', email);\n    return await this._client.post(login, data, {\n      headers: {'Content-Type': 'multipart/form-data' }\n    }).then((response) => response.data.access_token);\n  }\n}\n\nexport const getClient = (accessToken?, url?): Client => new Client(accessToken, url);\n\nconfig.ts\n\n// config.ts\ninterface ConfigOptions {\n  backend: { url: string };\n  auth0: any;\n  authentication: { provider: string };\n  authorization: { embedUrl: string };\n}\ndeclare global {\n  interface Window {\n    env: any;\n  }\n}\n\nconst Config: ConfigOptions = {\n  backend: {\n    url:\n      process.env.REACT_APP_BACKEND_URL ||\n      window?.env?.BACKEND_URL ||\n      \"http://localhost:8008\",\n  },\n  auth0: {\n    domain:\n      process.env.AUTH0_DOMAIN ||\n      window?.env?.AUTH0_DOMAIN ||\n      \"acalla-demoapp.us.auth0.com\",\n    clientId:\n      process.env.AUTH0_CLIENT_ID ||\n      window?.env?.AUTH0_CLIENT_ID ||\n      \"myClientID\",\n    audience:\n      process.env.AUTH0_AUDIENCE ||\n      window?.env?.AUTH0_AUDIENCE ||\n      \"https://demoapi.server.com/v1/\",\n  },\n  authentication: {\n    provider: \"auth0\",\n  },\n  authorization: {\n    embedUrl: window?.env?.AUTHZ_EMBED_URL || \"http://localhost:3000\",\n  }\n};\n\nexport default Config;\n\nCheat Sheet\n\n// Single-line comments start with two slashes.\n/* Multiline comments start with slash-star,\n   and end with star-slash */\n\n// Statements can be terminated by ;\ndoStuff();\n\n// ... but they don't have to be, as semicolons are automatically inserted\n// wherever there's a newline, except in certain cases.\ndoStuff()\n\n// Because those cases can cause unexpected results, we'll keep on using\n// semicolons in this guide.\n\n///////////////////////////////////\n// 1. Numbers, Strings and Operators\n\n// JavaScript has one number type (which is a 64-bit IEEE 754 double).\n// Doubles have a 52-bit mantissa, which is enough to store integers\n// up to about 9✕10¹⁵ precisely.\n3; // = 3\n1.5; // = 1.5\n\n// Some basic arithmetic works as you'd expect.\n1 + 1; // = 2\n0.1 + 0.2; // = 0.30000000000000004\n8 - 1; // = 7\n10 * 2; // = 20\n35 / 5; // = 7\n\n// Including uneven division.\n5 / 2; // = 2.5\n\n// And modulo division.\n10 % 2; // = 0\n30 % 4; // = 2\n18.5 % 7; // = 4.5\n\n// Bitwise operations also work; when you perform a bitwise operation your float\n// is converted to a signed int up to 32 bits.\n1  10; // = false\n2 = 2; // = true\n\n// Strings are concatenated with +\n\"Hello \" + \"world!\"; // = \"Hello world!\"\n\n// ... which works with more than just strings\n\"1, 2, \" + 3; // = \"1, 2, 3\"\n\"Hello \" + [\"world\", \"!\"]; // = \"Hello world,!\"\n\n// and are compared with\n\"a\"  {\n    return number % 2 === 0;\n};\n\nisEven(7); // false\n\n// The \"equivalent\" of this function in the traditional syntax would look like this:\n\nfunction isEven(number) {\n    return number % 2 === 0;\n};\n\n// I put the word \"equivalent\" in double quotes because a function defined\n// using the lambda syntax cannnot be called before the definition.\n// The following is an example of invalid usage:\n\nadd(1, 8);\n\nconst add = (firstNumber, secondNumber) => {\n    return firstNumber + secondNumber;\n};\n\nCheat Sheet Typescript\n\n\n// There are 3 basic types in TypeScript\nlet isDone: boolean = false;\nlet lines: number = 42;\nlet name: string = \"Anders\";\n\n// But you can omit the type annotation if the variables are derived\n// from explicit literals\nlet isDone = false;\nlet lines = 42;\nlet name = \"Anders\";\n\n// When it's impossible to know, there is the \"Any\" type\nlet notSure: any = 4;\nnotSure = \"maybe a string instead\";\nnotSure = false; // okay, definitely a boolean\n\n// Use const keyword for constants\nconst numLivesForCat = 9;\nnumLivesForCat = 1; // Error\n\n// For collections, there are typed arrays and generic arrays\nlet list: number[] = [1, 2, 3];\n// Alternatively, using the generic array type\nlet list: Array = [1, 2, 3];\n\n// For enumerations:\nenum Color { Red, Green, Blue };\nlet c: Color = Color.Green;\nconsole.log(Color[c]); // \"Green\"\n\n// Lastly, \"void\" is used in the special case of a function returning nothing\nfunction bigHorribleAlert(): void {\n  alert(\"I'm a little annoying box!\");\n}\n\n// Functions are first class citizens, support the lambda \"fat arrow\" syntax and\n// use type inference\n\n// The following are equivalent, the same signature will be inferred by the\n// compiler, and same JavaScript will be emitted\nlet f1 = function (i: number): number { return i * i; }\n// Return type inferred\nlet f2 = function (i: number) { return i * i; }\n// \"Fat arrow\" syntax\nlet f3 = (i: number): number => { return i * i; }\n// \"Fat arrow\" syntax with return type inferred\nlet f4 = (i: number) => { return i * i; }\n// \"Fat arrow\" syntax with return type inferred, braceless means no return\n// keyword needed\nlet f5 = (i: number) => i * i;\n\n// Interfaces are structural, anything that has the properties is compliant with\n// the interface\ninterface Person {\n  name: string;\n  // Optional properties, marked with a \"?\"\n  age?: number;\n  // And of course functions\n  move(): void;\n}\n\n// Object that implements the \"Person\" interface\n// Can be treated as a Person since it has the name and move properties\nlet p: Person = { name: \"Bobby\", move: () => { } };\n// Objects that have the optional property:\nlet validPerson: Person = { name: \"Bobby\", age: 42, move: () => { } };\n// Is not a person because age is not a number\nlet invalidPerson: Person = { name: \"Bobby\", age: true };\n\n// Interfaces can also describe a function type\ninterface SearchFunc {\n  (source: string, subString: string): boolean;\n}\n// Only the parameters' types are important, names are not important.\nlet mySearch: SearchFunc;\nmySearch = function (src: string, sub: string) {\n  return src.search(sub) != -1;\n}\n\n// Classes - members are public by default\nclass Point {\n  // Properties\n  x: number;\n\n  // Constructor - the public/private keywords in this context will generate\n  // the boiler plate code for the property and the initialization in the\n  // constructor.\n  // In this example, \"y\" will be defined just like \"x\" is, but with less code\n  // Default values are also supported\n\n  constructor(x: number, public y: number = 0) {\n    this.x = x;\n  }\n\n  // Functions\n  dist(): number { return Math.sqrt(this.x * this.x + this.y * this.y); }\n\n  // Static members\n  static origin = new Point(0, 0);\n}\n\n// Classes can be explicitly marked as implementing an interface.\n// Any missing properties will then cause an error at compile-time.\nclass PointPerson implements Person {\n    name: string\n    move() {}\n}\n\nlet p1 = new Point(10, 20);\nlet p2 = new Point(25); //y will be 0\n\n// Inheritance\nclass Point3D extends Point {\n  constructor(x: number, y: number, public z: number = 0) {\n    super(x, y); // Explicit call to the super class constructor is mandatory\n  }\n\n  // Overwrite\n  dist(): number {\n    let d = super.dist();\n    return Math.sqrt(d * d + this.z * this.z);\n  }\n}\n\n// Modules, \".\" can be used as separator for sub modules\nmodule Geometry {\n  export class Square {\n    constructor(public sideLength: number = 0) {\n    }\n    area() {\n      return Math.pow(this.sideLength, 2);\n    }\n  }\n}\n\nlet s1 = new Geometry.Square(5);\n\n// Local alias for referencing a module\nimport G = Geometry;\n\nlet s2 = new G.Square(10);\n\n// Generics\n// Classes\nclass Tuple {\n  constructor(public item1: T1, public item2: T2) {\n  }\n}\n\n// Interfaces\ninterface Pair {\n  item1: T;\n  item2: T;\n}\n\n// And functions\nlet pairToTuple = function (p: Pair) {\n  return new Tuple(p.item1, p.item2);\n};\n\nlet tuple = pairToTuple({ item1: \"hello\", item2: \"world\" });\n\n// Including references to a definition file:\n///\n\n// Template Strings (strings that use backticks)\n// String Interpolation with Template Strings\nlet name = 'Tyrone';\nlet greeting = Hi ${name}, how are you?\n// Multiline Strings with Template Strings\nlet multiline = `This is an example\nof a multiline string`;\n\n// READONLY: New Feature in TypeScript 3.1\ninterface Person {\n  readonly name: string;\n  readonly age: number;\n}\n\nvar p1: Person = { name: \"Tyrone\", age: 42 };\np1.age = 25; // Error, p1.age is read-only\n\nvar p2 = { name: \"John\", age: 60 };\nvar p3: Person = p2; // Ok, read-only alias for p2\np3.age = 35; // Error, p3.age is read-only\np2.age = 45; // Ok, but also changes p3.age because of aliasing\n\nclass Car {\n  readonly make: string;\n  readonly model: string;\n  readonly year = 2018;\n\n  constructor() {\n    this.make = \"Unknown Make\"; // Assignment permitted in constructor\n    this.model = \"Unknown Model\"; // Assignment permitted in constructor\n  }\n}\n\nlet numbers: Array = [0, 1, 2, 3, 4];\nlet moreNumbers: ReadonlyArray = numbers;\nmoreNumbers[5] = 5; // Error, elements are read-only\nmoreNumbers.push(5); // Error, no push method (because it mutates array)\nmoreNumbers.length = 3; // Error, length is read-only\nnumbers = moreNumbers; // Error, mutating methods are missing\n\n// Tagged Union Types for modelling state that can be in one of many shapes\ntype State =\n  | { type: \"loading\" }\n  | { type: \"success\", value: number }\n  | { type: \"error\", message: string };\n\ndeclare const state: State;\nif (state.type === \"success\") {\n  console.log(state.value);\n} else if (state.type === \"error\") {\n  console.error(state.message);\n}\n\n// Template Literal Types\n// Use to create complex string types\ntype OrderSize = \"regular\" | \"large\";\ntype OrderItem = \"Espresso\" | \"Cappuccino\";\ntype Order = A ${OrderSize} ${OrderItem};\n\nlet order1: Order = \"A regular Cappuccino\";\nlet order2: Order = \"A large Espresso\";\nlet order3: Order = \"A small Espresso\"; // Error\n\n// Iterators and Generators\n\n// for..of statement\n// iterate over the list of values on the object being iterated\nlet arrayOfAnyType = [1, \"string\", false];\nfor (const val of arrayOfAnyType) {\n    console.log(val); // 1, \"string\", false\n}\n\nlet list = [4, 5, 6];\nfor (const i of list) {\n   console.log(i); // 4, 5, 6\n}\n\n// for..in statement\n// iterate over the list of keys on the object being iterated\nfor (const i in list) {\n   console.log(i); // 0, 1, 2\n}\n\n// Type Assertion\n\nlet foo = {} // Creating foo as an empty object\nfoo.bar = 123 // Error: property 'bar' does not exist on {}\nfoo.baz = 'hello world' // Error: property 'baz' does not exist on {}\n\n// Because the inferred type of foo is {} (an object with 0 properties), you\n// are not allowed to add bar and baz to it. However with type assertion,\n// the following will pass:\n\ninterface Foo {\n  bar: number;\n  baz: string;\n}\n\nlet foo = {} as Foo; // Type assertion here\nfoo.bar = 123;\nfoo.baz = 'hello world'\n\nResources\n\nreact cheatsheet\nhttps://learnxinyminutes.com/docs/typescript/\n",
            "tags": [
                "js",
                "javascript",
                "typescript"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/mac-setup-development/",
            "title": "Mac Setup 2022",
            "description": "How I set up my M1 MacBook Pro software development...",
            "content": "\nMacBook Pro Specification\n\n13-inch\nApple M1 Pro M1 2020\n16 GB RAM\n512 GB SSD\nQWERTY = English/Hebrew\nmacOS Monterey (Update always)\n\nHomebrew\n\nInstall Homebrew as package manager for macOS:\n\npaste in terminal and follow the instructions\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\nUpdate everything in Homebrew to recent version:\n\nbrew update\n\nAdd additional source for casks:\n\nbrew tap homebrew/cask-versions\n\nInstall GUI applications (read more about these in GUI Applications):\n\nbrew install --cask \\\n  appcleaner \\\n  dbeaver-community \\\n  deepl \\\n  disk-inventory-x \\\n  google-chrome  \\\n  google-drive \\\n  grammarly \\\n  itsycal \\\n  firefox \\\n  visual-studio-code \\\n  all-in-one-messenger \\\n  sublime-text \\\n  docker \\\n  rectangle \\\n  discord \\\n  vlc \\\n  figma \\\n  grammarly \\\n  macx-youtube-downloader \\\n  notion \\\n  postman \\\n  tor-browser \\\n  transmission \\\n  utm \\\n  mongodb-compass \\\n  obs \\\n  spotify \\\n  iterm2 \\\n  rectangle \\\n  sublime-text \\\n  syncthing \\\n  viber \\\n  visual-studio-code \\\n  yandex-disk \\\n  zoom\n\nInstall terminal applications (read more about these in Terminal Applications):\n\nbrew install \\\n  git \\\n  ffmpeg \\\n  nvm \\\n  jupyterlab\nAdditional GUI Applications\nKotatogram\n\nKotatogram - Experimental fork of Telegram Desktop. Folders with features\n\nGUI Applications\n\nGoogle Chrome\n\nGoogle Chrome (web development, web browsing)\n  Preferences\n    set default browser\n    always show bookmarks\n    import bookmarks from previous machine\n  Chrome Developer Tools\n    Network -> only \"Fetch/XHR\"\n  Search Shortcuts. Add Shortucts for different search engines.\n    chrome://settings/searchEngines\n    Yandex, search only in Russia.\n        Shortcut: vv\n        url: https://yandex.ru/{yandex:searchPath}?text=%s&{yandex:referralID}&lr=101443&rstr=-225\n    Youtube\n      Shortcut: yy\n      url: https://www.youtube.com/results?search_query=%s&page={startPage?}&utm_source=opensearch\n\n  Chrome Extensions\n    ChatGPT for Search Engines - Show ChatGPT output on every search request\n    DeepL Translate - AI translator\n    DoubleSubs  - dual subs on youtube/netflix + web translator\n    Google Translate\n    React Developer Tools\n    Pocket - The easiest, fastest way to capture articles, videos, and more.\n    Session Buddy (Manage Browser Tabs and Bookmarks)\n    LanguageTool (multilingual grammar, style, and spell checker)\n    RSS Feed Reader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android)\n    Inoreader (Easy to subscribe/unsubscribe to blogs/no need email + iOS/Android)\n    30 Seconds of Knowledge (random code snippet on a new tab)\n    JSON Formatter\n    picture-in-picture (youtube/video above other screens)\n    Visual CSS Editor (Customize any website visually)\n    Squish - AI-powered summary tool. Turn any body of text into a few sentences with one click.\n    Zotero - Add/sync scientific PDF documents\n    Video Downloader Plus\n    Opus Guide (Step-by-step for instructions)\n\nDisk Inventory X\nDisk Inventory X (disk usage utility for macOS)\n\nDocker\nDocker (Docker, see setup)\n  used for running databases (e.g. PostgreSQL, MongoDB) in container without cluttering the Mac\n  Preferences\n    enable \"Use Docker Compose\"\n\nFirefox\nFirefox (web development)\n\nVisual Studio Code\nVisual Studio Code (web development IDE)\n\nSettings / Synced\n\nSublime Text\nSublime Text (editor)\n\nMaccy\nMaccy (clipboard manager)\n  enable \"Launch at Login\"\n\nRectangle\n\nMove and resize windows in macOS using keyboard shortcuts or snap areas\nhttps://rectangleapp.com\n\nOBS\nOBS (for video recording and live streaming)\n  for Native Mac Screen recorder\n    Base (Canvas) 2880x1800 (Ratio: 16:10)\n    Output 1728x1080\n\nSpotify\nSpotify\n\nSoundcloud\nhttps://soundcloud.com\n\nSyncthing\nsyncthing - Sync folders/files between devices. I use to backup all photos/video from mobile to PC\n\nTransmission\n Transmission (A torrent client that I use. Very minimal in its UI but very powerful and has all the features that I need)\n\nUTM\nUTM (Virtual machines UI using QEMU)\n  download ubuntu for arm, doc\n  On error with shared folder: Could not connect: Connection refused open in browser: http://127.0.0.1:9843/\n  For Debian install spice-webdavd for shared folder. https://packages.debian.org/search?keywords=spice-webdavd, https://github.com/utmapp/UTM/issues/1204\nsudo apt install spice-vdagent spice-webdavd -y\n\nVLC\nVLC (video player)\n  use as default for video files\n\nTerminal Applications\n\nnvm\nnvm (node version manager)\njupyterlab\njupyterlab (Jupyter - python development, fast code snippets)\n  jupyter notebook - to start jupyter notebook\nffmpeg\nffmpeg (Converting video and audio)\n  compress video:\n    ffmpeg -i input.mp4 -c:v libx264 -crf 23 -preset slow -c:a aac -b:a 192k output.mp4\nor\n  ffmpeg -i input.mp4 output.avi\n    convert video to .gif:\n    ffmpeg \\\n  -i input.mp4 \\\n  -ss 00:00:00.000 \\\n  -pix_fmt rgb24 \\\n  -r 10 \\\n  -s 960x540 \\\n  -t 00:00:10.000 \\\n  output.gif\nNVM for Node/npm\n\nThe node version manager (NVM) is used to install and manage multiple Node versions. After you have installed it via Homebrew in a previous step, type the following commands to complete the installation:\n\necho \"source $(brew --prefix nvm)/nvm.sh\" >> ~/.zshrc\n\nsource ~/.zshrc\nor alias\nzshsource\n\nNow install the latest LTS version on the command line:\n\nnvm install\n\nAfterward, check whether the installation was successful and whether the node package manager (npm) got installed along the way:\n\nnode -v && npm -v\n\nUpdate npm to its latest version:\n\nnpm install -g npm@latest\n\nAnd set defaults for npm:\n\nnpm set init.author.name \"your name\"\nnpm set init.author.email \"you@example.com\"\nnpm set init.author.url \"example.com\"\n\nIf you are a library author, log in to npm too:\n\nnpm adduser\n\nThat's it. If you want to list all your Node.js installation, type the following:\n\nnvm list\n\nIf you want to install a newer Node.js version, then type:\n\nnvm install  --reinstall-packages-from=$(nvm current)\nnvm use\nnvm alias default\n\nOptionally install yarn if you use it as alternative to npm:\n\nnpm install -g yarn\nyarn -v\nIf you want to list all globally installed packages, run this command:\n\nnpm list -g --depth=0\n\nThat's it. You have a running version of Node.js and its package manager.\n\nOH MY ZSH\n\nMacOS already comes with zsh as default shell. Install Oh My Zsh for an improved (plugins, themes, ...) experience. Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.\n\nInstall:\n\nsh -c \"$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n\nUpdate everything (e.g. plugins) in Oh My Zsh to recent version:\n\nomz update\n\nInstall fonts for themes:\n\nbrew tap homebrew/cask-fonts\nbrew install --cask font-hack-nerd-font\n\niTerm2\n\nInstall theme\n\nTheme description\n\nbrew install romkatv/powerlevel10k/powerlevel10k\necho \"source $(brew --prefix)/opt/powerlevel10k/powerlevel10k.zsh-theme\" >>~/.zshrc\n\nEnable suggestions\ngit clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions\necho \"plugins=(zsh-autosuggestions)\" >>~/.zshrc\n\nOpen new tab(CMD+T)/restart iTerm to proceed with theme setup\n\nTerminal Script and Aliases\n\nUpdate .zprofile. Еhe changes will take effect after restarting the terminal\n\nvi ~/.zprofile\n\nAutomatic software updates\n\nAdd script to zprofile that updates everything:\n\nUpdate, upgrade all and cleanup\nsoftwareupdate - system software update tool\n\nWe can execute this command on strartup, but i prefer handle it. When I kick of upd command in terminal, it will update everythin I need:\n\nalias upd='brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i'\n\nAdd aliases to latest versions pip & python\n\nalias pip=pip3\nalias python=python3\n\nFinal view of .zprofile\n...\nalias pip=pip3\nalias python=python3\nalias upd='omz update; brew update; brew upgrade; brew cu -a --cleanup -y -v; brew cleanup; softwareupdate -i -a; i'\n\nLinks\nhttps://www.robinwieruch.de/mac-setup-web-development/\nhttps://sourabhbajaj.com/mac-setup/iTerm/ack.html\nhttps://www.engineeringwithutsav.com/blog/spice-up-your-macos-terminal\n",
            "tags": [
                "mac",
                "mac setup web developer",
                "mac setup javascript"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/markdown-syntax/",
            "title": "Markdown Cheat Sheet",
            "description": "Markdown cheatsheet",
            "content": "\nThis article offers an example of the basic Markdown syntax that can be used and also shows whether the basic elements of HTML are decorated with CSS.\n\nHeaders\n\n\nHeader 1\n\nHeader 2\nHeader 1\n\nHeader 2\nh1\nh2\nh3\nh4\nh5\nh6\nh1\nh2\nh3\nh4\nh5\nh6\n\nParagraph\n\nTo insert an empty string, you need to put the word wrap symbol twice (press Enter)\n\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Consequuntur eius in labore quidem, sequi suscipit!\n\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Aliquam aut commodi debitis ipsam nobis perspiciatis sequi, sint unde vitae.\n\nImages\n\nImage alt text\nImage alt text\nImage alt text\n\n[img]: http://foo.com/img.jpg\n\nEmphasis\n\nitalic\nitalic\n\nbold\nbold\nbold italic*\nbold italic___\n\nstrikethrough\n\ncode\n\nitalic\nitalic\n\nbold\nbold\nbold italic*\nbold italic___\n\nstrikethrough\n\ncode\n\nLinks\n\nlink\n\nlink\n[google]: http://google.com\n\nBlockquotes\n\nThe blockquote element represents the content that is quoted from another source, optionally with a quotation that must be in the element footer or cite, and optional line changes such as annotations and abbreviations.\n\nBlock quote without attribution\nTiam, ad mint andaepu dandae nostion secatur sequo quae.\nNotethat you can use the syntax Markdown inside the block quote.\n\nBlock quote with authorship\nDon't communicate by sharing memory, share memory by communicating.\n— Rob Pike\n\n: The above quote is taken from Rob Pike’s book talk during Gopherfest, November 18, 2015.\nThis is an example quote,\nin which before each line\nangle bracket is used.\nThis is an example quote,\nin which the corner bracket is placed only before the beginning of the new paragraph.\nSecond paragraph.\nThis is an example quote,\nin which before each line\nangle bracket is used.\nThis is an example quote,\nin which the corner bracket is placed only before the beginning of the new paragraph.\nSecond paragraph.\nLevel One Citation\n> Second Level Citation\n>> Third Level Citation\nLevel One Citation\nLevel One Citation\n> Second Level Citation\n>> Third Level Citation\nLevel One Citation\n\nTables\n\n   | Name  | Age |\n   | ----- | --- |\n   | Bob   | 27  |\n   | Alice | 23  |\n\n   | Name  | Age |\n   | ----- | --- |\n   | Bob   | 27  |\n   | Alice | 23  |\n\nThe cells in the delimitation row use only symbols - and :. The symbol : is placed at the beginning, at the end, or on both sides of the cell contents of the dividing row to indicate the alignment of the text in the corresponding column on the left, right side, or center.\n\n| Column on the left | Column on the right | Column on the center |\n| :----------------- | ------------------: | :------------------: |\n| Text               |                Text |         Text         |\n\n| Column on the left | Column on the right | Column on the center |\n| :----------------- | ------------------: | :------------------: |\n| Text               |                Text |         Text         |\n\nMarkdown inside the table\n\n| Italics   | Bold     | Code   |\n| --------- | -------- | ------ |\n| italics | bold | code |\n\n| Italics   | Bold     | Code   |\n| --------- | -------- | ------ |\n| italics | bold | code |\n\nCode Blocks\n\nCode block with inverted quotes\n\n  Example HTML5 Document\n\n\n  Test\n\nCode block with four spaces indent\n\n      Example HTML5 Document\n\n      Test\n\n\nCode Unit with Hugo Internal Shorted Backlight\n\n{{}}\n\n  Example HTML5 Document\n\n\n  Test\n\n\n{{}}\n\nLists\n\nItem 1\nItem 2\n\nItem 1\nItem 2\n\n[ ] Checkbox off\n[x] Checkbox on\n\nItem 1\nItem 2\n\nItem 1\nItem 2\n\nItem 1\nItem 2\n\n[ ] Checkbox off\n[x] Checkbox on\n\nItem 1\nItem 2\n\n\nMake the headers uniform. At the end of the title, do not put a point.\n\n| Correct                              | Wrong                            |\n| ------------------------------------ | -------------------------------- |\n| Getting the  Creating a Cluster | Get the  Creating a Cluster |\n| Get  Create Cluster             |\n\nIf you want to describe the sequence of actions, use the numbered list. At the end of the lines, put a period.\nIf the order of items is not important, use the marked list. Make it one of the ways:\n\n    If the entries in the list are separate sentences, start them with a capital letter and put a period at the end.\n    If the introductory phrase and the list make up one sentence, the entries in the list should start with a lowercase letter and end with a semicolon. The last list item ends with a dot.\n    If the list consists of parameter names or values (without explanation), do not put characters at the end of lines.\n\nOrdered list\n\nFirst item\nSecond item\nThird item\n\nTo create an ordered numbered list, use the digits with the symbol . or ). The recommended markup format is 1 and ..\n\nFirst item\nSecond item\nThird item\n\nFirst item\nSecond item\nThird item\n\nTo create a nested ordered list, add a indent to the entries in the child list. The allowed indentation is from two to five spaces. The recommended indent size is four spaces.\n\nFor example, markup:\n\nFirst paragraph\n    Sub-paragraph\n    Sub-paragraph\nSecond paragraph\n\nFirst paragraph\n    Sub-paragraph\n    Sub-paragraph\nSecond paragraph\n\nUnordered list\n\nList item\nAnother item\nAnd another item\n\nNested list\n\nFruit\n  Apple\n  Orange\n  Banana\nDairy\n  Milk\n  Cheese\n\nOther elements - abbr, sub, sup, kbd, mark\n\nGIF is a bitmap image format.\n\nH2O\n\nXn + Yn = Zn\n\nPress CTRL+ALT+Delete to end the session.\n\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n\n    Most salamanders are nocturnal\n\n💡 Data structure is a container that stores data in a specific format. This container decides how the outside world can read or change this data.\n\nMath LaTeX syntax\n\n| Math Element                  | Code                            | Pronunciation |\n| ----------------------------- | ------------------------------- | ------------- |\n| $x_{12345}$                   | $x_{12345}$                   |               |\n| $\\quad x_12$                  | $\\quad x_12$                  |               |\n| $a_{ij}b_{kl}=\\delta_{i}^{l}$ | $a_{ij}b_{kl}=\\delta_{i}^{l}$ |               |\n| $\\alpha$                      | $\\alpha$                      | \\alpha        |\n| $\\beta$                       | $\\beta$                       | \\beta         |\n| $\\gamma$                      | $\\gamma$                      | \\gamma        |\n| $\\delta$                      | $\\delta$                      | \\delta        |\n| $\\epsilon$                    | $\\epsilon$                    | \\epsilon      |\n| $\\pi$                         | $\\pi$                         | \\pi           |\n| $\\theta$                      | $\\theta$                      | \\theta        |\n| $\\lambda$                     | $\\lambda$                     | \\lambda       |\n| $\\mu$                         | $\\mu$                         | \\mu           |\n| $\\sigma$                      | $\\sigma$                      | \\sigma        |\n| $\\omega$                      | $\\omega$                      | \\omega        |\n| $\\Gamma$                      | $\\Gamma$                      | \\Gamma        |\n| $\\Delta$                      | $\\Delta$                      | \\Delta        |\n| $\\Sigma$                      | $\\Sigma$                      | \\Sigma        |\n| $\\Theta$                      | $\\Theta$                      | \\Theta        |\n| $\\Omega$                      | $\\Omega$                      | \\Omega        |\n| $\\infty$                      | $\\infty$                      | \\infty        |\n| $\\sum$                        | $\\sum$                        | \\sum          |\n| $\\prod$                       | $\\prod$                       | \\prod         |\n| $\\int$                        | $\\int$                        | \\int          |\n| $\\oint$                       | $\\oint$                       | \\oint         |\n| $\\partial$                    | $\\partial$                    | \\partial      |\n| $\\nabla$                      | $\\nabla$                      | \\nabla        |\n| $\\pm$                         | $\\pm$                         | \\pm           |\n| $\\times$                      | $\\times$                      | \\times        |\n| $\\div$                        | $\\div$                        | \\div          |\n| $\\approx$                     | $\\approx$                     | \\approx       |\n| $\\geq$                        | $\\geq$                        | \\geq          |\n| $\\leq$                        | $\\leq$                        | \\leq          |\n| $\\neq$                        | $\\neq$                        | \\neq          |\n\nResources\n\nStyle from Google",
            "tags": [
                "markdown",
                "css",
                "html",
                "themes"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/other-snippets",
            "title": "Some code snippets",
            "description": "Some code snippets",
            "content": "\npandoc mardown -> pdf\n\nCreate pdf file from .md in multiple folders\n\nprepare:\n\nbrew install basictex\n\nsearch for cyrillic fonts\nfc-list | grep к\\\n\nbrew tap homebrew/cask-fonts\nbrew install --cask font-m-plus\nbrew tap homebrew/cask-fonts\nbrew install --cask font-m-plus\nbrew install --cask font-m-plus-1\nbrew install --cask font-m-plus-1-code\n\npandoc --pdf-engine xelatex \\\n--variable mainfont=\"M+ 1p\" --variable sansfont=\"M+ 1p\" --variable monofont=\"M+ 1m\" \\\n-V geometry:\"top=1cm, bottom=2cm, left=1cm, right=1cm\" \\\n--file-scope \\\n--highlight-style=tango \\\n-s \\\n--toc-depth=1 \\\n--variable=toc-title:\" \" \\\n--top-level-division=chapter \\\n--standalone \\\n--self-contained \\\n--from=markdown \\\n $(find . -name '*.ru.md') \\\n-o book.pdf\n\nConvert video -> audio with ffmpeg in current directory\n\n#!/bin/bash\n\nCheck if ffmpeg is installed\ncommand -v ffmpeg >/dev/null 2>&1 || { echo >&2 \"ffmpeg is required but not installed. Aborting.\"; exit 1; }\n\nGet a list of all video files in the current directory\nvideo_files=(*.{mp4,mkv,flv,avi})\n\nCheck if there are any video files in the current directory\nif [ ${#video_files[@]} -eq 0 ]\n  then\n    echo \"No video files found in the current directory.\"\n    exit 1\nfi\n\nLoop through all video files and convert them to audio files\nfor video_file in \"${video_files[@]}\"\ndo\nGet the file name without the extension\n  file_name=\"${video_file%.*}\"\n\nConvert the video file to an audio file in the current directory\n  ffmpeg -i $video_file -vn -acodec libmp3lame -ab 128k $file_name.mp3\n\n  echo \"Conversion of $video_file completed. The audio file is located in the current directory.\"\ndone\n\necho \"All conversions completed.\"\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/photos/22-07-02-israel-haifa-bahai-gardens/",
            "title": "Israel - Haifa - Bahai Gardens",
            "description": "Israel - Haifa - Bahai Gardens",
            "content": "\nGoogle maps Route\n\n{{}}\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/posts/python-bitwise-operators",
            "title": "Python bitwise operators",
            "description": "Python bitwise operators",
            "content": "\n>> 100 >> 100 >> 100 > Right Shift\n\nThe rightmost bits always get dropped.\nEvery time you shift a bit to the right by one position, you halve its underlying value.\n\nRight Shift\n>> 100 >> 1\n50\n>> 100 >> 2\n25\n>> 100 >> 3\n12\n>> 5 >> 10\n0\n\n$\na >> n = [a/2^n]\n$\n\nthe right shift operator automatically floors the result.\n>> 5 >> 1  # Bitwise right shift\n2\n>> 5 // 2  # Floor division (integer division)\n2\n>> 5 / 2   # Floating-point division\n2.5\n>> -2 >> 5\n-1\n\n&\n\n0 & 0 = 0\n0 & 1 = 0\n1 & 0 = 0\n1 & 1 = 0\n\nFor numbers:\n\n27 & 23\n\nCovert to binary\n\n\n    27 -> 11011\n    23 -> 10111\n\nturns to (in binary)\n\n    11011 & 10111 = 10011 -> 19\n\n    27 & 23 = 19\n\nResources\nhttps://realpython.com/python-bitwise-operators/",
            "tags": [
                "Python"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/python-snippets/",
            "title": "Python Cheat Sheet",
            "description": "Python Cheat Sheet",
            "content": "\n\ndef sum_of_digits(n):\n    sum = 0\n    while n:\n        sum += n % 10\n        n //= 10\n    return sum\n\nSingle line comments start with a number symbol.\n\n\"\"\" Multiline strings can be written\n    using three \"s, and are often used\n    as documentation.\n\"\"\"\n\n####################################################\n1. Primitive Datatypes and Operators\n####################################################\n\nYou have numbers\n3  # => 3\n\nMath is what you would expect\n1 + 1   # => 2\n8 - 1   # => 7\n10 * 2  # => 20\n35 / 5  # => 7.0\n\nInteger division rounds down for both positive and negative numbers.\n5 // 3       # => 1\n-5 // 3      # => -2\n5.0 // 3.0   # => 1.0 # works on floats too\n-5.0 // 3.0  # => -2.0\n\nThe result of division is always a float\n10.0 / 3  # => 3.3333333333333335\n\nModulo operation\n7 % 3   # => 1\ni % j have the same sign as j, unlike C\n-7 % 3  # => 2\n\nExponentiation (x**y, x to the yth power)\n2**3  # => 8\n\nEnforce precedence with parentheses\n1 + 3 * 2    # => 7\n(1 + 3) * 2  # => 8\n\nBoolean values are primitives (Note: the capitalization)\nTrue   # => True\nFalse  # => False\n\nnegate with not\nnot True   # => False\nnot False  # => True\n\nBoolean Operators\nNote \"and\" and \"or\" are case-sensitive\nTrue and False  # => False\nFalse or True   # => True\n\nTrue and False are actually 1 and 0 but with different keywords\nTrue + True # => 2\nTrue * 8    # => 8\nFalse - 5   # => -5\n\nComparison operators look at the numerical value of True and False\n0 == False  # => True\n1 == True   # => True\n2 == True   # => False\n-5 != False # => True\n\nNone, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False.\nAll other values are True\nbool(0)     # => False\nbool(\"\")    # => False\nbool([])    # => False\nbool({})    # => False\nbool(())    # => False\nbool(set()) # => False\nbool(4)     # => True\nbool(-6)    # => True\n\nUsing boolean logical operators on ints casts them to booleans for evaluation, but their non-cast value is returned\nDon't mix up with bool(ints) and bitwise and/or (&,|)\nbool(0)     # => False\nbool(2)     # => True\n0 and 2     # => 0\nbool(-5)    # => True\nbool(2)     # => True\n-5 or 0     # => -5\n\nEquality is ==\n1 == 1  # => True\n2 == 1  # => False\n\nInequality is !=\n1 != 1  # => False\n2 != 1  # => True\n\nMore comparisons\n1  True\n1 > 10  # => False\n2  True\n2 >= 2  # => True\n\nSeeing whether a value is in a range\n1  True\n2  False\nChaining makes this look nicer\n1  True\n2  False\n\n(is vs. ==) is checks if two variables refer to the same object, but == checks\nif the objects pointed to have the same values.\na = [1, 2, 3, 4]  # Point a at a new list, [1, 2, 3, 4]\nb = a             # Point b at what a is pointing to\nb is a            # => True, a and b refer to the same object\nb == a            # => True, a's and b's objects are equal\nb = [1, 2, 3, 4]  # Point b at a new list, [1, 2, 3, 4]\nb is a            # => False, a and b do not refer to the same object\nb == a            # => True, a's and b's objects are equal\n\nStrings are created with \" or '\n\"This is a string.\"\n'This is also a string.'\n\nStrings can be added too\n\"Hello \" + \"world!\"  # => \"Hello world!\"\nString literals (but not variables) can be concatenated without using '+'\n\"Hello \" \"world!\"    # => \"Hello world!\"\n\nA string can be treated like a list of characters\n\"Hello world!\"[0]  # => 'H'\n\nYou can find the length of a string\nlen(\"This is a string\")  # => 16\n\nYou can also format using f-strings or formatted string literals (in Python 3.6+)\nname = \"Reiko\"\nf\"She said her name is {name}.\" # => \"She said her name is Reiko\"\nYou can basically put any Python expression inside the braces and it will be output in the string.\nf\"{name} is {len(name)} characters long.\" # => \"Reiko is 5 characters long.\"\n\nNone is an object\nNone  # => None\n\nDon't use the equality \"==\" symbol to compare objects to None\nUse \"is\" instead. This checks for equality of object identity.\n\"etc\" is None  # => False\nNone is None   # => True\n\nNone, 0, and empty strings/lists/dicts/tuples/sets all evaluate to False.\nAll other values are True\nbool(0)     # => False\nbool(\"\")    # => False\nbool([])    # => False\nbool({})    # => False\nbool(())    # => False\nbool(set()) # => False\n\n####################################################\n2. Variables and Collections\n####################################################\n\nPython has a print function\nprint(\"I'm Python. Nice to meet you!\")  # => I'm Python. Nice to meet you!\n\nBy default the print function also prints out a newline at the end.\nUse the optional argument end to change the end string.\nprint(\"Hello, World\", end=\"!\")  # => Hello, World!\n\nSimple way to get input data from console\ninput_string_var = input(\"Enter some data: \") # Returns the data as a string\n\nThere are no declarations, only assignments.\nConvention is to use lower_case_with_underscores\nsome_var = 5\nsome_var  # => 5\n\nAccessing a previously unassigned variable is an exception.\nSee Control Flow to learn more about exception handling.\nsome_unknown_var  # Raises a NameError\n\nif can be used as an expression\nEquivalent of C's '?:' ternary operator\n\"yay!\" if 0 > 1 else \"nay!\"  # => \"nay!\"\n\nLists store sequences\nli = []\nYou can start with a prefilled list\nother_li = [4, 5, 6]\n\nAdd stuff to the end of a list with append\nli.append(1)    # li is now [1]\nli.append(2)    # li is now [1, 2]\nli.append(4)    # li is now [1, 2, 4]\nli.append(3)    # li is now [1, 2, 4, 3]\nRemove from the end with pop\nli.pop()        # => 3 and li is now [1, 2, 4]\nLet's put it back\nli.append(3)    # li is now [1, 2, 4, 3] again.\n\nAccess a list like you would any array\nli[0]   # => 1\nLook at the last element\nli[-1]  # => 3\n\nLooking out of bounds is an IndexError\nli[4]  # Raises an IndexError\n\nYou can look at ranges with slice syntax.\nThe start index is included, the end index is not\n(It's a closed/open range for you mathy types.)\nli[1:3]   # Return list from index 1 to 3 => [2, 4]\nli[2:]    # Return list starting from index 2 => [4, 3]\nli[:3]    # Return list from beginning until index 3  => [1, 2, 4]\nli[::2]   # Return list selecting every second entry => [1, 4]\nli[::-1]  # Return list in reverse order => [3, 4, 2, 1]\nUse any combination of these to make advanced slices\nli[start:end:step]\n\nMake a one layer deep copy using slices\nli2 = li[:]  # => li2 = [1, 2, 4, 3] but (li2 is li) will result in false.\n\nRemove arbitrary elements from a list with \"del\"\ndel li[2]  # li is now [1, 2, 3]\n\nRemove first occurrence of a value\nli.remove(2)  # li is now [1, 3]\nli.remove(2)  # Raises a ValueError as 2 is not in the list\n\nInsert an element at a specific index\nli.insert(1, 2)  # li is now [1, 2, 3] again\n\nGet the index of the first item found matching the argument\nli.index(2)  # => 1\nli.index(4)  # Raises a ValueError as 4 is not in the list\n\nYou can add lists\nNote: values for li and for other_li are not modified.\nli + other_li  # => [1, 2, 3, 4, 5, 6]\n\nConcatenate lists with \"extend()\"\nli.extend(other_li)  # Now li is [1, 2, 3, 4, 5, 6]\n\nCheck for existence in a list with \"in\"\n1 in li  # => True\n\nExamine the length with \"len()\"\nlen(li)  # => 6\n\nTuples are like lists but are immutable.\ntup = (1, 2, 3)\ntup[0]      # => 1\ntup[0] = 3  # Raises a TypeError\n\nNote that a tuple of length one has to have a comma after the last element but\ntuples of other lengths, even zero, do not.\ntype((1))   # =>\ntype((1,))  # =>\ntype(())    # =>\n\nYou can do most of the list operations on tuples too\nlen(tup)         # => 3\ntup + (4, 5, 6)  # => (1, 2, 3, 4, 5, 6)\ntup[:2]          # => (1, 2)\n2 in tup         # => True\n\nYou can unpack tuples (or lists) into variables\na, b, c = (1, 2, 3)  # a is now 1, b is now 2 and c is now 3\nYou can also do extended unpacking\na, *b, c = (1, 2, 3, 4)  # a is now 1, b is now [2, 3] and c is now 4\nTuples are created by default if you leave out the parentheses\nd, e, f = 4, 5, 6  # tuple 4, 5, 6 is unpacked into variables d, e and f\nrespectively such that d = 4, e = 5 and f = 6\nNow look how easy it is to swap two values\ne, d = d, e  # d is now 5 and e is now 4\n\nDictionaries store mappings from keys to values\nempty_dict = {}\nHere is a prefilled dictionary\nfilled_dict = {\"one\": 1, \"two\": 2, \"three\": 3}\n\nNote keys for dictionaries have to be immutable types. This is to ensure that\nthe key can be converted to a constant hash value for quick look-ups.\nImmutable types include ints, floats, strings, tuples.\ninvalid_dict = {[1,2,3]: \"123\"}  # => Raises a TypeError: unhashable type: 'list'\nvalid_dict = {(1,2,3):[1,2,3]}   # Values can be of any type, however.\n\nLook up values with []\nfilled_dict[\"one\"]  # => 1\n\nGet all keys as an iterable with \"keys()\". We need to wrap the call in list()\nto turn it into a list. We'll talk about those later.  Note - for Python\nversions  [\"three\", \"two\", \"one\"] in Python  [\"one\", \"two\", \"three\"] in Python 3.7+\n\nGet all values as an iterable with \"values()\". Once again we need to wrap it\nin list() to get it out of the iterable. Note - Same as above regarding key\nordering.\nlist(filled_dict.values())  # => [3, 2, 1]  in Python  [1, 2, 3] in Python 3.7+\n\nCheck for existence of keys in a dictionary with \"in\"\n\"one\" in filled_dict  # => True\n1 in filled_dict      # => False\n\nLooking up a non-existing key is a KeyError\nfilled_dict[\"four\"]  # KeyError\n\nUse \"get()\" method to avoid the KeyError\nfilled_dict.get(\"one\")      # => 1\nfilled_dict.get(\"four\")     # => None\nThe get method supports a default argument when the value is missing\nfilled_dict.get(\"one\", 4)   # => 1\nfilled_dict.get(\"four\", 4)  # => 4\n\n\"setdefault()\" inserts into a dictionary only if the given key isn't present\nfilled_dict.setdefault(\"five\", 5)  # filled_dict[\"five\"] is set to 5\nfilled_dict.setdefault(\"five\", 6)  # filled_dict[\"five\"] is still 5\n\nAdding to a dictionary\nfilled_dict.update({\"four\":4})  # => {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4}\nfilled_dict[\"four\"] = 4         # another way to add to dict\n\nRemove keys from a dictionary with del\ndel filled_dict[\"one\"]  # Removes the key \"one\" from filled dict\n\nFrom Python 3.5 you can also use the additional unpacking options\n{'a': 1, **{'b': 2}}  # => {'a': 1, 'b': 2}\n{'a': 1, **{'a': 2}}  # => {'a': 2}\n\nSets store ... well sets\nempty_set = set()\nInitialize a set with a bunch of values. Yeah, it looks a bit like a dict. Sorry.\nsome_set = {1, 1, 2, 2, 3, 4}  # some_set is now {1, 2, 3, 4}\n\nSimilar to keys of a dictionary, elements of a set have to be immutable.\ninvalid_set = {[1], 1}  # => Raises a TypeError: unhashable type: 'list'\nvalid_set = {(1,), 1}\n\nAdd one more item to the set\nfilled_set = some_set\nfilled_set.add(5)  # filled_set is now {1, 2, 3, 4, 5}\nSets do not have duplicate elements\nfilled_set.add(5)  # it remains as before {1, 2, 3, 4, 5}\n\nDo set intersection with &\nother_set = {3, 4, 5, 6}\nfilled_set & other_set  # => {3, 4, 5}\n\nDo set union with |\nfilled_set | other_set  # => {1, 2, 3, 4, 5, 6}\n\nDo set difference with -\n{1, 2, 3, 4} - {2, 3, 5}  # => {1, 4}\n\nDo set symmetric difference with ^\n{1, 2, 3, 4} ^ {2, 3, 5}  # => {1, 4, 5}\n\nCheck if set on the left is a superset of set on the right\n{1, 2} >= {1, 2, 3} # => False\n\nCheck if set on the left is a subset of set on the right\n{1, 2}  True\n\nCheck for existence in a set with in\n2 in filled_set   # => True\n10 in filled_set  # => False\n\nMake a one layer deep copy\nfilled_set = some_set.copy()  # filled_set is {1, 2, 3, 4, 5}\nfilled_set is some_set        # => False\n\n\n####################################################\n3. Control Flow and Iterables\n####################################################\n\nLet's just make a variable\nsome_var = 5\n\nHere is an if statement. Indentation is significant in Python!\nConvention is to use four spaces, not tabs.\nThis prints \"some_var is smaller than 10\"\nif some_var > 10:\n    print(\"some_var is totally bigger than 10.\")\nelif some_var  dict_keys(['one', 'two', 'three']). This is an object that implements our Iterable interface.\n\nWe can loop over it.\nfor i in our_iterable:\n    print(i)  # Prints one, two, three\n\nHowever we cannot address elements by index.\nour_iterable[1]  # Raises a TypeError\n\nAn iterable is an object that knows how to create an iterator.\nour_iterator = iter(our_iterable)\n\nOur iterator is an object that can remember the state as we traverse through it.\nWe get the next object with \"next()\".\nnext(our_iterator)  # => \"one\"\n\nIt maintains state as we iterate.\nnext(our_iterator)  # => \"two\"\nnext(our_iterator)  # => \"three\"\n\nAfter the iterator has returned all of its data, it raises a StopIteration exception\nnext(our_iterator)  # Raises StopIteration\n\nWe can also loop over it, in fact, \"for\" does this implicitly!\nour_iterator = iter(our_iterable)\nfor i in our_iterator:\n    print(i)  # Prints one, two, three\n\nYou can grab all the elements of an iterable or iterator by calling list() on it.\nlist(our_iterable)  # => Returns [\"one\", \"two\", \"three\"]\nlist(our_iterator)  # => Returns [] because state is saved\n\n\n####################################################\n4. Functions\n####################################################\n\nUse \"def\" to create new functions\ndef add(x, y):\n    print(\"x is {} and y is {}\".format(x, y))\n    return x + y  # Return values with a return statement\n\nCalling functions with parameters\nadd(5, 6)  # => prints out \"x is 5 and y is 6\" and returns 11\n\nAnother way to call functions is with keyword arguments\nadd(y=6, x=5)  # Keyword arguments can arrive in any order.\n\nYou can define functions that take a variable number of\npositional arguments\ndef varargs(*args):\n    return args\n\nvarargs(1, 2, 3)  # => (1, 2, 3)\n\nYou can define functions that take a variable number of\nkeyword arguments, as well\ndef keyword_args(**kwargs):\n    return kwargs\n\nLet's call it to see what happens\nkeyword_args(big=\"foot\", loch=\"ness\")  # => {\"big\": \"foot\", \"loch\": \"ness\"}\n\nYou can do both at once, if you like\ndef all_the_args(args, *kwargs):\n    print(args)\n    print(kwargs)\n\"\"\"\nall_the_args(1, 2, a=3, b=4) prints:\n    (1, 2)\n    {\"a\": 3, \"b\": 4}\n\"\"\"\n\nWhen calling functions, you can do the opposite of args/kwargs!\nUse * to expand tuples and use ** to expand kwargs.\nargs = (1, 2, 3, 4)\nkwargs = {\"a\": 3, \"b\": 4}\nall_the_args(*args)            # equivalent to all_the_args(1, 2, 3, 4)\nall_the_args(**kwargs)         # equivalent to all_the_args(a=3, b=4)\nall_the_args(args, *kwargs)  # equivalent to all_the_args(1, 2, 3, 4, a=3, b=4)\n\nReturning multiple values (with tuple assignments)\ndef swap(x, y):\n    return y, x  # Return multiple values as a tuple without the parenthesis.\n(Note: parenthesis have been excluded but can be included)\n\nx = 1\ny = 2\nx, y = swap(x, y)     # => x = 2, y = 1\n(x, y) = swap(x,y)  # Again parenthesis have been excluded but can be included.\n\nFunction Scope\nx = 5\n\ndef set_x(num):\nLocal var x not the same as global variable x\n    x = num    # => 43\n    print(x)   # => 43\n\ndef set_global_x(num):\n    global x\n    print(x)   # => 5\n    x = num    # global var x is now set to 6\n    print(x)   # => 6\n\nset_x(43)\nset_global_x(6)\n\nPython has first class functions\ndef create_adder(x):\n    def adder(y):\n        return x + y\n    return adder\n\nadd_10 = create_adder(10)\nadd_10(3)   # => 13\n\nThere are also anonymous functions\n(lambda x: x > 2)(3)                  # => True\n(lambda x, y: x * 2 + y * 2)(2, 1)  # => 5\n\nThere are built-in higher order functions\nlist(map(add_10, [1, 2, 3]))          # => [11, 12, 13]\nlist(map(max, [1, 2, 3], [4, 2, 1]))  # => [4, 2, 3]\n\nlist(filter(lambda x: x > 5, [3, 4, 5, 6, 7]))  # => [6, 7]\n\nWe can use list comprehensions for nice maps and filters\nList comprehension stores the output as a list which can itself be a nested list\n[add_10(i) for i in [1, 2, 3]]         # => [11, 12, 13]\n[x for x in [3, 4, 5, 6, 7] if x > 5]  # => [6, 7]\n\nYou can construct set and dict comprehensions as well.\n{x for x in 'abcddeef' if x not in 'abc'}  # => {'d', 'e', 'f'}\n{x: x**2 for x in range(5)}  # => {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n####################################################\n5. Modules\n####################################################\n\nYou can import modules\nimport math\nprint(math.sqrt(16))  # => 4.0\n\nYou can get specific functions from a module\nfrom math import ceil, floor\nprint(ceil(3.7))   # => 4.0\nprint(floor(3.7))  # => 3.0\n\nYou can import all functions from a module.\nWarning: this is not recommended\nfrom math import *\n\nYou can shorten module names\nimport math as m\nmath.sqrt(16) == m.sqrt(16)  # => True\n\nPython modules are just ordinary Python files. You\ncan write your own, and import them. The name of the\nmodule is the same as the name of the file.\n\nYou can find out which functions and attributes\nare defined in a module.\nimport math\ndir(math)\n\nIf you have a Python script named math.py in the same\nfolder as your current script, the file math.py will\nbe loaded instead of the built-in Python module.\nThis happens because the local folder has priority\nover Python's built-in libraries.\n\n\n####################################################\n6. Classes\n####################################################\n\nWe use the \"class\" statement to create a class\nclass Human:\n\nA class attribute. It is shared by all instances of this class\n    species = \"H. sapiens\"\n\nBasic initializer, this is called when this class is instantiated.\nNote that the double leading and trailing underscores denote objects\nor attributes that are used by Python but that live in user-controlled\nnamespaces. Methods(or objects or attributes) like: init, str,\nrepr etc. are called special methods (or sometimes called dunder methods)\nYou should not invent such names on your own.\n    def init(self, name):\nAssign the argument to the instance's name attribute\n        self.name = name\n\nInitialize property\n        self._age = 0\n\nAn instance method. All methods take \"self\" as the first argument\n    def say(self, msg):\n        print(\"{name}: {message}\".format(name=self.name, message=msg))\n\nAnother instance method\n    def sing(self):\n        return 'yo... yo... microphone check... one two... one two...'\n\nA class method is shared among all instances\nThey are called with the calling class as the first argument\n    @classmethod\n    def get_species(cls):\n        return cls.species\n\nA static method is called without a class or instance reference\n    @staticmethod\n    def grunt():\n        return \"grunt\"\n\nA property is just like a getter.\nIt turns the method age() into a read-only attribute of the same name.\nThere's no need to write trivial getters and setters in Python, though.\n    @property\n    def age(self):\n        return self._age\n\nThis allows the property to be set\n    @age.setter\n    def age(self, age):\n        self._age = age\n\nThis allows the property to be deleted\n    @age.deleter\n    def age(self):\n        del self._age\n\nWhen a Python interpreter reads a source file it executes all its code.\nThis name check makes sure this code block is only executed when this\nmodule is the main program.\nif name == 'main':\nInstantiate a class\n    i = Human(name=\"Ian\")\n    i.say(\"hi\")                     # \"Ian: hi\"\n    j = Human(\"Joel\")\n    j.say(\"hello\")                  # \"Joel: hello\"\ni and j are instances of type Human, or in other words: they are Human objects\n\nCall our class method\n    i.say(i.get_species())          # \"Ian: H. sapiens\"\nChange the shared attribute\n    Human.species = \"H. neanderthalensis\"\n    i.say(i.get_species())          # => \"Ian: H. neanderthalensis\"\n    j.say(j.get_species())          # => \"Joel: H. neanderthalensis\"\n\nCall the static method\n    print(Human.grunt())            # => \"grunt\"\n\nStatic methods can be called by instances too\n    print(i.grunt())                # => \"grunt\"\n\nUpdate the property for this instance\n    i.age = 42\nGet the property\n    i.say(i.age)                    # => \"Ian: 42\"\n    j.say(j.age)                    # => \"Joel: 0\"\nDelete the property\n    del i.age\ni.age                         # => this would raise an AttributeError\n\n\n####################################################\n6.1 Inheritance\n####################################################\n\nInheritance allows new child classes to be defined that inherit methods and\nvariables from their parent class.\n\nUsing the Human class defined above as the base or parent class, we can\ndefine a child class, Superhero, which inherits the class variables like\n\"species\", \"name\", and \"age\", as well as methods, like \"sing\" and \"grunt\"\nfrom the Human class, but can also have its own unique properties.\n\nTo take advantage of modularization by file you could place the classes above in their own files,\nsay, human.py\n\nTo import functions from other files use the following format\nfrom \"filename-without-extension\" import \"function-or-class\"\n\nfrom human import Human\n\nSpecify the parent class(es) as parameters to the class definition\nclass Superhero(Human):\n\nIf the child class should inherit all of the parent's definitions without\nany modifications, you can just use the \"pass\" keyword (and nothing else)\nbut in this case it is commented out to allow for a unique child class:\npass\n\nChild classes can override their parents' attributes\n    species = 'Superhuman'\n\nChildren automatically inherit their parent class's constructor including\nits arguments, but can also define additional arguments or definitions\nand override its methods such as the class constructor.\nThis constructor inherits the \"name\" argument from the \"Human\" class and\nadds the \"superpower\" and \"movie\" arguments:\n    def init(self, name, movie=False,\n                 superpowers=[\"super strength\", \"bulletproofing\"]):\n\nadd additional class attributes:\n        self.fictional = True\n        self.movie = movie\nbe aware of mutable default values, since defaults are shared\n        self.superpowers = superpowers\n\nThe \"super\" function lets you access the parent class's methods\nthat are overridden by the child, in this case, the init method.\nThis calls the parent class constructor:\n        super().init(name)\n\noverride the sing method\n    def sing(self):\n        return 'Dun, dun, DUN!'\n\nadd an additional instance method\n    def boast(self):\n        for power in self.superpowers:\n            print(\"I wield the power of {pow}!\".format(pow=power))\n\n\nif name == 'main':\n    sup = Superhero(name=\"Tick\")\n\nInstance type checks\n    if isinstance(sup, Human):\n        print('I am human')\n    if type(sup) is Superhero:\n        print('I am a superhero')\n\nGet the Method Resolution search Order used by both getattr() and super()\nThis attribute is dynamic and can be updated\n    print(Superhero.mro)    # => (,\n=> , )\n\nCalls parent method but uses its own class attribute\n    print(sup.get_species())    # => Superhuman\n\nCalls overridden method\n    print(sup.sing())           # => Dun, dun, DUN!\n\nCalls method from Human\n    sup.say('Spoon')            # => Tick: Spoon\n\nCall method that exists only in Superhero\n    sup.boast()                 # => I wield the power of super strength!\n=> I wield the power of bulletproofing!\n\nInherited class attribute\n    sup.age = 31\n    print(sup.age)              # => 31\n\nAttribute that only exists within Superhero\n    print('Am I Oscar eligible? ' + str(sup.movie))\n\n####################################################\n6.2 Multiple Inheritance\n####################################################\n\nAnother class definition\nbat.py\nclass Bat:\n\n    species = 'Baty'\n\n    def init(self, can_fly=True):\n        self.fly = can_fly\n\nThis class also has a say method\n    def say(self, msg):\n        msg = '... ... ...'\n        return msg\n\nAnd its own method as well\n    def sonar(self):\n        return '))) ... ((('\n\nif name == 'main':\n    b = Bat()\n    print(b.say('hello'))\n    print(b.fly)\n\nAnd yet another class definition that inherits from Superhero and Bat\nsuperhero.py\nfrom superhero import Superhero\nfrom bat import Bat\n\nDefine Batman as a child that inherits from both Superhero and Bat\nclass Batman(Superhero, Bat):\n\n    def init(self, args, *kwargs):\nTypically to inherit attributes you have to call super:\nsuper(Batman, self).init(args, *kwargs)\nHowever we are dealing with multiple inheritance here, and super()\nonly works with the next base class in the MRO list.\nSo instead we explicitly call init for all ancestors.\nThe use of args and *kwargs allows for a clean way to pass arguments,\nwith each parent \"peeling a layer of the onion\".\n        Superhero.init(self, 'anonymous', movie=True,\n                           superpowers=['Wealthy'], args, *kwargs)\n        Bat.init(self, args, can_fly=False, *kwargs)\noverride the value for the name attribute\n        self.name = 'Sad Affleck'\n\n    def sing(self):\n        return 'nan nan nan nan nan batman!'\n\n\nif name == 'main':\n    sup = Batman()\n\nGet the Method Resolution search Order used by both getattr() and super().\nThis attribute is dynamic and can be updated\n    print(Batman.mro)       # => (,\n=> ,\n=> ,\n=> , )\n\nCalls parent method but uses its own class attribute\n    print(sup.get_species())    # => Superhuman\n\nCalls overridden method\n    print(sup.sing())           # => nan nan nan nan nan batman!\n\nCalls method from Human, because inheritance order matters\n    sup.say('I agree')          # => Sad Affleck: I agree\n\nCall method that exists only in 2nd ancestor\n    print(sup.sonar())          # => ))) ... (((\n\nInherited class attribute\n    sup.age = 100\n    print(sup.age)              # => 100\n\nInherited attribute from 2nd ancestor whose default value was overridden.\n    print('Can I fly? ' + str(sup.fly)) # => Can I fly? False\n\n\n\n####################################################\n7. Advanced\n####################################################\n\nGenerators help you make lazy code.\ndef double_numbers(iterable):\n    for i in iterable:\n        yield i + i\n\nGenerators are memory-efficient because they only load the data needed to\nprocess the next value in the iterable. This allows them to perform\noperations on otherwise prohibitively large value ranges.\nNOTE: range replaces xrange in Python 3.\nfor i in double_numbers(range(1, 900000000)):  # range is a generator.\n    print(i)\n    if i >= 30:\n        break\n\nJust as you can create a list comprehension, you can create generator\ncomprehensions as well.\nvalues = (-x for x in [1,2,3,4,5])\nfor x in values:\n    print(x)  # prints -1 -2 -3 -4 -5 to console/terminal\n\nYou can also cast a generator comprehension directly to a list.\nvalues = (-x for x in [1,2,3,4,5])\ngen_to_list = list(values)\nprint(gen_to_list)  # => [-1, -2, -3, -4, -5]\n\nDecorators\nIn this example beg wraps say. If say_please is True then it\nwill change the returned message.\nfrom functools import wraps\n\n\ndef beg(target_function):\n    @wraps(target_function)\n    def wrapper(args, *kwargs):\n        msg, say_please = target_function(args, *kwargs)\n        if say_please:\n            return \"{} {}\".format(msg, \"Please! I am poor :(\")\n        return msg\n\n    return wrapper\n\n\n@beg\ndef say(say_please=False):\n    msg = \"Can you buy me a beer?\"\n    return msg, say_please\n\n\nprint(say())                 # Can you buy me a beer?\nprint(say(say_please=True))  # Can you buy me a beer? Please! I am poor :(\n\n\n\n",
            "tags": [
                "Python",
                "CheatSheet"
            ],
            "lang": "en"
        },
        {
            "uri": "/posts/serverless-flask-lambda-api-gateway-mongodb/",
            "title": "Serverless: Flask+API Gateway+Lambda+MongoDB",
            "description": "Create AWS serverless application on Flask + API Gateway + Lambda + MongoDB",
            "content": "\nmongodb free tier vs documentdb\n\nProject structure\n\nsrc\n  app.py\n  mongo.py\n.env\nrequirements.txt\nserverless.yml\n\nSources\n\nAdd AIM user\n\nSetup specific user for serverless deployment\n\nusername: serverless-deployer\n\naws aim\ndocumentation\n\nSet policy\n\nCreate:\n\nServerLessDeployerPolicyGroup\nServerLessDeployerPolicy\n\nPolicy:\n\n{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"apigateway:*\",\n                \"cloudformation:CancelUpdateStack\",\n                \"cloudformation:ContinueUpdateRollback\",\n                \"cloudformation:CreateChangeSet\",\n                \"cloudformation:CreateStack\",\n                \"cloudformation:CreateUploadBucket\",\n                \"cloudformation:DeleteStack\",\n                \"cloudformation:Describe*\",\n                \"cloudformation:EstimateTemplateCost\",\n                \"cloudformation:ExecuteChangeSet\",\n                \"cloudformation:Get*\",\n                \"cloudformation:List*\",\n                \"cloudformation:UpdateStack\",\n                \"cloudformation:UpdateTerminationProtection\",\n                \"cloudformation:ValidateTemplate\",\n                \"dynamodb:CreateTable\",\n                \"dynamodb:DeleteTable\",\n                \"dynamodb:DescribeTable\",\n                \"dynamodb:DescribeTimeToLive\",\n                \"dynamodb:UpdateTimeToLive\",\n                \"ec2:AttachInternetGateway\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:CreateInternetGateway\",\n                \"ec2:CreateNetworkAcl\",\n                \"ec2:CreateNetworkAclEntry\",\n                \"ec2:CreateRouteTable\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:CreateSubnet\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateVpc\",\n                \"ec2:DeleteInternetGateway\",\n                \"ec2:DeleteNetworkAcl\",\n                \"ec2:DeleteNetworkAclEntry\",\n                \"ec2:DeleteRouteTable\",\n                \"ec2:DeleteSecurityGroup\",\n                \"ec2:DeleteSubnet\",\n                \"ec2:DeleteVpc\",\n                \"ec2:Describe*\",\n                \"ec2:DetachInternetGateway\",\n                \"ec2:ModifyVpcAttribute\",\n                \"events:DeleteRule\",\n                \"events:DescribeRule\",\n                \"events:ListRuleNamesByTarget\",\n                \"events:ListRules\",\n                \"events:ListTargetsByRule\",\n                \"events:PutRule\",\n                \"events:PutTargets\",\n                \"events:RemoveTargets\",\n                \"iam:AttachRolePolicy\",\n                \"iam:CreateRole\",\n                \"iam:DeleteRole\",\n                \"iam:DeleteRolePolicy\",\n                \"iam:DetachRolePolicy\",\n                \"iam:GetRole\",\n                \"iam:PassRole\",\n                \"iam:PutRolePolicy\",\n                \"iot:CreateTopicRule\",\n                \"iot:DeleteTopicRule\",\n                \"iot:DisableTopicRule\",\n                \"iot:EnableTopicRule\",\n                \"iot:ReplaceTopicRule\",\n                \"kinesis:CreateStream\",\n                \"kinesis:DeleteStream\",\n                \"kinesis:DescribeStream\",\n                \"lambda:*\",\n\t\t\t\t\"logs:CreateLogDelivery\",\n                \"logs:CreateLogGroup\",\n                \"logs:DeleteLogGroup\",\n                \"logs:DescribeLogGroups\",\n                \"logs:DescribeLogStreams\",\n                \"logs:FilterLogEvents\",\n                \"logs:GetLogEvents\",\n                \"logs:PutSubscriptionFilter\",\n                \"s3:CreateBucket\",\n                \"s3:DeleteBucket\",\n                \"s3:DeleteBucketPolicy\",\n                \"s3:DeleteObject\",\n                \"s3:DeleteObjectVersion\",\n                \"s3:GetObject\",\n                \"s3:GetObjectVersion\",\n                \"s3:ListAllMyBuckets\",\n                \"s3:ListBucket\",\n                \"s3:PutBucketNotification\",\n                \"s3:PutBucketPolicy\",\n                \"s3:PutBucketTagging\",\n                \"s3:PutBucketWebsite\",\n                \"s3:PutEncryptionConfiguration\",\n                \"s3:PutObject\",\n                \"sns:CreateTopic\",\n                \"sns:DeleteTopic\",\n                \"sns:GetSubscriptionAttributes\",\n                \"sns:GetTopicAttributes\",\n                \"sns:ListSubscriptions\",\n                \"sns:ListSubscriptionsByTopic\",\n                \"sns:ListTopics\",\n                \"sns:SetSubscriptionAttributes\",\n                \"sns:SetTopicAttributes\",\n                \"sns:Subscribe\",\n                \"sns:Unsubscribe\",\n                \"states:CreateStateMachine\",\n                \"states:DeleteStateMachine\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n\nCreate user\n\ncopy the API Key & Secret\n\nNeed during setup aws cli/serverless\n\nCreate serverless.yml\n\nIn the root folder create:\n\norg: romankurnovskii\napp: app-name\nservice: app-service-name\n\nframeworkVersion: '3'\n\nuseDotenv: true\n\ncustom:\n  wsgi:\n    app: src/app.app\n    packRequirements: false\n\nprovider:\n  name: aws\n  deploymentMethod: direct\n  region: eu-west-1\n  runtime: python3.9\n  architecture: arm64\n  versionFunctions: false\n  memorySize: 128\n\nfunctions:\n  api:\n    handler: wsgi_handler.handler\n    events:\n      httpApi: '*'\n    environment:\n      MONGO_CONNECTION_STRING: ${env:MONGO_CONNECTION_STRING}\n      MONGO_COLLECTION_DB_NAME: ${env:MONGO_COLLECTION_DB_NAME}\n\npackage:\n  patterns:\n    '!.dynamodb/**'\n    '!.git/**'\n    '!.vscode/**'\n    '!.env'\n    '!node_modules/**'\n    '!tmp/**'\n    '!venv/**'\n    '!pycache/**'\n\nplugins:\n  serverless-wsgi\n  serverless-python-requirements\n\nCreate Flask app\n\nPrerequisites\n\npython -m venv ./venv\nsource ./venv/bin/activate\n\nApp\n\n\nsrc/app.py\n\nfrom flask import Flask, ObjectId, request, jsonify, make_response\nfrom flask_cors import CORS\nimport json\nfrom src.mongo import my_db\n\n\nusers_collection = my_db.users\n\n\napp = Flask(name)\ncors = CORS(app)\n\n\n@app.route(\"/\", methods=['GET'])\ndef get_user(user_id):\n    user_id = request.args.get('id')\n    user = users_collection.find_one({\"_id\": ObjectId(user_id)})\n    if not user:\n        return jsonify({'error': 'data not found'}), 404\n    return jsonify({'user': user})\n\n@app.route('/', methods=['PUT'])\ndef create_record():\n    record = json.loads(request.data)\n    user_id = record.get('user_id', None)\n    users_collection.update_one({\"_id\": ObjectId(user_id)}, record)\n\n\n@app.route(\"/\")\ndef hello():\n    return jsonify(message='Hello!')\n\n\n@app.errorhandler(404)\ndef resource_not_found(e):\n    return make_response(jsonify(error='Not found!'), 404)\n\n\ndef internal_server_error(e):\n    return 'error', 500\n\n\napp.register_error_handler(500, internal_server_error)\n\nsrc/mongo.py\n\nimport os\n\nfrom pymongo import MongoClient\n\nMONGO_CONNECTION_STRING = os.environ.get(\n    \"MONGO_CONNECTION_STRING\", default=\"mongodb://localhost:27017/\"\n)\nMONGO_COLLECTION_DB_NAME = os.environ.get(\n    \"MONGO_COLLECTION_DB_NAME\", default=\"test-mydb\"\n)\n\n\ndb_client = MongoClient(MONGO_CONNECTION_STRING)\nmy_db = db_client[MONGO_COLLECTION_DB_NAME]\n\n.env\n\nMONGO_CONNECTION_STRING=mongodb+srv://login:password@cluster0.XXXXX.mongodb.net/mydb?retryWrites=true&w=majority\nMONGO_COLLECTION_DB_NAME=mydb\n\nsrc/requirements.txt\n\ncertifi==2022.6.15\ncharset-normalizer==2.1.1\nclick==7.1.2\ndnspython==2.2.1\necdsa==0.18.0\nFlask==1.1.4\nFlask-Cors==3.0.10\nidna==3.3\nimportlib-metadata==4.12.0\nitsdangerous==1.1.0\nJinja2==2.11.3\njmespath==1.0.1\nMarkupSafe==2.0.1\npyasn1==0.4.8\npymongo==4.2.0\npython-dateutil==2.8.2\npython-dotenv==0.20.0\nrequests==2.28.1\nrsa==4.9\nsix==1.16.0\nurllib3==1.26.12\nWerkzeug==1.0.1\nzipp==3.8.1\n\nDeployment\n\nserverless login\n\ninstall dependencies with:\n\nnpm install\n\nand\n\npip install -r requirements.txt\n\nand then perform deployment with:\n\nserverless deploy\n\nAfter running deploy, you should see output similar to:\n\nDeploying app-service-name to stage dev (eu-west-1)\n\n✔ Service deployed to stack app-service-name (182s)\n\nLocal development\n\nThanks to capabilities of serverless-wsgi, it is also possible to run your application locally, however, in order to do that, you will need to first install werkzeug dependency, as well as all other dependencies listed in requirements.txt. It is recommended to use a dedicated virtual environment for that purpose. You can install all needed dependencies with the following commands:\n\nAlready in requirements.txt:\n\npip install werkzeug\npip install -r requirements.txt\n\nAt this point, you can run your application locally with the following command:\n\nserverless wsgi serve\n\nFor additional local development capabilities of serverless-wsgi plugin, please refer to corresponding GitHub repository.",
            "tags": [
                "aws",
                "flask",
                "serverless",
                "mongodb",
                "lambda"
            ],
            "lang": "en"
        },
        {
            "uri": "/search/_index",
            "title": "Search page",
            "content": "",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/_index",
            "title": "Roadmaps",
            "content": "\nList style view",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/_index",
            "title": "Algorithms 101",
            "description": "LeetCode Cookbook",
            "content": "\nSmart / OKR\n\nWhat is SMART and OKR\n\nSMART\n\nSpecific:\nGoal: prepare for contests. Pass 50->75->100% of contest problems in time\n  Improve python skills.\n  Improve understanding of common algorithms and data structures.\n\nMeasurable: How will we know that change has occurred?\nsolve top 100 questions that cover common algorithms and data structures.\n\nAchievable:\nparticipate in LeetCode contest, solve 50%+ problems in time.\n\nRelevant: Is it possible to achieve this objective?\nachievable with practice.\nimprove skills in solving business problems more efficiently, quickly, understandable.\n\nTime-Bound: When will this objective be accomplished?\n1-2 hours a day, 5-6 days a week, ~5 problems a week\nfirst contest after 20% problems pass.\n20 weeks from start.\nSummarize results on 28 Feb 2023\n\nOKR + roadmap\n\npass 20 problems: (4 weeks, 12 Nov 2022)\n  Ability to define algorithm/idea of solving problem.\n  participate in contest, solve minimum 1-2 problems in time.\nnext participate in contest/solve contest tasks every week:\n  solve minimum 1-2 problems.\n  fix results, correct next goal keys if I go ahead.\npass 40 problems: (10 Dec 2022)\n  participate in contest, solve minimum 2 problems in time.\nafter 50 problems have a rest one week. (24 Dec 2022)\npass 70 problems: (28 Jan 2023)\n  solve next 10 medium problems without hints effectively.\n80-100 problems: (28 Feb 2023)\n  have understanding in which topics I have gaps.\n  emphasize problem solving on these topics in addition to the tasks on the list.\nSum up results (28 Feb 2023)\n\nSolving plan\n\nopen task\nread\nfirst thoughts\nspend 1510 minutes on coding/drawing/understanding algo\nfinished or not, read hints\nspend 10 minutes on fixing if needed\nread solution, discussions\nif there is a new algo, read theory the rest of first hour, practice\ncode from scratch with comments/code snippets\nrepeat 7-9 until tests pass\n\nPrepare environment\n\nvscode\n\nto observe any change in python use nodemon npm package\n\nnpm i -g nodemon\n\nrun python file:\n\nnodemon --exec python p.py\n\nTemplate\n\nfrom typing import List\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        return 1\n\nnums = [1,2,3]\ntarget = 5\n\ns = Solution()\nres = s.twoSum(nums, target)\n\nprint(res)\n\nProblems list\n\nleetcode Top 100 Interview Questions\nList of problems cis mutable. Will take first not solved until all first top 100 problems are solved. It can take + ~5-15 problems\n\nProblems order\n\nMore info:\n\nTop MAANG interview questions 2022\n\nTutorial subscriptions\n\nhttps://www.enjoyalgorithms.com/data-structures-and-algorithms-course/\nhttps://www.scaler.com/topics/data-structures/\nhttps://leetcode.com/subscribe/\nhttps://algo.monster/subscribe\nhttps://www.algoexpert.io/purchase#algoexpert\nSenior Algorithms | Interactive tutorial\n01.01 Data Structures and Algorithms | Interactive\nContests\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/algorithms",
            "title": "Algorithm Patterns",
            "description": "LeetCode Cookbook - Algorithms",
            "content": "\nIntro\n\nBig-O Cheat Sheet\n\nSort\n\ndef insertion_sort(array):\n    for i in range(1, len(array)):\n        value = array[i]\n        while i > 0 and array[i - 1] > value:\n            array[i] = array[i - 1]\n            i -= 1\n            array[i] = value\n    return array\n\ndef selection_sort(array):\n    for i in range(len(array) - 1):\n        min_value = i\n        for j in range(i + 1, len(array)):\n            if array[j] > 2\n\nDynamic programming (DP)\n\nBreadth First Search (BFS)\n\nBFS on Tree:\ndef bfs(root):\n    queue = deque([root])\n    while len(queue) > 0:\n        node = queue.popleft()\n        for child in node.children:\n            if is_goal(child):\n                return FOUND(child)\n            queue.append(child)\n    return NOT_FOUND\n\nBFS on Graph:\ndef bfs(root):\n    queue = deque([root])\n    visited = set([root])\n    while len(queue) > 0:\n        node = queue.popleft()\n        for neighbor in get_neighbors(node):\n            if neighbor in visited:\n                continue\n            queue.append(neighbor)\n            visited.add(neighbor)\n\nBFS on a Matrix:\nnum_rows, num_cols = len(grid), len(grid[0])\ndef get_neighbors(coord):\n    row, col = coord\n    delta_row = [-1, 0, 1, 0]\n    delta_col = [0, 1, 0, -1]\n    res = []\n    for i in range(len(delta_row)):\n        neighbor_row = row + delta_row[i]\n        neighbor_col = col + delta_col[i]\n        if 0  0:\n        node = queue.popleft()\n        for neighbor in get_neighbors(node):\n            if neighbor in visited:\n                continue\nDo stuff with the node if required\n...\n            queue.append(neighbor)\n            visited.add(neighbor)\n\nDepth-first search (DFS)\n\nDFS on Tree:\ndef dfs(root, target):\n    if root is None:\n        return None\n    if root.val == target:\n        return root\n    left = dfs(root.left, target)\n    if left is not None:\n        return left\n    return dfs(root.right, target)\n\nDFS on Graph:\ndef dfs(root, visited):\n    for neighbor in get_neighbors(root):\n        if neighbor in visited:\n            continue\n        visited.add(neighbor)\n        dfs(neighbor, visited)\n\nDFS on two-dimensional array:\n\nLet's imagine you have a big maze made of walls and corridors, and you want to find a way from the entrance to the exit. You can put a robot at the entrance, and you want to tell the robot what to do to find the exit.\n\nThe first thing you might tell the robot is to always go as far as it can in one direction before turning. This is what depth-first search does.\n\nThe robot starts at the entrance and goes as far as it can down the first corridor it finds.\nIf it comes to a dead end, it goes back to the last intersection it passed and tries the next corridor.\nIf it comes to the exit, it stops and says \"I found the exit!\".\n\nExample:\n\nDefine the maze as a two-dimensional array\nmaze = [\n  ['.', '.', '#', '#', '#', '#', '#', '#'],\n  ['#', '.', '.', '.', '#', '.', '.', '#'],\n  ['#', '.', '#', '.', '#', '.', '.', '#'],\n  ['#', '.', '.', '.', '.', '#', '.', '#'],\n  ['#', '#', '#', '#', '.', '#', '.', '#'],\n  ['#', '.', '.', '.', '.', '.', '.', '#'],\n  ['#', '.', '#', '#', '#', '#', '.', '.'],\n  ['#', '#', '#', '#', '#', '#', '#', '.'],\n]\n\nDefine the starting point and the destination\nstart = (0, 0)\nend = (len(maze)-1, len(maze[0])-1)\n\nDefine a function to find the exit using depth-first search\ndef dfs(current, visited):\nMark the current cell as visited\n  visited.add(current)\n\nBase case: If we've reached the destination, return True\n  if current == end:\n    return True\n\nTry all possible directions from the current cell\n  for delta in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n    next_cell = (current[0] + delta[0], current[1] + delta[1])\n    if is_valid_cell(next_cell) and next_cell not in visited:\n      if dfs(next_cell, visited):\n        return True\n\nIf we couldn't find the exit from this cell, backtrack to the previous cell\n  return False\n\nCall the depth-first search function with the starting point and an empty set of visited cells\nvisited = set()\nif dfs(start, visited):\n  print(\"I found the exit!\")\nelse:\n  print(\"I couldn't find the exit.\")\n\nSliding Window\n\nUsage: Use when need to handle the input data in specific window size.\n\nsliding-image\n\nExample: Sliding window technique to find the largest sum of 4 consecutive numbers.\n\nTemplate:\n\nwhile j  k:\n        while condition > k:\n            i+=1 # remove calculation for i\n        j+=1\nreturn ans\n\nExamples\n\nProblem: Find the largest sum of K consecutive entries, given an array of size N\n\nAdd the first K components together and save the result in the currentSum variable. Because this is the first sum, it is also the current maximum; thus, save it in the variable maximumSum.\nAs the window size is ww, we move the window one place to the right and compute the sum of the items in the window.\nUpdate the maximum if the currentSum is greater than the maximumSum, and repeat step 2.\n\ndef max_sum(arr, k):\n\tn = len(arr)    # length of the array\n\nlength of array must be greater\nwindow size\n\tif n Usage: Use two pointers to iterate the input data. Generally, both pointers move in the opposite direction at a constant interval.\n\ntwo-pointers.jpg\n\nPractice questions\nTwo pointers intro\n\nBacktracking\n\nBased on Depth-first search (DFS)\n\nUsage:\n\nFinding all permutations, combinations, subsets and solving sudoku are classic combinatorial problems.\n\nImagine you are trying to solve a puzzle, like a Sudoku. When you are solving a puzzle, sometimes you reach a point where you can't make any more progress using the current path. That's when you need to backtrack.\n\nBacktracking is a general algorithmic technique that is used to find all (or some) solutions to a problem by incrementally building candidates, and checking if the candidate is feasible or not. If the candidate is not feasible, the algorithm goes back (backtracks) to the previous step and tries again with a different candidate. The process continues until a solution is found, or all candidates have been tried.\n\nBacktracking is an algorithmic technique for solving problems recursively by trying to build a solution incrementally, one piece at a time, removing those solutions that fail to satisfy the constraints of the problem at any point of time.\n\nBacktracking algorithm is derived from the Recursion algorithm, with the option to revert if a recursive solution fails, i.e. in case a solution fails, the program traces back to the moment where it failed and builds on another solution. So basically it tries out all the possible solutions and finds the correct one.\nBacktracking == DFS on a tree\n\nHowto:\n\nBacktracking is drawing tree\nWhen drawing the tree, bear in mind:\n   how do we know if we have reached a solution?\n   how do we branch (generate possible children)?\n\n\nExample:\n\nLet's say we want to generate all possible combinations of 1, 2, and 3 of length 2. The possible combinations are: (1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2).\n\nThis process generates all possible combinations of length k:\n\ndef backtrack(nums, path, res, k):\nnums: the list of available numbers\npath: the current path of selected numbers\nres: the list of all valid combinations\nk: the length of each combination\n\n    if len(path) == k: # base case\n        res.append(path[:])\n        return\n\n    for i in range(len(nums)):\n        path.append(nums[i])\n        backtrack(nums[:i] + nums[i+1:], path, res, k)\n        path.pop()\n\n\nnums = [1, 2, 3]\nk = 2\nres = []\nbacktrack(nums, [], res, k)\nprint(res)\n[[1, 2], [1, 3], [2, 1], [2, 3], [3, 1], [3, 2]]\n\nAlgorithm:\n\nWe start with an empty path and empty result list.\nWe loop through the available numbers (1, 2, 3) and add the first number to the path.\nWe make a recursive call to backtrack with the remaining numbers (2, 3) and a path that includes the first number (e.g., [1]). This adds all possible combinations of length k-1 with the first number.\nAfter the recursive call, we remove the first number from the path.\nWe repeat this process for the other available numbers, generating all possible combinations of length k.\nWhen we reach the base case (len(path) == k), we add the current path to the result list.\nWe return the result list of all possible combinations.\n\nThe base case is when the length of the path is equal to k. At this point, we add the current path to the result list and return.\n\nThe recursive case involves looping through the available numbers, adding the current number to the path, making a recursive call with the remaining numbers, and removing the current number from the path after the recursive call.\n\npath:\n\nIn the backtrack function, path refers to the list of numbers that have been selected so far to form a valid combination.\n\nInitially, path is an empty list []. In each recursive call, a number from nums is selected and added to path.\n\nFor example, if nums = [1, 2, 3] and the current path is [1], the function will call backtrack([2, 3], [1], res, k) to consider all possible combinations with 1 in the first position, followed by all possible combinations of length k-1 of [2, 3] in the second position.\n\nOnce all possible combinations with 1 in the first position have been explored, the number 1 will be removed from path, and the function will try the next number from nums, which in this case is 2. The function continues in this way until all valid combinations of length k have been found and added to the res list.\n\nProblem examples:\n\nLeetCode 17. Letter Combinations of a Phone Number\n[LeetCode 22. Generate Parentheses]\n[LeetCode 46. Permutations]\n\nExample of LeetCode 78 problem:\n\n\n\n\n\nAlgorithm:\n\nWe define a backtrack function named backtrack(first, curr) which takes the index of first element to add and a current combination as arguments.\n\nIf the current combination is done, we add the combination to the final output.\n\nOtherwise, we iterate over the indexes i from first to the length of the entire sequence n.\n\n    Add integer nums[i] into the current combination curr.\n    Proceed to add more integers into the combination: backtrack(i + 1, curr).\n    Backtrack by removing nums[i] from curr.\n\nclass Solution:\n    def subsets(self, nums: List[int]) -> List[List[int]]:\n        def backtrack(first = 0, curr = []):\nif the combination is done\n            if len(curr) == k:\n                output.append(curr[:])\n                return\n            for i in range(first, n):\nadd nums[i] into the current combination\n                curr.append(nums[i])\nuse next integers to complete the combination\n                backtrack(i + 1, curr)\nbacktrack\n                curr.pop()\n\n        output = []\n        n = len(nums)\n        for k in range(n + 1):\n            backtrack()\n        return output\n\n[    [],\n[1], [3], [4],\n[1, 3], [1, 4], [3, 4],\n[1, 3, 4]\n]\n\nDutch National Flag problem\n\nThe Dutch National Flag problem is a sorting problem that asks us to sort an array of colors, like a bunch of different colored socks. We want to put all the socks of the same color together in the array.\n\nThe colors in this problem are represented by numbers. We use the numbers 0, 1, and 2 to represent the colors red, white, and blue. So, we have an array of numbers, and we want to sort them in such a way that all the 0's are at the beginning of the array, then all the 1's, and finally all the 2's are at the end.\n\nFor example, if we have an array [2, 0, 2, 1, 1, 0], we want to sort it so that it becomes [0, 0, 1, 1, 2, 2].\n\nOne way to solve this problem is to use a technique called the Dutch National Flag algorithm. The idea behind this algorithm is to use three pointers: a low pointer, a mid pointer, and a high pointer.\n\nThe low pointer starts at the beginning of the array, the high pointer starts at the end of the array, and the mid pointer starts at the beginning of the array.\n\nWe then iterate through the array with the mid pointer.\nIf the value at the mid pointer is 0, we swap it with the value at the low pointer and increment both pointers. - If the value at the mid pointer is 1, we leave it where it is and just increment the mid pointer.\nIf the value at the mid pointer is 2, we swap it with the value at the high pointer and decrement the high pointer.\n\nWe keep doing this until the mid pointer passes the high pointer, at which point the array is sorted.\n\nSo, in our sock example, we start with the low pointer at the beginning of the array, the mid pointer also at the beginning of the array, and the high pointer at the end of the array. Then, we iterate through the array with the mid pointer, swapping socks as needed until the array is sorted by color.\n\nResources\n\nhttps://www.geeksforgeeks.org/learn-data-structures-and-algorithms-dsa-tutorial/\nhttps://algo.monster/templates\nhttps://interviewnoodle.com/grokking-leetcode-a-smarter-way-to-prepare-for-coding-interviews-e86d5c9fe4e1\ndata structures\nCompetitive Programming Library\nAlgorithms for Competitive Programming\nSolutions to Introduction to Algorithms Third Edition",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/codeforces/_index",
            "title": "Codeforces",
            "description": "Codeforces",
            "content": "\nPython template for contests\nCompetitive Programming Helper (cph) | VSCode extension\nCompetitive Programming | browser extension\n\nLinks\n\nPython Visualize/Debug code online\nPython collections.Counter\nhttps://github.com/archishmanghos/DSA-Contests/\nhttps://github.com/debochdilamo/Competative-Programming/tree/CodeForces-Solutions\nhttps://github.com/DilamoWondimu/Competative-programming/tree/main/CodeForces-Solutions\nhttps://github.com/hkirat/Algorithmic-Resources\nhttps://github.com/valentk777/Competitive-Programming/\nCompetitive Programming Library\n\n\nCodeforces rating",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/codeforces/827-div-4-1742",
            "title": "Round #827/1742 (Div. 4)",
            "description": "Codeforces Round #827 (Div. 4) / 1742",
            "content": "\n#TODO\n\nContest date: 2022-10-13\n\nContest problems\nEditorial\n\nA. Sum\n\nhttps://codeforces.com/contest/1742/problem/A\n\nSolution:\n\n\n\ndef run():\n    for _ in range(inpi()):\n        print(solve())\n\n\nif name == \"main\":\n    CODE_DEBUG = 0\n    if os.environ.get(\"CODE_DEBUG\") or CODE_DEBUG:\n        sys.stdin = open(\"./input.txt\", \"r\")\n        start_time = time.time()\n        run()\n        print(\"\\n--- %s seconds ---\\n\" % (time.time() - start_time))\n    else:\n        run()\n\nB. Increasing\n\nhttps://codeforces.com/contest/1742/problem/B\n\n\n\nSolution:\n\n\n\nExplanation from Codeforces:\n\nC. Stripes\n\nhttps://codeforces.com/contest/1742/problem/C\n\n\nSolution:\n\n\n\nExplanation from Codeforces:\n\nD. Coprime\n\nhttps://codeforces.com/contest/1742/problem/D\n\n\nSolution:\n\nE. Scuza\n\nhttps://codeforces.com/contest/1742/problem/E\n\nSolution:\n\nF. Smaller\n\nhttps://codeforces.com/contest/1742/problem/F\n\n\nSlow Solution:\n\nG. Orray\n\nhttps://codeforces.com/contest/1742/problem/G1\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/codeforces/835-div-4-1760",
            "title": "Round #835/1760 (Div. 4)",
            "description": "Codeforces Round #835 (Div. 4) / 1760",
            "content": "\n#TODO D E F G\n\nContest date: 2022-11-21\n\nContest problems\nEditorial\n\nA. Medium Number\n\nhttps://codeforces.com/contest/1760/problem/A\n\nSolution:\n\ndef solve():\n    ar = inpl()\n    ar.sort()\n    return ar[1]\n\n\ndef run():\n    for _ in range(inpi()):\n        print(solve())\n\n\nif name == \"main\":\n    CODE_DEBUG = 0\n    if os.environ.get(\"CODE_DEBUG\") or CODE_DEBUG:\n        sys.stdin = open(\"./input.txt\", \"r\")\n        start_time = time.time()\n        run()\n        print(\"\\n--- %s seconds ---\\n\" % (time.time() - start_time))\n    else:\n        run()\n\nB. Atilla's Favorite Problem\n\nhttps://codeforces.com/contest/1760/problem/B\n\n\nMinimum size of alphabet is the ordinal number of a letter in the string.\n\nGo through each letter and find the max letter's code with built-in function ord\n\nSolution:\n\ndef solve():\n    n = inp()\n    s = inp()\n\n    last_i = 0\n    for i in s:\n        last_i = max(last_i, ord(i) - 97 + 1) # for letter 'a' unicode code is 97\n\n    return last_i\n\nExplanation from Codeforces:\n\nTo solve the problem we need to find the character with the highest alphabetical order in our string, since Atilla will need at least that alphabet size and won't need more. To do this iterate through the string and find the character with the highest alphabetical order. Output the maximum alphabetical order found. The solution can be done in $𝑂(𝑛)$.\n\nC. Advantage\n\nhttps://codeforces.com/contest/1760/problem/C\n\nNeed to know the strongest participant.\nCompare current with strongest.\nIf current participant is the strongest, then compare it with the second strongest.\n\n\nSolution:\n\nFind two strongest.\nLoop and calculate diff.\n\ndef solve():\n    n = inp()\n    ar = inpl()\n\n    result = ar[:]\n\n    ar.sort()\n    strongest, strongest2 = ar[-1], ar[-1]\n    if len(ar) > 1:\n        strongest2 = ar[-2]\n\n    for i, x in enumerate(result):\n        if x == strongest:\n            result[i] = x - strongest2\n        else:\n            result[i] = x - strongest\n\n    return list_to_string_list(result)\n\nExplanation from Codeforces:\n\nMake a copy of the array 𝑠: call it 𝑡. Sort 𝑡 in non-decreasing order, so that 𝑡1 is the maximum strength and $𝑡_2$ — the second maximum strength.\n\nThen for everyone but the best person, they should compare with the best person who has strength $𝑡1$. So for all $𝑖$ such that $𝑠𝑖 ≠ 𝑡1$, we should output $𝑠𝑖−𝑡1$. Otherwise, output $𝑠𝑖−𝑡_2$ — the second highest strength, which is the next best person.\n\nD. Challenging Valleys\n\nhttps://codeforces.com/contest/1760/problem/D\n\n\nSolution:\n\nE. Binary Inversions\n\nhttps://codeforces.com/contest/1760/problem/E\n\nSolution:\n\nF. Quests\n\nhttps://codeforces.com/contest/1760/problem/F\n\n\nSlow Solution:\n\nG. SlavicG's Favorite Problem\n\nhttps://codeforces.com/contest/1760/problem/G\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/codeforces/849-div-4-1791",
            "title": "Round #849/1791 (Div. 4)",
            "description": "Codeforces Round #849 (Div. 4) / 1791",
            "content": "\n#TODO G2\n\nContest date: 2023-02-03\n\nContest problems\nEditorial\n\nA. Codeforces Checking\n\nhttps://codeforces.com/contest/1791/problem/A\n\nSolution:\n\ndef solve(letter):\n    code = \"codeforces\"\n    if letter in code:\n        print(\"YES\")\n    else:\n        print(\"NO\")\n\nfor _ in range(int(input())):\n    letter = input()\n    solve(letter)\n\nB. Following Directions\n\nhttps://codeforces.com/contest/1791/problem/B\n geometry, implementation, *800\n\nSolution:\n\ndef solve(n, s):\n    x = 0\n    y = 0\n    for move in s:\n        if move == 'L':\n            x -= 1\n        elif move == 'R':\n            x += 1\n        elif move == 'U':\n            y += 1\n        elif move == 'D':\n            y -= 1\n        if x == 1 and y == 1:\n            print(\"YES\")\n            break\n    else:\n        print(\"NO\")\n\nfor _ in range(int(input())): # attempts\n    num = int(input())\n    letter = input()\n    solve(num, letter)\n\nC. Prepend and Append\n\nhttps://codeforces.com/contest/1791/problem/C\nimplementation, two pointers, *800\n\nIn this problem we are allowed:\nto remove first letter of the binary string and last.\nWe can do this while first letter is not equal to the last according to the definition of the problem. (Add 𝟶 to one end of the string and 𝟷 to the other end of the string.)\n\nSolution:\n\ndef solve(n, s):\n    res = n\n    if res  0:\n        first = s[0]\n        last = s[-1]\n        if first == last:\n            return res\n        s = s[1:-1]\n        res -= 2\n    return res\n\nfor _ in range(int(input())): # attempts\n    n = int(input())\n    s = input()\n    print(solve(n, s))\n\nOptimized:\n\nUse deque from collection module\n\n    >>> deque('123')\n    deque(['1', '2', '3'])\n\nfrom collections import deque\n\ndef solve(n, s):\n    s = deque(s)\n    while len(s) and s[0] != s[-1]:\n        s.popleft()\n        s.pop()\n    print(len(s))\n\nD. Distinct Split\n\nhttps://codeforces.com/contest/1791/problem/D\n brute force, greedy, strings, *1000\n\nIf we get a string abcabcd we need to split into two strings.\n\nNote 1: result number could not be more than string length.\n\nabcabcd can be splited into abc and abcd.\n    len('abc') = 3 #3 distinct letters\n    len('abcd') = 4\n    3 + 4 = 7\n\nOutput: maximum possible value of 𝑓(𝑎)+𝑓(𝑏) such that 𝑎+𝑏=𝑠.\n\nLet's look at other examples and possible edge cases.\n\nNote 2: function for a string 𝑥 - is the number of distinct characters. (From problem statement.)\n\n    'aaaaa' => 1 # 1 distinct\n\nIf we concatenate two strings into one s, we need to keep order of the letters.\nNeed to split this string s in a such a way so that repeated letters fall into different parts of string s (a and b).\n\nabcab => abc and ab better that abca and b because:\n\n    f('abc') = 3    f('abca') = 3\n    f('ab') = 2     f('b') = 1\n              5   > 4\n\nApproach 1:\n\nDivide string into two parts starting left part from len 1. For example:\n    s = 'abcabc'\n    a = 'a'\n    b = 'bcabc'\nIncrease left part and decrease right part.\n   On each step calculate sum of distinct letters. For example:\n        set(a) + set(b)\n        max_result = max(max_result, len(set(a)) + len(set(b)))\n\nSolution:\n\ndef solve():\ninput\n    n = int(input())\n    s = input()\n\n    point = 0\n    max_n = 0\n    a_set = set(s[point])\n    b_set = set(s[point+1:])\n    while point   dp, greedy, sortings, *1100\n\nExplanation from Codeforces:\n\nWe can notice that by performing any number of operations, the parity of the count of negative numbers won't ever change.\n\nThus, if the number of negative numbers is initially even, we can make it equal to 0 by performing some operations.\n\nSo, for an even count of negative numbers, the answer is the sum of the absolute values of all numbers (since we can make all of them positive). And if the count of negative numbers is odd, we must have one negative number at the end.\n\nWe will choose the one smallest by absolute value and keep the rest positive (for simplicity, we consider −0 as a negative number).\n\nSolution:\n\ndef solve():\n    n = inpi()\n    a = inpl()\n\n    count_minus = 0\n    count_zeros = 0\n    min_val = abs(a[0])\n    s = 0 # sum\n\n    for i in range(n):\n        if a[i] i** thrice, it won't change after any further operations.\n\nProblem here is to implement a solution how to save information how many times there was a change of a[i].\n\nOne way is to use Fenwick Tree\n\nUse FenwickTree template\nSave there count of a[i] changes. No need to calculate more than 3 times\nWhen need to print a[x] calculate up to three times and print result.\n\n\nclass Fenwick: #also known as Binary Indexed Tree (BIT)\nno need to change here anything\n    def init(self, n):\n        self.n = n\n        self.bit = [0] * (n+1)\n\n    def add(self, idx, val):\n        while idx  0:\n            ret += self.bit[idx]\n            idx -= idx & -idx\n        return ret\n\n    def range_sum(self, l, r):\nReturn the sum of the elements from l (inclusive) to r (exclusive)\n        return self.prefix_sum(r - 1) - self.prefix_sum(l - 1)\n\n    def prefix_sum(self, x):\nreturn sum upto and including element x\n        z = x\n        res = 0\n        while z >= 0:\n            res += self.bit[z]\nStrip trailing zeros from z, and then take away one\n            z = (z & (z + 1)) - 1\n        return res\n\n\ndef solve():\n    n, q = list(map(int, input().split()))\n    a = list(map(int, input().split()))\n    tree = BIT(n)\n    while q:\n        q -= 1\n        t, *params = map(int, input().split())\n        if t == 1:\n            l, r = params\n            tree.add(l, 1)\n            tree.add(r + 1, -1)\n        else:\n            x, = params\n            need = min(3, tree.query(x)) # get count of times need to change a[i]\n            cur = a[x - 1]\n            for i in range(need):\n                cur = sum_of_digits(cur)\n            print(cur)\n\nGood to know\n\nSegment Tree template tutorial\nA Visual Introduction to Fenwick Tree | medium\nFenwick Tree | cp-algorithms\nSegment Tree | cp-algorithms\nДерево отрезков | algorithmica\nДерево Фенвика | algorithmica\nДерево Фенвика | habr\n\nG1. Teleporters (Easy Version)\n\nhttps://codeforces.com/contest/1791/problem/G1\n greedy, sortings *1100\n\nStatement:\n\nYou are playing a game where you are given a list of teleporters (0,2,…,𝑛), each located at a point on the number line. The number line includes all integers from 0 to n.\n\nAt point i, you can do one of three actions:\nMove left one unit: this action costs 1 coin.\nMove right one unit: this action also costs 1 coin.\nUse a teleporter at point i: this action costs $a_i$ coins. When you use a teleporter, you are immediately teleported back to point 0.\nLast statement in the problem means that at any point i on the number line, you have the option to use a teleporter located at that point, which will immediately transport you back to point 0. However, using a teleporter costs a certain number of coins, denoted by $a_i$. For example, if you are at point i = 5, and you use the teleporter located at point 5, you will be immediately transported back to point 0, but you will have to pay the cost $a_i$ in coins to do so.\nEssentially, the teleporters provide a way for you to quickly move back to the starting point (point 0) without having to take multiple steps, but this convenience comes at a cost. You can only use each teleporter once, and you must have enough coins to pay the cost of using it.\n\nYou start at point 0, and you have c coins to spend. Your goal is to use as many teleporters as possible while still having at least 1 coin left over. You cannot use a teleporter more than once.\n\nWrite a function max_teleporters(n, c, a) that takes in\nthe length of the number line n\nthe number of coins you have c\nthe list of teleporter costs a.\n\nThe function should return the maximum number of teleporters you can use given your starting position at 0 and the number of coins you have to spend.\n\nYou may assume that n, c, and all elements of a are positive integers.\n\nFor example, the following input should return 2:\n\nn = 8\nc = 32\na = [100, 52, 13, 6, 9, 4, 100, 35]\n\nmax_teleporters(n, c, a) => 2\n\nWill use greedy algorithm.\n\nApproach:\n\nGiven input: n = 8, c = 32, a = [100, 52, 13, 6, 9, 4, 100, 35]\n\nStep 1: Calculate the cost of each teleporter\n\n  For each teleporter at position i, add the index i and the cost a[i]\n  The resulting array cost contains the costs to use each teleporter, accounting for the cost of moving to that teleporter's position: [101, 54, 16, 10, 14, 10, 107, 43]\n\nStep 2: Sort the cost array in ascending order\n\n  Sorting the array ensures that we use the cheapest teleporters first\n\nStep 3: Compute the maximum number of teleporters we can use\n\n  Initialize a variable used to 0 to keep track of how many teleporters we've used\n  Iterate over the sorted cost array, adding the cost of each teleporter to a running total total\n  If the current total is less than or equal to c, we can use the current teleporter, so increment used\n  If the current total is greater than c, we've run out of coins and can't use any more teleporters, so exit the loop\n  Return the final value of used\n\nStep 4: Return the maximum number of teleporters we can use\n\n  The maximum number of teleporters we can use is the value of used computed in Step 3: 2\n\n\nSolution:\n\ndef max_teleporters(n, c, a):\nFirst, we add the index of each teleporter to its cost, so that we can easily\ncalculate the cost to reach each teleporter from the starting position (0).\n    for i in range(n):\n        a[i] += i + 1\n\nWe then sort the list of teleporter costs in ascending order, so that we can\nuse the cheapest teleporters first.\n    a.sort()\n\nWe iterate over the sorted list of teleporter costs, using as many teleporters\nas possible while still having at least one coin remaining. We keep track of the\nnumber of teleporters used in the \"used\" variable.\n    used = 0\n    for cost in a:\n        if cost   binary, search, greedy, sortings *1900\n\nExplanation from Codeforces:\n\nPlease also refer to the tutorial for the easy version.\n\nIf we are not at the first taken portal, the problem is still independent for each portal, but this time the cost of a portal is $𝑚𝑖𝑛(𝑎𝑖+𝑖,𝑎𝑖+𝑛+1−𝑖)$ (since we can come to a portal either from point 0 or point $𝑛+1$).\n\nSo, we again sort the portals by their costs. But this time, we need to make sure that the first taken portal is taken from point 0, so we will iterate over all portals and check the maximum amount of portals we can take if we use it as the first one.\n\nWe can check this using prefix sums over the minimum cost array and binary searching, checking if the amount of considered portals taken doesn't exceed the number of coins we initially have (we also have to deal with the case when the portal we are considering is included both times as the initial portal and in the minimum cost prefix).",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/codeforces/cp-template",
            "title": "Python template for contests",
            "description": "Python template for contests",
            "content": "\n\nimport math\nimport os\nimport sys\nimport time\nfrom collections import defaultdict, Counter\n\n\nINF = sys.maxsize\n\n------ INPUTS\n\ninp = lambda: sys.stdin.readline().strip().rstrip(\"\\r\\n\") #read line as string. Ex: input 1 2 3 => '1 2 3'\ninpi = lambda: int(inp()) #read input as integer. input 1 => 1\ninpl = lambda: list(map(int, inp().split())) #read line as list of integers. Ex: [1, 2, 3]\ninp_strl = lambda: list(inp().split()) #read line as list of strings. Ex: ['1', '2', '3']\n\nlist_to_string = lambda _v: \"\".join(map(str, _v)) # [1,2,3] => '123'\nlist_to_string_list = lambda _v: \" \".join(map(str, _v)) # [1,2,'3'] => '1 2 3'\n\n------ SOLUTION\n\ndef solve():\ninput\n    return\n\ndef run():\nsolve()\n    print(solve())\n\nif name == \"main\":\n    CODE_DEBUG = 0\n    if os.environ.get(\"CODE_DEBUG\") or CODE_DEBUG:\n        sys.stdin = open(\"./input.txt\", \"r\")\n        start_time = time.time()\n        run()\n        print(\"\\n--- %s seconds ---\\n\" % (time.time() - start_time))\n    else:\n        run()\n`",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/_index",
            "title": "Data Structures",
            "description": null,
            "content": "\nTree\n\nclass Node:\n    def init(self, value):\n        self.value = value\n        self.children = {}\n\n    def insert(self, s, idx):\nidx: index of the current character in s\n        if idx != len(s):\n            self.children.setdefault(s[idx], Node(s[idx]))\n            self.children.get(s[idx]).insert(s, idx + 1)\n\nFenwick Tree\n\nclass Fenwick: #also known as Binary Indexed Tree (BIT)\n    def init(self, n):\n        self.n = n\n        self.bit = [0] * (n+1)\n\n    def add(self, idx, val):\n        while idx  0:\n            ret += self.bit[idx]\n            idx -= idx & -idx\n        return ret\n\n    def range_sum(self, l, r):\nReturn the sum of the elements from l (inclusive) to r (exclusive)\n        return self.prefix_sum(r - 1) - self.prefix_sum(l - 1)\n\n    def prefix_sum(self, x):\nreturn sum upto and including element x\n        z = x\n        res = 0\n        while z >= 0:\n            res += self.bit[z]\nStrip trailing zeros from z, and then take away one\n            z = (z & (z + 1)) - 1\n        return res\n\n\nA Visual Introduction to Fenwick Tree | medium\nFenwick Tree\nДерево Фенвика | algorithmica\nДерево Фенвика | habr\n\nResources\n\ndata structures\ndata structures tutorial | programiz\nCompetitive Programming Library\nAlgorithms for Competitive Programming\n",
            "tags": [
                "Data Structures"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/binary-tree",
            "title": "Binary Tree",
            "description": "Binary Tree",
            "content": "\n1. What is a Binary Tree?\n\nA binary tree is a data structure in which each node has at most two children, which are referred to as the left child and the right child.\n\n2. Representing a Binary Tree in Python\n\nTo represent a binary tree in Python, we can create a class called Node to represent each node in the tree. Each node will have a value and references to its left and right children.\n\nIf a node doesn't have a left or right child, the reference will be set to None. Here's an example implementation:\n\nclass Node:\n    def init(self, value, left=None, right=None):\n        self.value = value\n        self.left = left\n        self.right = right\n\nOn this step None will look like this:\n\n3. Adding Nodes to a Binary Tree\n\nOnce we have a representation of a node, we can start adding nodes to the tree to create the structure of the binary tree.\n\nTo add a node, we need to find the correct position in the tree where the new node should be added. This is typically done by starting at the root node and comparing the value of the new node to the value of the current node.\n\nIf the new node's value is less than the current node's value, we move to the left child. If the new node's value is greater than the current node's value, we move to the right child. We repeat this process until we find a position where there is no left or right child (i.e., the current node is a leaf node), and we can add the new node there.\n\nHere's an example implementation of a function to add a node to a binary tree:\n\ndef insert(root, value):\n    if root is None:\n        return Node(value) # basicaly create a new root Node\n    if value Visualize Binary Tree\n\n4. Full Binary Tree Class\n\nclass Node:\n    def init(self, value):\n        self.value = value\n        self.left = None\n        self.right = None\n\n    def insert(self, value):\n        if self.value:\n            if value  self.value:\n            if self.right is None:\n                return False\n            else:\n                return self.right.search(value)\n        else:\n            return True\n\n    def remove(self, value, parent=None):\n        if value  self.value:\n            if self.right:\n                self.right.remove(value, self)\n        else:\n            if self.left is None and self.right is None:\n                if parent:\n                    if parent.left == self:\n                        parent.left = None\n                    else:\n                        parent.right = None\n                else:\n                    self.value = None\n            elif self.left and self.right is None:\n                if parent:\n                    if parent.left == self:\n                        parent.left = self.left\n                    else:\n                        parent.right = self.left\n                else:\n                    self.value = self.left.value\n                    self.right = self.left.right\n                    self.left = self.left.left\n            elif self.right and self.left is None:\n                if parent:\n                    if parent.left == self:\n                        parent.left = self.right\n                    else:\n                        parent.right = self.right\n                else:\n                    self.value = self.right.value\n                    self.left = self.right.left\n                    self.right = self.right.right\n            else:\n                min_larger_node = self.right\n                while min_larger_node.left:\n                    min_larger_node = min_larger_node.left\n                self.value = min_larger_node.value\n                if self.right == min_larger_node:\n                    self.right = min_larger_node.right\n                else:\n                    min_larger_node.parent.left = min_larger_node.right\n\nLinks\n\nVisualize Binary Tree",
            "tags": [
                "Data Structures"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/data-structures/segment-tree",
            "title": "Segment Tree",
            "description": null,
            "content": "\nA Segment Tree is a data structure used for efficiently processing queries over intervals or ranges. It is commonly used for solving problems that involve finding information about a set of elements and their sub-intervals.\n\nSince a Segment Tree is a binary tree, a simple linear array can be used to represent the Segment Tree. Before building the Segment Tree, one must figure what needs to be stored in the Segment Tree's node?.\n\nEach leaf node represents a single element, and each internal node represents the union of its children's ranges. Queries are performed by traversing the tree from the root to the leaves, and aggregating information about the ranges that intersect with the query interval.\n\nExample 1, consider a set of numbers and the task of finding the minimum value in a range of these numbers. The Segment Tree can be used to solve this problem by storing the minimum value of each range in the tree's internal nodes, and answering queries by searching for the smallest value in the portion of the tree that covers the query interval.\n\nExample 2, if the question is to find the sum of all the elements in an array from L indices to R, then at each node (except leaf nodes) the sum of its children nodes is stored.\n\nThe Segment Tree can be constructed in O(n log n) time, where n is the number of elements in the original set, and it can answer queries in O(log n) time. This makes it an efficient data structure for processing queries over large datasets.\n\nTo study the topic \"segment tree\" you need to know the following concepts:\n\narrays\nloops\nconditional operators\nbitwise operations\n\nA Segment Tree is a dynamic data structure used to perform operations on and update intervals. It supports two operations: Element update (update) on a given range and request (query) on the sum of elements in a given range.\n\nLet's perform the following task: we have an array and we want to find the sum of the elements in a given range.\n\nFor this task, we can use a segment tree. It is constructed as a binary tree, where each node represents an interval and the value of the node is the sum of the elements in that interval.\n\nFundamentals:\n\nDefinition of a sum element in a segment tree:\n\nA sum element in a segment tree is the sum of all elements in the range it represents.\n\nConstructing a segment tree:\n\nA segment tree can be constructed from an array of numbers. Each node in the tree represents a range of elements in the array and stores the sum of the elements in that range.\n\nImplementation of operations:\n\nThe implementation of various operations in a segment tree essentially depends on its structure. However, there are several operations that are often used in various tasks:\n\nUpdate** value in the array: This operation allows you to change the value of an element in an array. It is usually implemented using a recursive tree traversal.\n\nQuery Value**: This operation allows you to query the value of an element in an array. It is also usually implemented using recursive tree traversal.\n\nQuery for a sum**: This operation allows you to query the sum of values in an array on a given interval. It is usually implemented by recursive tree traversal and sum counting\n\nBuilding a spanning tree\n\nSince the tree is binary, each vertex will have up to two descendants.\n\nGraphically it looks as follows (for an array of 8 elements):\n\nsegment-tree-structure.png\n\nAt the topmost vertex the segment from the beginning of the array to the last element is fixed.\n\nOn the left is the left half of the parent ([0 1 2 3]). On the right is the right half ()[4 5 6 7]). And so on up to the last node with a segment of one element.\n\nTake the array a = [1, 3, -2, 8, -7]. We use it to build a tree of segments to write the sums of these segments in each node.\n\nThe structure of such a tree is as follows:\n\nsegment-tree-sum.png\n\n💡 The tree contains less than 2n vertices. 2*n-1\n\nThe number of vertices in the worst case is estimated by the sum $n + \\frac{n}{2} + \\frac{n}{4} + \\frac{n}{8} + \\ldots + 1 > array = [1, 3, -2, 8, -7]\n> build_tree(array)\n\n  4 [0, 0, 0, 0, 1, 1, 3, -2, 8, -7]\n  3 [0, 0, 0, 1, 1, 1, 3, -2, 8, -7]\n  2 [0, 0, 2, 1, 1, 1, 3, -2, 8, -7]\n  1 [0, 3, 2, 1, 1, 1, 3, -2, 8, -7]\n  0 [3, 3, 2, 1, 1, 1, 3, -2, 8, -7]\n\nCalculating the sum on the segment:\n\nThe function gets the indexes of the original array.\n\nWhen we created the tree from the source array, we placed each individual element on the new index [n + i].\n\n💡 So when the function takes an index, we first find the bottommost element in the tree. It is located in the new array by the index [length_of_source_array + index]\n\ncalculate the sum on the segment\ndef query_tree(l, r):\n  global tree, n\n\n  sum = 0\n  l += n # current item index\n  r += n\n  while l > a = [1, 3, -2, 8, -7]\n> n = len(a)\n> tree = build_tree(a)\n> query_tree(0, 4) # sum([1, 3, -2, 8, -7])\n3\n> query_tree(1, 3) # sum([3, -2, 8])\n9\n> query_tree(4, 4)\n-7\n\n\nWe get the SegmentTree class:\n\nSum function or any other function can be turned on at the time of tree generation\n\nclass SegmentTree:\n    def init(self, a):\n        self.n = len(a)\n        self.tree = [0] * 2 * self.n\n        for i in range(self.n):\n            self.tree[self.n + i] = a[i]\n        for i in range(self.n - 1, 0, -1):\n            self.tree[i] = self.tree[2i] + self.tree[2i+1]\n\n    def calculate_sum(self, l, r):\n        sum = 0\n        l += self.n\n        r += self.n\n        while l >= 1\n    while idx:\n        self.data[idx] = self._func(self.data[2 * idx], self.data[2 * idx + 1])\n        idx >>= 1\n\ndef len(self):\n    return self._len\n\ndef query(self, start, stop):\n    \"\"\"func of data[start, stop)\"\"\"\n    start += self._size\n    stop += self._size\n    if start==stop:\n        return self._default\n    res_left = res_right = self._default\n    while start >= 1\n        stop >>= 1\n\n    return self._func(res_left, res_right)\n\ndef repr(self):\n    return \"SegmentTree({0})\".format(self.data)\n\nThe build_tree method builds a segment tree, and query allows you to perform query operations.\n\nLinks\n\nhttps://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/",
            "tags": [
                "Data Structures"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/helpers",
            "title": "Helpers",
            "description": "LeetCode Cookbook - Help functions",
            "content": "\nCreate Linked List\n\nDefinition for singly-linked list:\n\nclass Node:\n    def init(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\nvalues = [2, 4, 3]\n\ndef create_linked_node(values):\n    head = Node(values[0]) # start node, head of the linkedlist\n    current = head   # current node in the linked list where we change/add next node\n    for i in values[1:]:\n        node = Node(i)\n        current.next = node\n        current = current.next  # now current node is last created\n    return head\n\nlinked_list = create_linked_node(values)\n\nObject structure:\n\ncreate linkedlist in python\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/plan",
            "title": "Plan",
            "description": null,
            "content": "\nQ1\n\n|   #   | Problem                                                                                                                  | Difficulty | Topics                                                                                                                                 |\n| :---: | ------------------------------------------------------------------------------------------------------------------------ | ---------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n|   1   | 1. Two Sum                                                                                 | Easy       | Array, Hash Table                                                                             |\n|   2   | 13. Roman to Integer                                                             | Easy       | Hash Table, Math, String                                                    |\n|   3   | 14. Longest Common Prefix                                                   | Easy       | String                                                                                                              |\n|   4   | 20. Valid Parentheses                                                           | Easy       | String, Stack                                                                                     |\n|   5   | 21. Merge Two Sorted Lists                                                 | Easy       | Linked List, Recursion                                                                   |\n|   6   | 26. Remove Duplicates from Sorted Array                       | Easy       | Array, Two pointers                                                                         |\n|   7   | 66. Plus One                                                                             | Easy       | Array, Math                                                                                         |\n|   8   | 69. Sqrt(x)                                                                                 | Easy       | Math, Binary Search,                                                                        |\n|   9   | 70. Climbing Stairs                                                               | Easy       | Math, Dynamic Programming, Memoization                        |\n|  10   | 88. Merge Sorted Array                                                         | Easy       | Array, Two pointers, Sorting                                            |\n|  11   | 94. Binary Tree Inorder Traversal                                   | Easy       | Stack, Tree, Depth-First Search, Binary Tree |\n|  12   | 2. Add Two Numbers                                                               | Medium     | Linked List, Math, Recursion                                            |\n|  13   | 3. Longest Substring Without Repeating Characters | Medium     | Hash Table, String, Sliding Window                                |\n|  14   | 5. Longest Palindromic Substring                                   | Medium     | String, Dynamic Programming                                                         |\n|  15   | 7. Reverse Integer                                                               | Medium     | Math                                                                                                                  |\n|  16   | 11. Container With Most Water                                         | Medium     | Array, Two pointers, Greedy                                              |\n|  17   | 15. 3Sum                                                                                   | Medium     | Array, Two pointers, Sorting                                            |\n|  18   | 17. Letter Combinations of a Phone Number                 | Medium     | Hash Table, String, Backtracking                                    |\n|  19   | 19. Remove Nth Node From End of List                           | Medium     | Linked List, Two pointers                                                             |\n|  20   | 22. Generate Parentheses                                                   | Medium     | String, Dynamic Programming, Backtracking                  |\n\nIntermediate results\n\nAppeared intuitive understanding of algorithms.\nIn most cases, one hour is not enough to solve the problem.\n\nIf you start sketching an intuitive algorithm, then in the process comes an understanding and an improved solution.\n\nUpdate plan by solution:\nAfter reading, if there is no exact solution:\nAssume/analyze/draw the proposed algorithm\nView solutions with explanations\nCompare with your own / analyze\nCode\n\nQ2\n\n|   #   | Problem                                                                                                                                      | Difficulty | Topics                                                                                                                          |\n| :---: | -------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------------------------------------------------------------------------------------------------------------------------- |\n|  21   | 28. Find the Index of the First Occurrence in a String                 | Medium     | String, Two pointers, String Matching                   |\n|  22   | 29. Divide Two Integers                                                                         | Medium     | Math, Bit Manipulation                                                            |\n|  23   | 33. Search in Rotated Sorted Array                                                  | Medium     | Array, Binary Search,                                                               |\n|  24   | 34. Find First and Last Position of Element in Sorted Array | Medium     | Array, Binary Search,                                                               |\n|  25   | 36. Valid Sudoku                                                                                       | Medium     | Array, Hash Table, Matrix                                           |\n|  26   | 38. Count and Say                                                                                     | Medium     | String                                                                                                       |\n|  27   | 46. Permutations                                                                                       | Medium     | Array, Backtracking                                                                  |\n|  28   | 48. Rotate Image                                                                                       | Medium     | Array, Math, Matrix                                                       |\n|  29   | 49. Group Anagrams                                                                                   | Medium     | Array, Hash Table, String, Sorting              |\n|  30   | 50. Pow(x, n)                                                                                                       | Medium     | Math, Recursion                                                                          |\n|  31   | 53. Maximum Subarray                                                                                                | Medium     | Array, Divide and Conquer, Dynamic Programming |\n\nIntermediate results\n\nRevise training tactics. Prepare list of top coding patterns.\n\nPractice on each coding pattern.\n\nQ3\n\n|   #   | Problem                                        | Difficulty | Topics                                                                                                               |\n| :---: | ---------------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------- |\n|  32   | 55. Jump Game         | Medium     | Array, Greedy, Dynamic Programming              |\n|  33   | 56. Merge Intervals   | Medium     | Array, Sorting,                                                                |\n|  34   | 62. Unique Paths      | Medium     | Math, Dynamic Programming, Combinatorics] |\n|  35   | 73. Set Matrix Zeroes | Medium     | Array, Hash Table, Matrix                                |\n|  36   | 75. Sort Colors       | Medium     | Array, Two Pointers, Sorting                          |\n|  37   | 78. Subsets           | Medium     | Array, Backtracking,  Bit Manipulation       |\n\nStarted participating in contests.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/_index",
            "title": "LeetCode Problems",
            "description": null,
            "content": "\nPython template\n\nTips I learned\n\nDon't code before \"design\" and understand the solution algorithm\nFirst 20 problems. Spent 5 min for thinking. Next look for 2-3 different approaches. Understand. Decide. Write.\n\nResources\n\nhttps://walkccc.me/LeetCode/problems/\nhttps://books.halfrost.com/leetcode/\nhttps://grandyang.com/leetcode/\nCompetitive Programmer's Handbook\n",
            "tags": [
                "leetcode"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/_index",
            "title": "Easy",
            "description": null,
            "content": "",
            "tags": [
                "leetcode"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/1-two-sum",
            "title": "1. Two Sum",
            "description": "LeetCode 1. Two Sum",
            "content": "\nLeetCode problem\n\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\n\nYou can return the answer in any order.\n\nExample 1:\n\n    Input: nums = [2,7,11,15], target = 9\n    Output: [0,1]\n    Explanation: Because nums[0] + nums[1] == 9, we return [0, 1].\n\nTopics:\narray, hash table\nIf we fix one of the numbers, say x, we have to scan the entire array to find the next number y which is value - x where value is the input parameter. Can we change our array somehow so that this search becomes faster?\nThe second train of thought is, without changing the array, can we use additional space somehow? Like maybe a hash map to speed up the search?\n\nFirst accepted\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        nums_set = set(nums)  # to search in a O(1)\n        while nums:\n            n1 = nums.pop() # get last\n            n2 = target - n1\n\n            if n1 == n2:\n                if n2 in nums:\n                    return [len(nums), nums.index(n2)]\n            elif n2 in nums_set:\n                return [len(nums), nums.index(n2)]\n\nfaster than 64.51%\nMemory Usage: 15.5 MB, less than 9.15%\n\nTime complexity:\nset(nums): O(n)\nwhile nums: O(n)\nnums.pop: O(1)\nnums.index: O(n)\n\nBetter solution\nremember indexes of \"passed\" n's from nums\n\nclass Solution:\n    def twoSum(self, nums: List[int], target: int) -> List[int]:\n        hashmap = {}\n        for idx, n1 in enumerate(nums):\n            n2 = target - n1\n            if n2 in hashmap:\n                return [idx, hashmap[n2]]\n            hashmap[n1] = idx\n\nfaster than 53.00%\nMemory Usage: 15.1 MB, less than 52.14%\n",
            "tags": [
                "Array",
                "Hash Table"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/13-roman-to-integer",
            "title": "13. Roman to Integer",
            "description": "Leetcode 13. Roman to Integer - Solution",
            "content": "\nLeetCode problem\n\nRoman numerals are represented by seven different symbols: I, V, X, L, C, D and M.\n\n    Symbol       Value\n    I             1\n    V             5\n    X             10\n    L             50\n    C             100\n    D             500\n    M             1000\n\nFor example, 2 is written as II in Roman numeral, just two ones added together. 12 is written as XII, which is simply X + II. The number 27 is written as XXVII, which is XX + V + II.\n\nRoman numerals are usually written largest to smallest from left to right. However, the numeral for four is not IIII. Instead, the number four is written as IV. Because the one is before the five we subtract it making four. The same principle applies to the number nine, which is written as IX. There are six instances where subtraction is used:\n\nI can be placed before V (5) and X (10) to make 4 and 9.\nX can be placed before L (50) and C (100) to make 40 and 90.\nC can be placed before D (500) and M (1000) to make 400 and 900.\n\nGiven a roman numeral, convert it to an integer.\n\nExample 1:\n\n    Input: s = \"III\"\n    Output: 3\n    Explanation: III = 3.\n\nExample 2:\n\n    Input: s = \"MCMXCIV\"\n    Output: 1994\n    Explanation: M = 1000, CM = 900, XC = 90 and IV = 4.\n\nFirst accepted\n\ntest-case\n\nclass Solution:\n    def romanToInt(self, s: str) -> int:\n        dict = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000}\n        n_sum = 0\n        prev = 0\n        for c in reversed(s):\n            n = dict[c]\n            n = -n if n in (1,10,100) and prev in (n5, n10) else n\n            n_sum += n\n            prev = abs(n)\n        return n_sum\n",
            "tags": [
                "Hash Table",
                "Math",
                "String"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/14-longest-common-prefix",
            "title": "14. Longest Common Prefix",
            "description": "Leetcode 14. Longest Common Prefix",
            "content": "\nLeetCode problem\n\nWrite a function to find the longest common prefix string amongst an array of strings.\n\nIf there is no common prefix, return an empty string \"\".\n\nExample 1:\n\n    Input: strs = [\"flower\",\"flow\",\"flight\"]\n    Output: \"fl\"\n\nExample 2:\n\n    Input: strs = [\"dog\",\"racecar\",\"car\"]\n    Output: \"\"\n    Explanation: There is no common prefix among the input strings.\n\nFirst accepted\n\nIdea:\n\ntest-case\n\nclass Solution:\n    def longestCommonPrefix(self, strs: List[str]) -> str:\n        strs.sort()\n        l = strs[0]\n        r = strs[-1]\n        if l == r:\n            return l\n        res = \"\"\n        for i in range(0, len(l)):\n            if l[i] == r[i]:\n                res += l[i]\n            else:\n                return res\n        return res\n",
            "tags": [
                "String"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/20-valid-parentheses",
            "title": "20. Valid Parentheses",
            "description": "Leetcode 20. Valid Parentheses solution",
            "content": "\nLeetCode problem\n\nGiven a string s containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.\n\nAn input string is valid if:\n\nOpen brackets must be closed by the same type of brackets.\nOpen brackets must be closed in the correct order.\nEvery close bracket has a corresponding open bracket of the same type.\n\nExample 1:\n\n    Input: s = \"()[]{}\"\n    Output: true\n\nExample 2:\n\n    Input: s = \"()[]{}\"\n    Output: true\n\nFirst accepted\n\nIdea:\n\nLoop through string\nIf current \"closes\" the last in stack, then remove last from stack\nElse: add current to stack\n\nclass Solution:\n    def isValid(self, s: str) -> bool:\n        stack = []\n        par_dict = {'(': ')', '{': '}', '[': ']'}\n        last_value = None\n        for i in s:\n            second_value = par_dict.get(last_value, None)\n            if i == second_value:\n                stack.pop()\n                last_value = stack[-1] if stack else None\n            else:\n                stack.append(i)\n                last_value = i\n        return not stack\n\nBetter solution\n\nclass Solution:\n    def isValid(self, s: str) -> bool:\n        par_dict = {'(': ')', '{': '}', '[': ']'}\n        stack = []\n        for char in s:\n            if char in par_dict:   # If it's an opening bracket, add it to the stack\n                stack.append(char)\n            elif stack: # If there's something in the stack\n                if char == par_dict[stack[-1]]:\nIf it's a closing bracket for the last opened bracket, remove it from the stack.\n                    stack.pop()\n                else:   # It's not a closing bracket for the last opened bracket. Invalid string.\n                    return False\n            else:   # Not an opening bracket or closing bracket. Invalid string.\n                return False\n        return stack == []\n",
            "tags": [
                "String",
                "Stack"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/21-merge-two-sorted-lists",
            "title": "21. Merge Two Sorted Lists",
            "description": "Leetcode 21. Merge Two Sorted Lists",
            "content": "\nLeetCode problem\n\nYou are given the heads of two sorted linked lists list1 and list2.\n\nMerge the two lists in a one sorted list. The list should be made by splicing together the nodes of the first two lists.\n\nReturn the head of the merged linked list.\n\nExample 1:\n\n21. Merge Two Sorted Lists\n\n    Input: list1 = [1,2,4], list2 = [1,3,4]\n    Output: [1,1,2,3,4,4]\n\nExample 2:\n\n    Input: list1 = [], list2 = [0]\n    Output: [0]\n\nFirst accepted\n\nIdea:\n\nGet smallest head. Loop and update its next.\n\nDefinition for singly-linked list.\nclass ListNode:\ndef init(self, val=0, next=None):\nself.val = val\nself.next = next\nclass Solution:\n    def mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n        res = ListNode()\n        current = res\n        while l1 and l2:\n            if l1.val  Recursion\n\ndef mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n\tif l1 and l2:\n\t\tif l1.val > l2.val:\n\t\t\tl1, l2 = l2, l1 #swap smaller and larger: make l1 the one with the smaller first value\n\t\tl1.next = self.mergeTwoLists(l1.next, l2) # move forward in the list which starts with the smaller value\n\treturn l1 or l2 # return whichever of the two lists remains at the end\nLoop\n\ndef mergeTwoLists(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n        res = ListNode()\n        current = res\n        while l1 and l2:\n            if l1.val  b.val:\n                a, b = b, a\n            a.next = self.mergeTwoLists(a.next, b)\n        return a or b\nFirst make sure that a is the \"better\" one (meaning b is None or has larger/equal value). Then merge the remainders behind a.\n\ndef mergeTwoLists(self, a, b):\n    if not a or b and a.val > b.val:\n        a, b = b, a\n    if a:\n        a.next = self.mergeTwoLists(a.next, b)\n    return a\n",
            "tags": [
                "Linked List",
                "Recursion"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/26-remove-duplicates-from-sorted-array",
            "title": "26. Remove Duplicates from Sorted Array",
            "description": "LeetCode 26. Remove Duplicates from Sorted Array",
            "content": "\nLeetCode problem\n\nGiven an integer array nums sorted in non-decreasing order, remove the duplicates in-place such that each unique element appears only once. The relative order of the elements should be kept the same.\n\nSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\n\nReturn k after placing the final result in the first k slots of nums.\n\nDo not allocate extra space for another array. You must do this by modifying the input array in-place with O(1) extra memory.\n\nCustom Judge:\n\nThe judge will test your solution with the following code:\n\n    int[] nums = [...]; // Input array\n    int[] expectedNums = [...]; // The expected answer with correct length\n\n    int k = removeDuplicates(nums); // Calls your implementation\n\n    assert k == expectedNums.length;\n    for (int i = 0; i  int:\n        k = 1\n        if len(nums) == 1:\n            return k\n\n        p1 = 0\n        p2 = 1\n\n        while p2  int:\n        if len(nums) == 1:\n            return 1\n\n        k = 1\n        i = 1\n        for n in nums:\n            if nums[i-1] != n:\n                nums[i] = n\n                i += 1\n                k += 1\n\n        return k\n",
            "tags": [
                "Array",
                "Two pointers"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/66-plus-one",
            "title": "66. Plus One",
            "description": "LeetCode 66. Plus One",
            "content": "\nLeetCode problem\n\nYou are given a large integer represented as an integer array digits, where each digits[i] is the ith digit of the integer. The digits are ordered from most significant to least significant in left-to-right order. The large integer does not contain any leading 0's.\n\nIncrement the large integer by one and return the resulting array of digits.\n\nExample 1:\n\n    Input: digits = [1,2,3]\n    Output: [1,2,4]\n    Explanation: The array represents the integer 123.\n    Incrementing by one gives 123 + 1 = 124.\n    Thus, the result should be [1,2,4].\n\nExample 2:\n\n    Input: digits = [4,3,2,1]\n    Output: [4,3,2,2]\n    Explanation: The array represents the integer 4321.\n    Incrementing by one gives 4321 + 1 = 4322.\n    Thus, the result should be [4,3,2,2].\n\nFirst accepted\n\nclass Solution:\n    def plusOne(self, digits: List[int]) -> List[int]:\n        i = len(digits) - 1\n\n        while i >= 0 and digits[i] == 9:\n            digits[i] = 0\n            i -= 1\n\n        if i < 0:\n            return [1] + digits\n        digits[i] = digits[i] + 1\n\n        return digits\n",
            "tags": [
                "Array",
                "Math"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/69-sqrtx",
            "title": "69. Sqrt(x)",
            "description": "Leetcode 69. Sqrt(x)",
            "content": "\nLeetCode problem\n\nGiven a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.\n\nYou must not use any built-in exponent function or operator.\n\nFor example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.\n\nExample 1:\n\n    Input: x = 4\n    Output: 2\n    Explanation: The square root of 4 is 2, so we return 2.\n\nExample 2:\n\n    Input: x = 8\n    Output: 2\n    Explanation: The square root of 8 is 2.82842..., and since we round it down to the nearest integer, 2 is returned.\n\nFirst accepted\n\nclass Solution:\n    def mySqrt(self, x: int, div=2) -> int:\n        s = x // div\n        s1 = (s + div) // 2\n        if s1 * s1 > x:\n            s1 = self.mySqrt(x, s1)\n            return s1\n        else:\n            return s1\n",
            "tags": [
                "Math",
                "Binary Search"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/70-climbing-stairs",
            "title": "70. Climbing Stairs",
            "description": "LeetCode 70. Climbing Stairs",
            "content": "\nLeetCode problem\n\nYou are climbing a staircase. It takes n steps to reach the top.\n\nEach time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?\n\nExample 1:\n\n    Input: n = 2\n    Output: 2\n    Explanation: There are two ways to climb to the top.\n    1 step + 1 step\n    2 steps\n\nExample 2:\n\n    Input: n = 3\n    Output: 3\n    Explanation: There are three ways to climb to the top.\n    1 step + 1 step + 1 step\n    1 step + 2 steps\n    2 steps + 1 step\n\nFirst accepted\n\nIdea:\n\nTried to calculate by hand. There is a sequence Fibonacci here\n\nclass Solution:\n    def climbStairs(self, n: int) -> int:\n        if n == 1:\n            return 1\n        if n == 2:\n            return 2\n\n        prev1 = 1\n        prev2 = 2\n        current = 2\n        while n > 2:\n            current = prev1 + prev2\n            prev1 = prev2\n            prev2 = current\n            n -= 1\n        return current\n",
            "tags": [
                "Math",
                "Dynamic Programming",
                "Memoization"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/88-merge-sorted-array",
            "title": "88. Merge Sorted Array",
            "description": "LeetCode 88. Merge Sorted Array",
            "content": "\nLeetCode problem\n\nYou are given two integer arrays nums1 and nums2, sorted in non-decreasing order, and two integers m and n, representing the number of elements in nums1 and nums2 respectively.\n\nMerge nums1 and nums2 into a single array sorted in non-decreasing order.\n\nThe final sorted array should not be returned by the function, but instead be stored inside the array nums1. To accommodate this, nums1 has a length of m + n, where the first m elements denote the elements that should be merged, and the last n elements are set to 0 and should be ignored. nums2 has a length of n.\n\nExample 1:\n\n    Input: nums1 = [1,2,3,0,0,0], m = 3, nums2 = [2,5,6], n = 3\n    Output: [1,2,2,3,5,6]\n    Explanation: The arrays we are merging are [1,2,3] and [2,5,6].\n    The result of the merge is [1,2,2,3,5,6] with the underlined elements coming from nums1.\n\nExample 2:\n\n    Input: nums1 = [1], m = 1, nums2 = [], n = 0\n    Output: [1]\n    Explanation: The arrays we are merging are [1] and [].\n    The result of the merge is [1].\n\nExample 3:\n\n    Input: nums1 = [0], m = 0, nums2 = [1], n = 1\n    Output: [1]\n    Explanation: The arrays we are merging are [] and [1].\n    The result of the merge is [1].\n    Note that because m = 0, there are no elements in nums1. The 0 is only there to ensure the merge result can fit in nums1.\n\nFirst accepted\n\nclass Solution:\n    def merge(self, nums1: List[int], m: int, nums2: List[int], n: int) -> None:\n        \"\"\"\n        Do not return anything, modify nums1 in-place instead.\n        \"\"\"\n        i = len(nums1) - n\n        for j in nums2:\n            nums1[i] = j\n            i += 1\n        nums1.sort()\n",
            "tags": [
                "Array",
                "Two Pointers",
                "Sorting"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/easy/94-binary-tree-inorder-traversal",
            "title": "94. Binary Tree Inorder Traversal",
            "description": "LeetCode 94. Binary Tree Inorder Traversal",
            "content": "\n##TODO\n\nimplement**: create binary tree from list of values\n\nLeetCode problem\n\nGiven the root of a binary tree, return the inorder traversal of its nodes' values.\n\n\nExample 1:\n\n\n\n    Input: root = [1,null,2,3]\n    Output: [1,3,2]\n\nExample 2:\n\n    Input: root = []\n    Output: []\n\nExample 3:\n\n    Input: root = [1]\n    Output: [1]\n\nThoughts\n\nDon't understand what needed. Why:\n1-null-2-3 becomes 1-3-2\n[1,2,5,7,8,9,10] becomes [7,2,8,1,9,5,10]\n\n\nIn 1-null-2-3 1 becomes the first because we loop to its left node which is null, then come back and first value here is 1.\n\nFirst accepted\n\nDefinition for a binary tree node.\nclass TreeNode:\ndef init(self, val=0, left=None, right=None):\nself.val = val\nself.left = left\nself.right = right\nclass Solution:\n    def inorderTraversal(self, root: Optional[TreeNode]) -> List[int]:\n\nadd all left, then add right\n        def get_child(head):\n            if head:\n                get_child(head.left)\n                result.append(head.val)\n                get_child(head.right)\n\n        result = []\n        get_child(root)\n        return result\n\nBetter solution\n\nMorris Traversal\n\nResources\n\nLeetCode explanation\n",
            "tags": [
                "Stack",
                "Tree",
                "Depth-First Search",
                "Binary Tree"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/_index",
            "title": "Medium",
            "description": null,
            "content": "",
            "tags": [
                "leetcode"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/17-letter-combinations-of-a-phone-number",
            "title": "17. Letter Combinations of a Phone Number",
            "description": "LeetCode 17. Letter Combinations of a Phone Number",
            "content": "\nLeetCode problem\n\nGiven a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. Return the answer in any order.\n\nA mapping of digits to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters.\n\n17\n\nExample 1:\n\n    Input: digits = \"23\"\n    Output: [\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"]\n\nExample 2:\n\n    Input: digits = \"\"\n    Output: []\n\nFirst accepted\n\nclass Solution:\n    def letterCombinations(self, digits: str) -> List[str]:\n        if not digits:\n            return []\n\n        letters = ['', '', 'abc', 'def',\n                      'ghi', 'jkl', 'mno',\n                      'pqrs', 'tuv', 'wxyz']\n\n        result = ['']\n\n        for d in digits:\n            d = int(d)\n            tmp = []\n            for letter in letters[d]:\n                for word in result:\n                    word += letter\n                    tmp.append(word)\n            result = tmp\n\n        return result\n",
            "tags": [
                "Hash Table",
                "String",
                "Backtracking"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/19-remove-nth-node-from-end-of-list",
            "title": "19. Remove Nth Node From End of List",
            "description": "LeetCode 19. Remove Nth Node From End of List",
            "content": "\nLeetCode problem\n\nGiven the 'head' of a linked list, remove the 'nth' node from the end of the list and return its head.\n\n\nExample 1:\n\nLeetCode 19. Remove Nth Node From End of List\n\n    Input: head = [1,2,3,4,5], n = 2\n    Output: [1,2,3,5]\n\n\nExample 2:\n\n    Input: head = [1], n = 1\n    Output: []\n\nFirst accepted\n\nCreate Linked List\n\nIdea:\n\nTwo pointers.\nSecond pointer starts from nth position.\nRun while second pointer exist.\n\nFirst version:\n\nDefinition for singly-linked list.\nclass ListNode:\ndef init(self, val=0, next=None):\nself.val = val\nself.next = next\nclass Solution:\n    def removeNthFromEnd(self, head, n: int):\n        p1 = head\n        p2 = head\n\n        for _ in range(n):\n            p1 = p1.next\n\n        if not p1:\n            return head.next # in case: head=[1], n=1 -> return []\n\n        while p1.next:\n            p1 = p1.next\n            p2 = p2.next\n\n        p2.next = p2.next.next\n\n        return head\n",
            "tags": [
                "Medium",
                "Linked List",
                "Two Pointers"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/2-add-two-numbers",
            "title": "2. Add Two Numbers",
            "description": "LeetCode 2. Add Two Numbers",
            "content": "\nBefore task: Create function for test data - code snippet\n\nYou are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order, and each of their nodes contains a single digit. Add the two numbers and return the sum as a linked list.\n\nYou may assume the two numbers do not contain any leading zero, except the number 0 itself.\n\nExample 1:\n\n    Input: l1 = [2,4,3], l2 = [5,6,4]\n    Output: [7,0,8]\n    Explanation: 342 + 465 = 807.\n\nFirst accepted\n\nIdea:\n\nLoop through lists\nadd each value to the list\nreverse list\ncalculate sum\ncreate linked list from reversed sum\n\nclass Solution:\n    def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n\n        def createLinkedNode(values):\n            head = ListNode(values[0])\n            current = head\n            for i in values[1:]:\n                node = ListNode(i)\n                current.next = node\n                current = current.next\n            return head\n\n\n        res = None\n\n        vals_l1 = []\n        cur = l1\n        while cur:\n            vals_l1.append(cur.val)\n            cur = cur.next\n\n        vals_l2 = []\n        cur = l2\n        while cur:\n            vals_l2.append(cur.val)\n            cur = cur.next\n\n        s_l1 = ''\n        for i in reversed(vals_l1):\n            s_l1 += str(i)\n\n        s_l2 = ''\n        for i in reversed(vals_l2):\n            s_l2 += str(i)\n\n        ll_sum = int(s_l1) + int(s_l2)\n        values = []\n        for val in reversed(str(ll_sum)):\n            values.append(int(val))\n\n        res = createLinkedNode(values)\n        return res\n\nBetter solution\n\n\n\nIdea:\n\nJust like how you would sum two numbers on a piece of paper.\n\ntest-case\n\nclass Solution:\n    def addTwoNumbers(self, l1: Optional[ListNode], l2: Optional[ListNode]) -> Optional[ListNode]:\n        dummyHead = ListNode(0)\n        curr = dummyHead\n        carry = 0\n        while l1 != None or l2 != None or carry != 0:\n            l1Val = l1.val if l1 else 0\n            l2Val = l2.val if l2 else 0\n            columnSum = l1Val + l2Val + carry\n            carry = columnSum // 10\n            newNode = ListNode(columnSum % 10)\n            curr.next = newNode\n            curr = newNode\n            l1 = l1.next if l1 else None\n            l2 = l2.next if l2 else None\n        return dummyHead.next\n`",
            "tags": [
                "Linked List",
                "Math",
                "Recursion"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/22-generate-parentheses",
            "title": "22. Generate Parentheses",
            "description": "LeetCode 22. Generate Parentheses",
            "content": "\nLeetCode problem\n\nGiven n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.\n\nExample 1:\n\n    Input: n = 3\n    Output: [\"((()))\",\"(()())\",\"(())()\",\"()(())\",\"()()()\"]\n\nExample 2:\n\n    Input: n = 1\n    Output: [\"()\"]\n\nPrerequirements\n\nBacktracking pattern\n\nFirst accepted\n\nIdea:\n\ntest-case\n\nclass Solution:\n  def generateParenthesis(self, n):\n    ans = []\n\n    def dfs(l: int, r: int, s: str) -> None:\n      if l == 0 and r == 0:\n        ans.append(s)\n      if l > 0:\n        dfs(l - 1, r, s + '(')\n      if l < r:\n        dfs(l, r - 1, s + ')')\n\n    dfs(n, n, '')\n    return ans\n",
            "tags": [
                "String",
                "Dynamic Programming",
                "Backtracking"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/29-divide-two-integers",
            "title": "29. Divide Two Integers",
            "description": "LeetCode 29. Divide Two Integers",
            "content": "\nLeetCode problem\n\nGiven two integers dividend and divisor, divide two integers without using multiplication, division, and mod operator.\n\nThe integer division should truncate toward zero, which means losing its fractional part. For example, 8.345 would be truncated to 8, and -2.7335 would be truncated to -2.\n\nReturn the quotient after dividing dividend by divisor.\n\nNote: Assume we are dealing with an environment that could only store integers within the 32-bit signed integer range: [−231, 231 − 1]. For this problem, if the quotient is strictly greater than 231 - 1, then return 231 - 1, and if the quotient is strictly less than -231, then return -231.\n\nExample 1:\n\n  Input: dividend = 10, divisor = 3\n  Output: 3\n  Explanation: 10/3 = 3.33333.. which is truncated to 3.\n\nExample 2:\n\n  Input: dividend = 7, divisor = -3\n  Output: -2\n  Explanation: 7/-3 = -2.33333.. which is truncated to -2.\n\nCode\n\nIdea:\n\nRemove decimals from both divisor and divident\nRemember the result sign (positive or ` int:\n        res = 0\n\n        dd = abs(dividend)\n        ds = abs(divisor)\n\n        sign = -1 if (dividend > 0 and divisor  0) else 1\n\n        while dd >= ds:\n            dd -= ds\n            res += 1\n\n        return sign * res\n\nImprove idea:\n\nSum divisor after \"success\" subtract until result of subtract is > 0\nSubtract divisor back until we can subtract it from dividend\n\nclass Solution:\n    def divide(self, dividend: int, divisor: int) -> int:\n        res = 0\n\n        dd = abs(dividend)\n        ds = abs(divisor)\n\n        sign = -1 if (dividend > 0 and divisor  0) else 1\n\n        if divisor == -1 and dividend == -2147483648:\n            return 2147483647\n        elif divisor == 1:\n            return sign * dd\n\n        while dd >= ds:\n            tmp = ds\n            multiples = 1 # count of subtracts\n            while dd >= tmp: ## sum divisor\n                dd -= tmp\n                res += multiples # hense sum count of subtracts\n\n                tmp += tmp\n                multiples += multiples\n            else:\n                if dd >= ds:\n                    dd -= ds\n                    res += 1\n\n        return sign * res\n\nBetter idea\n\nIdea: Bit manipulation\n\nclass Solution:\n  def divide(self, dividend, divisor):\n      positive = (dividend = divisor:\n              curr_divisor, num_divisors = divisor, 1\n              while dividend >= curr_divisor:\n                  dividend -= curr_divisor\n                  res += num_divisors\n\n                  curr_divisor = curr_divisor  int:\n    if dividend == -2**31 and divisor == -1:\n      return 2**31 - 1\n\n    sign = -1 if (dividend > 0) ^ (divisor > 0) else 1\n    ans = 0\n    dvd = abs(dividend)\n    dvs = abs(divisor)\n\n    while dvd >= dvs:\n      k = 1\n      while k * 2 * dvs <= dvd:\n        k <<= 1\n      dvd -= k * dvs\n      ans += k\n\n    return sign * ans\n`",
            "tags": [
                "Math",
                "Bit Manipulation"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/3-longest-substring-without-repeating-characters",
            "title": "3. Longest Substring Without Repeating Characters",
            "description": "LeetCode 3. Longest Substring Without Repeating Characters",
            "content": "\nLeetCode problem\n\nGiven a string s, find the length of the longest substring without repeating characters.\n\n\nExample 1:\n\n    Input: s = \"abcabcbb\"\n    Output: 3\n    Explanation: The answer is \"abc\", with the length of 3.\n\nFirst accepted\n\nIdea:\n\nLoop through string\nCalculate max count of elements in substring\nIf get double element, then go back until get this element and do step 2.\nProceed the main loop\n\nLongest Substring Without Repeating Characters\n\nclass Solution:\n    def lengthOfLongestSubstring(self, s: str) -> int:\n        uniqs = set()\n        len_max = 0\n        len_current = 0\n        idx = 0\n        for i in s:\n            if i in uniqs:\n                len_max = max(len_max, len_current)\n                len_current = 1\n                uniqs = set(i)\n                for j in reversed(s[:idx]):\n                    if j == i:\n                        break\n                    else:\n                        len_current += 1\n                        uniqs.add(j)\n\n            else:\n                uniqs.add(i)\n                len_current += 1\n            len_max = max(len_max, len_current)\n            idx += 1\n        return len_max\n\nBetter solution\nSliding Window - template\n\nWindow Sliding Technique is a computational technique which aims to reduce the use of nested loop and replace it with a single loop, thereby reducing the time complexity.\nThe Sliding window technique can reduce the time complexity to O(n).\n\nTips for identifying this kind of problem where we could use the sliding window technique:\n\nThe problem will be based on an array, string, or list data structure.\nYou need to find the subrange in this array or string that should provide the longest, shortest, or target values.\nA classic problem: to find the largest/smallest sum of given k (for example, three) consecutive numbers in an array.\n\nclass Solution:\n    def lengthOfLongestSubstring(self, s: str) -> int:\n        n = len(s)\n        ans = 0\nmp stores the current index of a character\n        mp = {}\n\n        i = 0\ntry to extend the range [i, j]\n        for j in range(n):\n            if s[j] in mp:\n                i = max(mp[s[j]], i)\n\n            ans = max(ans, j - i + 1)\n            mp[s[j]] = j + 1\n\n        return ans\n\nclass Solution:\n    def lengthOfLongestSubstring(self, s: str) -> int:\n        chars = [None] * 128\n        left = right = 0\n        res = 0\n        while right < len(s):\n            r = s[right]\n\n            index = chars[ord(r)]\n            if index is not None and left <= index < right:\n                left = index + 1\n\n            res = max(res, right - left + 1)\n\n            chars[ord(r)] = right\n            right += 1\n        return res\n\nclass Solution():\n    def lengthOfLongestSubstring(self, s):\n        max_len = 0\n        substr = ''\n        for char in s:\n            if char not in substr:\n                substr += char\n                max_len = max(max_len, len(substr))\n            else:\n                start = substr.index(char) + 1\n                substr = substr[start:] + char\n        return max_len\n\nResources\n\nhttps://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2694302/JS-or-98-or-Sliding-window-or-With-exlanation\nhttps://leetcode.com/problems/longest-substring-without-repeating-characters/discuss/2133524/JavaC%2B%2B-A-reall-Detailed-Explanation\n",
            "tags": [
                "Hash Table",
                "String",
                "Sliding Window"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/33-search-in-rotated-sorted-array",
            "title": "33. Search in Rotated Sorted Array",
            "description": "LeetCode 33. Search in Rotated Sorted Array",
            "content": "\nLeetCode problem\n\nThere is an integer array nums sorted in ascending order (with distinct values).\n\nPrior to being passed to your function, nums is possibly rotated at an unknown pivot index k (`1  int:\n    left = 0\n    right = len(nums) - 1\n\n    while left <= right:\n      mid = (left + right) // 2\n      if nums[mid] == target:\n        return mid\n\n      if nums[left] <= nums[mid]:\n        if nums[left] <= target < nums[mid]:\n          right = mid - 1\n        else:\n          left = mid + 1\n      else:\n        if nums[mid] < target <= nums[right]:\n          left = mid + 1\n        else:\n          right = mid - 1\n\n    return -1\n",
            "tags": [
                "Math",
                "Bit Manipulation"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/34-find-first-and-last-position-of-element-in-sorted-array",
            "title": "34. Find First and Last Position of Element in Sorted Array",
            "description": "LeetCode 34. Find First and Last Position of Element in Sorted Array",
            "content": "\nLeetCode problem\n\nGiven an array of integers nums sorted in non-decreasing order, find the starting and ending position of a given target value.\n\nIf target is not found in the array, return [-1, -1].\n\nYou must write an algorithm with O(log n) runtime complexity.\n\n\nExample 1:\n\n  Input: nums = [5,7,7,8,8,10], target = 8\n  Output: [3,4]\n\nExample 2:\n\n  Input: nums = [5,7,7,8,8,10], target = 6\n  Output: [-1,-1]\n\nExample 3:\n\n  Input: nums = [], target = 0\n  Output: [-1,-1]\n\nCode\n\nIdea:\n\nFind target index (target_index) using Binary Search\n   If not exist then return [-1, -1]\n   If exist then goto step 2\nWe got the middle index. For now this is the most left and most right index.\nDivide nums into two arrays: left_nums and right_nums:\n   left_nums = nums[0:target_index]\n   right_nums = nums[target_index:]\nFind the most left target in left_nums. (Set right border in subarray)\nFind the most right target in right_nums. (Set left border in subarray)\n\n\nclass Solution:\n    def searchRange(self, nums: List[int], target: int) -> List[int]:\n\n        def find_target():\n            left = 0\n            right = len(nums) - 1\n\n            while left  List[int]:\n    l = bisect_left(nums, target)\n    if l == len(nums) or nums[l] != target:\n      return -1, -1\n    r = bisect_right(nums, target) - 1\n    return l, r\n",
            "tags": [
                "Array",
                "Bit Manipulation"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/36-valid-sudoku",
            "title": "36. Valid Sudoku",
            "description": "LeetCode 36. Valid Sudoku",
            "content": "\nLeetCode problem\n\nDetermine if a 9 x 9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:\n\nEach row must contain the digits 1-9 without repetition.\nEach column must contain the digits 1-9 without repetition.\nEach of the nine 3 x 3 sub-boxes of the grid must contain the digits 1-9 without repetition.\n\nNote:\n\nA Sudoku board (partially filled) could be valid but is not necessarily solvable.\nOnly the filled cells need to be validated according to the mentioned rules.\n\nExample 1:\n\nexample\n\n    Input: board =\n    [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"]\n    ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"]\n    ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"]\n    ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"]\n    ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"]\n    ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"]\n    ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"]\n    ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"]\n    ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n    Output: true\n\nExample 2:\n\n    Input: board =\n    [[\"8\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"]\n    ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"]\n    ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"]\n    ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"]\n    ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"]\n    ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"]\n    ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"]\n    ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"]\n    ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]\n    Output: false\n    Explanation: Same as Example 1, except with the 5 in the top left corner being modified to 8. Since there are two 8's in the top left 3x3 sub-box, it is invalid.\n\nCode\n\nIdea:\n\ntest-case\n\nclass Solution:\n    def isValidSudoku(self, board: List[List[str]]) -> bool:\n        exist = set()\n        for i in range(9):\n            for j in range(9):\n                x = boardi\n                if x != '.':\n                    uniqs = (\n                        (i, x),\n                        (x, j),\n                        (int(i/3), int(j/3), x) ) # devide 3 because of third check in 3x3 block\n                    for z in uniqs:\n                        if z in exist:\n                            return False\n                        exist.add(z)\n        return True\n",
            "tags": [
                "Array",
                "Hash Table",
                "Matrix"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/38-count-and-say",
            "title": "38. Count and Say",
            "description": "LeetCode 38. Count and Say",
            "content": "\nLeetCode problem\n\nThe count-and-say sequence is a sequence of digit strings defined by the recursive formula:\n\ncountAndSay(1) = \"1\"\ncountAndSay(n) is the way you would \"say\" the digit string from countAndSay(n-1), which is then converted into a different digit string.\nTo determine how you \"say\" a digit string, split it into the minimal number of substrings such that each substring contains exactly one unique digit. Then for each substring, say the number of digits, then say the digit. Finally, concatenate every said digit.\n\nFor example, the saying and conversion for digit string \"3322251\":\nexample\n\nGiven a positive integer n, return the nth term of the count-and-say sequence.\n\nExample 1:\n\n    Input: n = 1\n    Output: \"1\"\n    Explanation: This is the base case.\n\nExample 2:\n\n    Input: n = 4\n    Output: \"1211\"\n    Explanation:\n    countAndSay(1) = \"1\"\n    countAndSay(2) = say \"1\" = one 1 = \"11\"\n    countAndSay(3) = say \"11\" = two 1's = \"21\"\n    countAndSay(4) = say \"21\" = one 2 + one 1 = \"12\" + \"11\" = \"1211\"\n\nIdea:\n\nclass Solution:\n    def countAndSay(self, n: int) -> str:\n        res = '1'\n        while n > 1:\n            l = len(res)\n            new_str = ''\n            i = 0\n            while i < l:\n                count = 1\n                while i < l - 1 and res[i] == res[i+1]:\n                    count += 1\n                    i += 1\n                new_str += str(count) + res[i]\n                i += 1\n            res = new_str\n            n -= 1\n\n        return res\n",
            "tags": [
                "String"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/46-permutations",
            "title": "46. Permutations",
            "description": "LeetCode 46. Permutations",
            "content": "\nLeetCode problem\n\nGiven an array nums of distinct integers, return all the possible permutations. You can return the answer in any order.\n\nExample 1:\n\n    Input: nums = [1,2,3]\n    Output: [[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]]\n\nExample 2:\n\n    Input: nums = [0,1]\n    Output: [[0,1],[1,0]]\n\nExample 3:\n\n    Input: nums = [1]\n    Output: [[1]]\n\nIdea:\n\nDraw a decigion tree\nFix when branch is ready to return\n\ntest-case\n\nImplementation:\nRecursive:\n   Go through every value in nums\n   Pop value\n   call perm() with updated nums\n   from each call(step) append 'poped' value from step 2\n\nclass Solution:\n    def permute(self, nums: List[int]) -> List[List[int]]:\n\n        result_permutation = []\n\n        if len(nums) == 1: # base case\n            return [nums[:]]\n\n        for _ in nums:\n            tmp_removed = nums.pop(0) # remove current element before next step\n\n            permutations = self.permute(nums)\n\n            for perm in permutations:\n                perm.append(tmp_removed)\n\n            nums.append(tmp_removed)\n            result_permutation.extend(permutations)\n\n        return result_permutation\n\nResources\n\nhttps://www.youtube.com/watch?v=s7AvT7cGdSo\nhttps://walkccc.me/LeetCode/problems/0046/",
            "tags": [
                "Array",
                "Backtracking"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/48-rotate-image",
            "title": "48. Rotate Image",
            "description": "LeetCode 48. Rotate Image",
            "content": "\nLeetCode problem\n\nYou are given an n x n 2D matrix representing an image, rotate the image by 90 degrees (clockwise).\n\nYou have to rotate the image in-place, which means you have to modify the input 2D matrix directly. DO NOT allocate another 2D matrix and do the rotation.\n\nExample 1:\n\nExample LeetCode 48. Rotate Image\n\n    Input: matrix = [[1,2,3],[4,5,6],[7,8,9]]\n    Output: [[7,4,1],[8,5,2],[9,6,3]]\n\nExample 2:\n\nExample LeetCode 48. Rotate Image\n\n    Input: matrix = [[5,1,9,11],[2,4,8,10],[13,3,6,7],[15,14,12,16]]\n    Output: [[15,13,2,5],[14,3,4,1],[12,6,8,9],[16,7,10,11]]\n\n\nIdea:\n\n\nclass Solution:\n    def rotate(self, matrix: List[List[int]]) -> None:\n        \"\"\"\n        Do not return anything, modify matrix in-place instead.\n        \"\"\"\n        l = 0\n        r = len(matrix) - 1\n\n        while l  None:\n    matrix.reverse()\n\n    for i in range(len(matrix)):\n      for j in range(i + 1, len(matrix)):\n        matrixi, matrixj = matrixj, matrixi\n\nResources\n\nhttps://www.youtube.com/watch?v=fMSJSS7eO1w\nhttps://walkccc.me/LeetCode/problems/0048/",
            "tags": [
                "Array",
                "Math",
                "Matrix"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/49-group-anagrams",
            "title": "49. Group Anagrams",
            "description": "LeetCode 49. Group Anagrams",
            "content": "\nLeetCode problem\n\nGiven an array of strings strs, group the anagrams together. You can return the answer in any order.\n\nAn Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\n\nExample 1:\n\n    Input: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"]\n    Output: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]]\n\nExample 2:\n\n    Input: strs = [\"\"]\n    Output: [[\"\"]]\n\nExample 3:\n\n    Input: strs = [\"a\"]\n    Output: [[\"a\"]]\n\nIdea:\n\nclass Solution:\n    def groupAnagrams(self, strs: List[str]) -> List[List[str]]:\n        dd = {}\n        for s in strs:\n            s_sort = \"\".join(sorted(s))\n            values = dd.get(s_sort, [])\n            values.append(s)\n            dd[s_sort] = values\n        return dd.values()\n\nApproach 2:\n\nIntuition:\n\nTwo strings are anagrams if and only if their character counts (respective number of occurrences of each character) are the same.\n\nAlgorithm:\n\nWe can transform each string s into a character count, count\\text{count}count, consisting of 26 non-negative integers representing the number of a's, b's, z's, etc. We use these counts as the basis for our hash map.\n\nIn python, the representation will be a tuple of the counts. For example, abbccc will be (1, 2, 3, 0, 0, ..., 0), where again there are 26 entries total.\n\nexample\n\nclass Solution:\n    def groupAnagrams(strs):\n        ans = collections.defaultdict(list)\n        for s in strs:\n            count = [0] * 26\n            for c in s:\n                count[ord(c) - ord('a')] += 1\n            ans[tuple(count)].append(s)\n        return ans.values()\n\nResources\n\nLeetCode expl",
            "tags": [
                "Array",
                "Hash table",
                "String",
                "Sorting"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/5-longest-palindromic-substring",
            "title": "5. Longest Palindromic Substring",
            "description": "Leetcode 5. Longest Palindromic Substring",
            "content": "\nLeetCode problem\n\nGiven a string s, return the longest palindromic substring in s.\n\nA string is called a palindrome string if the reverse of that string is the same as the original string.\n\nExample 1:\n\n    Input: s = \"babad\"\n    Output: \"bab\"\n    Explanation: \"aba\" is also a valid answer.\n\nExample 2:\n\n    Input: s = \"cbbd\"\n    Output: \"bb\"\n\nFirst accepted\n\nHints\nHow can we reuse a previously computed palindrome to compute a larger palindrome?\nHow can we reuse a previously computed palindrome to compute a larger palindrome?\nComplexity based hint:\nIf we use brute-force and check whether for every start and end position a substring is a palindrome we have O(n^2) start - end pairs and O(n) palindromic checks. Can we reduce the time for palindromic checks to O(1) by reusing some previous computation.\n\n\nIdea:\n\nWe start at index = 0 and iterate through all values until n. At each index we call the function getPalindrome that will check the values to the left and right of the provided indices. It will continue to do so until the longest palindrome within the given range is found.\n\ntest-case\n\nLeetcode diagram explained\nLink to diagram\n\nclass Solution:\n    def longestPalindrome(self, s: str) -> str:\n        def getPalindrome(left, right):\n            while(left >= 0 and\n                  right  len_max:\n                pal_left = left\n                pal_right = right\n                len_max = pal_len\n\n            left, right = getPalindrome(i, i+1)\n            pal_len = right - left + 1\n            if pal_len > len_max:\n                pal_left = left\n                pal_right = right\n                len_max = pal_len\n\n        return s[pal_left:pal_right+1]\n\nBetter solution\n\nManacher's algorithm\n\nThere is an O(n) algorithm called Manacher's algorithm.\n\nclass Solution:\n  def longestPalindrome(self, s: str) -> str:\n@ and $ signs are sentinels appended to each end to avoid bounds checking\n    t = '#'.join('@' + s + '$')\n    n = len(t)\nt[i - maxExtends[i]..i) ==\nt[i + 1..i + maxExtends[i]]\n    maxExtends = [0] * n\n    center = 0\n\n    for i in range(1, n - 1):\n      rightBoundary = center + maxExtends[center]\n      mirrorIndex = center - (i - center)\n      maxExtends[i] = rightBoundary > i and \\\n          min(rightBoundary - i, maxExtends[mirrorIndex])\n\nAttempt to expand palindrome centered at i\n      while t[i + 1 + maxExtends[i]] == t[i - 1 - maxExtends[i]]:\n        maxExtends[i] += 1\n\nIf palindrome centered at i expand past rightBoundary,\nadjust center based on expanded palindrome.\n      if i + maxExtends[i] > rightBoundary:\n        center = i\n\nFind the maxExtend and bestCenter\n    maxExtend, bestCenter = max((extend, i)\n                                for i, extend in enumerate(maxExtends))\n    return s[(bestCenter - maxExtend) // 2:(bestCenter + maxExtend) // 2]\n\nResources\n\nManacher's algorithm\nErrichto:Leetcode problem Longest Palindromic Substring (two solutions)\nhttps://redquark.org/leetcode/0005-longest-palindromic-substring/\n\nRU\nРазбор задачи с интервью. Литкод 5. Longest Palindromic Substring\nАлгоритмика: Алгоритм Манакера\nВикипедия:Алгоритм Манакера,%D1%80%D0%B5%D1%88%D0%B0%D1%82%D1%8C%20%D0%B8%20%D0%B1%D0%BE%D0%BB%D0%B5%D0%B5%20%D0%BE%D0%B1%D1%89%D0%B8%D0%B5%20%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B8)\n",
            "tags": [
                "String",
                "Dynamic Programming"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/50",
            "title": "50. Pow(x, n)",
            "description": "LeetCode 50. Pow(x, n)",
            "content": "\nLeetCode problem\n\nImplement pow(x, n), which calculates x raised to the power n (i.e., x^n).\n\nExample 1:\n\n    Input: x = 2.00000, n = 10\n    Output: 1024.00000\n\nExample 2:\n\n    Input: x = 2.10000, n = 3\n    Output: 9.26100\n\nExample 3:\n\n    Input: x = 2.00000, n = -2\n    Output: 0.25000\n    Explanation: 2-2 = 1/22 = 1/4 = 0.25\n\nApproach 1:\n\nclass Solution:\n    def myPow(self, x: float, n: int) -> float:\n        return x ** n\n\nApproach 2:\n\nRecursive\n\nclass Solution:\n    def myPow(self, x, n):\n        if not n:\n            return 1\n        if n >= 1\n        return pow\n",
            "tags": [
                "Math",
                "Recursion"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/53",
            "title": "53. Maximum Subarray",
            "description": "LeetCode 53. Maximum Subarray",
            "content": "\nLeetCode problem\n\nGiven an integer array nums, find the subarray which has the largest sum and return its sum.\n\nExample 1:\n\n    Input: nums = [-2,1,-3,4,-1,2,1,-5,4]\n    Output: 6\n    Explanation: [4,-1,2,1] has the largest sum = 6.\n\nExample 2:\n\n    Input: nums = [1]\n    Output: 1\n\nExample 3:\n\n    Input: nums = [5,4,-1,7,8]\n    Output: 23\n\nApproach 1:\n\nclass Solution:\n    def maxSubArray(self, nums: List[int]) -> int:\n\n        max_ = nums[0]\n        max2 = nums[0]\n\n        if len(nums) == 1:\n            return max_\n\n        for i in range(1, len(nums)):\n            max_ = max(nums[i], nums[i] + max_)\n            max2 = max(max_, max2)\n\n        return max2\n\n",
            "tags": [
                "Array",
                "Divide and Conquer",
                "Dynamic Programming"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/55",
            "title": "55. Jump Game",
            "description": "LeetCode 55. Jump Game",
            "content": "\nLeetCode problem\n\nYou are given an integer array nums. You are initially positioned at the array's first index, and each element in the array represents your maximum jump length at that position.\n\nReturn true if you can reach the last index, or false otherwise.\n\nExample 1:\n\n    Input: nums = [2,3,1,1,4]\n    Output: true\n    Explanation: Jump 1 step from index 0 to 1, then 3 steps to the last index.\n\nExample 2:\n\n    Input: nums = [3,2,1,0,4]\n    Output: false\n    Explanation: You will always arrive at index 3 no matter what. Its maximum jump length is 0, which makes it impossible to reach the last index.\n\n\nApproach 1:\n\nIdea: go forward on each step and mark next cell if can achieve it.\n\nclass Solution:\n    def canJump(self, nums: List[int]) -> bool:\n        last_i = len(nums)\n        if last_i == 1:\n            return True\n        nn = [0] * last_i\n        nn[0] = nums[0]\n        for i in range(last_i):\n            el = nums[i]\n            if el or nn[i+1]:\n                for j in range(el):\n                    nn[i+j+1] = el\n                    if nn[last_i - 1]:\n                        return True\n            else:\n                return False\n        return False\n\nApproach 2:\n\nGoing forwards. m tells the maximum index we can reach so far.\n\nclass Solution:\n    def canJump(self, nums):\n        m = 0\n        for i, n in enumerate(nums):\n            if i > m:\n                return False\n            m = max(m, i + n)\n        return True\n\nclass Solution:\n  def canJump(self, nums: List[int]) -> bool:\n    i = 0\n    m = 0\n    while i < len(nums) and i <= m:\n      m = max(m, i + nums[i])\n      i += 1\n    return i == len(nums)\n`",
            "tags": [
                "Array",
                "Greedy",
                "Dynamic Programming"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/56",
            "title": "56. Merge Intervals",
            "description": "LeetCode 56. Merge Intervals",
            "content": "\n\nLeetCode problem\n\nGiven an array of intervals where intervals[i] = [starti, endi], merge all overlapping intervals, and return an array of the non-overlapping intervals that cover all the intervals in the input.\n\nExample 1:\n\n    Input: intervals = [[1,3],[2,6],[8,10],[15,18]]\n    Output: [[1,6],[8,10],[15,18]]\n    Explanation: Since intervals [1,3] and [2,6] overlap, merge them into [1,6].\n\nExample 2:\n\n    Input: intervals = [[1,4],[4,5]]\n    Output: [[1,5]]\n    Explanation: Intervals [1,4] and [4,5] are considered overlapping.\n\nApproach 1:\n\n\nclass Solution:\n    def merge(self, intervals: List[List[int]]) -> List[List[int]]:\n        intervals.sort()\n        res = [intervals[0]]\n        for ir in range(1, len(intervals)):\n            if intervalsir >= res-1 and intervalsir = intervalsir and res-1  List[List[int]]:\n    ans = []\n    for interval in sorted(intervals):\n      if not ans or ans-1 < interval[0]:\n        ans.append(interval)\n      else:\n        ans-1 = max(ans-1, interval[1])\n    return ans\n",
            "tags": [
                "Array",
                "Sorting"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/62",
            "title": "62. Unique Paths",
            "description": "LeetCode 62. Unique Paths",
            "content": "\nLeetCode problem\n\nThere is a robot on an m x n grid. The robot is initially located at the top-left corner (i.e., grid0). The robot tries to move to the bottom-right corner (i.e., gridm - 1). The robot can only move either down or right at any point in time.\n\nGiven the two integers m and n, return the number of possible unique paths that the robot can take to reach the bottom-right corner.\n\nThe test cases are generated so that the answer will be less than or equal to 2 * 10^9.\n\n\n\nExample 1:\n\nLeetCode 62. Unique Paths\n\n    Input: m = 3, n = 7\n    Output: 28\nExample 2:\n\n    Input: m = 3, n = 2\n    Output: 3\n    Explanation: From the top-left corner, there are a total of 3 ways to reach the bottom-right corner:\n    Right -> Down -> Down\n    Down -> Down -> Right\n    Down -> Right -> Down\n\nApproach 1:\n\nLeetCode Submission\n\nclass Solution:\n    def uniquePaths(self, m: int, n: int) -> int:\n        if m == 1 or n == 1:\n            return 1\n        matrix = [ [1 for j in range(n)] for i in range(m)]\n\n        for i in range(1, m):\n            for j in range(1, n):\n                max_above = 0\n                max_left = 1\n                if i > 0:\n                    max_above = matrixi-1\n                if j > 0:\n                    max_left = matrixi\n                matrixi = max_above + max_left\n\n        m = matrixi\n        return m\n\nclass Solution:\n  def uniquePaths(self, m: int, n: int) -> int:\n    matrix = [[1] * n for _ in range(m)]\n\n    for i in range(1, m):\n      for j in range(1, n):\n        matrixi = matrixi - 1 + matrixi\n\n    return matrix-1\n",
            "tags": [
                "Math",
                "Dynamic Programming",
                "Combinatorics"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/7-reverse-integer",
            "title": "7. Reverse Integer",
            "description": "LeetCode 7. Reverse Integer",
            "content": "\nLeetCode problem\n\nGiven a signed 32-bit integer x, return x with its digits reversed. If reversing x causes the value to go outside the signed 32-bit integer range [-231, 231 - 1], then return 0.\n\nAssume the environment does not allow you to store 64-bit integers (signed or unsigned).\n\nExample 1:\n\nInput: x = 123\nOutput: 321\n\nExample 2:\n\nInput: x = -123\nOutput: -321\n\nExample 3:\n\nInput: x = 120\nOutput: 21\n\nFirst accepted\n\nIdea:\n\nConvert number to int\nRemove minus if exist (or convert module of number)\nreverse\n\nclass Solution:\n    def reverse(self, x: int) -> int:\n        reversed_int = []\n        str_int = str(x)\n        if x = -2147483648 and res  int:\n        s = str(abs(x))\n        rev = int(s[::-1])\n\n        if rev > 2147483647:\n            return 0\n\n        return rev if x > 0 else (rev * -1)\n",
            "tags": [
                "Math"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/73",
            "title": "73. Set Matrix Zeroes",
            "description": "LeetCode 73. Set Matrix Zeroes",
            "content": "\nLeetCode problem\n\nGiven an m x n integer matrix matrix, if an element is 0, set its entire row and column to 0's.\n\nYou must do it in place.\n\nExample 1:\n\n73. Set Matrix Zeroes\n\n    Input: matrix = [[1,1,1],[1,0,1],[1,1,1]]\n    Output: [[1,0,1],[0,0,0],[1,0,1]]\n\nExample 2:\n\n73. Set Matrix Zeroes\n\n    Input: matrix = [[0,1,2,0],[3,4,5,2],[1,3,1,5]]\n    Output: [[0,0,0,0],[0,4,5,0],[0,3,1,0]]\n\nApproach 1:\n\nIdea:\n\n\n\n\nLeetCode Submission\n\nclass Solution:\n  def setZeroes(self, matrix: List[List[int]]) -> None:\n\n    rows = len(matrix)\n    cols = len(matrix[0])\n\n1. Check first row/column for zero's\n    first_row_has_zero = 0 in matrix[0]\n    first_col_has_zero = 0 in list(zip(*matrix))[0]\n\n2. Check other cells in matrix and save info in the 1st row/col if cell has zero's\n    for i in range(1, rows):\n      for j in range(1, cols):\n        if matrixi == 0:\n          matrix0 = 0 # 1st row\n          matrixi = 0 # 1st col\n\n3. Loop again through first row/column and overwrite cells according to the data from 1st row/column\nexcept the 1st row/col\n    for i in range(1, rows):\n      for j in range(1, cols):\n        if matrix0 == 0 or matrixi == 0:\n          matrixi = 0\n\nFill 0s for the 1st row if needed\n    if first_row_has_zero:\n      matrix[0] = [0] * cols\n\nFill 0s for the 1st col if needed\n    if first_col_has_zero:\n      for row in matrix:\n        row[0] = 0\n",
            "tags": [
                "Array",
                "Hash Table",
                "Matrix"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/75",
            "title": "75. Sort Colors",
            "description": "LeetCode 75. Sort Colors",
            "content": "\nLeetCode problem\n\nThis problem is also known as the Dutch National Flag problem. One solution is to use three pointers to partition the array into three sections: red, white, and blue.\n\nHere's the algorithm:\n\nInitialize three pointers: left, mid, and right.\nInitialize left to 0, mid to 0, and right to n-1, where n is the length of the input array.\nWhile mid is less than or equal to right:\n   If nums[mid] is 0, swap nums[mid] with nums[left], increment mid and left.\n   If nums[mid] is 1, increment mid.\n   If nums[mid] is 2, swap nums[mid] with nums[right], decrement right.\nReturn the sorted array.\n\nclass Solution:\n    def sortColors(self, nums: List[int]) -> None:\n        \"\"\"\n        Do not return anything, modify nums in-place instead.\n        \"\"\"\n\n        l, m, r = 0, 0, len(nums) - 1\n        while m <= r:\n            if nums[m] == 0:\n                nums[m], nums[l] = nums[l], nums[m]\n                l += 1\n                m += 1\n            elif nums[m] == 1:\n                m += 1\n            else:\n                nums[m], nums[r] = nums[r], nums[m]\n                r -= 1\n",
            "tags": [
                "Array",
                "Two Pointers",
                "Sorting"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/algorithms-101/problems/medium/78",
            "title": "78. Subsets",
            "description": "LeetCode 78. Subsets",
            "content": "\nLeetCode problem\n\nIn this solution, we start with an empty list in the results array.\n\nFor each element in the nums array, we append that element to all of the subsets in the results array to create new subsets, and then add these new subsets to the results array.\n\nBy doing this for all elements in nums, we generate all possible subsets.\n\n\nclass Solution:\n    def subsets(self, nums: List[int]) -> List[List[int]]:\n        res = [[]]\n        for i in nums:\n            for j in range(len(res)):\n                cur = []\n                cur.append(i)\n                cur.extend(res[j])\n                res.append(cur)\n        return res\n\nApproach 2:\n\nclass Solution:\n  def subsets(self, nums: List[int]) -> List[List[int]]:\n    res = []\n\n    def dfs(start: int, path: List[int]) -> None:\n      res.append(path)\n\n      for i in range(start, len(nums)):\n        dfs(i + 1, path + [nums[i]])\n\n    dfs(0, [])\n    return res\n\nThis is a recursive solution that uses a depth-first search (DFS) approach to generate all possible subsets of the input list nums. The function takes two parameters start and path, where\nstart represents the starting index of the current subset\npath represents the current subset being constructed.\n\nThe base case of the recursion is when start is greater than or equal to the length of nums, at which point the current path is added to the final result res.\n\nFor each recursive call, the function iterates through the remaining elements of nums starting at index start, and appends each element to the path list. Then, the function recursively calls itself with the next index i+1 as the new starting point for the next subset, and the updated path list.\n\nAs the recursion returns, each subset is added to the res list, and the path list is updated by removing the last element that was added in the previous recursive call.\n\nFinally, the function is initialized with an empty path list and a starting index start of 0, and the final res list is returned after all subsets have been generated.\n\n\nLeetCode Editorial:\n\nEditorial\n\nApproach 1: Cascading\n\nCascading\n\nApproach 2: Backtracking\n\nBacktracking\nBacktracking\n\nApproach 3: Lexicographic (Binary Sorted) Subsets\nLexicographic (Binary Sorted) Subsets\n",
            "tags": [
                "Array",
                "Backtracking",
                "Bit Manipulation"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/archive/",
            "title": "Docs",
            "content": "\nDocs EN | RU\nPosts EN | RU\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/_index",
            "title": "AWS Certified Developer (DVA-C01 -> DVA-C02)",
            "description": "My plan for preparing for and taking the AWS Certified Developer exam",
            "content": "\nTL;DR\n\nPassed exam in one month.\nCreated an app with questions and progress that helped me a lot\nLQVKGDT16BF1Q6CX\n\nNote\nThe AWS Certified Developer - Associate exam is changing February 28, 2023. The last date to take the current exam is February 27, 2023.\nTo keep the docs up to date I will add new and latest information.\n\nDVA-C01 vs DVA-C02\n\nnew domain: Domain 3: Deployment\n  focus will be on testing and deploying your code into different environments including development, test, and production environments. You’ll need to know how CloudFormation, the AWS Cloud Development Kit (CDK), and AWS SAM are used to deploy applications.\nDomains 4 and 5 (“Refactoring” along with “Monitoring and Troubleshooting”) from the DVA-C01 exam guide have been consolidated into Domain 4 (“Troubleshooting and Optimization”) in the DVA-C02 exam guide\nPlus 2% questions in Development with AWS Services domain\n\nTest questions for DVA-C02 - here and here\n\nCriteria\n\nIn order to pass the exam, you must score more than 720/1000 (unspecified) points. Criterion will be a minimum threshold of 75/100%, unless conditions change in preparation.\n\nStudy Plan\n\nFind out what the exam requirements are\nHave a list of topics that will be on the exam\nPractice each service for comprehension\nRead extra theory that will not be covered during practice\nGo through the test general questions\nRepeat 3-5 repeat until the result of failed block greater than 80 points\n\nEntrypoint:\n\nAWS Certified Developer Exam Information\n\nPrepare\n\nThe AWS website has:\nExam Preparation Guide DVA-C01\nFrom 27 Feb 2023 Exam Preparation Guide DVA-C02\n\n\nTo pass the exam, you need to know certain services from the 4 domains: Development with AWS Services, Security, Deployment, Refactoring, Monitoring and Troubleshooting\n\nList of services on the exam\nVersion 2.1 DVA-C01\nVersion 1.0 DVA-C02\n\n\nAnalytics:\n\nAmazon Athena (new in DVA-C02)\nAmazon OpenSearch Service (Amazon Elasticsearch Service)\nAmazon Kinesis\n\nApplication Integration:\n\nAWS AppSync (new in DVA-C02)\nAmazon EventBridge (Amazon CloudWatch Events)\nAmazon Simple Notification Service (Amazon SNS)\nAmazon Simple Queue Service (Amazon SQS)\nAWS Step Functions\n\nCompute:\n\nAmazon EC2\nAWS Elastic Beanstalk\nAWS Lambda\nAWS Serverless Application Model (AWS SAM) (new in DVA-C02)\n\nContainers:\n\nAWS Copilot (new in DVA-C02)\nAmazon Elastic Container Registry (Amazon ECR)\nAmazon Elastic Container Service (Amazon ECS)\nAmazon Elastic Kubernetes Services (Amazon EKS)\n\nDatabase:\n\nAmazon Aurora (new in DVA-C02)\nAmazon DynamoDB\nAmazon ElastiCache\nAmazon MemoryDB for Redis (new in DVA-C02)\nAmazon RDS\n\nDeveloper Tools:\nAWS Amplify (new in DVA-C02)\nAWS Cloud9 (new in DVA-C02)\nAWS CloudShell (new in DVA-C02)\nAWS CodeArtifact\nAWS CodeBuild\nAWS CodeCommit\nAWS CodeDeploy\nAmazon CodeGuru\nAWS CodePipeline\nAWS CodeStar\nAWS Fault Injection Simulator\nAWS X-Ray\n\nManagement and Governance:\n\nAWS AppConfig (new in DVA-C02)\nAWS Cloud Development Kit (AWS CDK) (new in DVA-C02)\nAWS CloudFormation\nAWS CloudTrail (new in DVA-C02)\nAmazon CloudWatch\nAmazon CloudWatch Logs (new in DVA-C02)\nAWS Command Line Interface (AWS CLI) (new in DVA-C02)\nAWS Systems Manager (new in DVA-C02)\n\nNetworking and Content Delivery:\n\nAmazon API Gateway\nAmazon CloudFront\nElastic Load Balancing\nAmazon Route 53 (new in DVA-C02)\nAmazon VPC (new in DVA-C02)\n\nSecurity, Identity, and Compliance:\n\nAWS Certificate Manager (ACM) (new in DVA-C02)\nAWS Certificate Manager Private Certificate Authority (new in DVA-C02)\nAmazon Cognito\nAWS Identity and Access Management (IAM)\nAWS Key Management Service (AWS KMS)\nAWS Secrets Manager (new in DVA-C02)\nAWS Security Token Service (AWS STS) (new in DVA-C02)\nAWS WAF (new in DVA-C02)\n\nStorage:\n\nAmazon Elastic Block Store (Amazon EBS) (new in DVA-C02)\nAmazon Elastic File System (Amazon EFS) (new in DVA-C02)\nAmazon S3\nAmazon S3 Glacier (new in DVA-C02)\n\nTraining plan\n\nOpened a training plan for any tutorial to understand where to start learning. Have chosen cloudacademy service (but for example FreeCodeCamp has a free course with content).\n\nAnother option is to use free AWS Workshops\n\nAWS Developer - Associate (DVA-C01) Certification Preparation\n\nDon't see coverage of the following services, so I add them to the block when related topics are covered:\n\nAnalytics:\n\nAmazon Elasticsearch Service (Amazon ES) -> OpenSearch Service\n\nDeveloper Tools:\n\nAWS CodeArtifact\nAWS Fault Injection Simulator\n\nMy roadmap\n\nThe following is my roadmap for the study. There may be adjustments.\n\nAWS Identity and Access Management (IAM)\nAmazon EC2\nAWS Elastic Beanstalk\nAWS Lambda\nAmazon S3\nAmazon DynamoDB\nAmazon ElastiCache\nAmazon RDS\nAmazon API Gateway\nAmazon CloudFront\nElastic Load Balancing (ELB)\nAmazon Kinesis\nAmazon OpenSearch Service (Amazon Elasticsearch Service)\nAmazon CloudWatch\nAWS CloudFormation\nAWS CodeCommit\nAWS CodeDeploy\nAWS CodeBuild\nAWS CodePipeline\nAmazon CodeGuru\nAWS CodeStar\nAWS CodeArtifact\nAWS X-Ray\nAWS Fault Injection Simulator\nAmazon Elastic Container Registry (Amazon ECR)\nAmazon Elastic Container Service (Amazon ECS)\nAWS Fargate\nAmazon Elastic Kubernetes Services (Amazon EKS)\nAmazon Cognito\nRoute 53\nAWS Key Management Service (AWS KMS)\nAmazon EventBridge (Amazon CloudWatch Events)\nAmazon Simple Notification Service (Amazon SNS)\nAmazon Simple Queue Service (Amazon SQS)\nAWS Step Functions\n\nResources\n\nAWS Certified Developer\nA brief overview of the official documentation\nExam Preparation Guide\nSample Exam Questions\nhttps://github.com/itsmostafa/certified-aws-developer-associate-notes\nhttps://github.com/arnaudj/mooc-aws-certified-developer-associate-2020-notes\nFreeCodeCamp Youtube - AWS Certified Developer - Associate 2020\nHow-To Labs from AWS\nAWS Ramp-Up guides: Downloadable AWS Ramp-Up Guides offer a variety of resources to help you build your skills and knowledge of the AWS Cloud.\nCoursera's AWS Courses(Free to enroll via audit): AWS also provides various specializations in partnership with coursera\nAWS Architecture center: Provides reference architecture diagrams, vetted architecture solutions, Well-Architected best practices, patterns, icons, and more. This expert guidance was contributed by cloud architecture experts from AWS, including AWS Solutions Architects, Professional Services Consultants, and Partners.\nAWS Whitepapers: Expand your knowledge of the cloud with AWS technical content authored by AWS and the AWS community, including technical whitepapers, technical guides, reference material, and reference architecture diagrams.\nBack to Basics: Back to Basics' is a video series that explains, examines, and decomposes basic cloud architecture pattern best practices.\nAWS Heroes Content Library: AWS Hero authored content including blogs, videos, slide presentations, podcasts, and more.\nhttps://amazon.qwiklabs.com/catalog\nAWS Workshops: This website lists workshops created by the teams at Amazon Web Services (AWS). Workshops are hands-on events designed to teach or introduce practical skills, techniques, or concepts which you can use to solve business problems.\nhttps://wellarchitectedlabs.com/\nhttps://testseries.edugorilla.com/tests/1359/aws-certified-developer-associate\n\nCommunity posts\n\nhttps://dev.to/romankurnovskii/aws-certified-developer-associate-prepare-2np\nhttps://www.reddit.com/user/romankurnovskii/comments/x8rgig/what_is_the_topics_order_to_cover_to_get_prepared/?utm_source=share&utm_medium=web2x&context=3\nhttps://twitter.com/romankurnovskii/status/1567746601136832512\n",
            "tags": [
                "AWS"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/api-gateway/",
            "title": "API Gateway",
            "description": "Create, maintain, and secure APIs at any scale with Amazon API Gateway",
            "content": "\nAbout\n\nDocumentation\nUser Guide\n\nAPI Gateway provides the opportunity to create and expand your own REST and WebSocket APIs at any size.\n\nAPI endpoints can be cached to accommodate for frequent similar requests.\n\nUse Cases\n\nBuild a network for micros­ervices archit­ectures.\n\nAmazon CloudWatch metrics - Collects near-real-time metrics\n  Examples: 4XXError (client-side errors), 5XXError(server-side errors), CacheHitCount\nAmazon CloudWatch Logs** - Debug issues related to request execution\nAWS CloudTrail - Record of actions taken by a user, role, or an AWS service in API Gateway\nAWS X-Ray - Trace your request across different AWS Services\n\nDigests\n\nConcepts\n\nREST API, HTTP API, WebSocket API\nDeployment - point-in-time snapshot of your API Gateway API\nEndpoint - https://api-id.execute-api.region-id.amazonaws.com\n    Edge-optimized\n    Private\n    Regional\n    Stage - A logical reference to a lifecycle state of your API.\n    Route - URL path, Latency based routing,\n    Integration - Lambda, HTTP, Private VPC, CORS\n    Import/Export - Open API\n    AM User should have permission to enable logging\n\nAmazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale.\nStage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API. The act like environment variables and can be used in your API setup and mapping templates.\nWith deployment stages in API Gateway you can manage multiple release stages for each API, such as: alpha, beta, and production. Using stage variables you can configure an API deployment stage to interact with different backend endpoints.\nWhen you build an API Gateway API with standard Lambda integration using the API Gateway console, the console automatically adds the required permissions. However, when you set up a stage variable to call a Lambda function through our API, you must manually add these permissions.\nIntegration timeout for AWS, Lambda, Lambda proxy, HTTP, HTTP proxy - 50 ms to 29 seconds\nYou can enable API caching to cache your endpoint's responses, this reduces the number of calls made to your endpoint and improves the latency of requests to your API\nAWS Gateway Integration types:\n  AWS_ Proxy - lambda proxy integration\n  HTTP - http custom integration\n  HTTP_PROXY - http proxy\n\nPractice\n\nCreating a RESTful API Using Amazon API Gateway\n\nQuestions\n\nQ1\n\nYou are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)__ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\n\nA) HTTP proxy\nB) mock\nC) AWS service proxy\nD) Lambda function\n\n\n\nExplanation\n\n\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\n\nAmazon API Gateway supports mock integrations for API methods.\n\nB\n\nQ2\n\nA developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\n\nWhich architectures meet these requirements? (Select TWO.)\n\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store.\nCreate a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store\nCreate an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets.\nCreate a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store.\nEnable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store\n\n\nExplanation\n\n\nAWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\n\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\n\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n\n1, 2\n\nQ3\n\nA company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\n\nWhich AWS service should be used to refactor this architecture?\n\nAWS X-Ray\nAmazon SQS\nAWS Lambda\nAmazon API Gateway\n\n\nExplanation\n\n\n4\n\nQ4\n\nVeronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\n\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\n\nEnable request validation\nInclude the Amazon Resource Name (ARN) of the Lambda function\nChange the integration type\nCreate a mapping template\n\n\nExplanation\n\n\nAPI Gateway mapping template\n\n4\n\nQ5\n\nA developer is designing a full-stack serverless application. Files for the website are stored in an Amazon S3 bucket. AWS Lambda functions that use Amazon API Gateway endpoints return results from an Amazon DynamoDB table. The developer must create a solution that securely provides registration and authentication for the application while minimizing the amount of configuration.\n\nWhich solution meets these requirements?\n\nCreate an Amazon Cognito user pool and an app client. Configure the app client to use the user pool and provide the hosted web UI provided for sign-up and sign-in.\nConfigure an Amazon Cognito identity pool. Map the users with IAM roles that are configured to access the S3 bucket that stores the website.\nConfigure and launch an Amazon EC2 instance to set up an identity provider with an Amazon Cognito user pool. Configure the user pool to provide the hosted web UI for sign-up and sign-in.\nCreate an IAM policy that allows access to the website that is stored in the S3 bucket. Attach the policy to an IAM group. Add IAM users to the group.\n\n\nExplanation\n\n\n2\n\nQ6\n\nA company has moved a legacy on-premises application to AWS by performing a lift and shift. The application exposes a REST API that can be used to retrieve billing information. The application is running on a single Amazon EC2 instance. The application code cannot support concurrent invocations. Many clients access the API, and the company adds new clients all the time.\n\nA developer is concerned that the application might become overwhelmed by too many requests. The developer needs to limit the number of requests to the API for all current and future clients. The developer must not change the API, the application, or the client code.\n\nWhat should the developer do to meet these requirements?\n\nPlace the API behind an Amazon API Gateway API. Set the server-side throttling limits.\nPlace the API behind a Network Load Balancer. Set the target group throttling limits.\nPlace the API behind an Application Load Balancer. Set the target group throttling limits.\nPlace the API behind an Amazon API Gateway API. Set the per-client throttling limits.\n\n\nExplanation\n\n\n4\n",
            "tags": [
                "aws",
                "API Gateway"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudformation/_index",
            "title": "CloudFormation",
            "description": "Speed up cloud provisioning with infrastructure as code",
            "content": "\nAbout\n\nCloudF­orm­ation enables the user to design & provision AWS infras­tru­cture deploy­ments predic­tably & repeat­edly\n\nDocumentation\nUser Guide\n\nCloudFormation Flow\n\nTerminology\n\n| Component   | Description                                                                                                                                                                       |\n| ----------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Templates   | The JSON or YAML text file that contains the instructions for building out the AWS environment                                                                                    |\n| Stacks      | The entire environment described by the template and created, updated, and deleted as a single unit                                                                               |\n| StackSets   | AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation |\n| Change Sets | A summary of proposed changes to your stack that will allow you to see how those changes might impact your existing resources before implementing them                            |\n\nTemplate\n\nThe main sections in a CloudFormation template:\n\nParameters - specify variables requiring user input\nConditions - define entities based on a condition e.g. provision resources based on environment, or specify AMI for and EC2 instance to deploy based on region\nResources - the AWS resources to create\nMappings - create custom mappings e.g. RegionMap for Region : AMI\nTransforms - reference code located in S3 e.g. Lambda code or reusable snippets of CloudFormation code\n\nDigest\n\nA CloudFormation template will consist of a set of resources defined. These resources will be part of a single stack, once built. CloudFormation will treat all the resources as a collection of resources\nCloudFormation supports JSON and YAM for its template languages.\nAll ID's are unique to each region, account, and VPC. It is best practice to not embed such IU's inside a CloudFormation template. Instead, define parameters, mappings and conditions to create a dynamic template that could be run across VP's, Regions or even accounts\nCloudformation stackset vs changeset vs nested stack\nNested stacks - stacks created as part of other stacks. You create a nested stack within another stack by using the AWS: CloudFormation:Stack resource. For example, assume that you have a load balancer configuration that you use for most of your stacks. Instead of copying and pasting\nthe same configurations into your templates, you can create a dedicated template for the load balancer. Then, you just use the resource to reference that template from within other templates\nChange Sets will produce a summary of changes and their impact on the resources.\nStackSets is used for deploying or managing template resources across accounts and/or regions.\nSting, Number, List are supported data type in CFT\nIncluding lambda function as zipfile parameter in CFT is the easiest way to deploy lambda function\nIf stack creation fails, AWS CloudFormation rolls back any changes by deleting the resources that it created.\nFn:FindInMap to perform a dynamic lookup in Cloud formation template\nTransform section of Cloud formation specifies version of SAM model to use.\nTwo templates, one for Intra and one for App.\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Provision\n\nSame type services: CloudF­orm­ation, Service Catalog, OpsWorks, Market­place\n\nManage infrastructure with DevOps\n\nAutomate, test, and deploy infrastructure templates with continuous integration and delivery (CI/CD) automations.\n\nScale production stacks\n\nRun anything from a single Amazon Elastic Compute Cloud (EC2) instance to a complex multi-region application.\n\nShare best practices\n\nDefine an Amazon Virtual Private Cloud (VPC) subnet or provisioning services like AWS OpsWorks or Amazon Elastic Container Service (ECS) with ease.\n\nCompare\n\nCloudFormation vs Elastic Beanstalk\n\n   | CloudFormation                               | Elastic Beanstalk                                                                        |\n   | -------------------------------------------- | ---------------------------------------------------------------------------------------- |\n   | “Template-driven provisioning”               | “Web apps made easy”                                                                     |\n   | Deploys infrastructure using code            | Deploys applications on EC2 (PaaS)                                                       |\n   | Can be used to deploy almost any AWS service | Deploys web applications based on Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker |\n   | Uses JSON or YAML template files             | Uses ZIP or WAR files                                                                    |\n   | Similar to Terraform                         | Similar to Google App Engine                                                             |\n\nComparing CloudFormation Init and EC2 User Data\n\nCloudFormation Init and EC2 User Data\n\nWith EC2 Instance user data, developers are able to run commands and scripts during the launch of an EC2 instance. User data can be used to install necessary packages, update the ownership of files and directories, or even update or run services. Developers who are familiar with shell scripting may find user data as the easiest way to incorporate launch instructions for EC2 instances. \n\nEC2 Instance user data allows for a procedural-based approach to configuring an instance during launch.\n\nThe following snippet represents a UserData property of an EC2 instance defined within a CloudFormation template:\n\nalt\n\nThe shell script above begins with an update and installation of the httpd Apache service using the yum package manager. The systemctl start and enable commands start the Apache server and allow it to serve content from the EC2 instance. The cat command adds an HTML snippet to the index.html file located in the /var/www/html/ directory of the instance. Once the user data script is completed, you can view the HTML content by accessing the EC2 instance's public URL.\n\nIt's important to note that this user data script runs only when the EC2 instance is launched. \n\nCloudFormation Init (cfn-init)\n\nAWS provides CloudFormation helper scripts like cfn-init to help fine-tune your stack templates to better fit your needs. CloudFormation cfn-init allows developers to establish a desired state of their instance using metadata. This means that these configurations can be updated and run on the same instance over time.\n\nThe following snippet represents the AWS::CloudFormation::Init metadata type for an EC2 instance defined within a CloudFormation template:\n\nalt\n\nThe config section details the packages, files, and services to be configured on the EC2 instance. This eases the burden of managing a bash script since each type of configuration is held in its own dedicated section. The snippet performs the same configuration on the EC2 instance as the previous user data example. \n\nUnlike EC2 user data, the cfn-init script does not run automatically. The next lab step will cover how to utilize helper scripts in a CloudFormation stack deployment.\n\nKey Differences\n\nInstance user data is procedural-based, while CloudFormation init can be used to achieve the desired state of an instance\nWhen you update the instance user data in CloudFormation and perform a stack update, the instance is terminated and replaced. However, when you update the CloudFormation init metadata and perform a stack update, the instance will be updated in place\nInstance user data is run only once during the instance launch\nThe success or failure of a user data script does not affect a CloudFormation stack creation process. With the CloudFormation signal helper script, a successful stack creation relies on a successful instance configuration (More on this in the next lab step)\n\nPractice\n\nInitializing Amazon EC2 Instances with AWS CloudFormation Init\n\nQuestions\n\nQ1\n\nYou are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.\n\nWhat steps can you take in this situation? (Choose 2 answers)\n\nUse a template to first create Resource A with the ARN as an output value.\nUse a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt.\nHard code the ARN value output from creating Resource A into the second template.\nJust create Resource B. \n\n\nExplanation\n\n\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n\n2\n",
            "tags": [
                "aws",
                "cloudformation"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudformation/initializing-ec2-with-cloudformation/",
            "title": "Initializing Amazon EC2 Instances with AWS CloudFormation Init",
            "description": "Initializing Amazon EC2 Instances with AWS CloudFormation Init",
            "content": "\nLab\n\nInitializing Amazon EC2 Instances with AWS CloudFormation Init\n\nEstablishing Desired EC2 Instance State with AWS CloudFormation Init\n\nIn the AWS Console search bar, search for cloudformation and click the CloudFormation result under Services:\n\nalt\n\n2\\. Click the Create stack dropdown menu and select With new resources:\n\nalt\n\n3\\. In the Create stack form, in the Specify template section, ensure Amazon S3 URL **is selected for the **Template source.\n\nPaste in the following URL in the Amazon S3 URL field:\n\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: Provision a Single Amazon EC2 Instance with CFN Helper Scripts\n\nParameters:\n  AmiID:\n    Description: The ID of the AMI.\n    Type: AWS::SSM::Parameter::Value\n    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-x86_64-gp2\n\nResources:\n  WebServer:\n    Type: AWS::EC2::Instance\n    Properties:\n      ImageId: !Ref AmiID\n      InstanceType: t3.micro\n      SecurityGroupIds:\n        !Ref WebServerSecurityGroup\n      UserData:\nUpdate aws-cfn-bootstrap\nRun cfn-init to initialize WebServer content\nReturn cfn-init run result to CloudFormation upon completion\n        Fn::Base64:\n          !Sub |\n            #!/bin/bash -xe\n            yum update -y aws-cfn-bootstrap\n            /opt/aws/bin/cfn-init -v --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region}\n            /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region}\n    CreationPolicy:\n      ResourceSignal:\n        Count: 1\n        Timeout: PT5M\n    Metadata:\n      AWS::CloudFormation::Init:\n        config:\n          packages:\n            yum:\n              httpd: []\n          files:\n            \"/var/www/html/index.html\":\n              content: |\n\n                    Cloud Academy EC2 Instance\n                    This content has been initialized with AWS CloudFormation Helper Scripts\n\n              mode: '000644'\n          services:\n            sysvinit:\n              httpd:\n                enabled: 'true'\n                ensureRunning: 'true'\n\n  WebServerSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: SSH and HTTP\n      SecurityGroupIngress:\n      CidrIp: 0.0.0.0/0\n        FromPort: 22\n        IpProtocol: tcp\n        ToPort: 22\n      CidrIp: 0.0.0.0/0\n        FromPort: 80\n        IpProtocol: tcp\n        ToPort: 80\n\nOutputs:\n  WebServerPublicDNS:\n    Description: Public DNS of EC2 instance\n    Value: !GetAtt WebServer.PublicDnsName\n\nalt\n\nThe CloudFormation stack template is stored in a public S3 bucket. The EC2 instance resource definition is shown below:\n\nalt\n\nThe WebServer EC2 instance is defined above. It is a size t3.micro instance that references a WebServerSecurityGroup resource for its security group and the AmiID parameter for its image ID. Both of these referenced configurations are defined elsewhere in the template. \n\nThe UserData script defined next performs the following tasks once the EC2 instance is created:\n\nUpdates the aws-cfn-bootstrap package to retrieve the latest version of the helper scripts\nRuns the cfn-init helper script to execute the WebServer instance Metadata scripts\nRuns the cfn-signalhelper script to notify CloudFormation after all the service(s) (Apache in this case) is installed and configured on the EC2 instance\n\nNote: The cfn-init helper script is not executed automatically. You must run the cfn-init script within the EC2 instance UserData in order to execute your metadata scripts.\n\nThe cfn-signal helper script works hand-in-hand with the CreationPolicy configuration. The ResourceSignal property has a Count of 1 and a Timeout of PT5M. This instructs CloudFormation to wait for up to 5 minutes to receive 1 resource signal from the EC2 instance.\n\nThe cfn-signal helper script call in the UserData uses $? to retrieve the return code of the previous script. If the cfn-init script is successful and the EC2 instance is configured properly, cfn-signal returns a success to CloudFormation which then transitions the EC2 instance to the CREATE_COMPLETEstatus. If the cfn-init script is unsuccessful or the timeout of 5 minutes expires before receiving a signal, then the EC2 instance will be transitioned to a CREATE_FAILEDstatus and the stack deployment will fail. \n\nThe EC2 instance Metadata configuration is the same as the previous lab step. It defines a AWS::CloudFormation::Init script to install the httpd package using yum, generate an index.html file within /var/www/html/ and start the httpd service to serve the content from the EC2 instance.\n\n5\\. Click *Next *to continue:\n\nalt\n\n6\\. Enter web-server-stack for the Stack name and click Next:\n\nalt\n\n7\\. You will not be configuring additional stack options. Scroll to the bottom of the page and click Next.\n\n8\\. On the review page, scroll to the bottom and click *Create stack *to deploy your stack:\n\nalt\n\nYour stack will begin deploying and you will be brought to the Events page of your web-server-stack:\n\nalt\n\nThe stack can take up to 3 minutes to deploy successfully. \n\n9\\. If the Events section does not automatically refresh after 3 minutes, click the refresh icon:\n\nalt\n\nalt\n\nThe WebServer instance remains in a CREATE\\_IN\\_PROGRESS status until CloudFormation receives a SUCCESS signal from the instance. In the screenshot above, the UniqueId of i-0fd18c8deb52983d5 belongs to the WebServer instance. \n\nAfter the success signal is received, the WebServer instance is transitioned into the CREATE_COMPLETE status. \n\nWithout the CloudFormation signal helper script, CloudFormation would have transitioned the EC2 instance to a completed status when the resource was created instead of waiting until the Apache service has been installed and running on the instance. \n\n10\\. Click the Outputs tab on the web-server-stack page:\n\nalt\n\n11\\. Right-click and open the WebServerPublicDNS URL in a new browser tab:\n\nalt\n\nThe HTMLpage generated in the cfn-init script is now being served from the Apache server running within your WebServer EC2 instance:\n\nalt\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudfront/_index",
            "title": "CloudFront",
            "description": "Securely deliver content with low latency and high transfer speeds",
            "content": "\nAbout\n\nSecurely deliver data, videos, applic­ations, and APIs to customers globally with low latency, and high transfer speeds\n\nCloudFront is a distributed content delivery network (CDN) that enables easy delivery of web content to end users from a pool of web servers around the globe\n\nDocumentation\nUser Guide\n\n\n\nCloudFront is a global service:\n\nIngress to upload objects.\nEgress to distribute content.\n\nTerminology\n\nEdge Location:** The location where content is cached to be accessed by users. These are READ/WRITE.\nCDN:** A collection of Edge Locations that can distribute content around the world.\nOrigin:** The origin of all files the CDN will distribute. E.g.\n  an S3 bucket hosting some images, or hosting a static website\n  an EC2 instance running a website with dynamic content\n  an ELB pointing to several EC2 instances\n  a DNS endpoint using Route53\n  any origin server, even non-AWS\nDistribution**: The name of the CDN.\n  Web Distribution: Used for delivering content over HTTP/HTTPS. Can be either an S3 bucket or a web server (EC2/non-AWS). Cannot serve multimedia content.\n  RTMP Distribution: Uses RTMP for media streaming and flash multimedia content. Probably what Netflix uses.\n\nPrice\n\nCurrent price\n\nThere is an option for reserved capacity over 12 months or longer (starts at 10TB of data transfer in a single region).\n\n   | Pay                             | do not pay                                        |\n   | ------------------------------- | ------------------------------------------------- |\n   | Data Transfer Out to Internet   | Data transfer between AWS regions and CloudFront. |\n   | Data Transfer Out to Origin     | Regional edge cache.                              |\n   | Number of HTTP/HTTPS Requests   | AWS ACM SSL/TLS certificates.                     |\n   | Invalidation Requests           | Shared CloudFront certificates.                   |\n   | Dedicated IP Custom SSL         |\n   | Field level encryption requests |\n\nUse Cases\n\nType: Content delivery networks\n\nPractice\n\nConfiguring a Static Website With S3 And CloudFront\n\nQuestions\n\nQ1\n\nA company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.\n\nWhich API command can a developer use to ensure cache storage consistency with minimal duplication?\n\nA) CreateReusableDelegationSet with Route 53\nB) CreateStackSet with CloudFormation\nC) CreateGlobalReplicationGroup with ElastiCache\nD) CreateCachePolicy with CloudFront\n\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html\n\nD\n\nQ2\n\nA developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\n\nWhich architectures meet these requirements? (Select TWO.)\n\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store.\nCreate a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store\nCreate an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets.\nCreate a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store.\nEnable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store\n\n\nExplanation\n\n\nAWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\n\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\n\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n\n1, 2\n",
            "tags": [
                "aws",
                "CloudFront"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudfront/configuring-static-website-s3-and-cloudfront/",
            "title": "Configuring a Static Website With S3 And CloudFront",
            "description": "Configuring a Static Website With S3 And CloudFront",
            "content": "\nPractice\n\nLab link\n\nCreating an Amazon S3 Bucket for a Static Website\n\n1\\. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n\nalt\n\nYou will be placed in the Amazon S3 console.\n\n2\\. To start creating a new Amazon S3 bucket, in the top-right, click Create bucket:\n\nalt\n\nThe Amazon S3 bucket creation form will load.\n\n3\\. Under General configuration, enter the following:\n\nBucket name**: Enter _calabs-bucket-&lt;UniqueNumber&gt; _(Append a unique number to the end of calabs-bucket-)\nRegion: Ensure **US West (Oregon) us-west-2 is selected\n\nalt\n\nYou have added a unique number to the bucket name because Amazon S3 bucket names must be unique regardless of the AWS region in which the bucket is created.\n\nA bucket name must also be DNS compliant. Here are some of the rules it must adhere to:\n\nThey must be at least 3 and no more than 63 characters long.\nThey may contain lowercase letters, numbers, periods, and/or hyphens.\nEach label must start and end with a lowercase letter or a number.\nThey cannot be formatted as an IP address (for example, 192.168.1.1).\n\nThe following are examples of valid bucket names:\n\ncalabs-bucket-1\ncloudacademybucket\ncloudacademy.bucket\ncalabs.1\nca-labs-bucket\n\nMake a note of the name of your bucket, you will use it later.\n\n4\\. Make sure to select ACLs Enabled:\n\nalt\n\n5\\. In the Block Public Access section, un-check the Block all public access check-box:\n\nalt\n\n6\\. To acknowledge that you want to make this bucket publicly accessible, check I acknowledge that the current settings might result in this bucket and the objects within becoming public:\n\nalt\n\n7\\. To finish creating your Amazon S3 bucket, scroll to the bottom of the form and click Create bucket:\n\nalt\n\nNote: If you see an error because your bucket name is not unique, append a different unique number to the bucket name. For example, change \"calabs-bucket\" to \"calabs-bucket-1\" (or a unique number/character string) and click Create bucket again.\n\nThe Buckets list page will load and you will see a notification that your bucket has been created:\n\nalt\n\nNext, you will enable static website hosting for your bucket.\n\n8\\. In the list, click the name of your bucket:\n\nalt\n\nYou will see an overview of your Amazon S3 bucket, and a row of tabs with Objects selected.\n\n9\\. In the row of tabs under Bucket overview, click Properties:\n\nalt\n\nThe Properties tab allows you to enable and disable various Amazon S3 bucket features, including:\n\nBucket Versioning**: Old versions can be kept when objects are updated\nDefault encryption**: A bucket can be configured to encrypt all objects by default\nServer access logging**: Web-server style access logs can be enabled\nRequester pays**: When enabled, the entity downloading data from this bucket will pay data transfer costs incurred\n\n10\\. Scroll to the bottom of the Properties page and in the Static website hosting section, on the right, click Edit:\n\nalt\n\nThe Edit static website hosting form will load.\n\n11\\. In the Static website hosting field, select Enable:\n\nalt\n\nThe form will expand to reveal more configuration options.\n\n12\\. Enter the following, leaving all other fields at their defaults:\n\nIndex document**: Enter index.html\nError document**: Enter error/index.html\n\nalt\n\n13\\. To finish enabling static website hosting, scroll to the bottom, and click Save changes:\n\nalt\n\nThe bucket overview page will load and you'll see a notification that you have successfully enabled static website hosting:\n\nalt\n\nYour S3 bucket is ready to host content.\n\nNext, you will create a bucket policy that will apply to all objects uploaded to your bucket.\n\n14\\. In the row of tabs, click Permissions:\n\nalt\n\n15\\. Scroll down to the Bucket policy section, and on the right, click Edit.\n\nThe Edit bucket policy form will load.\n\nAmazon S3 bucket policies are defined in JavaScript Object Notation, commonly referred to as JSON.\n\n16\\. In the Policy editor, copy and paste the following and replace YOUR_BUCKET_NAME with the name of your S3 bucket:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AddPerm\",\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": \"arn:aws:s3:::YOUR_BUCKET_NAME/*\"\n    }\n  ]\n}\n\nThis policy will allow public access to all objects in your S3 bucket.\n\nThis is a permissive policy. In a non-lab environment, security concerns may require you to implement a more restrictive policy. To learn more about bucket policies, visit the AWS Policies and Permissions in Amazon S3 documentation.\n\n17\\. To save your bucket policy, at the bottom of the page, click Save changes.\n\nThe bucket overview page will load and you will see a notification that the policy has been edited.\n\nNext, you will download a basic website from a public GitHub repository and load it into your S3 bucket.\n\n18\\. To download a zip file containing a basic website, click here.\n\nThis is an example website provided by CloudAcademy that is suitable for static website hosting.\n\n19\\. Extract the zip to your local file system.\n\nExact instructions will vary depending on your operating system and browser. In most browsers, you can click the downloaded file and a file extraction utility will open.\n\n20\\. In the row of tabs, click Objects.\n\n21\\. To begin uploading the website to your Amazon S3 bucket, scroll down and click Upload:\n\nalt\n\nThe Upload form will load.\n\n22\\. In the Files and folders section, click Add files:\n\nalt\n\nA file picker will open.\n\n23\\. Using the file picker, select all files and folders from inside the zip file you downloaded and extracted earlier.\n\nIf your extraction utility extracted the files to a folder called static-website-example-master, ensure you upload all the files and folders inside it but not the static-website-example-master folder itself. To be able to access the website, the index.html file must be at the top-level of your Amazon S3 bucket.\n\nYou may find it easier to drag and drop the files and folders onto the Upload page instead of using the file picker.\n\nYou may also see a browser confirmation dialog asking you to confirm you want to upload the files.\n\nOnce selected, the files and folders from the zip file will appear in the Files and folders section.\n\n24\\. Scroll to the bottom of the page and click Upload.\n\nYou will see a blue notification displaying the progress of the upload, and when complete you will see a green Upload succeeded notification.\n\n25\\. To exit the Upload form, on the right, click Close.\n\nThe bucket overview page will load.\n\nYour Objects section should look similar to:\n\nalt\n\nNext, you will test that your website is accessible.\n\n26\\. To retrieve the endpoint for your bucket, click the Properties tab, scroll to the bottom, and click the copy icon next to the Bucket website endpoint:\n\nalt\n\n27\\. Paste the endpoint into the address bar of a new browser tab.\n\nYou will see a website load that looks like this:\n\nalt\n\nThis website is being served by your Amazon S3 bucket.\n\nCreating an Amazon CloudFront Distribution for the Static Website\n\n\n1\\. In the AWS Management Console search bar, enter CloudFront, and click the CloudFront result under Services:\n\nalt\n\nThe Amazon CloudFront console will load.\n\n2\\. To start creating a distribution, click Create a CloudFront Distribution:\n\nalt\n\n3\\. Under Origin, in the Origin Domain text-box, enter the Amazon S3 static website hosting endpoint that you created earlier:\n\nalt\n\n4\\. Scroll down to the* Settings* settings, in the Default Root Object text-box, enter index.html:\n\nalt\n\nYou are setting this field because Amazon CloudFront doesn't always transparently relay requests to the origin. If you did not set a default root object on the distribution you would see an AccessDenied error when you access the CloudFront distribution's domain later in this lab step.\n\nTo learn more, see the How CloudFront Works if You Don't Define a Root Object section of the AWS developer guide for Specifying a Default Root Object.\n\n5\\. Leave all other settings at their default values, scroll to the bottom, and click Create Distribution.\n\nThe CloudFront distribution list page will load and you will see your distribution listed.\n\nYou will see the Last Modified of your distribution is Deploying:\n\nalt\n\nIt can take up to 15 minutes to deploy a new Amazon CloudFront distribution. Once complete, the Status will change to Enabled.\n\nThere are two main types of origin that Amazon CloudFront supports, Amazon S3 buckets, and custom origins. A custom origin could be a website being served by an EC2 instance, or it could be a web server outside of AWS. To learn more while your CloudFront distribution is deploying, visit the AWS Using Amazon S3 Origins, MediaPackage Channels, and Custom Origins for Web Distributions page. \n\nOnce your deployment is complete, continue with the instructions.\n\n6\\. To view details of your distribution, click the random alphanumeric ID:\n\nalt\n\nNote: Your ID will be different.\n\nalt\n\n7\\. Copy the value of the Distribution Domain Name field:\n\nalt\n\n\n8\\. Paste the domain name into the address bar of a new browser tab.\n\nYou will see the website that you uploaded to your Amazon S3 bucket display:\n\nalt\n\nYou are accessing the website through your Amazon CloudFront distribution.\n\nNote: The instructions below are optional. Perform them if there is enough time left in the lab.\n\n9\\. On the website, click through and visit the different pages a few times to generate traffic.\n\nIf you have a different web browser available, try accessing the site in the other browser.\n",
            "tags": [
                "aws",
                "elasticache"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudwatch/_index",
            "title": "CloudWatch",
            "description": "Amazon CloudWatch",
            "content": "\nAbout\n\nCloudWatch offers a reliable, scalable, & flexible monitoring solution that can easily start.\n\nCloudWatch can provide a dependable, scalable, & flexible monitoring solution that is simple to set up.\n\nDocumentation\nUser Guide\n\nAmazon CloudWatch Flow\n\nCloudWatch vs CloudTrail\n\n   | CloudWatch                                              | CloudTrail                                                               |\n   | ------------------------------------------------------- | ------------------------------------------------------------------------ |\n   | Performance monitoring                                  | Auditing                                                                 |\n   | Log events across AWS Services – think operations       | Log API activity across AWS services – think activities, or who to blame |\n   | Higher-level comprehensive monitoring and event service | More low-level, granular                                                 |\n   | Log from multiple accounts                              | Log from multiple accounts                                               |\n   | Logs stored indefinitely                                | Logs stored to S3 or CloudWatch indefinitely                             |\n   | Alarms history for 14 days                              | No native alarming; can use CloudWatch alarms                            |\n\nDigest\n\n\nStandard vs High resolution metrics.\nCloudWatch unified agent collect both system metrics and log files from Amazon EC2 Instances and on-premise servers. The agent supports both Windows Server and Linux, and enables to select the metrics to be collected, including sub-resource metrics such as per-CPU core. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server.\nCloudwatch events could be used for scheduling lambda functions.\nRAM Utilization needs custom metrics\nwhich use-cases are covered in Standard metrics and which ones would need custom metrics.  (memory utilization needs custom metrics)\nBasic (5 min) vs detailed (1 min) vs high resolution (1s) monitoring.\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Operate\n\nSame type services: CloudWatch, CloudTrail, Systems Manager, Cost & usage report, Cost explorer, Managed Services\t\n\nPractice\n\nIntroduction to CloudWatch\n\nQuestions\n\nQ1\n\nA Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.\n\nWhich of the following enables monitoring of the metric in Amazon CloudWatch?\n\nRetrieve the log data from AWS CloudTrail using the LookupEvents API Call\nRetrieve the log data from CloudWatch using the GetMetricData API call\nLaunch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application\nInstall the Amazon CloudWatch logs agent on the EC2 instance that the application is running on\n\nExplanation\n\n\n4\n\nQ2\n\nA developer must use AWS X-Ray to monitor an application that is running on an Amazon EC2 instance. Developer has prepared the application by using the X-Ray SDK.\n\nWhat should the developer do to perform the monitoring?\n\nConfigure the X-Ray SDK sampling rule and target. Activate the X-Ray daemon from the EC2 console or the AWS CLI with the modify-instance-attribute command to set the XRayEnabled flag.\nInstall the X-Ray daemon. Assign an IAM role to the EC2 instance with a policy that allows writes to X-Ray.\nInstall the X-Ray daemon. Configure it to forward data to Amazon EventBridge (Amazon CloudWatch Events). Grant the EC2 instance permission to write to Event Bridge (CloudWatch Events).\nDeploy the X-Ray SDK with the application, and instrument the application code. Use the SDK logger to capture and send the events.\n\n\nExplanation\n\n\n3\n\nQ3\n\nA developer is designing an AWS Lambda function to perform a maintenance activity. The developer will use Amazon EventBridge (Amazon CloudWatch Events) to invoke the function on an hourly schedule. The developer wants the function to log information at different levels of detail according to the value of a log level variable. The developer must design the function so that the log level can be set without requiring a change to the function code.\n\nWhich solution will meet these requirements?\n\nA. Add a custom log level parameter for the Lambda function. Set the parameter by using the Lambda console.\nSet the log level in the Amazon CloudWatch Logs console.\nSet the log level in a Lambda environment variable.\nAdd a custom log level parameter for the Lambda function. Set the parameter by using the AWS CLI.\n\n\nExplanation\n\n\n3\n",
            "tags": [
                "aws",
                "cloudwatch"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cloudwatch/introduction-to-cloudwatch/",
            "title": "Introduction to CloudWatch",
            "description": "Introduction to CloudWatch",
            "content": "\nLab\n\nIntroduction to CloudWatch\n\nExplore CloudWatch\n\n1\\. AWS has done an excellent job defining CloudWatch key concepts. Read the abbreviated excerpt from their official documentation below to obtain an understanding of Metrics, Namespaces and Alarms: \nMetrics\n\nA metric is the fundamental concept in CloudWatch and represents a time-ordered set of data points. These data points can be either your custom metrics or metrics from other services in AWS. You or AWS products publish metric data points into CloudWatch and you retrieve statistics about those data points as an ordered set of time-series data. Metrics exist only in the region in which they are created.\n\nThink of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another.\n\nNamespaces\n\nCloudWatch namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.\n\nNote: In this lab you will see namespaces that AWS has created for you, and a custom namespace created by the steps performed in this lab.\n\nAlarms\n\nYou can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.\n\nAlarms invoke actions for sustained state changes only. CloudWatch alarms will not invoke actions simply because they are in a particular state. The state must have changed and been maintained for a specified number of periods.\n\nThe interested student can take a look at the full version of the documentation here. Due to time constraints, you should look at additional documentation once you have completed the lab.\n\n2\\. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n\nalt\n\n3\\. Click Metrics > All metrics in the left navigation pane. At this point, there are most likely no custom namespaces. But several AWS namespaces may already be established for you. What metrics are listed on the *All metrics *tab depends on a couple of factors:\n\nHow quickly you arrived at this view after starting your lab. This lab creates an EC2 instance and EBS volume when you start the lab. After a couple of minutes of delay, metrics for the EC2 and EBS namespaces are included.\nHow recently your Cloud Academy AWS account has been used to complete other Cloud Academy labs. If the AWS account you logged in to recently completed other labs, you may see namespace related to metrics collected in those labs.\n\nalt\n\n4\\. Spend a few minutes to explore what metrics and namespaces look like in the CloudWatch console.  Simply select any namespace and then any particular metric. As an example, the EC2 namespace and CPUUtilization **metric for the **HighCPUInstance are selected in the image below:\n\nalt\n\nNote: The image above is for illustrative purposes only, you do not need to choose the same instance or metric to explore CloudWatch metrics.\n\nThe longer the instance has been running, the more data points will appear in the graph. By default, EC2 metrics are collected every five minutes. You may need to adjust the displayed timeline to 1 week (1w) or further in the past to see some metrics.\n\nMonitoring EC2 Instances\n\n1\\. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\nalt\n\n2\\. Click Instances **from the navigation pane and select the box near the instance name. A wealth of instance information is displayed in the **Details tab:\n\nalt\n\nWhen you started the Lab, Cloud Academy configured the lab environment for you. This includes a medium instance named HighCPUInstance.\n\nNote: Your information will vary. There is additional instance information not shown in the example above.\n\n3\\. Switch to the Monitoring tab and take a look at the standard metrics:\n\nalt\n\nNote: If you don't see an instance yet, it's possible that it's still provisioning in the background. Refresh the page every minute or so until it appears.\n\nThese are the standard metrics that CloudWatch monitors for all your EC2 instances. Please refer to the documentation for details. (Due to possible time constraints, please look up additional information in the documentation after completing this lab.)\n\nYou should be aware that all the metrics in this tab related to Disk (Disk Reads, Disk Read Operations, Disk Writes, Disk Write Operations) pertain to ephemeral storage disks. Those metrics will not represent anything if you have launched an EBS backed instance. To see the metrics related to EBS volumes you need to look elsewhere. Next you will take a look at the metrics of the EBS volume for this particular instance.\n\nNote: Ephemeral storage is also known as instance storage. It is temporary storage that is added to your instance, unlike EBS which is an attached volume that is permanent in nature.\n\n4\\. To enable and disable detailed monitoring, click Manage detailed monitoring:\n\nalt\n\nThe Detailed monitoring page will open :\n\nalt\n\nHere you can enable and disable detailed monitoring by checking or unchecking the Enable checkbox followed by clicking Save. \n\n5\\. Click Cancel as we will not be enabling detailed monitoring in this lab:\n\nalt\n\n6\\. Reselect the HighCPUInstance , click the Storage tab. Scroll down and click on the Volume Id (lower right):\n\nalt\n\n7\\. Select the volume and click on the Monitoring tab to see the metrics for this EBS volume:\n\nalt\n\nAs you can see, Amazon does quite a bit out of the box with respect to monitoring EC2 Instances and EBS volumes. However, you can enable Detailed Monitoring for even more control over the monitoring frequency of EC2 instances. CloudWatch monitors EC2 instances every 5 minutes by default. If you need more frequent monitoring, you can enable CloudWatch's Detailed Monitoring feature to monitor your instances every minute. You can enable Detailed Monitoring during the instance launch or change it anytime afterwards. Note: Detailed Monitoring does come with an associated cost.\n\nInstall the EC2 Monitoring Scripts\n\n\n1\\. Navigate to EC2 Instances by clicking here.\n\n2\\. Click on Launch instances:\n\nalt\n\n3\\. In the Application and OS Images section, select the Amazon Linux option under Quick Start:\n\nalt\n\n4\\. In the Instance Type section, you should not change any options. Simply make sure the default *t2.micro *is selected:\n\nalt\n\n5\\. In the Key pair section, select the keypair:\nalt\nNote: Your keypair may differ from the screenshot.\nReminder: The PEM or PPK formatted key pair can be downloaded directly from the Your lab data section of the Cloud Academy Lab page at any time.\n\n6\\. Scroll down and expand the Advanced details section. Under IAM instance profile, select the IAM role provided. It will have a name that looks similar to cloudacademylabs-EC2MonitoringRole-XXXXXXXXXX :\n\nalt\n\n7\\. Scroll down to Detailed CloudWatch monitoring and select Enable:\n\nalt\n\n8\\. Scroll down to User data and copy and paste the following bash script code in the User data (As text) field:\n\nThis is where the magic happens. Next you will insert the code to execute during the instance launch. However, in order to send metrics to CloudWatch, you need to configure some credentials first. You can use either Access Keys or IAM roles for this task. In this Lab, you will follow the best practices and use IAM roles. There is an instance role already created in you account configured with the proper permissions.\n\nCopy code\n\n#!/bin/bash\nyum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64\nwget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip\nunzip CloudWatchMonitoringScripts-1.2.2.zip\nrm CloudWatchMonitoringScripts-1.2.2.zip\necho \"*/1 * * * * /aws-scripts-mon/mon-put-instance-data.pl --mem-util --disk-space-util --disk-path=/ --from-cron\" > monitoring.txt\ncrontab monitoring.txt\nrm monitoring.txt\n\nalt\n\nThis bash script will get executed the first time the instances launches. In summary, the script will:\n\nInstall Perl libraries\nRetrieve and install the AWS CloudWatch Monitoring scripts\nConfigure crontab to run the monitoring script every minute\n\n9\\. In the Summary section, click Launch instance:\n\nalt\n\nA confirmation page will let you know that your instance is launching:\nalt\n\n 10\\. Click View all instances.\n\nNotice the Name for the new instance is blank by default. Although not mandatory, it is helpful to have a name. Move your mouse into the blank space in the Name column. It turns to an edit pencil. Use the pencil to change your Instance Name to Monitoring Scripts:\n\nalt\n\nalt\n\nWait until the* Instance State* is Running for the new Instance. It typically takes less than one minute for the state to transition from Pending to Running.\n\nalt\n\n11\\. Navigate back to CloudWatch by clicking here **and clickAll metrics from the navigation pane. Notice that there is a new namespace called **System/Linux **under Custom namespaces**:\n\nalt\n\nThis name is configured when you send the custom metrics.\n\nNote: If you don't see the new Namespace wait a few minutes and refresh the page. CloudWatch takes some time to display the information in the dashboard. Recall that the newly installed monitoring scripts send data every minute based on the crontab configuration setup in the User data bash script for the instance.\n\n12\\. Click on the new System/Linux namespace:\n\nalt\n\nThere are two metrics being monitored by CloudWatch in the custom System/Linux namespace. (Filesystem, InstanceId, MountPath and InstanceId)\n\n13\\. Click the metric on the left (Filesystem...), then select the checkbox so the first metric is graphed.\n\nalt\n\n14\\. Click Linux System, so the Metrics path is All > Linux System again. Now select the metric on the right (InstanceId) and select its checkbox as well. \n\nalt\n\nalt\n\nalt\n\n15\\. Switch to the Graphed metrics tab. If you selected both metrics correctly the tab will include a \"(2)\" at the end of it indicating how many metrics are graphed. Your graph should look similar to the following:\n\nalt\n\nIt is simple to customize the display to meet your needs for the metrics displayed.\n\n16\\. Click the custom graph period drop-down above the graph display area and select *15 *from the *Minutes *row:\n\nalt\n\n17\\. Select the Period drop-down column menu for each metric in the lower Graphed metrics tab and choose 1 Minute:\n\nalt\n\nYou can now see the highest resolution metrics that are being sent to CloudWatch every minute. (You may need to refresh the chart after setting the new periods)\n\n18\\. Select Maximum for the *Statistic *column. Instead of an average of the datapoints, the maximum will be graphed. (Note: In the lab example it is probably the same since the disk really has not been touched) Your configuration should look like:\n\nalt\n\nCreating Your First CloudWatch Alarm\n\n1\\. Navigate to CloudWatch by clicking here, click on Alarms **> All Alarms** in the left pane:\n\nalt\n\nThere are no Alarms configured, so there are no records found. Further, the three types of Alarms are all at zero (0).\n\nNote: More information on Alarm states will be covered soon.\n\n 2\\. Click Create Alarm and click Select metric. Select the EC2 namespace:\n\nalt\n\nMany different metrics are displayed for both the HighCPUInstance and the Monitoring Scripts instances.\n\nalt\n\n3\\. Click Per-instance metrics, scroll down and select the metric with HighCPUInstance **under **Instance Name **and CPUUtilization under **Metric Name:\n\nalt\n\nTip: You may need to use the arrows in the upper right to find the HighCPUInstance on another page. Alternatively, you can make note of the last 3 or 4 characters in the InstanceId from the EC2 console, then enter those in the Search Metrics field. The search applies to all pages of information.\n\nOnce selected it is graphed immediately. Notice that you could tailor the graph to a specific Time Range (upper-right). For example, the time range can be specified in Relative or Absolute terms. \n\n4\\. Click Select metric when ready. \n\n 5\\. Under Conditions, set the following values leaving the defaults for the rest:\n\nWhenever High CPU is...**: _Greater/Equal\nThan...**: 50\n\nalt\n\nAn alarm watches for a metric to go beyond an allowable value range when monitored over time. If violated the alarm's state is changed. There are three possibles states for an alarm:\nOK—The metric is within the defined threshold\nIn alarm—The metric is outside of the defined threshold\nInsufficient data—The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state\n\n6\\. Click *Next *and fill out the form as described:\n\nAlarm state trigger**: In alarm\nSelect an SNS topic: **Create new topic\n\nInsert a valid e-mail and click on Create topic.\n\nalt\n\n7\\. Click Next and fill the form as described before clicking Next:\n\nDefine a unique name**: High CPU\nAlarm description**: When CPU utilization >= 50%\n\nalt\n\nTip: Be sure to use your valid email address in the Email list field so you can verify the Alarm later. AWS Simple Notification Service (SNS) is used to send the email when the alarm is triggered. However, you will not need to configure anything in SNS.\n\n8\\. Click Create alarm when ready.\n\n9\\. Check for an email from AWS Notifications. Open up the email and click the Confirm subscription link:\n\nalt\n\nYou should receive a subscription confirmation. (For example, a confirmation message from Amazon Simple Notification Service (SNS) in a new browser tab if using a browser-based email client like Gmail.)  To summarize, you have created a new alarm, along with a new SNS topic. Since you subscribed to this new SNS topic, every time the state of the alarm switches to ALARM you will receive an email. You may not receive an alarm email if the time it took to confirm the SNS topic subscription took longer than the time it took for the alarm to trigger. Emails will only be sent to subscribers at the time of the alarm transition.\n\n10\\. You should be put to the Alarm page:\n\nalt\n\nNote your Alarm state may differ. For example, you may be in an Insufficient data **state briefly and then transition either to **In alarm or OK. \n\nTroubleshooting Tip: If the state of your alarm does not change to In alarm almost immediately, it is probably because you picked the incorrect instance. The HighCPUInstance is designed to trigger an alarm due to a high CPU utilization metric. The Monitoring Scripts instance is not taxed at all. To remedy the situation you can either create a new alarm with the correct instance, lower the threshold to something artificially low (1), or change the >= to = 500 (which can never happen for the single CPU instance). Wait five minutes until the alarm is disabled, then edit the alarm to trigger when CPUUtilization >= 50.\n\n13\\. Now move to the History tab:\n\nalt\n\nYour History is likely similar to the example shown above. The oldest event is the furthest down. In succession, the Alarm was created; the state changed from INSUFFICIENT DATA to ALARM; SNS sent off an email notification.\n\n15\\. Spend a few minutes exploring the latest alarm history and try to understand what is going on with each entry. You can see more details for each entry by clicking the date.\n\nCreate an Alarm using the EC2 console\n\n1\\. Navigate to EC2 Instances by clicking here.\n\n2\\. Select the Monitoring Scripts instance, then switch to the Status Checks tab:\n\nalt\n\n3\\. Click Actions **> **Create Status Check Alarm:\n\nalt\n\nThis dialog is similar in function to the create alarm wizard you saw in an earlier Lab Step.\n\n4\\. For the Alarm notification, select the SNS topic name you set up before.\n\nalt\n\nOther fields can be kept at their defaults. The Alarm thresholds section uses Status check failed: either to trigger the alarm for either instance or system status check failures:\n\nalt\n\n5\\. Click Create when ready. An alarm creation confirmation message is displayed:\n\nalt\n\nNow you know two different ways to create alarms: one from CloudWatch and the other from the EC2 console. Next, you will learn how to attach EC2 actions to alarms.\n\n6\\. Return to the* Alarms by clicking here. *Notice that the first alarm you created is stuck in the In alarm state.\n\nalt\n\nThe alarm is stuck in the In alarm state because the instance is running an application that consumes 100% of the CPU utilization. Clearly an indicator that something may have gone wrong with the instance. Imagine that you are managing a production environment and you have an instance that is becoming unavailable intermittently because of high CPU utilization. You would like to receive a notification every time the CPU utilization is high, but this can happen anytime, in the middle of the night, or during a weekend or holiday. It would be helpful to have a pre-defined action in this case -- at least until you find a definitive solution for the problem.\n\nTo help you address the scenario, you can set EC2 actions on your alarms. \n\n8\\. Select the High CPU alarm and then Actions > Edit:\n\nalt\n\nTo make your alarm more suitable to the training environment needs, set a new EC2 Action to Reboot this instance whenever the state of this alarm is ALARM.\n\nClick on Next and click on Add EC2 action **under **EC2 action. Select Reboot this instance.\n\nalt\n\n10\\. Click Update alarm when ready.\n\nAlthough the changes have been made to the alarm, the alarm remains in the In alarm state. CloudWatch will only perform actions when the state transitions to the *In alarm *state from another state. In the next instruction, you will modify the alarm to quickly have it change to the *OK *state and then change it back to return to the *In alarm *state. \n\n11\\. Select the High CPU **Alarm and choose **Actions > Edit. Toggle the relationship from >= to = and save the alarm to have it transition to the In alarm state.\n\nalt\n\n_Note: _The state change may not be immediate and may take up to 2 minutes.\n\n13\\. Navigate back to the Instances by clicking here and watch CloudWatch reboot the instance when the Alarm Status changes to In alarm.\n\nIn case you miss it, you can return to the alarm in CloudWatch and see the Reboot Action listed in the *History *section:\n\nalt\n\nCheck your email client and confirm that you received a notification of the alarm:\n\nalt\n\nSharing CloudWatch Metrics with others\n\n1\\. Go to CloudWatch by clicking here and click on Metrics **> **All metrics.\n\n2\\. Select an interesting metric, such as the DiskspaceUtilization metric for the Monitoring Scripts instance, and click Actions > Share:\n\nalt\n\nThis metric can be found under* System/Linux* > Filesystem, InstanceId, MountPath.\n\n3\\. In the Share Graph URL dialog, right-click and copy the URL, then Close the dialog:\n\nalt\n\n4\\. The URL for the specific graph you were looking at is copied into the clipboard. You can paste it into a test email to confirm this. For example:\n\nalt\n\nThe URL is quite complex. To confirm that it is indeed correct, test it out in another browser tab.\n\n5\\. Open another browser tab. Paste the URL into the address field and refresh your browser. You should see the exact same graph as the one you shared earlier. Notice that you need to be logged into the AWS console in order to view the information referenced by the URL. For security reasons, you can only share URLs with other AWS Identity and Access Management (IAM) users who have the appropriate CloudWatch IAM permissions in your AWS account.",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codeartifact/",
            "title": "CodeArtifact",
            "description": "Amazon CodeArtifact",
            "content": "\nAbout\n\nAWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.\n\nDocumentation\nUser Guide\n\nAmazon CodeArtifact Flow\n\nCodeAr­tifact is a secure storage, publishing, and sharing of software code packages used in a development process organisation's software development. CodeAr­tifact makes it easy for small organisations to store, publish, and share software packages.\n\nCodeArtifact can be configured to automatically fetch software packages and dependencies from public artifact repositories.\n\nCodeArtifact works with commonly used package managers and build tools like Maven, Gradle, npm, yarn, twine, pip, and NuGet making it easy to integrate into existing development workflows.\n\nPrice\n\nPay only for what you use – the size of the artifacts stored, the number of requests made, and the amount of data transferred out of an AWS Region. CodeArtifact includes a monthly free tier for storage and requests\n\nCurrent price\n\nUse Cases\n\nType: Developer Tools\n\nAlternatives\n\nJFrog Artifactory\nDocker hub\nSonatype Nexus Platform\nHelm\nAzure DevOps Services\nGithub\n\nUsage\n\n$ aws codeartifact list-domains --region us-east-1\n\nPractice\n\nGetting started using the console\n",
            "tags": [
                "aws",
                "codeartifact"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codebuild/",
            "title": "CodeBuild",
            "description": "Amazon CodeBuild - Build and test code with continuous scaling. Pay only for the build time you use.",
            "content": "\nAbout\n\nCodeBuild is a fully managed service that assembles source code, runs unit tests, & also generates artefacts ready to deploy.\n\nDocumentation\nUser Guide\n\nCodeBuild is a code creation service that also produces code artefacts upon request.\n\nCodeBuild is an alternative to other build tools such as Jenkins.\n\nCodeBuild is integrated with KMS for encryption of build artifacts, IAM for build permissions, VPC for network security, and CloudTrail for logging API calls.\n\nCodeBuild is a fully managed build service to compile source code, run unit tests and produce artifacts that are ready for deployment. Not the best fit for serverless template deployment or serverless application initialization.\n\nbuildspec.yml\n\nBuild instructions can be defined in the code (buildspec.yml).\n\nCodeBuild Local Build\n\nIn case you need to do deep troubleshooting beyond analyzing log files.\n\nCan run CodeBuild locally on your computer using Docker.\n\nLeverages the CodeBuild agent.\n\nPrice\n\nCurrent price\n\nYou pay based on the time it takes to complete the builds.\n\nLab\n\ncicd-aws-code-services\nChapters:\nLogging in to the Amazon Web Services Console\nCreating an AWS CodeCommit Repository\nCommitting Code to Your AWS CodeCommit Repository\nBuilding and Testing with AWS CodeBuild\nDeploying with AWS CodeDeploy\nAutomating Your Deployment with AWS CodePipeline\nFollowing the Continuous Deployment Pipeline\nRecovering Automatically from a Failed Deployment\nUsing AWS CodeDeploy Blue/Green Deployments in Your Pipeline\n\ncodedeploy-codepipeline-codestar overview\n\nQuestions\n\nQ1\n\nYou are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\n\nWhat is the easiest way to develop these simple applications?\n\nInstall AWS SAM CLI and run “sam init \\[options\\]” with the templates’ data. \nUse AWS step function visual workflow and insert your templates in the states\nSave your template in the Serverless Application Repository and use AWS SAM\n\n\nExplanation\n\n\nAWS SAM - AWS Serverless Application Model\n\nhttps://aws.amazon.com/serverless/sam/\n\n1\n",
            "tags": [
                "aws",
                "codebuild"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codecommit/_index",
            "title": "CodeCommit",
            "description": "Amazon CodeCommit",
            "content": "\nAbout\n\nCodeCommit is a version control service (Git repository service) that enables the user to personally store & manage Git archives in the AWS cloud.\n\nDocumentation\nUser Guide\n\nCodeCommit is integrated with Jenkins, CodeBuild and other CI tools.\n\nPrice\n\n5 active users free per month with the\n\nCurrent price\n\nPractice\n\nIntroduction to CodeCommit\n",
            "tags": [
                "aws",
                "codecommit"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codecommit/introduction-codecommit/",
            "title": "Introduction to CodeCommit",
            "description": "Tutorial Introduction to CodeCommit",
            "content": "\nLab\nhttps://cloudacademy.com/lab/introduction-codecommit/\n\nCreate a repository\n\n1\\. In the AWS Management Console search bar, enter CodeCommit, and click the CodeCommit result under Services:\n\nalt\n\n2\\. Click Create repository:\n\nalt\n\n3\\. In the Create repository form enter the following values accepting the defaults for values not specified:\n\nRepository name:** lab-repository\n\nalt\n\nYou can leave the Description field empty for this lab. Usually this field would contain a short description of the purpose of the repository. Attaching meaningful descriptions to repositories makes managing large numbers of repositories easier.\n\n4\\. Click Create to create the repository.\n\nCreating credentials to access your repository\n\nIn the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n\nalt\n\n2\\. Under Access Management, click Users in the left-hand sidebar menu:\n\nalt\n\n3\\. In the IAM user list, click student:\n\nalt\n\n4\\. Click the Security credentials tab:\n\nalt\n\n5\\. Scroll down to the HTTPS Git credentials for AWS CodeCommit section and click Generate credentials:\n\nalt\n\n6\\. In the box that opens, click Download credentials:\n\nalt\n\nYour browser will download a file called credentials.csv.\n\nKeep these credentials saved, you will use them in later steps.\n\nAccessing a shell with Git available\n\nIn the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\nalt\n\n2\\. In the left-hand side menu, click Instances:\n\nalt**\n\n3\\. Select the instance and in the row of buttons above the instance list, click Connect:\n\nalt\n\n4\\. In the Connect to instance dialog, ensure EC2 Instance Connect is selected and enter in the User name field:\n\nalt\n\n5\\. To open a shell on this instance, click Connect.\n\nEC2 Instance Connect allows you to connect to the instance over SSH using your web browser. With EC2 Instance Connect a new browser window opens an SSH shell on a Linux host that has git installed.\n\nKeep this window open, you will use it in later steps.\n\nAdding files to your repository\n\nNavigate to the CodeCommit Console.\n\n2\\. In the list of repositories, click lab-repository:\n\nalt\n\n3\\. Click Clone URL and select Clone HTTPS in the drop-down menu that opens:\n\nalt\n\nThe URL of the repository has been copied to your clipboard.\n\n4\\. To copy the repository to your Linux host, in your shell, type git clone followed by a space and paste the repository URL: \n\ngit clone https://git-codecommit.us-west-2.amazonaws.com/v1/repos/lab-repository\n\n5\\. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\n\nalt**\n\nYou can ignore the warning about cloning an empty repository.\n\nYou have copied the repository from AWS CodeCommit and stored it locally on the Linux host.\n\n6\\. To change to the directory of the repository, enter the cd command:\n\n\ncd lab-repository\n\n7\\. To create a file, enter the following command:\n\necho \"lab\" > lab.txt\n\nWith this command you have created a file called lab.txt that can be added to your local repository.\n\nTo add the file to your local repository, enter the following commands:\n\ngit add lab.txt\ngit commit -m \"Lab commit\"\n\nIn Git terminology, with the first command you are \"staging\" the file before you commit it. This process enables you to specify which files you want to add to the repository and which ones you want to ignore when committing.\n\nYou will see output similar to the following:\n\nalt**\n\nYou can ignore the message about your name and email address. Usually when using Git you will configure the name and email address so that your commits are labeled with these details.\n\nYou have added the lab.txt file to your local copy of the repository on the Linux host.\n\nPushing your commit to your remote repository\n\n1\\. In the shell on the Linux host, enter the following command:\n\ngit push\n\nIn Git terminology, with this command you are \"pushing\" your local commit from your \"local\" repository to the \"remote\" repository that you \"cloned\" from.\n\n2\\. When prompted, enter the username and password from the credentials file you downloaded in the Creating credentials to access your repository step:\n\nalt**\n\nYou have copied the file from the local repository on the Linux host, to the repository hosted in AWS CodeCommit.\n\nNavigate to the CodeCommit Console.\n\n4\\. In the Repositories list click lab-repository:\n\nalt\n\nYou will see the lab.txt file you pushed in the previous Lab step listed.\n\nalt\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codedeploy/",
            "title": "CodeDeploy",
            "description": "Amazon CodeDeploy - Automate code deployments to maintain application uptime",
            "content": "\nAbout\n\nCodeDeploy is a fully managed deployment service that automates software deploy­ments to a variety of compute services such as EC2, Fargate, Lambda, & on-pre­mises servers\n\nDocumentation\nUser Guide\n\nCodeDeploy can also deploy a serverless Lambda function.\n\nCodeDeploy can be connected to CodePipeline and use artifacts from there.\n\n{{}}\n\nPlatforms\n\nNeed to choose the compute platform:\n\nEC2/On-premises.\nAWS Lambda.\nAmazon ECS.\n\nAppSpec File\n\nThe application specification file (AppSpec file) is a YAML-formatted, or JSON-formatted file used by CodeDeploy to manage a deployment.\n\nThe AppSpec file defines the deployment actions you want AWS CodeDeploy to execute.\n\nDeployment types\n\nIn-place deployment (EC2 only)**\nBlue/green deployments:**\n\n  AWS Lambda: Traffic is shifted from one version of a Lambda function to a new version of the same Lambda function.\n\n  Amazon ECS: Traffic is shifted from a task set in your Amazon ECS service to an updated, replacement task set in the same Amazon ECS service.\n\n  EC2/On-Premises: Traffic is shifted from one set of instances in the original environment to a replacement set of instances.\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Developer Tools\n\nPractice\n\nContinuous Integration and Deployment with AWS Code Services\n\nQuestions\n\nQ1\n\nWhat will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\n\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups.\nNothing will happen, as the custom deployment configuration was unused.\nAll deployment groups associated with the custom deployment configuration will also be deleted.\nAll deployments associated with the custom deployment configuration will be terminated.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\n\nCan delete only if unused.\n\n1\n\nQ2\n\nWhat happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\n\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks.\nThe instances used in the deployment group will change.\nAll details associated with that deployment group will also be deleted from AWS CodeDeploy.\nThe instances that were participating in the deployment group will run once again.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\n\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n\n3\n",
            "tags": [
                "aws",
                "codedeploy"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codeguru/_index",
            "title": "CodeGuru",
            "description": "Automate code reviews and optimize application performance with ML-powered recommendations",
            "content": "\nAbout\n\nAmazon CodeGuru is a developer tool that provides intelligent recommendations to improve code quality and identify an application’s most expensive lines of code.\n\nCodeGuru is a developer tool powered by machine learning that provides intell­igent recomm­end­ations for improving code quality & identi­fying an applic­ation’s most expensive lines of code.\n\nDocumentation\nUser Guide\n\nCodeGuru Flow\n\nAmazon CodeGuru is comprised of two services: CoduGuru Reviewer and CodeGuru Profiler. Reviewer is what listens for pull requests in a repository and reviews code changes.\n\nAmazon CodeGuru Reviewer is triggered by pull requests to Code Commit, then makes suggestions as comments in the pull request wherever it sees fit.\n\nAmazon CodeGuru Reviewer generates suggestions in its reviews as comments in pull requests.\n\nType detection\n\nSecurity Detection\nSecrets Detection\nCode Quality\n\nBenefits\n\nAmazon CodeGuru Reviewer\n\nCatch code problems before they hit production\nFix security vulnerabilities\nProactively improve code quality with continuous monitoring\n\nCodeGuru Profiler\n\nTroubleshoot performance issues\nDiscover anomalies and common issues in your application performance\nCatch your most expensive line of code today\n\nPrice\n\nCurrent price\n\nPractice\n\nAutomating Code Reviews with Amazon CodeGuru\n",
            "tags": [
                "aws",
                "codeguru"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codeguru/automating-code-reviews-amazon-codeguru/",
            "title": "Automating Code Reviews with Amazon CodeGuru",
            "description": null,
            "content": "\nLab\n\nAutomating Code Reviews with Amazon CodeGuru\n\nAssociating Amazon CodeGuru with a CodeCommit Repository\n\n1\\. Navigate to the Amazon CodeCommit console.\n\n2\\. Click java-web-app:\n\nalt\n\n3\\. Notice that at the moment, only a README file has been committed to the master branch. Next, you'll associate CodeGuru with this repository, so that CodeGuru can begin to analyze the code therein.\n\n4\\. Go to the CodeGuru dashboard.\n\n5\\. Click Associate Repository and run analysis:\n\nalt\n\n6\\. Select AWS CodeCommit as the provider, choose java-web-app from the repository dropdown, enter _master _into Source branch and click Associate:\n\nalt\n\nIn roughly one minute, you'll see that CodeGuru has associated with your repository:\n\nalt\n\nTriggering an Amazon CodeGuru Review\n\n1\\. Navigate to  :8080 in your browser. \n\n Note: This is the IP of an EC2 instance that can be found in the EC2 console.\n\n2\\. Click the file icon in the top left to open the file tree:\n\nalt\n\nNote: During the creation of this lab, two things were performed automatically. One is that the CodeCommit repository you visited earlier was cloned to the directory you're looking at in the IDE now. Another is that the framework for a Java web app was added in addition to the single README you saw. This is so that you can see the benefits of CodeGuru without having to work heavily with code.\n\nIn this lab step, you'll push all the new code to the nearly-empty Code Commit repository, to trigger a CodeGuru review.\n\n3\\. Open the terminal in your IDE:\n\nalt\n\n4\\. In the terminal, add the new files to a Git branch, and commit and push the changes:\n\ncd /cloudacademy/lab\ngit add .\ngit checkout -b trigger_branch\ngit commit -m \"trigger a CodeGuru analysis by pushing Java code\"\ngit push origin trigger_branch\n\nThis will create a Git commit that includes all the Java files in a branch called trigger_branch, so that you can make a pull request in CodeCommit. Since CodeGuru analyses are triggered by pull requests, this is what will trigger a CodeGuru analysis.\n\n5\\. Back on the CodeCommit dashboard, click Create pull request:\n\nalt\n\n6\\. Set the Destination to master and the source to trigger_branch and click Compare:\n\nalt\n\n7\\. Type Trigger a CodeGuru Reviewer Analysis into the Title field and click Create pull request:\n\nalt\n\nThis will create a pull request and trigger a CodeGuru review.\n\nViewing Amazon CodeGuru Comments\n\n1\\. If you weren't automatically brought to the pull request details page after creating your pull request, click Pull Requests beneath Repositories on the left side of the page:\n\nalt\n\n2\\. Click the only available pull request:\n\nalt\n\n3\\. Notice the section mentioning CodeGuru Reviewer:\n\nalt\n\nThis section will display in each pull request made in any repository associated with CodeGuru. As of the time this lab was released, CodeGuru is still in preview. As the section on your pull request details tab mentions, because it's in preview mode, CodeGuru can take a while to process a pull request. There isn't a way to track its progress, and you currently won't be alerted when that processing begins or finishes.\n\n4\\. Select the Changes tab:\n\nalt\n\n5\\. In the Go to file filter, enter dockerservlet and click the result to navigate to the file:\n\nalt\n\nYou may need to scroll down the page to find the DockerServlet.java file changes. This file is known to have CodeGuru Reviewer comments that usually appear a few minutes after creating the pull request.\n\n6\\. Scroll down to line 60 to see an example of a comment from CodeGuru Reviewer (If you don't see any comment you may try refreshing the page every minute until one appears):\n\nalt\n\nYou can then make updates as you see fit, and submit more pull requests to see if you've addressed CodeGuru's suggestions.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codepipeline/",
            "title": "CodePipeline",
            "description": "Amazon CodePipeline Automate continuous delivery pipelines for fast and reliable updates",
            "content": "\nAbout\n\nAWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n\nDocumentation\nUser Guide\n\nCodePipeline automates the build, test, and deploy phases of your release process every time there is a code change, based on the release model you define.\n\nYou can easily integrate AWS CodePipeline with third-party services such as GitHub or with your own custom plugin. With AWS CodePipeline, you only pay for what you use.\n\nCodePipeline Flow\n\nAlternatives\n\nBamboo.\nCircleCI.\nJenkins.\nTravis CI.\nGitLab.\nTeamCity.\nAzure DevOps Server.\nGoogle Cloud Build.\n\nTerminology\n\nPipelines\n\nA workflow that describes how software changes go through the release process.\n\nArtifacts\n\nFiles or changes that will be worked on by the actions and stages in the pipeline.\nEach pipeline stage can create “artifacts”.\nArtifacts are passed, stored in Amazon S3, and then passed on to the next stage.\n\nStages\n\nPipelines are broken up into stages, e.g., build stage, deployment stage.\nEach stage can have sequential actions and or parallel actions.\nStage examples would be build, test, deploy, load test etc.\nManual approval can be defined at any stage.\n\nActions\n\nStages contain at least one action, these actions take some action on artifacts and will have artifacts as either an input, and output, or both.\n\nTransitions\n\nThe progressing from one stage to another inside of a pipeline.\n\nPrice\n\nCurrent price\n\nQuestions\n\nQ1\n\nYou are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\n\nWhich actions should you take to make this function perform correctly? (2 answers)\n\nRestart all Amazon EC2 instances that are running a Windows operating system.\nProvide the IAM user credentials to integrate AWS CodePipeline.\nFill out the required fields for your proxy host.\nModify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n\n2, 3\n",
            "tags": [
                "aws",
                "codepipeline"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codestar/_index",
            "title": "CodeStar",
            "description": "Amazon CodeStar - Quickly develop, build, and deploy applications on AWS",
            "content": "\nAbout\n\ncodestar-overview\n\nAWS CodeStar enables you to quickly develop, build, and deploy applications on AWS. AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place.\n\nDocumentation\nUser Guide\n\nCodeStar enables to quickly develop, build, & deploy applic­ations on AWS.\n\nWith AWS CodeStar, you can create, manage, and scale automated code reviews with a single click. You can also monitor the performance and scalability of your code review process with the built-in metrics dashboard.\n\nEach AWS CodeStar project comes with a project management dashboard, including an integrated issue tracking capability powered by Atlassian JIRA Software.\n\nAlternatives\n\nAlternatives to AWS CodeStar:\n\nJenkins.\nAzure DevOps Projects.\nGitHub.\nGitLab.\nCircleCI.\nCloudBees CI.\nPlesk.\nCopado CI/CD.\n\nPrice\n\nThere is no additional charge for using AWS CodeStar. You only pay for the AWS resources that you provision for developing and running your application (for example, Amazon EC2 instances).\n\nCurrent price\n\nPractice\n\nDevelop and Deploy an Application with AWS CodeStar\n",
            "tags": [
                "aws",
                "codestar"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/codestar/develop-and-deploy-app-with-codestar",
            "title": "Develop and Deploy an Application with AWS CodeStar",
            "description": "Develop and Deploy an Application with AWS CodeStar",
            "content": "\nLab\n\nDevelop and Deploy an Application with AWS CodeStar\n\nCreating an AWS CodeStar Project\n\nIn the AWS Management Console search bar, enter CodeStar, and click the CodeStar result under Services:\n\nalt\n\n2\\. On the welcome page, click Create project.\n\nTake a moment to see all of the different templates available in AWS CodeStar.\n\nCheck the following boxes on the left filter bar to narrow down the listed templates:\n\nAWS services: EC2\nApplication category: **Web application\nProgramming languages: **Node.js\n\nThe choice of Application category **and **Programming language will be driven by the requirements of your project and skills available to you. The choice of AWS services may not be as easy. Some guidelines for choosing between the alternatives are:\n\nAWS Elastic Beanstalk**: A good choice for a fully managed application environment running on EC2 servers. This option allows you to stay focused on your code.\nAmazon* *EC2**: Preferable when you want to host the application on servers that you manage yourself, including on-premise servers.\nAWS Lambda**: Choose this option if you want to run a serverless application. \n\n4\\. Select the *Express.js *project template:\n\nalt\n\nExpress.js is a popular Node.js web application framework.\n\n5\\. In the next step of the Create project wizard, enter the following:\n\nProject name**: ca-app-&lt;Unique_String&gt; (Replace &lt;Unique_String&gt; with a 6 characters. The name must be unique for the region because of AWS CodeCommit repository name restrictions)\nProject ID**: Accept the default value\n\nThe instructions in this Lab use ca-app for the project name, but you should use a different name or the project creation may fail if it is already in use.\n\nMake sure that CodeCommit is selected under Project repository:\n\nalt\n\nYou will see the EC2 Configuration section of the form.\n\n7\\. Ensure the following values are selected:\n\nInstance type**: _t2.micro _(default value)\nVPC**: Select the non-default VPC (The VPC without \"(Default)\"),or the VPC with only two subnets if there is no (Default) label\nSubnet: Select the subnet in the **us-west-2a availability zone\n\nIf you can't see which subnet is in us-west-2a hover your mouse over each subnet.\n\nClick Next and then* Create Project*:\n\nalt\n\nConnecting to the Virtual Machine using EC2 Instance Connect\n\nIn the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\nalt\n\n2\\. To see available instances, click Instances in the left-hand menu:\n\nalt\n\nThe instances list page will open, and you will see an instance named cloudacademylabs:\n\nalt\n\nIf you don't see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n\n3\\. Right-click the cloudacademylabs instance, and click Connect:\n\nalt\n\nThe Connect to your instance form will load.\n\n4\\. In the form, ensure the EC2 Instance Connect tab is selected:\n\nalt\n\nYou will see the instance's Instance ID and Public IP address displayed.\n\n5\\. In the User name textbox, enter ec2-user:\n\nalt\n\nNote: Ensure there is no space after ec2-user or connect will fail. \n\n6\\. To open a browser-based shell, click Connect:\n\nalt\n\nIf you see an error it's likely that the environment hasn't finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\n\nalt\n\nA browser-based shell will open in a new window ready for you to use.\n\nKeep this window open, you will use it in later lab steps.\n\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\n\nTouring the AWS CodeStar Project Website\n\nObserve the tiles that are included in your Dashboard:\n\nIDE: References for how to get started with a variety of integrated development environments (IDEs) under **Access your project code. You will simply use the EC2 instance to edit the code interact with CodeCommit in this lab.\nRepository**: You can see the main details related to the code repository here. The most recent code commits for the selected branch:\n    alt\n    The View commits button opens the detailed view list of the commits. Currently, there is only a master branch and the initial commit to display. The committer, AWS CodeStar, made the initial commit during the project creation. Each commit also includes a button on the right to view the code changes in AWS CodeCommit. You will look at the code in a future Lab Step.\n\nPipeline**: This shows a graphical representation of the release pipeline for your project:\n    alt\n    Any time you commit a code change to the master branch, the pipeline will automatically deploy your application. As your application grows and the requirements for your release pipeline change, you can modify the pipeline by clicking Edit. For example, you may want to add an automated test stage, invoke an AWS Lambda function, or modify the deployment group to deploy to an Auto Scaling group. The *Release change *button can be used to force a deployment of the latest commit. That can be useful if you modify the pipeline or something went wrong with the release. If something does go wrong with a pipeline stage, you will see the bar on the left turn red.\n\nMonitoring: This shows the **CPUUtilization and other metrics of the EC2 instance where your application is deployed.\n    alt\nIssues**: This Lab doesn't include a JIRA project, but for projects requiring issue tracking you can find link to JIRA from here.\n\n3\\. Click View application in the upper-right to view the application included in the template:\n\nalt\n\nDepending on your time of day, the background will change. You will commit a code change later to modify the appearance of the application.\n\n4\\. Look at the Project resources tab under the Overview.\n\nThe most interesting thing to see here is the list of all the Project Resources created by the project template:\nalt\n\nAWS CodeStar saved you a lot of time compared to manually configuring everything that is included. Notice that AWS CloudFormation includes a stack resource. That is how AWS CodeStar works behind the scenes. Each project template creates a stack in AWS CloudFormation. Of course, you don't need to know any of the details. AWS CodeStar does everything for you so you can focus on development.\n\nIf you need to delete an AWS CodeStar project, you can do so from the CodeStar project page. You will be given a choice of keeping the associated resources or also deleting the associated resources.\n\nDeveloping Your AWS CodeStar Project\n\nIn the AWS Management Console search bar, enter IAM, and click the IAM result under Services:\n\nalt\n\n2\\. Click on Users in the left navigation panel.\n\nalt\n\n3\\. In the Users table, click on student.\n\nalt Note: You will see error messages. This is normal. You only have the permissions required to complete the Lab.\n\n4\\. Click on the Security credentials tab.\n\nalt\n\n5\\. Scroll down to the HTTPS Git credentials for AWS CodeCommit section, and click Generate credentials:\n\nalt\n\nThis will show a pop-up dialog showing you your credentials. \n\n6\\. Click Download credentials:\n\nalt\n\nYour browser will download a file containing a username and password. Keep this file, you will use the credentials to connect to your AWS CodeStar repository.\n\n7\\. Return to your AWS CodeStar project's Repository tab and click HTTPS under Clone repository:\n\nalt\n\n This copies the HTTPS url of the CodeCommit repository to your clipboard.\n\n8\\. Paste the repository into the file with your code repository credentials.\n\nYou will use this URL later to access your repository.\n\n9\\. Return to the SSH shell connected to the dev-instance EC2 instance and enter cd to ensure you are in your home directory of /home/ec2-user.\n\nRefresh the instance connect browser tab if the session has expired.\n\n10\\. To tell Git to cache your credentials for a few hours, enter the following command:\n\ngit config --global credential.helper 'cache --timeout=10800'\n\n\n11\\. Tell Git your user name:\n\n\ngit config --global user.name \"student\" \n\nThis name will show up on the commits in your project dashboard.\n\n12\\. To clone your AWS CodeStar project repository, enter:\n\ngit clone\n\nReplace &lt;YOUR\\_PROJECT\\_REPOSITORY_URL&gt; with the URL you copied in a previous instruction.\n\nYour URL will be similar to https://git-codecommit.us-west-2.amazonaws.com/v1/repos/ca-app.\n\n13\\. When prompted, enter the Username and Password you saved in a text file earlier in this Lab Step.\n\nTip: The password generated by AWS is long and it is easy to make a typo when entering it. To avoid errors copy and paste the password.\n\n14\\. Change the repository directory name to ca-app:\nmv ca-app- ca-app\n\nNote: Change ca-app- to the name of your repository.\n\nThis won't change the repository name. It will only simplify the instructions at the command-line by not having to enter your unique string following ca-app in this and later Lab Steps. \n\n15\\. Change into the directory:\n\ncd ca-app\n\n16\\. Enter ls to get a quick overview of the project structure.\n\nThere are several files:\n\napp.js**: JavaScript file that starts the server\nappspec.yml**: Configuration file that instructs AWS CodeDeploy what steps to perform to deploy your application\npackage.json**: Metadata and dependencies related to your project\nREADME.md**: Text file explaining the project template\n\nThere is no need to get into the details of the file contents at this time. However, it is good to know that the appspec.yml file specifies scripts that run during the deployment of your application. The scripts are contained in one of the two project directories:\n\npublic**: Static assets used for your application\nscripts**: Scripts executed by AWS CodeDeploy during the deployment of your application\n\nNow you can get the server running on your development machine.\n\n17\\. Install the project dependencies using Node package manager (npm) and start the Node.js server:\n\nnpm install\nnode app.js\n\nWhile the server is running you won't be able to enter new commands. That won't be a problem. Now you can test that the development server is serving the application.\n\nNavigate to Instances in the EC2 service in the AWS Console.\n\n19\\. Select the instance named cloudacademylabs:\n\nalt\n\nIn the Description tab, you will see a field called Public DNS (IPv4).\n\n20\\. To copy the public DNS, click the click the copy icon under Public IPv4 DNS:\n\nalt\n\n21\\. Open a new browser taband paste the public DNS and append :3000 to the end and press enter:\n\nalt\n\nNow that you verified the application works on the development machine, you can make some code changes.\n\n22\\. Return to the SSH shell and press Ctrl+C to kill the running Node.js server.\n\nEnter the following multiline command at the shell prompt to update a file in the project:\n\necho 'var idx = Math.floor(new Date().getHours());\nvar body = document.getElementsByTagName(\"body\")[0];\nvar idxStep = 1;\nvar refreshRate = 1000;\n\nfunction adjustIdx() {\n  if (idx = 23) {\n    // Start decreasing idx\n    idxStep = -1;\n  }\n  idx += idxStep;\n  body.className = \"heaven-\" + idx;\n}\n\nbody.className = \"heaven-\" + idx;\nsetInterval(adjustIdx, refreshRate);' > public/js/set-background.js\n\nTest the changes by running the server again with node app.js and refresh the browser tab with your development application. \n\nYou will see a similar page as the previous one, but the color will change roughly once a second.\n\n25\\. Stop the Node.js server with Ctrl+C.\n\n26\\. View the local repository status:\n\ngit status\n\nalt\n\nThis tells you that you are on the master branch and working from the initial code commit. The output also shows the set-background.js file was modified. You need to add the file to stage it before committing.\n\n27\\. Add the modified file to the staged changes in the commit:\n\ngit add public/js/set-background.js\n\n28\\. Commit the staged changes to the local repository and add a short message about the changes:\n\ngit commit -m \"animation\"\n\n29\\. Push the changes in your local repository to the remote AWS CodeStar project repository so they are synchronized:\n\ngit push\n\nNow that you have made a change to your code, you will see how the changes are deployed in the next Lab Step.\n\nSummary \n\nIn this Lab Step, you committed a code change to your AWS CodeStar project repository. You created the required credentials and tested the application on your development server.\n\nDeploying Your AWS CodeStar Project\n\n1\\. Return to your AWS CodeStar project view.\n\nThere are a few things to notice since you were here last:\n\nYour commit is now visible in the *Repository *> *Most recent commit *tile\nYour Monitoring > CPUUtilization tile might show some spikes if your application has already been deployed\nYour Pipeline tab may show one of the pipeline stages In progress or you may see a recent timestamp inside each stage box telling you the new version has been deployed.\n\nIf you missed the release flowing through the stages of the pipeline, click Release change and click Continue in the pop-up.\n\n2\\. To inspect the code, in the Repository tab, click the most recent Commit ID:\n\nalt\n\nYour commit Id will be different.\n\n3\\. Look at the code changes:\n\nalt\n\nAdditions appear in green and removals would appear in red, if any were present. This is an easy way to keep track of what is happening to the code in your AWS CodeStar project.\n\nNavigate back to the Pipeline tab. Click on AWS Code Deploy under Deploy:\n\nalt\n\nThis opens your application in AWS CodeDeploy:\n\nalt\n\nYou can see the Deployment Groups created for deploying your application. In this case there will be just one with a Name **ending in **-Env. The Status **column will tell you if your last deployment **Succeeded **or failed. The time of the **Last attempted deployment and Last successful deployment are also recorded.\n\n5\\. Click the name of your deployment group beginning with ca-app:\n\nalt\n\nNotice that by default Rollback enabled is false. That means if your deployment fails, AWS CodeDeploy will not attempt to deploy the last successful version. That is something you might consider changing when you use AWS CodeStar for one of your projects.\n\n6\\. Scroll down the page and inspect the Deployment group deployment history section.\n\nEach deployment that was attempted to be deployed is recorded here along with a link to where the artifacts are located on Amazon S3.\n\n7\\. Click on the most recent deployment in the Deployment Id column:\n\nalt\n\nYour deployment will have a different deployment id.\n\nThis opens a page with details of the most recent deployment:\n\nDeployment status**: shows the state of the deployment operation\nDeployment details**: shows information similar to what you saw on the AWS CodeDeploy application page\nRevision details**: shows information about the revision deployed, including the location in AWS S3\nDeployment lifecycle events*: tells you the start and end times as well as the *Duration **of the deployment\n\n8\\. To view the deployment life-cycle events, click View events down the bottom:\n\nalt\n\n9\\. To view the events, scroll down to the event list:\n\nalt\n\nYou will see events similar to the above.\n\nIn case of a failed deployment, one of the events will record the failure and provide a link under the Logs column to investigate the command and logs related to the failure. If you recall, the appspec.yml file in the code project was used to instruct AWS CodeDeploy on how to deploy your application. Your project provides different scripts to run for some of the events listed in the table.\n\n10\\. Finally, return to the AWS CodeStar and click View application.\n\nYou will see the latest version of your application including the animation commit deployed and available to the world.\n\nManaging Your AWS CodeStar Project Team\n\n1\\. Return to your AWS CodeStar project's Overview and click on Add team members:\n\n alt\n\n2\\. Click on the User **drop-down menu and click on **Logan.\n\n3\\. Set the team member values for Logan to:\n\nEmail address**: test@cloudacademy.com\nProject Role**: Contributor\nRemote Access**: Checked (This allows the team member to upload an SSH public key to connect to EC2 instances)\n\nThe difference between the default Project Roles is:\n\nViewer**: Access to the project dashboard and able to view a few project resources\nContributor**: Everything Viewer can access plus view, modify, and access all project resources\nOwner**: Everything Contributor has plus adding and removing team members, and deleting the project\n\n4\\. Click Add team member:\n\nalt\n\nAfter adding a team member, you will be asked to create a profile for yourself.\n\n5\\. In the Create user profile form, enter the following values before clicking Create user profile:\n\nDisplay name**: student\nEmail address**: student@cloudacademy.com\n\nalt\n\nYou will see Logan and *student *appear in the *Team members *list.\n\n6\\. Click Add team member and select Bessie from the drop-down menu.\n\n7\\. Enter the following values and click Add:\n\nEmail address**: bessie@cloudacademy.com\nProject Role**: Viewer\nRemote Access**: Unchecked\n\nNow you can briefly experience the differences between the project roles.\n\n8\\. At the top of this Lab page, click on the Open Environment button.\n\nThis will sign you out of the student user and allow you to sign in as a different user.\n\n9\\. Log in to AWS using the team member in the viewer role:\n\nUser Name**: Bessie\nPassword**: Lab-Viewer1\n\nNavigate to AWS CodeStar in the AWS Console.\n\n11\\. Click on your project name.\n\nObserve that the viewer role has access to view the same tabs as your student user.\n\n13\\. Click on the Repository > Commit ID and see that a viewer is allowed to view code changes.\n\n14\\. Return to the project Pipeline section and click Release change, then Release.\n\nYou will receive an error message stating that you are not authorized to perform that action:\n\nalt\n\nAt the top of this Lab page, click on the Open Environment button and sign in again with the following credentials:\n\nUser Name**: Logan\nPassword**: Lab-Contributor1\n\nThe user Logan is in the contributor role, which has additional permissions than the viewer role.\n\n16\\. Click Release change, then Continue.\n\nThe contributor has permission to perform this action:\n\nalt\n\n17\\. Click Team in the left sidebar.\n\nNotice that you can only remove yourself from the team and not other members. That is a distinction between the contributor and owner roles.\n\n18\\. One last time, in the lab, click on the Open Environment button and sign in with the student credentials given in the Credentialssection of the Lab.\n\nCleaning Up Your AWS CodeStar Project\n\n1\\. Return to your AWS CodeStar project dashboard and click on Settings:\n\n alt\n\n2\\. Click Delete project.\n\n3\\. Enter delete in the pop-up dialog:\n\nalt\n\n4\\. Click Delete:\n\nIn a few seconds you will return to the AWS CodeStar start page and all of the resources in the project will begin terminating. \n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/cognito/",
            "title": "Cognito",
            "description": "Amazon Cognito",
            "content": "\nAbout\n\nAmazon Cognito - Simple and Secure User Sign-Up, Sign-In, and Access Control\n\nDocumentation\nUser Guide\n\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.\n\nUsers can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.\n\nAlternatives\n\nAuth0\nMicrosoft Azure Active Directory\nOneLogin\nGoogle Cloud Identity Platform\nIBM Security Verify\nKeycloak\n\nTerminology\n\nCredentials: The temporary security credentials, which include an access key ID, a secret access key, and a security token.\n\nAssumedRoleUser: The ARN and the assumed role ID, which are identifiers for the temporary security credentials that you can programatically refer to.\n\nPrice\n\nPay only for what you use. First 50,000 (monthly active users (MAUs) - Free.\n\nCurrent price\n\nUse Cases\n\nType: Identity & access management\n\nSame type services: Identity & Access Management (IAM), Single Sign-On, Cognito, Directory Service, Resource Access Manager, Organisations\t\n\nWorkflow\n\nThe process of authenticating a user with Cognito is as follows:\n\nThe user signs in with a Web ID provider (Google, Facebook, Amazon, etc.)\nThe Web ID provider returns a JWT token to the user\nThe user application makes an STS API call: sts assume-role-with-web-identity\nSTS returns an API response with the temporary credentials\nThe user application now has AWS access e.g. for S3, DynamoDB, etc.\n\nPractice\n\nManage Authentication with Amazon Cognito\n\nQuestions\n\nQ1\n\nYou are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\n\nWhat action will best solve this issue?\n\nUse AWS Lambda to send the time-based one-time password by SMS\nIncrease the complexity of the password\nCreate and assign a role with a policy that enables Cognito to send SMS messages to users\nCreate and assign a role with a policy that enables Cognito to send Email messages to users\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n\n3\n\nQ2\n\nA developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\n\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\n\nUse Amazon Cognito to provide the sign-up and sign-in functionality\nUse AWS IAM to provide the sign-up and sign-in functionality\nConfigure an AWS Config rule to make the API call triggered by the post-authentication event\nInvoke an Amazon API Gateway method to make the API call triggered by the post-authentication event\nExecute an AWS Lambda function to make the API call triggered by the post-authentication event\n\n\nExplanation\n\n\nAmazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n\n1, 5\n",
            "tags": [
                "aws",
                "Cognito"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/dynamodb/_index",
            "title": "DynamoDB",
            "description": "Creating a DynamoDB Amazon DynamoDB Table",
            "content": "\nAbout\n\nFast and flexible NoSQL database service for performance with millisecond latency at any scale\n\nAmazon DynamoDB documentation\nDeveloper Guide\n\nHow DynamoDB works\n\nPricing\n\nAmazon DynamoDB pricing\n\nWhen you use the DynamoDB service, you are charged for reading, writing, and storing data in DynamoDB tables, as well as any additional features you enable. DynamoDB supports two resource provisioning modes that correspond to specific billing schemes for processing read and write operations on your tables: on-demand and with preparation. Click the following links to learn more about the billing options for each provisioning mode\n\nUse Cases\n\nType: Key-value\n\nEcommerce Websites, gaming websites etc.\n\nDigest\n\nGlobal tables are useful for having multiple copies of tables in different region.\nAll DynamoDB tables are encrypted at rest using an AWS owned CMK by default.\nItems - in DynamoDB is similar in many ways to rows, records, or tuples in other database systems. Each DynamoDB table contains zero or more items. An item is a collection of attributes that is uniquely identifiable for each record in that table.\nAttributes - Each item is composed of one or more attributes. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.\nEach item in the table has a unique identifier, a primary key, or a partition key that distinguishes the item from all of the others in the table. The primary key consists of one attribute.\nPrimary key\nPartition key\nPartition key and sort key (range attribute)\nA primary key can either be a sinale-attribute partition key or a composite partition-sort key.\nBoth partition and sort keys attributes must be defined as type string, number, or binary.\nGlobal secondary index - a partition key and a sort key that can be different from those on the base table; query at table level across all partitions; eventual consistency:\n  Different partition key and sort key from base table\n  Only eventually consistent\n  Can be created after table is created\n  Using a random prefix for the GSI partition key enables to have high cardinality for the partition key\nLocal secondary index - same partition key as the base table, but a different sort key: query on a single partition; eventual or strong consistency:\n  Same partition key, different sort key from base table\n  Eventual and strongly consistent\n  Should be created when creating a table\nCalculate RCU (read capacity unit) & WCU (write capacity unit):\n    1 RCU = 2 eventual consistent read of 4 KB, 1 strongly consistent read of 4 KB\n    1 WCU = 1 write per second for data for an item as large as 1 KB.\nDynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables. The data about these events appears in the stream in near real time and in the order that the events occurred.\nQueries or scan on GSI consume RCU on index not on table\nConsistency:\n  Auto scaling\n  Storing session state could be on elastic cache or dynamodb\n  Provisioned throughput - ProvisionedThroughputExceededException\n  Reserved capacity, On-demand, Burst. Adaptive\n  On-demand backups, point-in-time recovery\nBest practices when using Scan in dynamodb - Use parallel scan\n  to control the amount of data returned per request use the Limit parameter. This can help prevent situations where one worker consumes all the provisioned throuahput at the expense of all other workers\n  DynamoDB does not support item locking, and conditional writes are perfect for implementing optimistic concurrency.\n\nDynamoDB vs Aurora\n\n | Amazon DynamoDB                                                                                                                                                | Amazon Aurora                                                             |\n | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |\n | Was developed by Amazon in 2012                                                                                                                                | Was developed by Amazon in 2015.                                          |\n | It is hosted, scalable database service by Amazon with data stored in Amazon cloud                                                                             | It is MySQL and PostgreSQL compatible cloud service by Amazon             |\n | It does not provide concept of Referential Integrity. Hence, no Foreign Keys | It provides concept of Referential Integrity. Hence, no Foreign Keys      |\n | Eventual Consistency and Immediate Consistency are used to ensure consistency in distributed system                                                            | Immediate Consistency is used to ensure consistency in distributed system |\n | Its Primary database models are Document store and Key-value store                                                                                             | Its Primary database model is Relational DBMS                             |\n | It does not support Server-side scripting                                                                                                                      | It supports Server-side scripting                                         |\n | It supports sharding as partitioning method                                                                                                                    | Partitioning can be done with horizontal partitioning                     |\n | It does not support SQL query language                                                                                                                         | It supports SQL query language                                            |\n | It supports replication methods                                                                                                                                | It supports only one replication method – Master-slave replication        |\n | It does not offer API for user-defined Map/Reduce methods. But maybe implemented via Amazon Elastic MapReduce                                                  | It does not offer API for user-defined Map/Reduce methods                 |\n\n\nDynamoDB supports different consistency models when performing reads:\n\nEventually, consistent reads may not always reflect the latest data if there was recently write activity on the table. Since the data in this scenario rarely changes, eventually consistent reads, which are cheaper than strongly consistent reads, can be tolerated. \n\nDynamoDB Partition Key\n\nPractice\n\nIntroduction to DynamoDB\n\nResources\n\nAWS Database Blog\n\nQuestions\n\nQ1\n\nA developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\n\nWhich architectures meet these requirements? (Select TWO.)\n\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store.\nCreate a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store\nCreate an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets.\nCreate a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store.\nEnable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store\n\n\nExplanation\n\n\nAWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\n\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\n\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n\n1, 2\n",
            "tags": [
                "aws",
                "dynamodb"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/dynamodb/introduction-dynamodb/",
            "title": "Introduction to DynamoDB",
            "description": "Creating a DynamoDB Amazon DynamoDB Table",
            "content": "\nLab\n\nCreating a DynamoDB Table with a Partition Key\n\n1\\. From the AWS Management Console, in the search bar at the top, enter DynamoDB, and under Services, click the DynamoDB result:\n\nalt\n\nThe Amazon DynamoDB product overview page will load.\n\n2\\. To start creating a new DyanmoDB table, on the right-hand side, click Create table:\n\nalt\n\n3\\. In the Table details section, enter the following:\n\nTable Name**:\nPartition Key**: Enter Name and ensure type is\n\nalt\n\n4\\. In the Settings section, select Customize settings:\n\nalt\n\nChoosing this option allows you to specify values for the table's read and write capacities.\n\n5\\. In the Read/write capacity settings section, under Capacity mode, select Provisioned and enter the following:\n\nRead Capacity**: \n    Provisioned capacity units:\nWrite Capacity**: \n    Provisioned capacity units:\n\nalt\n\nAccept the defaults for all other options on this page.\n\n6\\. Scroll to the bottom and click Create table:\n\nalt\n\nThe Tables list view will load and you will see a notification that your table is being created. After a 30 seconds or so, you will see a success notification:\n\nalt\n\nCreating a DynamoDB Table with Local and Global Secondary Indexes\n\n1\\. On the right-hand side of the page, click Create table:\n\nalt\n\n2\\. Enter the following in the Table details section:\n\nTable name**: \nPartition key**:\n    Name: Enter \n    Type: Select \nSort key**:\n    Name: Enter \n    Type: Select \n\nalt\n\n3\\. In the Settings section, select Customize settings.\n\n4\\. Under Read/write capacity settings, ensure Provisioned is selected for Capacity mode, and enter the following:\n\nRead capacity**:\n    Provisioned capacity units: \nWrite capacity**:\n    Provisioned capacity units: \n\nalt\n\n5\\. Scroll down to the Secondary indexes section and click Create local index:\n\nalt\n\nThe New local secondary index dialog box will appear.\n\n6\\. Enter the following to configure your local secondary index:\n\nSort Key**:\n    Name: Enter \n    Type: Select \nAttribute projections**: Select \n\nalt\n\nAn LSI (Local Secondary Index) has the same partition key as the table's primary key and will share the provisioned capacity of the table in contrast to global secondary indexes which provision their own capacity.\n\n7\\. To finish creating the local secondary index, at the bottom, click Create index:\n\nalt\n\n8\\. Scroll to the bottom and click Create table.\n\nAfter roughly 30 seconds you will the table become active:\n\nalt\n\nIn contrast to a Local Secondary Index, a Global Secondary Index is an index with a partition and sort key that can be different from those in the table. It is considered \"global\" because queries on the index can span all of the data in a table, across all partitions.\n\n9\\. Click Create table once more to start creating another table.\n\n10\\. Enter the following in the Table details section:\n\nTable Name**: \nPartition key**:\n    Name: Enter \n    Type_: _Select \nSort key**:\n    Name: Enter \n    Type: Select \n\nalt\n\n11\\. In the Settings section, select Customize settings.\n\n12\\. In the Read/write capacity settings section, ensure the Capacity mode is Provisioned, and enter the following:\n\nRead capacity**:\n    Provisioned capacity units: Enter \nWrite capacity**:\n    Provisioned capacity units: Enter \n\nalt\n\n13\\. Scroll down to the Secondary indexes section, and click Create global index:\n\nalt\n\nThe New global secondary index dialog form will appear.\n\n14\\. Enter the following:\n\nPartition key**:\n    Name: Enter  \n    Type: Select \nSort key**:\n    Name: Enter \n    Type: Select \nAttribute projections**: Select \n\nalt\n\n15\\. To finish creating the global secondary index, at the bottom, click Create index.\n\n16\\. Click Create global index again and enter the following:\n\nPartition key**:\n    Name: Enter \n    Type: Select \nSort key**:\n    Name: Enter \n    Type: Select \nAttribute projections**: Select\n\nalt\n\n17\\. To finish creating the global secondary index, at the bottom, click Create index.\n\n18\\. Scroll to the bottom and click Create table.\n\nOnce again, you will see your table created after roughly 30 seconds.\n\nInserting Items Into a DynamoDB Table\n\n1\\. In the left-hand menu, click Explore items:\n\nalt\n\n2\\. In the Tables list, select \n\nYou will see nothing under Items returned because there are no items stored.\n\n3\\. On the right-hand side, click Create item:\n\nalt\n\nThe Create item form will load and you will see a list of Attributes.\n\n4\\. In the Value textbox next to Name - Partition key, enter a name for your forum (can be anything you wish):\n\nalt\n\n5\\. To add another attribute for this item, click Add new attribute and select String from the list of types:\n\nalt\n\n6\\. In the Attribute name textbox, enter Description and in the Value textbox, enter any value you'd like:\n\nalt\n\n7\\. At the bottom, click Create item:\n\nalt\n\n8\\. Repeat steps 3-7 three more times so that end up with four entries in the  table:\n\nalt\n\nSelect the table and click Create Item.\n\n10\\. Provide any values you'd like for , and , keeping in mind that the value must match the name of one of your forums.\n\nNote:   is a \" \" table with the  Local Secondary Index. For being able to save a   item, you have to provide:\n\n(the table Primary Key)\n(the table Sort Key)\n(the Local Secondary Index Sort Key)\n\nNote: You will have to click Add new attribute to add the CreationDate attribute and specify a value.\n\n11\\. At the bottom, click Create item.\n\n12\\. Repeat steps 9-11 three more times until you have four items in the  table:\n\nalt\n\nEditing DynamoDB Table Items\n\n1\\. On the Explore items page, select the  table:\n\nalt\n\n2\\. Select any item in the table and click on its name to get to the Item editor page:\n\nalt\n\n3\\. Click inside any value and make an update to its contents:\n\nalt\n\nWarning: Note that modifying the partition key will result in changing the values of the item keys. This will delete and recreate the item with new keys.\n\n4\\. At the bottom of the page, click Save changes:\n\nalt\n\nQuerying a DynamoDB Table\n\n1\\. In the left-hand menu, click PartiQL editor:\n\nalt\n\nThe PartiQL editor page will load.\n\nPartiQL is a SQL (Structured Query Language) compatible language for Amazon DynamoDB. As well as querying tables, you can use it to insert new items and update existing ones.\n\n2\\. Under Tables, click the three dots next to the   and click Scan table:\n\nalt\n\nThe Query 1 editor will be populated with a PartiQL query that selects all items from the  .\n\n3\\. To execute the PartiQL table, under the editor, click Run:\n\nalt\n\n4\\. Scroll down to see the results under Items returned:\n\nalt\n\nNotice that you have a choice of viewing the results in tabular form or in JSON (Java Script Object Notation):\n\nalt\n\n5\\. To query for a specific item, replace the contents of the Query 1 editor with the following, and click Run:\n\nSELECT * FROM \"Thread\" WHERE \"Subject\" = 'Intro to cool stuff'\n\nThis time, you will only see items returned that satisfy the value of the WHERE condition.\n\nNote: Change the value of the WHERE condition to match an item you created if you don't see a result.\n\nPartiQL supports most standard features of SQL which means you can query, select, and sort your data in sophisticated ways.\n\nTypically, using the Amazon DynamoDB Console to query items is useful for one-off reports and debugging or troubleshooting. Like most databases, DynamoDB can be accessed programmatically by other systems and software applications through either the AWS SDK (software development kit) or DyanmoDB's HTTP API (application programming interface).\n\nYou can learn more about using PartiQL with Amazon DynamoDB by visiting the Working with PartiQL Query Language section of the Amazon DynamoDB developer guide.\n\nDeleting a DynamoDB Table\n\n1\\. In the left-hand menu, click Tables:\n\nalt\n\n2\\. In the Tables table, select the Thread table:\n\nalt\n\n3\\. On the right-hand side, click Delete:\n\nalt\n\nThe Delete table confirmation modal will appear.\n\nNotice that you have the ability to create a backup for a table before deleting it.\n\n4\\. In the confirmation textbox, enter delete and click Delete table:\n\nalt\n\nYou will see a message summarizing the deletion:\n\nalt\n\n5\\. To continue, click Go to tables:\n\nalt\n\n6\\. To update the Tables table, click the refresh icon:\n\nalt\n\nYou will now see only two tables listed.\n",
            "tags": [
                "aws",
                "dynamodb"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/ec2/",
            "title": "EC2",
            "description": "A step-by-step guide to Amazon EC2",
            "content": "\nAbout\n\nAWS EC2\nAWS EC2 User Guide\n\nAmazon Elastic Compute Cloud (EC2) - one of the most popular AWS services.\n\nAllows:\nto run different types of cloud instances and pay-per-use models.\nto control computing resources at the operating system level\nworking in an Amazon computing environment.\n\nDigest\n\nEC2 & EBS\nEC2 (Elastic Compute Cloud) Instance\nEBS (Elastic Block Store) - Persistent storage volume\nAMI (Amazon Machine Image) - Packages OS and additional installations in a reusable template\nInstance and Instance Types: General Purpose (t-type and m-type), Compute Optimized(c-type), GPU Graphics, GPU Compute, Memory Optimized(r, × and z-type), and Storage Optimized(d, h and i-type)\nPurchasing Options: On Demand, Reserved, Scheduled, Spot, Dedicated Instance and Dedicated Host\nSpot: Partial hours are not billed if terminated by AWS EC2\nSecure login information for your instances using key pairs\nPlacement group: Cluster and Spread\n\nFor root:\n\nGeneral purpose SSD (balances price & performance)\nProvisioned OPS SD (Highest performance for mission critical low-latency or high throughput workloads)\nMagnetic HDD (previous generation)\n\nFor other:\n\nThroughput Provisioned HDD (low cost for frequently accessed, throughput intensive workloads)\nCold HDD (lowest cost for less frequently workloads)\nInstance Store - temporary storage volume in which data is deleted when you STOP or TERMINATE your instance\n\nPrice\n\nPricing models:\nOn Demand - pay a fixed rate by the hour/second with no commitment. You can provision and terminate it at any given time.\nReserved - you get capacity reservation, basically purchase an instance for a fixed time of period. The longer, the cheaper.\nSpot - Enables you to bid whatever price you want for instances or pay the spot price. Dedicated Hosts - physical EC2 server dedicated for your use.\n\nCurrent price\n\nPractice\n\n{{}}\n\nTL;DR\n\nChoose a region close to you\nGo to EC2 service\nClick on \"Instances\" in the menu and click on \"Launch instances\"\nChoose image: Amazon Linux 2\nChoose instance type: t2.micro\nMake sure \"Delete on Termination\" is checked in the storage section\nUnder the \"User data\" field the following:\n\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"Hello from web!\" > /var/www/html/index.html\n\nAdd tags with the following keys and values:\n  key \"Type\" and the value \"web\"\n  key \"Name\" and the value \"web-1\"\nIn the security group section, add a rule to accept HTTP traffic (TCP) on port 80 from anywhere\nClick on \"Review\" and then click on \"Launch\" after reviewing.\nIf you don't have a key pair, create one and download it.\nNow HTTP traffic (port 80) should be accepted from anywhere\n\nCreate an EC2 Instance\n\nGo to EC2 page -> Launch Instance\n\n01.png\n\nEC2 image\n\nChoose the image we want\n\nCreate keys\n\nLet's create a key to use to connect to the instance externally\n\n\n\nEnter any name you want. Leave all other parameters by default\n\n\n\nAfter the key is created it will start automatic downloading. You need it to connect to EC2 from your local terminal\n\nNetwork Settings\n\nUnder Network Settings I leave Allow SSH traffic from\n\nCreate\n\nClick Launch Instance\n\nThe Instance has been created and is available for connection\n\nConnecting to EC2 from the terminal\n\nConnect to EC2 from a local terminal\n\nLet's move previously created and downloaded mykey key to home folder of current user and give permissions to file CHMOD 400\n\ncd ~\ncd Downloads/\nmv mykey.pem $HOME\ncd ..\nchmod 400 mykey.pem\n\nTo connect, we need a public iPv4 address. Find it on the instance page\n\n\n\nConnect with the command ssh.\n\nssh -i mykey.pem ec2-user@52.24.109.78\n\nQuestions\n\nQ1\n\nA company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\n\nWhich approach meets these requirements?\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI.\nStore the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials.\nStore the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances.\nStore the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials\n\n\nExplanation\n\n\nAWS Secrets Manager\n\nSecrets Manager offers secret rotation\n\n4\n\nQ2\n\nAn organization needs to provision a new Amazon EC2 instance with a persistent block storage volume to migrate data from its on-premises network to AWS. The required maximum performance for the storage volume is 64,000 IOPS.\n\nIn this scenario, which of the following can be used to fulfill this requirement?\n\nDirectly attach multiple Instance Store volumes in an EC2 instance to deliver maximum IOPS performance.\nLaunch a Nitro-based EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS.\nLaunch an Amazon EFS file system and mount it to a Nitro-based Amazon EC2 instance and set the performance mode to Max I/O.\nLaunch any type of Amazon EC2 instance and attach a Provisioned IOPS SSD EBS volume (io1) with 64,000 IOPS.\n\n\nExplanation\n\n\nAn Amazon EBS volume is a durable, block-level storage device that you can attach to your instances. After you attach a volume to an instance, you can use it as you would use a physical hard drive. EBS volumes are flexible.\n\nThe AWS Nitro System is the underlying platform for the latest generation of EC2 instances that enables AWS to innovate faster, further reduce the cost of the customers, and deliver added benefits like increased security and new instance types.\n\nAmazon EBS is a persistent block storage volume. It can persist independently from the life of an instance. Since the scenario requires you to have an EBS volume with up to 64,000 IOPS, you have to launch a Nitro-based EC2 instance.\n\nAmazon EBS volume types\n\n2\n\nQ3\n\nA Database Specialist manages an EBS-Optimized Amazon RDS for MySQL DB instance with Provisioned IOPS storage. The users recently raised a database IO latency issue during peak hours when it was always under a heavy workload. Upon review, the Specialist noticed that the RDS DB instance was barely using the maximum IOPS configured but was fully utilizing the maximum bandwidth for the required throughput. CloudWatch metrics showed that CPU and Memory utilization were at optimum levels.\n\nWhich action should the Database Specialist take to fix the performance issue?\n\nChange the underlying EBS storage type of the instance to General Purpose (SSD).\nModify the DB instance to an EBS-Optimized instance class with higher maximum bandwidth.\nDisable EBS optimization on the MySQL DB instance to allow higher maximum bandwidth.\nModify the DB instance to increase the size and corresponding Provisioned IOPS allocated to the storage.\n\n\nExplanation\n\n\nAmazon RDS volumes are built using Amazon EBS volumes, except for Amazon Aurora, which uses an SSD-backed virtualized storage layer purpose-built for database workloads. RDS currently supports both magnetic and SSD-based storage volume types. There are two supported Amazon EBS SSD-based storage types, Provisioned IOPS (called io1) and General Purpose (called gp2).\n\nProvisioned IOPS storage is a storage type that delivers predictable performance and consistently low latency. If your workload is I/O constrained, using Provisioned IOPS SSD storage can increase the number of I/O requests that the system can process concurrently.\n\nProvisioned IOPS SSD storage provides a way to reserve I/O capacity by specifying IOPS. However, as with any other system capacity attribute, its maximum throughput under load is constrained by the resource that is consumed first. That resource might be network bandwidth, CPU, memory, or database internal resources.\n\nEBS–optimized instances deliver dedicated bandwidth to Amazon EBS. When attached to an EBS–optimized instance, Provisioned IOPS SSD (io1) volumes are designed to achieve their provisioned performance, 99.9% of the time. Choose an EBS–optimized instance that provides more dedicated Amazon EBS throughput than your application needs; otherwise, the connection between Amazon EBS and Amazon EC2 can become a performance bottleneck.\n\n2\n\nQ4\n\nA developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.\n\nHow can the application find this information?\n\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/.\nQuery the instance user data from http://169.254.169.254/latest/user-data/.\nQuery the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/.\nCheck the hosts file of the operating system.\n\n\nExplanation\n\n\n1\n\nQ5\n\nYou are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\n\nWhich actions should you take to make this function perform correctly? (2 answers)\n\nRestart all Amazon EC2 instances that are running a Windows operating system.\nProvide the IAM user credentials to integrate AWS CodePipeline.\nFill out the required fields for your proxy host.\nModify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n\n2, 3\n\nResources\n\nEC2 Linux Hands-On Lab\nEB FAQ\nEC2 Digest\nEB Digest\n\nCommunity posts\n\nhttps://dev.to/romankurnovskii/aws-ec2-cheat-sheet-2mhp",
            "tags": [
                "aws",
                "ec2"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/ecr/",
            "title": "Elastic Container Registry",
            "description": "Run highly secure, reliable, and scalable containers",
            "content": "\nAbout\n\nAmazon Elastic Container Registry (Amazon ECR) - Fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere\n\nDocumentation\nUser Guide\n\nHosted private Docker registry\n\nElastic Container Registry Flow\n\nAlternatives\n\nDocker Hub\nJFrog Artifactory\nAzure Container Registry\nHarbor\nGoogle Container Registry\nRed Hat Quay\nJFrog Container Registry\n\nPrice\n\nCurrent price\n\nUse Cases\n\nStore, encrypt, and manage container images\n\nManage software vulnerabilities\nStreamline your deployment workloads\nManage image lifecycle policies\n\nType: Containers\n\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\n\nPractice\n\nThis commands returns the command to execute to be able to login to ECR:\nLogin\n  get-login-password:aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\nCreate a repository:\n        aws ecr create-repository \\\n        --repository-name hello-repository \\\n        --image-scanning-configuration scanOnPush=true \\\n        --region region\n    Tag image\n  docker tag hello-world:latest aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository\nPush\n  docker push aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository\nPull\n  docker pull aws_account_id.dkr.ecr.region.amazonaws.com/hello-repository:latest\nDelete an image\n        aws ecr batch-delete-image \\\n        --repository-name hello-repository \\\n        --image-ids imageTag=latest \\\n        --region region\n    Delete a repository\n        aws ecr delete-repository \\\n      --repository-name hello-repository \\\n      --force \\\n      --region region\n\nLabs:\nUse AWS Fargate for Serverless Deployment of Container Applications\nQuick start: Publishing to Amazon ECR Public using the AWS CLI\n\n{{}}\n\nNotes:\nIf you get a 503 Service Temporarily Unavailable error, try again after 30 seconds to let the load balancer finish adding the task to the target group. \n",
            "tags": [
                "aws",
                "Elastic Container Registry",
                "ecr"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/ecs/",
            "title": "Elastic Container Service",
            "description": "Run highly secure, reliable, and scalable containers",
            "content": "\nAbout\n\nDocumentation\nUser Guide\n\nHighly secure, reliable, & scalable way to run contai­ners\n\nAmazon Elastic Container Service Flow\n\nAlternatives\n\nGoogle Container Engine (GKE)\nAzure Container Service\nIBM Bluemix Container Service\nJelastic Multi-Cloud PaaS\n\nTerminology\n\n| Amazon ECS      | Term\tDefinition                                                                         |\n| --------------- | --------------------------------------------------------------------------------------- |\n| Cluster         | Logical Grouping of EC2 Instances                                                       |\n| Container       | Instance\tEC2 instance running the ECS agent                                             |\n| Task Definition | Blueprint that describes how a docker container should launch                           |\n| Task            | A running container using settings in a Task Definition                                 |\n| Service         | Defines long running tasks – can control task count with Auto Scaling and attach an ELB |\n\nDigest\n\n\n\nMicroservices are built in multiple programming languages\nContainers simplify deployment of microservices:\n  Step 1 : Create a self contained Docker image\n    Application Runtime (JDK or Python), Application code and Dependencies\n  Step 2 : Run it as a container any where Local machine OR Corporate data center OR Cloud\nUse On-Demand instances or Spot instances\nLaunch type: EC2 or Fargate\nData volumes attached to containers\nDeployment type:\n  Rolling update\n  Blue/green deployment (powered by AWS CodeDeploy)\nTask Placement Strategies:\n  binpack - Leave least amount of unused CPU or memory. Minimizes number of container instances in use\n  random - Random task placement\n  spread - Based on specified values:\n    Host (instanceId)\n    (OR) Availability Zone(attribute:ecs.availability-zone)\n    (Alowed) Combine strategies and prioritize\nHow do you manage 100s of containers?\nECS - Fully managed service for container orchestration\n  Step 1 : Create a Cluster (Group of one or more EC2 instances)\n  Step 2: Deploy your microservice containers\nAWS Fargate: Serverless ECS. DON'T worry about EC2 instances.\nCloud Neutral: Kubernetes\n  AWS - AWS Elastic Kubernetes Service (EKS)\nLoad balancing:\n  Performed using Application Load Balancers\n    Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance\n    Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.myapp.com/microservice-a and www.myapp.com/microservice-b)\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Containers\n\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\n\nECS Auto Scaling\n\nBest practice:\n10 Microservices => 10 Task Definitions => 10 Task IAM Roles with individual permissions needed by each microservice\n\nECS vs EKS\n\nAmazon also provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\n\n| Amazon ECS                                                                                       | Amazon EKS                                                                                           |\n| ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |\n| Managed, highly available, highly scalable container platform                                    | Managed, highly available, highly scalable container platform                                        |\n| AWS-specific platform that supports Docker Containers                                            | Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments |\n| Considered simpler and easier to use                                                             | Considered more feature-rich and complex with a steep learning curve                                 |\n| Leverages AWS services like Route 53, ALB, and CloudWatch                                        | A hosted Kubernetes platform that handles many things internally                                     |\n| “Tasks” are instances of containers that are run on underlying compute but more of less isolated | “Pods” are containers collocated with one another and can have shared access to each other           |\n| Limited extensibility                                                                            | Extensible via a wide variety of third-party and community add-ons.                                  |\n\nQuestions\n\nQ1\n\nYou are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and _ for your Amazon ECS clusters and the CPU and _ metrics for your Amazon ECS services.\n\nmemory reservation and utilization; concurrent connections\nmemory utilization; memory reservation and utilization\nconcurrent connections; memory reservation and utilization\nmemory reservation and utilization; memory utilization\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_monitoring.html\n\n1, 2\n",
            "tags": [
                "aws",
                "Elastic Container Service",
                "ecs"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/eks/",
            "title": "Elastic Kubernetes Service",
            "description": "Amazon Elastic Kubernetes Service",
            "content": "\nAbout\n\nKubernetes (K8) Docker Container/Cluster management\n\nRun highly secure, reliable, and scalable containers\n\nDocumentation\nUser Guide\n\nElastic Kubernetes Service Flow\n\nElastic Kubernetes Service Flow\n\nAlternatives\n\nRed Hat OpenShift Container Platform\nAzure Kubernetes Service (AKS)\nRancher\nGoogle Kubernetes Engine (GKE)\nOracle Cloud Infrastructure Container Engine for Kubernetes\nMirantis Kubernetes Engine (formerly Docker Enterprise)\nKubernetes\nCloud Foundry\n\nPrice\n\nCurrent price\n\nUse Cases\n\nBuild and run web applications\nDeploy across hybrid environments\nModel machine learning (ML) workflows\n\nECS vs EKS\n\nAmazon provides the Elastic Container Service for Kubernetes (Amazon EKS) which can be used to deploy, manage, and scale containerized applications using Kubernetes on AWS.\n\n| Amazon ECS                                                                                       | Amazon EKS                                                                                           |\n| ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |\n| Managed, highly available, highly scalable container platform                                    | Managed, highly available, highly scalable container platform                                        |\n| AWS-specific platform that supports Docker Containers                                            | Compatible with upstream Kubernetes so it’s easy to lift and shift from other Kubernetes deployments |\n| Considered simpler and easier to use                                                             | Considered more feature-rich and complex with a steep learning curve                                 |\n| Leverages AWS services like Route 53, ALB, and CloudWatch                                        | A hosted Kubernetes platform that handles many things internally                                     |\n| “Tasks” are instances of containers that are run on underlying compute but more of less isolated | “Pods” are containers collocated with one another and can have shared access to each other           |\n| Limited extensibility                                                                            | Extensible via a wide variety of third-party and community add-ons.                                  |\n\nPractice\n\nBuilding a Cloud Native Application\n\n{{}}\n",
            "tags": [
                "aws",
                "EKS"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/elasticache/",
            "title": "ElastiCache",
            "description": "Amazon ElastiCache",
            "content": "\nAbout\n\nDocumentation\nUser Guide\n\nAmazon Elasticache is a fully managed Redis or Memcached in-memory data store.\n\nIt's great for use cases like two-tier web applications where the most frequently accesses data is stored in ElastiCache so response time is optimal.\n\nYou can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store for use cases that don't require durability like session stores, gaming leaderboards, streaming, and analytics.\n\nCompatible with Redis and Memcached\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: In-memory\n\n| Use Case                  | Benefit                                                                                                                                                                     |\n| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Web session store         | In cases with load-balanced web servers, store web session information in Redis so if a server is lost, the session info is not lost, and another web server can pick it up |\n| Database caching          | Use Memcached in front of AWS RDS to cache popular queries to offload work from RDS and return results faster to users                                                      |\n| Leaderboards              | Use Redis to provide a live leaderboard for millions of users of your mobile app                                                                                            |\n| Streaming data dashboards | Provide a landing spot for streaming sensor data on the factory floor, providing live real-time dashboard displays                                                          |\n\nCaching Engines\n\n| Memcached                                         | Redis                       |\n| ------------------------------------------------- | --------------------------- |\n| Simple, no-frills                                 | You need encryption         |\n| You need to elasticity (scale out and in)         | You need HIPAA compliance   |\n| You need to run multiple CPU cores and threads    | Support for clustering      |\n| You need to cache objects (e.g. database queries) | You need complex data types |\n| You need HA (replication                          | Backup and restore features |\n| Pub/Sub capability                                | Multi-AZ with Auto-Failover |\n| Non persistent. No backups                        |\n| Multi-node for partitioning of data (sharding)    |\n\nMemcached\n\nElastiCache manages Memcached nodes as a pool that can grow and shrink (similar to an EC2 Auto Scaling group); individual nodes are expendable and non-persistent.\n\nMemcached provides a simple caching solution that best supports object caching and lets you scale out horizontally. Ideal for offloading a DB's contents into a cache.\n\nRedis\n\nElastiCache manages Redis more as a relational database, i.e. Redis clusters are managed as persistent, stateful entities that include using multi-AZ redundancy for handling failover (similar to RDS).\n\nRedis supports complex data structures, hence would be ideal in cases where sorting and ranking datasets in memory are important (e.g. such as in leaderboards for games).\n\nCaching Strategies\n\nLazy Loading\n\nThe data that is read from the DB is stored in the cache. The data can become stale\nThe data becomes stale because there are no updates to the cache when data is changed in the database\n\nOnly cache data when it is requested. Cache miss penalty on initial request. Chance to produce stale data; can be mitigated by setting a TTL. Shorter TTL = less stale data.\n\nWrite-Through\n\nThe data is added/updated into the cache everytime the data is written to the DB (no stale data)\nBecause the data in the cache is updated every time it's written to the database, the data in the cache is always current.\n\nEvery database write will write to the cache as well. Data is never stale however there will be alot more operations to perform; and these resources are wasted if most of the data is never used.\n\nSession Store\n\nStores temporary session data in cache (with TTL) - Time to Live. Data expires after the given time\n\nPractice\n\nConfiguring a Lambda function to access Amazon ElastiCache in an Amazon VPC\n\nQuestions\n\nQ1\n\nWhat is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\n\nThe metrics reported by CloudWatch are more difficult to report.\nSecurity concerns and network latency over the public internet.\nThe ElastiCache cluster becomes more prone to failures.\nThe performance of the ElastiCache cluster is no longer controllable.\n\n\n\nExplanation\n\n\nElasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to Elasticache is required for test or development purposes, it can be done through a VPN.\n\n2\n\nQ2\n\nYou are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\n\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\n\nUse the AWS Management Console to create and configure the cluster.\nCreate a cron job to schedule the cluster deployment using the aws cloudformation deploy command\nCreate a configuration file with the .config extension and place it into the .ebextensions folder in the application package.\nBuild an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster.\n\n\nExplanation\n\n\n[AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n\n3\n",
            "tags": [
                "aws",
                "elasticache"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/elasticbeanstalk/",
            "title": "Elastic Beanstalk",
            "description": "AWS Elastic Beanstalk",
            "content": "\nAbout\n\nAWS Elastic Beanstalk\n\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\n\nDeploying new application versions to existing resources in AWS Elastic Beanstalk happens much faster (typically under a minute) and once again is mostly dependent on the size of the new application version.\n\nflow\nDigest\n\nWhen you want to use new run time capabilities with elastic bean stalk, it is better to use blue-green deployment\nSecurity group will not be removed when removing the stack with elastic bean stalk\nFor long running tasks - Use Elastic Beanstalk worker environment to process the tasks asynchronously\nLaunch configuration is used for modifying instance type, key pair, elastic block storage and other settings that can be configured only when launching the instance\nRolling with Additional Batch and Immutable both involve provisioning new servers to ensure capacity is not reduced. All At Once means the application will be offline for the duration of the update. Performing a Rolling Update without an additional batch of servers means a reduction in capacity. https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\nFor Blue green deployment - Use Elastic beanstalk swap URL feature or route 53 with weighted routing policies\nYou create your own Elastic Beanstalk platform using Packer, which is an open-source tool for creating machine images for many platforms, including AMIs for use\nwith Amazon Elastic Compute Cloud (Amazon EC2).\n\nPrice\n\nThere is no additional charge for AWS Elastic Beanstalk. Only the AWS resources required to store and run applications are charged.\n\nConcepts\n\nAWS doc\n\nApplications\n\nAn application is a collection of different elements, such as environments, environment configurations, and application versions.\n\nYou can have multiple application versions held within an application.\n\nApplication Version\n\nAn application version is a very specific reference to a section of deployable code. The application version will point typically to simple storage service (S3) where the deployable code may reside.\n\nEnvironment Configurations\n\nAn environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.\n\nEnvironment\n\nAn environment refers to an application version that has been deployed on AWS resources. These resources are configured and provisioned by AWS Elastic Beanstalk. At this stage the application is deployed as a solution and becomes operational within your environment.\n\nThe “environment” is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.\n\nEnvironment Tier\n\nReflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. If the application manages and handles HTTP requests, then the app will be run in a web server environment.\n\nConfiguration Template\n\nThis is the template that provides the baseline for creating a new, unique, environment configuration.\n\nPlatform\n\nCulmination of components in which you can build your application upon using Elastic Beanstalk. These are comprised of the OS of the instance, the programming language, the server type (web or application), and components of Elastic Beanstalk\n\nDeployment policies\n\nAll at once** – deploys the new version to all instances simultaneously and will be out of service for a short time.\nRolling** – deploys the new version in batches.\nRolling with additional batch** – deploys the new version in batches, but first launch a new batch of instances.\nImmutable** – deploys the new version to a new set of instances.\nTraffic splitting** – deploys the new version to a new set of instances and temporarily split incoming client traffic.\n\nPractice\n\n{{}}\n\nControlled deployment with AWS Elastic Beanstalk\n\nLab Controlled deployment with AWS Elastic Beanstalk\n\nIn this lab, we will deploy several application version updates in a load-balanced, auto-scaling environment.\n\nThe first update is deployed using a simple deployment. The second update is deployed using a `blue-green' deployment, where a separate environment is created to run the new version of the application, and the DNS switch switches incoming traffic to the new environment.\n\nThe final deployment architecture will look like this\n\n02.png\n\nLoading the application\n\nIn this review, I'm using the code that Cloudacademy provided me, but I have a ready-made launch script that you can download from Elastic Beanstalk: download\n\nCreate\n\nGo to Elastic Beanstalk page and click Create Application.\n\n03.png\n\nSet Name\n\nSpecify a name for the new application\n04.png\n\nChoose platform\n\nUnder Platform choose the desired platform of the application. In our case - Node.js.\n05.png\n\nDownload source code\n\nUnder Source code origin specify the version of the application and download the archive with the application.  Example\n\n06.png\n\nApplication Configuration\n\nChange the preset Configuration to Custom configuration:\n\n07.png\n\nClick Edit under Rolling updates and deployments\n\nIn the default configuration, updates are distributed to all instances at the same time. This leads to application downtime, which is unacceptable for production environments.\n\nWe will set Rolling and Batch size to 30%\n\n08.png\n\nNetwork\n\nBack in the main application form, click Edit in the Network configuration.\n\nOn the Modify network form, configure the following values, then Save. \n\nVPC: Select VPC with CIDR block 10.0.0.0/16. This will not be the default VPC.\nLoad balancer settings:\n    Load balancer subnets: Select subnets with CIDR blocks 10.0.100.0/24 **(us-west-2a)and **10.0.101.0/24 (us-west-2b). These are public subnets. The application load balancer requires at least two subnets in different availability zones\nInstance settings:\n    Instance subnets: Select a subnet with CIDR block 10.0.1.0/24. This is a private subnet.\n\n09.png\n\n10.png\n\nConfirmation.\n\nPress Create app.\n\nThe app creation process takes from 5 minutes.\n\nThen go to Dasboard\n11.png\n\nThis concludes the loading phase of the app in Elastic Beanstalk. Next, let's break down how to switch the downloading of the new version of the application to the clients.\n\nDownloading version 2 of the app\n\nDownloading version 2.0\n\nPress Upload and deploy and download the updated code. For example, you can change the text in the same source code for comparison.\n\n12.png\n\nSpecify new version and publication settings\n13.png\n\nVersion comparison\n\nNow we can compare both versions by following the links. In my case the applications look like this\n\n15.png\n14.png\n\nChanging the url of the apps\nNow let's swap the apps around. So that a user who previously went to one address will now see the 2nd version of the app.\n\nUnder Actions, click on Swap environment URLs and then select the app you want to swap\n\n16.png\n\nRemoving Elastic Beanstalk resources\n\nElastic Beanstalk runs EC2 instances as well as other services to deploy applications. But you can remove all services from a single window.\n\ngo to the Applications section\nSelect an application.f\nClick on Actions -> Terminate environment Translated with www.DeepL.com/Translator (free version)\n\n17.png\n\nQuestions\n\nQ1\n\nYou are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\n\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\n\nUse the AWS Management Console to create and configure the cluster.\nCreate a cron job to schedule the cluster deployment using the aws cloudformation deploy command\nCreate a configuration file with the .config extension and place it into the .ebextensions folder in the application package.\nBuild an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster.\n\n\nExplanation\n\n\n[AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n\n3\n\nQ2\n\nEmily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\n\nWhich of the following options would meet this requirement?\n\nUse an Amazon Elastic File System (EFS) volume to serve the static image files.\nConfigure the AWS ElasticBeanstalk proxy server to serve the static image files.\nUse an Amazon S3 bucket to serve the static image files.\nUse an Amazon Elastic Block Store (EBS) volume to serve the static image files.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\n\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n\n2\n\nQ3\n\nAn online shopping platform has been deployed to AWS using Elastic Beanstalk. They simply uploaded their Node.js application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring. Since the entire deployment process is automated, the DevOps team is not sure where to get the application log files of their shopping platform.\n\nIn Elastic Beanstalk, where does it store the application files and server log files?\n\nApplication files are stored in S3. The server log files can only be stored in the attached EBS volumes of the EC2 instances, which were launched by AWS Elastic Beanstalk.\nApplication files are stored in S3. The server log files can be stored directly in Glacier or in CloudWatch Logs.\nApplication files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs.\nApplication files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs.\n\n\nExplanation\n\n\nAWS Elastic Beanstalk stores your application files and optionally, server log files in Amazon S3. If you are using the AWS Management Console, the AWS Toolkit for Visual Studio, or AWS Toolkit for Eclipse, an Amazon S3 bucket will be created in your account and the files you upload will be automatically copied from your local client to Amazon S3.\n\nOptionally, you may configure Elastic Beanstalk to copy your server log files every hour to Amazon S3. You do this by editing the environment configuration settings.\n\nWith CloudWatch Logs, you can monitor and archive your Elastic Beanstalk application, system, and custom log files from Amazon EC2 instances of your environments. You can also configure alarms that make it easier for you to react to specific log stream events that your metric filters extract.\n\nThe CloudWatch Logs agent installed on each Amazon EC2 instance in your environment publishes metric data points to the CloudWatch service for each log group you configure.\n\nEach log group applies its own filter patterns to determine what log stream events to send to CloudWatch as data points. Log streams that belong to the same log group share the same retention, monitoring, and access control settings. You can configure Elastic Beanstalk to automatically stream logs to the CloudWatch service.\n\nThe option that says: Application files are stored in S3. The server log files can be optionally stored in CloudTrail or in CloudWatch Logs is incorrect because the server log files can optionally be stored in either S3 or CloudWatch Logs, but not directly to CloudTrail as this service is primarily used for auditing API calls.\n\n4\n\nQ4\n\nA former colleague reached out to you for consultation. He uploads a Django project in Elastic Beanstalk through CLI using instructions he read in a blog post, but for some reason he could not create the environment he needs for his project. He encounters an error message saying “The instance profile aws-elasticbeanstalk-ec2-role associated with the environment does not exist.”\n\nWhat are the possible causes of this issue? (Select TWO.)\n\nHe selected the wrong platform for the Django code.\nElastic Beanstalk CLI did not create one because your IAM role has no permission to create roles.\nInstance profile container for the role needs to be manually replaced every time a new environment is launched.\nYou have not associated an Elastic Beanstalk role to your CLI.\nIAM role already exists but has insufficient permissions that Elastic Beanstalk needs.\n\n\nExplanation\n\n\nAWS EB CLI cannot create the instance profile for your beanstalk environment if your IAM role has no access to creating roles.\n\nThis error is also thrown when the instance profile has insufficient or outdates policies that beanstalk needs to function. More details on this can be seen on the references provided.\n\n2, 5\n\nResources\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/tutorials.html\nTutorials and samples\n\nCommunity posts\n\nhttps://dev.to/romankurnovskii/todo-aws-aws-elastic-beanstalk-cheat-sheet-1718\nhttps://dev.to/romankurnovskii/aws-elastic-beanstalk-top-questions-certified-developer-exam-478g",
            "tags": [
                "aws",
                "Elastic Beanstalk"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/elasticloadbalancing/_index",
            "title": "Elastic Load Balancing",
            "description": "Distribute network traffic to improve application scalability",
            "content": "\nAbout\n\nElastic Load Balancing (ELB) automa­tically distri­butes incoming applic­ation traffic across multiple targets, such as EC2's, contai­ners, IP addresses, & Lambda functions.\n\nElastic Load Balancing is a best practice to assign incoming traffic to a single target, such as an EC2 Instance, and then distribute the rest of the traffic across the target's resources. An elastic load balancer distributes traffic across an arbitrary number of targets.\n\nDocumentation\nUser Guide\n\nTypes of Load Balancers\n\n   | Type                            | Description                                                                                                                                                                                                                                                                       |\n   | ------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n   | Application Load Balancer (ALB) | Operates at the Application Layer (OSI L7), handling HTTP/HTTPS traffic. Allows routing requests to specific web servers.                                                                                                                                                         |\n   | Network Load Balancer (NLB)     | Operates at the Network Layer (OSI L4), handling TCP traffic. Recommended for performance.                                                                                                                                                                                        |\n   | Classic Load Balancer (CLB)     | Operates at OSI L7 and OSI L4, however has limited functions. Requests are forwarded by the load balancer without “looking into” any of these requests. They just get forwarded to the backend section. Not recommended for use except for apps built in the EC2-Classic network. |\n\nCompare ALB vs NLB vs CLB\n\n| Basic load balancing features       | ALB  | NLB       | CLB  |\n| ----------------------------------- | ---- | --------- | ---- |\n| Balance load between targets        | Yes  | Yes       | Yes  |\n| Perform health checks on targets    | Yes  | Yes       | Yes  |\n| Highly available                    | Yes  | Yes       | Yes  |\n| Elastic                             | Yes  | Yes       | Yes  |\n| TLS Termination                     | Yes  | Yes       | Yes  |\n| Performance                         | Good | Very high | Good |\n| Send logs and metrics to CloudWatch | Yes  | Yes       | Yes  |\n| Layer 4 (TCP)                       | No   | Yes       | Yes  |\n| Layer 7 (HTTP)                      | Yes  | No        | Yes  |\n| Running costs                       | Low  | Low       | Low  |\n\n| Advanced load balancing features                    | ALB | NLB | CLB |\n| --------------------------------------------------- | --- | --- | --- |\n| Advanced routing options                            | Yes | N/A | No  |\n| Can send fixed response without backend             | Yes | No  | No  |\n| Supports user authentication                        | Yes | No  | No  |\n| Can serve multiple domains over HTTPS               | Yes | Yes | No  |\n| Preserve source IP                                  | No  | Yes | No  |\n| Can be used in EC2-Classic                          | No  | No  | Yes |\n| Supports application-defined sticky session cookies | No  | N/A | Yes |\n| Supports Docker containers                          | Yes | Yes | Yes |\n| Supports targets outside AWS                        | Yes | Yes | No  |\n| Supports websockets                                 | Yes | N/A | No  |\n| Can route to many ports on a given target           | Yes | Yes | No  |\n\nCompare ALB vs NLB vs CLB\n\nScaling\n\nflow\n\nVertical Scaling\n\nIncreasing the size of the instances (ie- increase in RAM and vCPUs )\nEx: from t2.micro to t3.2xlarge (doesn't have to be the same instance family)\nIn vertical scaling, you scale up/down\nVertical scaling usually happens in databases, to handle high workloads as your application grows\n\nHorizontal Scaling**\n\nIncreasing the no. of instances\nIn horizontal scaling, you scale out/in\nEx: ASG scaling out EC2 instances to match increased workload for your web application\n\nCross Zone Load Balancing\n\nALB\n\nEnabled by default. Cannot disable it\nNot charges for data transfer between AZs (inter AZ data transfer)\n\nflow\n\nCLB\n\nEnabled by default. Can disable it\nNot charged for data transfer between AZs\n\nflow\n\nNLB\n\nDisabled by default. Can enable it\nCharged for data transfer between AZs\n\nDigest\n\nELB(Elastic Load Balancing) distributes application or network traffic across multiple targets, such as EC2 instances, containers(ECS), and IP addresses, in multiple AZs.\nCross Zone Load Balancing – when enabled, each load balancer node distributes traffic across the registered targets in all enabled AZs.\n3 Types of Load balancers - Application, Network, Classic\nDeleting ELB won't delete the instances registered to it.\nTermination protection will be disabled by default; enable it to prevent accidental delete.\n504 error means the gateway has timed out and the application is not responding within the idle timeout period\nLook for the X-Forwarded-For header, if you need the end user IPv4 address\nASG (Auto Scaling Group) ensures you've the correct number of EC2 instances available. Specify minimum, maximum and desired number of instances.\nLifecycle hook - perform custom actions when instances launch or terminate\nCool down period - ensure not to launch additional instances before previous scaling activities complete\nLaunch configuration - Instance configuration template the ASG uses to launch EC2 instances\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Scale your network design\n\nAWS discourages the use of Classic Load Balancer in favor of its newer load balancers\n\nApplication Load Balancer is typically used for web applications.\n\nNetwork Load Balancer would be used for anything that ALBs don’t cover. A typical use case would be a near real-time data streaming service (video, stock quotes, etc.) Another typical case is that you would need to use an NLB if your application uses non-HTTP protocols\n\nPractice\n\nCreate Classic Load Balancer\n\nQuestions\n\nQ1\n\nWhich load balancer would you use for services which use HTTP or HTTPS traffic?\n\n\nExplanation\n\nApplication Load Balancer (ALB).\n\nQ2\n\nWhat are possible target groups for ALB (Application Load Balancer)?\n\n\nExplanation\n\n\nEC2 tasks\nECS instances\nLambda functions\nPrivate IP Addresses\n\nQ3\n\nYour would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.\n\nWhich solution below would be best to implement for this?\n\nEnable X-Forwarded For on the web servers and use a Classic Load Balancer\nConfigure proxy servers to forward the traffic to the correct instances\nUse an Application Load Balancer with path-based routing rules to forward the traffic to the correct instances\nUse an Application Load Balancer with host-based routing rules to forward the traffic to the correct instances\n\n\n\nExplanation\n\n\nApplication Load Balancer with host-based routing rules\n\nhttps://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\n\n4\n\nQ4\n\nWhat is the primary reason why you should be using an elastic load balancer for a website with high activity?\n\nELBs help you scale servers easily without manual intervention\nELBs can distribute traffic equally to your backend targets to handle the incoming traffic load\nELBs help tighten security through the use of security groups\nELBs boost your website’s overall performance\n\n\nExplanation\n\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions. It can handle the varying load of your application traffic in a single Availability Zone or across multiple Availability Zones.\n\nElastic Load Balancing offers three types of load balancers that all feature the high availability, automatic scaling, and robust security necessary to make your applications fault-tolerant. They are Application Load Balancer, Network Load Balancer, Classic Load Balancer, and Gateway Load Balancer.\n\n2\n\nQ5\n\nAfter a year of development, the company’s 100-node high-performance computing (HPC) application is now ready to be deployed to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Horizontal Pod Autoscaler (HPA). The application must be capable of receiving millions of UDP traffic per second from the public Internet while maintaining low latency.\n\nWhich of the following is the most operationally efficient solution that should be implemented to meet the above requirements?\n\nLaunch a Gateway Load Balancer and integrate an Elastic Fabric Adapter (EFA) to each Kubernetes pod deployed in the Amazon EKS cluster\nIntegrate the AWS Load Balancer Controller add-on to the EKS cluster. Launch a Network Load Balancer to load balance network traffic to individual Kubernetes pods.\nInstall the AWS Load Balancer Controller add-on to the EKS cluster and launch an Application Load Balancer to distribute the incoming traffic to the Kubernetes pods.\nSet up the kube-proxy Amazon EKS add-on to the cluster and configure the Source Network Address Translation (SNAT) of the Kubernetes pods by setting the AWS_VPC_K8S_CNI_EXTERNALSNAT configuration to true.\n\n\nExplanation\n\n\nNetwork Load Balancer operates at the connection level (Layer 4), routing connections to targets – Amazon EC2 instances, microservices, and containers – within Amazon Virtual Private Cloud (Amazon VPC) based on IP protocol data.\n\nIdeal for load balancing of both TCP and UDP traffic, Network Load Balancer is capable of handling millions of requests per second while maintaining ultra-low latencies. Network Load Balancer is optimized to handle sudden and volatile traffic patterns while using a single static IP address per Availability Zone.\n\nIt is integrated with other popular AWS services such as Auto Scaling, Amazon EC2 Container Service (ECS), Amazon CloudFormation, and AWS Certificate Manager (ACM).\n\nNetwork Load Balancer preserves the client-side source IP address, allowing the back-end EC2 instances to see the IP address of the client. This can then be used by applications for further processing.\n\nNetwork traffic is load balanced at L4 of the OSI model. To load balance application traffic at L7, you deploy a Kubernetes ingress, which provisions an AWS Application Load Balancer. An AWS Network Load Balancer can load balance network traffic to pods deployed to Amazon EC2 IP and instance targets or to AWS Fargate IP targets.\n\nThe AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources:\n\n– An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress.\n\n– An AWS Network Load Balancer (NLB) when you create a Kubernetes service of type LoadBalancer.\n\n2\n\n\n",
            "tags": [
                "aws",
                "ELB"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/elasticloadbalancing/create-amazon-load-balancing/",
            "title": "Create Classic Load Balancer",
            "description": "tutorial how to create AWS Classic Load Balancer",
            "content": "\nPractice\n\nCreating Classic Load Balancer\n\nPlanning the Classic Load Balancer\n\nWhen you connected to the AWS account provided in the former step, you had a few things that were already deployed. This is the current infrastructure that was already deployed for you:\n\n\n\nYou already have a VPC with some subnets and 2 EC2 instances running inside the VPC in different Availability Zones. Both instances are inside the same Security Group called , which is allowing HTTP access from port 80 to anywhere (0.0.0.0/0). Each EC2 instance is running the same web application. We want to configure an LB to create a central point of access to our application, and we also want to configure our architecture in a way that users can only access the application through the ELB.\n\nIn the end, we should have a solution similar to this one:\n\n\n\nTo do that we will have to create and configure a Classic Load Balancer, and properly configure the needed Security Groups to make sure that our application will work as expected.\n\nCreating a Classic Load Balancer and Registering EC2 Instances\n\nA Classic Load Balancer allows traffic to be balanced across many Amazon EC2 instances, it performs this balancing at the request and connection level.\n\n\n1\\. In the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\n\n\n2\\. In the left-hand menu, under Load Balancing, click Load Balancers:\n\n\n\n3\\. To start creating your classic load balancer, click Create Load Balancer:\n\n\n\nThree tiles will be displayed detailing the different types of load balancer supported by Amazon EC2.\n\n4\\. At the bottom of the page, click Classic Load Balancer:\n\n\n\n5\\. In the Classic Load Balancer tile, click Create:\n\n\n\nA multi-step wizard will open allowing you to configure and customize your load balancer.\n\n6\\. Under Basic Configuration, enter the following values:\n\nLoad Balancer name**: Enter classic-elb\nEnable advanced VPC configuration**: checked\n\n\n\nBe aware there are limitations on the name field, only the characters a-z, A-Z, 0-9 and hyphens are allowed.\n\nCreate LB Inside lets you select which VPC you want the load balancer to be created in, leave this at the default.\n\nThe Create an internal load balancer option determines whether the load balancer can accept public internet traffic or not. If checked, the load balancer will have a private IP address and will only be able to accept traffic from another source inside the VPC.\n\nThe default Listener Configuration, listening on port eighty (HTTP), is all that is required for this lab.\n\n7\\. Under Select Subnets, click the plus icon next to each subnet.\n\nAs you click for each subnet, it will move from the Available subnets table, to the Selected subnets table:\n\n\n\nAn Availability Zone, often referred to as an AZ, helps make your infrastructure more reliable. You can think of each zone as a separate data center (in many cases they are exactly that), they are guaranteed to have redundant power, networking, and connectivity within an AWS region.\n\nTo learn more about regions, availability zones, and redundancy in AWS, visit the documentation here.\n\nEach subnet is mapped to one availability zone. It's important to configure the selected subnets correctly. If a subnet containing an EC2 instance is not selected, the load balancer will not be able to communicate with that EC2 instance. \n\n8\\. To move to the next step of the wizard, click Next: Assign Security Groups:\n\n\n\n9\\. In the form, enter and select the following values:\n\nAssign a security group: Select **Create a new security group\nSecurity group name**: Enter elb-sg\nDescription**: Enter Security group for the classic load balancer\n\n\n\nYou will see a default security group rule allowing traffic on port eighty.\n\n10\\. In the default security group rule, in the Source drop-down, select Anywhere:\n\n\n\n11\\. To advance to the next page of the wizard, click Next: Configure Security Settings:\n\n\n\nThis wizard step display's a warning that your load balancer isn't configured to use HTTPS or SSL.\n\nIt's strongly recommended that you always enable encrypted traffic on your load balancers for security reasons. Configuring SSL is beyond the scope of this lab. If you would like to learn more about SSL and load balancing, it's covered in the Using Elastic Load Balancing & EC2 Auto Scaling to Support AWS Workloads course.\n\n12\\. To move to the next wizard step, click Next: Configure Health Check:\n\n\n\n13\\. In the Ping Path field, replace the contents with /:\n\n\n\nBy default, the fields on this page specify that the health check will be performed using the HTTP protocol on port eighty. This means the load balancer will assume an instance is healthy when the instance returns a 200 OK response.\n\nThe Advanced Details allow you to further customize different aspects of the health check:\n\nResponse Timeout**: How long to the load balancer should wait for a response from the EC2 instance.\nInterval**: Amount of time between health checks.\nUnhealthy threshold**: The number of consecutive failed healthy checks that must occur before the load balancer declares the EC2 instance unhealthy.\nHealthy threshold**: The number of consecutive health checks that must occur before declaring an EC2 instance healthy.\n\nTo learn more about Elastic Load Balancing health checks, see the AWS documentation here.\n\n14\\. To move to the next wizard step, click Next: Add EC2 Instances:\n\n\n\nThis step of the wizard displays the EC2 instances that currently exist and can be added to the load balancer:\n\n\n\n15\\. Select the instances named web-node:\n\n\n\nTake a look at the configuration options on this page:\n\nCross-Zone Load Balancing ensures that your LB distributes incoming requests evenly across all instances in its enabled Availability Zones. This means that the LB will ignore the default of round-robin and will also take into consideration the Availability Zone in which the instance is running. This reduces the need to maintain equivalent numbers of instances in each enabled Availability Zone and improves your application's ability to handle the loss of one or more instances.\n\nConnection Draining is used to ensure that a Classic Load Balancer stops sending requests to instances that are de-registering or unhealthy while keeping the existing connections open.\n\nLeave these options at their defaults.\n\n16\\. To advance to the next wizard step, click Next: Add Tags:\n\n\n\nIn a non-lab environment, it is best practice to add tags to resources you create. Tags help make managing, organizing, and filtering resources in AWS easier.\n\nTo read more about tagging resources in AWS, see this document from AWS.\n\n17\\. To proceed to the review step, click Review and Create:\n\n\n\nThis page allows you to review the load balancing settings you have configured:\n\n\n\n18\\. To create your load balancer, click Create:\n\n\n\nYou will see a notification that your load balancer has been successfully created:\n\n\n\n19\\. To return to the EC2 management console, click Close:\n\nConfiguring Security Groups for Load Balanced EC2 Instances\n\n\n1\\. In the list of load balancers, ensure your load balancer is selected:\n\n\n\nYou will see some tabs beneath the list and the Description tab will be selected.\n\nThis tab shows general information about your load balancer.\n\n2\\. To view information about instances registered with this load balancer, click the Instances tab:\n\n\n\nYou will see the instances and availability zones listed:\n\n\n\nThe instances will have a status of InService. This means the load balancer is performing successful health checks on the instances.\n\nNote: If you see the Status as OutOfService then the instances are still be registered. Wait a minute or two and then click the refresh icon in the top-right corner.\n\n3\\. To see the DNS of your load balancer, click the Description tab.\n\n4\\. Copy the domain name from the value of the DNS name field:\n\n\n\nWarning: Don't include the (A Record) part of the value when copying.\n\n5\\. In a new browser tab, paste the domain name, and press enter.\n\nYou will see an instance Id displayed:\n\n\n\nNote: Your instance Id will be different.\n\nAn application has been pre-installed on the EC2 instances that will respond to web requests with the instance Id of the instance serving the request.\n\nTo see the Id of the other EC2 instance, refresh the page. If the Id doesn't change, you may need to open an incognito or private browsing tab and visit the DNS name again.\n\nSeeing the Id change shows that the load balancer is working as expected, routing traffic to both registered instances.\n\nLeave this tab open and remember this is the tab for the load balancer, you will use it again later in the lab step.\n\n6\\. In the left-hand menu, under Instances, click Instances:\n\n\n\nYou will see two instances named web-node with a status of Running:\n\n\n\n7\\. Select one of the instances:\n\n\n\nYou will see tabs displayed below the list of instances.\n\n8\\. In the Details tab, in the Public IPv4 DNS field, click the copy icon:\n\n\n\nThe public DNS name of the EC2 has been copied to your clipboard.\n\n9\\. In a new browser tab, paste the DNS name and press enter.\n\nYou will see an instance Id displayed again.\n\nHowever, this time, because you are accessing the instance directly if you refresh or visit the DNS name in an incognito or private browsing tab, the Id won't change.\n\nNote that you are accessing the instance directly, this is allowed by the security group associated with the EC2 instances. Allowing load-balanced instances to be publicly accessible is a bad security practice, and there is rarely a good reason for it.\n\nIn the rest of this lab step, you will modify the EC2 instance's security group to only allow traffic from the load balancer.\n\nLeave this browser tab open and remember this is the tab for an EC2 instance, you will use this tab again later.\n\nNavigate to Load Balancers in the EC2 Management Console.\n\n11\\. Ensure the classic-elb load balancer is selected.\n\n12\\. In the Description tab, scroll down to the Security section:\n\n\n\nThis is the security group you configured when you created the load balancer.\n\n13\\. In the left-hand menu, under Network & Security, click Security Groups:\n\n\n\nYou will see a list of security groups:\n\n\n\nSelect the SG which has the Group Name starting with cloudacademylabs-  .\n\nThis is the security group of the EC2 instances.\n\nYou will see tabs displayed beneath the list.\n\n15\\. In the row of tabs, click Inbound rules:\n\n\n\n16\\. To modify the rules of this security group, click Edit inbound rules:\n\n\n\nYou want to allow only connections coming from the load balancer to the instances, however, the balancer doesn't have a particular IP address associated with it so you can't specify an IP address here. Instead, you will restrict the access by using the security group you created for the balancer.\n\nYou will change the current rule to deny access to anywhere and allow it only to members of the load balancer's security group.\n\n17\\. Delete the existing rule, and create a new one whose Type is HTTP. In the Source drop-down, ensure Custom is selected and in the box next to it, select elb-sg:\n\n\n\n18\\. To save your changes, in the bottom-right, click Save rules:\n\n\n\nWith your rule saved, reload the browser tab with the DNS of the load balancer.\n\nThis will continue to work, you will see an instance Id displayed.\n\n19\\. Reload the browser tab with the DNS of an instance in the address bar:\n\nThe exact behavior will vary depending upon your web browser.\n\nMost likely you see the loading symbol in the browser tab spinning indefinitely:\n\n\n\nIf you wait long enough, your browser will report that it timed out trying to reach the instance:\n\nChecking Your Load Balancer's Behavior During Instance Failures\n\n\nNavigate to Instances in the EC2 Management Console.\n\nYou will see two instances named web-node listed.\n\n2\\. To stop an instance, right-click one of them.\n\n3\\. In the menu that appears, click Instance state, and then click Stop instance:\n\nalt\n\nYou will see a dialog box asking you to confirm that you want to stop the instance.\n\n4\\. To confirm, click Stop:\n\nalt\n\nThe instance's Instance state column will change to Stopping. A few moments later you will see it changed to Stopped:\n\nalt\n\nStopping the instance will make it fail your load balancer's health checks.\n\nNavigate to Load Balancers in the EC2 Management Console.\n\n6\\. Ensure the classic-elb load balancer is selected.\n\n7\\. In the row of tabs below the load balancer list, click Instances:\n\nalt\n\nLook at the Status column in the instances table, one of the instances will still be InService, and the other will be OutOfService:\n\nalt\n\nThis means that there is only one instance serving the application, and therefore all the requests will be forwarded to the same instance.\n\nYou can test this behavior by clicking on the Description tab and accessing the *DNS name *of the load balancer in a new browser tab. Your request will be served by the instance that you didn't stop.\n\nLeave the browser tab with the load balancer's DNS name open. You will test it again after starting the stopped instance.\n\n8\\. To start the stopped instance, in the left-hand menu, under Instances, click Instances:\n\nalt\n\n9\\. Right-click the stopped instance.\n\n10\\. Click Instance state, and click Start instance:\n\nalt\n\nNote: You can also access this menu using the Actions button in the top-right.\n\nThe Instance state column will change to Pending, and a few moments later, to Running.\n\nTest accessing the load balancer by it's DNS name again. This time, you will see that both instances are serving requests.\n\nNote: You may need to open the load balancer's domain name in an incognito or private browsing tab to see both instance Ids.\n\nMonitoring your Classic Load Balancer\n\n\nNavigate to Load Balancers in the EC2 Management Console.\n\n2\\. In the list of load balancers, ensure the classic-elb load balancer is selected, and click the Monitoring tab:\n\nalt\n\nYou will see a number of graphs of different CloudWatch metrics.\n\nThe Elastic Load Balancing (ELB) service reports metrics to CloudWatch only when requests are flowing through the load balancer. If there are requests flowing through the load balancer, the load balancing service measures and sends its metrics in sixty-second intervals. If there are no requests flowing through the load balancer, or no data for a metric, the metric is not reported.\n\nThere are a few metrics related to a Classic Load Balancer, and most are self-explanatory if you are familiar with HTTP requests. If some of them are unfamiliar to you, visit the Amazon AWS documentation to read more.\n\nThe metrics called HealthyHostCount, and *UnHealthyHostCount *will count the number of Healthy and Unhealthy instances respectively. These metrics can be useful for you to identify a major problem in your AWS account. A healthy instance is one that is passing the health checks performed by the load balancer.\n\nYou could use CloudWatch Alarms to notify you when you have less than 2 instances running your application, though to be clear this is not a general rule: the number of instances that might identify a problem will vary depending on your environment.\n\nAlso notice that in these metrics, there is no way of seeing the Availability Zone to which the Healthy/Unhealthy instance belongs. In our lab, we stopped an instance for a few minutes, therefore you should be able to see something like this:\n\nalt\n\nIf the Healthy Hosts metric reaches zero, that means that people won't see anything when accessing your load balancer, and it is probable that you have a big problem in your infrastructure.\n\nThe Average Latency metric might be useful to identify potential issues in your setup. Maybe everything is working in your application, but you notice an increase in this metric. If you haven't changed anything in your application, that can be a potential issue - maybe you haven't provisioned enough EC2 instances, or you even have lots of instances but they don't have enough power to serve your increasing traffic.\n\nalt\n\nThe other metrics can be very useful for troubleshooting specific scenarios and will vary depending on your setup.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/eventbridge/",
            "title": "EventBridge",
            "description": "Amazon EventBridge - Build event-driven applications at scale across AWS, existing systems, or SaaS apps",
            "content": "\nAbout\n\nEventB­ridge is a serverless event bus that makes it easy to connect applic­ations together using data from apps, integrated SaaS apps, & AWS services.\n\nDocumentation\nUser Guide\n\nAmazon EventBridge Flow\n\nEventB­ridge is a low-cost alternative to building a new backend infrastructure for every new app. With Serverless EventB­ridge, you can connect your existing apps with a few lines of code. You don’t have to build a new backend for every new app you want to connect to.\n\nYou can use existing infrastructure as a provider of event data, and connect your apps using Serverless EventB­ridge.\n\nAlternatives\n\nAzure Service Bus\nTIBCO Cloud Integration (including BusinessWorks and Scribe)\nIBM App Connect\nGoogle Cloud Pub/Sub\nApache Camel\nPeregrine Connect\nSoftware AG webMethods\nIBM Cloud Pak for Integration\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Applic­ation integr­ation\n\nSame type services: SNS, SQS, AppSync, EventBridge\n\nRe-architect for speed\nExtend functionality via SaaS integrations\nMonitoring and Auditing\nCustomize SaaS with AI/ML\n\nEventBridge vs Amazon SNS\n\nIn comparison with Amazon SNS, EventBridge:\n\nIntegrates with more AWS services than SNS\nSupports registering message schemas\nHas sophisticated third-party integrations available\nSupports transforming event messages before sending them\n\nYou should choose to use Amazon EventBridge over Amazon SNS when the system you are building is expected to:\n\nSupport significant asynchronous functionality\nGrow significantly in terms of both usage and complexity\nHave changing requirements over time\nHave components built by different teams that interact\nNeed support for disparate event sources and targets\n\nAmazon EventBridge vs CloudWatch Events\n\nAmazon EventBridge extends CloudWatch Events - Build event-driven architectures\nOriginal goal with CloudWatch Events was to help with monitoring usecases specific to AWS services.\n  React to events from Your Applications, AWS services and Partner Services\n    Example: EC2 status change, change in your application or SaaS partner application\n  Event Targets can be a Lambda function, an SNS Topic, an SQS queues etc\n  Rules map events to targets (Make sure that IAM Roles have permissions)\n  Event buses receive the events:\n    Default event bus (for AWS services)\n    Custom event bus (custom applications)\n    Partner event bus (partner applications)\nOver time, Amazon EventBridge will replace Amazon CloudWatch Events\n\nPractice\n\n{{}}\n\nProcessing File Uploads Asynchronously with Amazon EventBridge\n\nQuestions\n\nQ1\n\nA food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\n\nWhich approach best meets these requirements?\n\nUse EventBridge with Kinesis Data Streams to send messages. \nUse a Step Function to send SQS messages.\nUse a Lambda function to send SNS messages.\nUse AWS Batch and SNS to send messages.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html\n\n3\n",
            "tags": [
                "AWS",
                "Amazon EventBridge"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/fargate/",
            "title": "Fargate",
            "description": "Serverless compute for containers",
            "content": "\nAbout\n\nServerless version of ECS.\n\nServerless compute for contai­ners.\n\nAWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers.\n\nDeploy and manage your applications, not infrastructure. Fargate removes the operational overhead of scaling, patching, securing, and managing servers.\n\nCompatible with both Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\n\nDocumentation\nUser Guide\n\n\nAWS Fargate Flow\n\nAlternatives\n\nGoogle Kubernetes Engine (GKE)\nRed Hat OpenShift Container Platform\nAzure Kubernetes Service (AKS)\nRancher\nAzure Container Instances\nCloud Foundry\nOracle Cloud Infrastructure Container Engine for Kubernetes\n\nPrice\n\nCurrent price\n\nUse Cases\n\nWeb apps, APIs, and microservices\nRun and scale container workloads\nSupport AI and ML training applications\n\nType: Containers\n\nSame type services: Elastic Container Service (ECS), Elastic Container Registry (ECR), Elastic Kubernetes Service (EKS), Fargate\n\nQuestions\n\nQ1\n\nHow AWS Fargate different from AWS ECS?\n\n\nExplanation\n\n\nIn AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances.\nWhile in AWS Fargate, you don't provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n\n\n\n",
            "tags": [
                "aws",
                "Fargate"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/fis/",
            "title": "Fault Injection Simulator",
            "description": "Improve resiliency and performance with controlled experiments with AWS Fault Injection Simulator",
            "content": "\nAbout\n\nAWS Fault Injection Simulator (FIS) is a fully managed service for running fault injection experiments on AWS that makes it easier to improve an application’s performance, observability, and resiliency.\n\nDocumentation\nUser Guide\n\nFault Injection Simulator Flow\n\nPrice\n\nCurrent price\n\nWith AWS FIS, you pay only for what you use. There are no upfront costs or minimum fees. You are charged based on the duration that an action is active. The AWS FIS price is $0.10 per action-minute.\n\nTerminology and Concepts\n\nEverything starts with an experiment template. The experiment template defines the targets that participate in the experiment. Supported targets are:\n\nEC2 Instances\nEKS node groups\nRDS clusters & instances\nIAM roles\n\nThe actions define the injected faults. You can run actions in parallel or sequence.\n\nSome action examples:\n\nAWS API level errors for the EC2 service\nStop/reboot/terminate EC2 instances\nRun SSM commands on EC2 instances to stress CPU or memory, add network latency, or kill a process\nReboot RDS instance\nFailover RDS cluster\nDrain ECS container instance\nTerminate EKS node group instance\n\nUse Cases\n\nPeriodic Game Days\nContinuous Delivery Pipeline Integration\n\nPractice\n\nTest instance stop and start using\n\nQuestions\n\nQ1\n\nWhat is Chaos Engineering?\n\n\nExplanation\n\n\nChaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\n\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\n\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n\n\n\n\n",
            "tags": [
                "aws",
                "Fault Injection Simulator",
                "FIS"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/iam/",
            "title": "IAM",
            "description": "A step-by-step guide to setting up AWS Identity and Access Management (IAM)",
            "content": "\nAbout\n\nIAM - AWS Identity and Access Management\n\nAWS IAM\nAWS IAM User Guide\n\nAWS Identity and Access Management (IAM) allows to securely control user access to AWS services and resources.\n\nDesigned for organizations with multiple users or systems that use AWS products such as Amazon EC2, Amazon RDS, and AWS Management Console.\n\nWith IAM, you can centrally manage users, security credentials such as access keys, and permissions that control user access to AWS resources.\n\nAmazon IAM Flow\n\nThere are three ways IAM authenticates a principal:\n\nUser Name/Password\nAccess Key\nAccess Key/Session Token\n\nDigest\n\nIAM consists of the following:\n  Users\n  Groups\n  Roles\n  Policy Documents\nIAM is Global. It doesn't apply to any specific region.\nThere is no charge to use IAM.\nIAM is compliant with Payment Card Industry (PCI) Data Security Standard (DSS)\nThe \"root account\" has complete Admin access.\nDon't use \"root account\"** for everyday use. Instead, create users. A new user will have NO permissions by default. Grant least privilege needed for their job.\nNew user will be assigned with password, Access Key ID & Secret Access Keys. The password will be used to login to AWS management console. Access Key ID & Secret Access Key will be used to login via the APIs and CLI\nAlways setup MFA on your root account.\nUse Groups to assign permissions to IAM users\nUse Roles to Delegate permissions. Role is more secure than creating individual user. Roles gives temporary credentials for access; whereas User has long term credentials.\nCreate and customize password rotation policies\nPolicies can be attached to users, groups and roles. Use AWS defined policies, assign permissions wherever possible. Policy is defined in JSON format and contains version, statements, - effect, action, resource, principal, and condition.\nSTS Security Token Service provides temporary security credentials to the trusted users. STS is global and there is no charge to use it.\nDigest: https://tutorialsdojo.com/aws-identity-and-access-management-iam/\nIAM best practices - Question might ask you to identify best practices among the given choices. https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\nDifference between when to use Role and User.\nIAM Policy Simulator - service for testing and troubleshooting IAM Policies. Details\n\nPractice\n\n{{}}\n\nGo to IAM page\n\nCreating IAM groups\n\nOn the User Groups page, click Create group\n\n01.png\n\nSpecify the name of the group. Mine is: DevOps.\nAdd permission to view EC2: AmazonEC2ReadOnlyAccess.\ncreate\n\nAmazon EC2 Read Only Access\n\nThe group was created\n\nIAM Group\n\nCreating IAM users\n\nOn the Users page, click Create user\n\n\nType in user name (login)\n\nPermissions\nAdd user to the group\n\nTags\nSkip section or put tags. It is useful and popular to set tags for resources in companies with a lot of connected AWS resources\n\nLogin/Password\nAt the last step, download the .csv file with login, keys and password. You will need the password later to log in as this user.\nOn this page there is a link to log in. We will use it in the next step\n\nLogging in as a new user\n\nChecking privileges.\nThis user has access to view EC2 instances. Let's check whether or not the S3 garbage cans have access.\n\nLet's try to create an S3 bucket\n\n\nAfter trying to create a recycle bucket, we get a window indicating no permissions\n\nQuestions\n\nQ1\n\nA client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\n\nWhat are the best practice guidelines for use of the root account?\n\nNever use the root account.\nUse the root account only to create administrator accounts.\nUse the root account to create your first IAM user and then lock away the root account.\nUse the root account to create all other accounts, and share the root account with one backup administrator.\n\n\nExplanation\n\nlock-away-credentials\n\n1\n\nQ2\n\nYour organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\n\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\n\nAssumeRoleWithSAML\nGetFederationToken\nAssumeRoleWithWebIdentity\nGetCallerIdentity\n\n\nExplanation\n\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\n\n1\n\nQ3\n\nAlice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\n\nWhich of the following APIs will be leveraged to provide temporary security credentials?\n\nAssumeRoleWithSAML\nGetFederationToken\nGetSessionToken\nAssumeRoleWithWebIdentity\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\n\n(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA\n\n3\n\nQ4\n\nA leading insurance firm has several new members in its development team. The solutions architect was instructed to provision access to certain IAM users who perform application development tasks in the VPC.\n\nThe access should allow the users to create and configure various AWS resources, such as deploying Windows EC2 servers. In addition, the users should be able to see the permissions in AWS Organizations to view information about the user’s organization, including the master account email and organization limitations.\n\nWhich of the following should the solutions architect implement to follow the standard security advice of granting the least privilege?\n\nAttach the PowerUserAccess AWS managed policy to the IAM users.\nAttach the AdministratorAccess AWS managed policy to the IAM users.\nCreate a new IAM role and attach the SystemAdministrator AWS managed policy to it. Assign the IAM Role to the IAM users.\nCreate a new IAM role and attach the AdministratorAccess AWS managed policy to it. Assign the IAM Role to the IAM users.\n\n\nExplanation\n\n\nAWS managed policies for job functions are designed to closely align to common job functions in the IT industry. You can use these policies to easily grant the permissions needed to carry out the tasks expected of someone in a specific job function.\n\nThese policies consolidate permissions for many services into a single policy that’s easier to work with than having permissions scattered across many policies.\n\nFor Developer Power Users, you can use the AWS managed policy name: PowerUserAccess if you have users who perform application development tasks. This policy will enable them to create and configure resources and services that support AWS aware application development.\n\nThe first statement of this policy uses the NotAction element to allow all actions for all AWS services and for all resources except AWS Identity and Access Management and AWS Organizations. The second statement grants IAM permissions to create a service-linked role.\n\nThis is required by some services that must access resources in another service, such as an Amazon S3 bucket. It also grants Organizations permissions to view information about the user’s organization, including the master account email and organization limitations.\n\n1\n\nQ5\n\nA company has 100 AWS accounts that are consolidated using AWS Organizations. The accountants from the finance department log in as IAM users in the TD-Finance AWS account. The finance team members need to read the consolidated billing information in the TD-Master AWS master account that pays the charges of all the member (linked) accounts. The required IAM access to the AWS billing services has already been provisioned in the master account.\n\nThe Security Officer should ensure that the finance team must not be able to view any other resources in the master account.\n\nWhich of the following grants the finance team the necessary permissions for the above requirement?\n\nSet up an IAM group for the finance users in the TD-Finance account then attach a ViewBilling permission and AWS managed ReadOnlyAccess IAM policy to the group.\nSet up individual IAM users for the finance users in the TD-Master account then attach the AWS managed ReadOnlyAccess IAM policy to the group with cross-account access.\nSet up an AWS IAM role in the TD-Finance account with the ViewBilling permission then grant the finance users in the TD-Master account the permission to assume that role.\nSet up an IAM role in the TD-Master account with the ViewBilling permission then grant the finance users in the TD-Finance account the permission to assume the role.\n\n\nExplanation\n\n\nYou can use the consolidated billing feature in AWS Organizations to consolidate billing and payment for multiple AWS accounts or multiple Amazon Internet Services Pvt. Ltd (AISPL) accounts. Every organization in AWS Organizations has a master (payer) account that pays the charges of all the member (linked) accounts.\n\nModifyAccount – Allow or deny IAM users permission to modify Account Settings.\nModifyAccount – Allow or deny IAM users permission to modify Account Settings.\nModifyBilling – Allow or deny IAM users permission to modify billing settings.\nModifyPaymentMethods – Allow or deny IAM users permission to modify payment methods.\nViewAccount – Allow or deny IAM users permission to view account settings.\nViewBilling – Allow or deny IAM users permission to view billing pages in the console.\nViewPaymentMethods – Allow or deny IAM users permission to view payment methods.\nViewUsage – Allow or deny IAM users permission to view AWS usage reports.\n\nUse policies to grant permissions to perform an operation in AWS. When you use an action in a policy, you usually allow or deny access to the API operation or CLI command with the same name. However, in some cases, a single action controls access to more than one operation.\n\n4\n\nResources\n\nSecurity best practices in IAM\nIAM Hands-On Lab\nIAM Workshops\nSecurity workshop\ntutorialsdojo digest\n\nCommunity posts\n\nhttps://dev.to/romankurnovskii/aws-iam-cheet-sheet-3if4",
            "tags": [
                "aws",
                "iam"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/kinesis/_index",
            "title": "Kinesis",
            "description": "Easily collect, process, and analyze video and data streams in real time",
            "content": "\nAbout\n\nKinesis makes it easy to collect, process, & analyze real-time, streaming data, so one can get timely insights.\n\nDocumentation\nUser Guide\n\nAmazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.\n\nKinesis flow\n\nReal-time-based\nFully managed\nScalable\n\nCapabilities\n\nKinesis Video Streams\n\nCapture, process, and store video streams\n\nKinesis Data Streams\n\nCapture, process, and store data streams\n\n\nKinesis Data Firehose\n\nLoad data streams into AWS data stores\n\n\n\nThe easiest way to capture, transform, and load data streams into AWS data stores for near real-time analytics\n\nKinesis Data Analytics\n\nAnalyze data streams with SQL or Apache Flink\n\nDigest\n\nKinesis data stream.\nHot shard vs cold shard.\nMerging shards will decrease streams capacity.\nKinesis adapter is the recommended way to consume streams from DynamoDB.\nIncoming write bandwidth and outgoing read bandwidth are used to calculate initial number of shards for kinesis stream.\nA single Kinesis Shard can handle 1MB per second write. 2MB per second read. It can also handle 1000 writes per second, and 5 read transactions a second\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Analytics\n\nSame type services: Athena, EMR, Redshift, Kinesis, Elasti­cSearch Service, Quicksight\n\n\nAnalysis of streaming social media data\nNetflix uses Amazon Kinesis to monitor the communications between all of its applications so it can detect and fix issues quickly, ensuring high service uptime and availability to its customers.\n\nPractice\n\nQuestions\n\nQ1\n\nYou built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.\n\nWhat steps will you take to keep a strong performance?\n\nRemove the hot shards\nMerge the hot shards\nSplit the hot shards\nIncrease the shard capacity\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\n\nSplit the hot shards\n\n3\n\nQ2\n\nJasmin needs to perform ad-hoc business analytics queries on well-structured data. Data comes in constantly at a high velocity. Jasmin's team can understand SQL.\n\nWhat AWS service(s) should Jasmin look to first?\n\nEMR using Hive\nEMR running Apache Spark\nKinesis Firehose + RDS\nKinesis Firehose + RedShift\n\n\nExplanation\n\n\nRedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol\n\nhttps://aws.amazon.com/kinesis/data-firehose/features/\n\n4\n\nResources\n\nFAQ\nOpenGuide\nHowTo\nIntroduction to Amazon Kinesis\nAWS Webcast - Introduction to Amazon Kinesis\n",
            "tags": [
                "aws",
                "kinesis"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/kinesis/sessionizing-clickstream-data-kinesis-data-analytics/",
            "title": "Sessionizing Clickstream Data with Amazon Kinesis Data Analytics",
            "description": "Sessionizing Clickstream Data with Amazon Kinesis Data Analytics",
            "content": "\nLab\n\nSessionizing Clickstream Data with Amazon Kinesis Data Analytics\n\nCreating an Amazon Kinesis Data Analytics Application\n\nIn the AWS Management Console search bar, enter Kinesis, and click the Kinesis result under Services:\n\nalt\n\nYou will be taken to the Amazon Kinesis dashboard.\n\nIn this lab, a Kinesis Data Stream has been pre-created for you. Under Data Streams you will see Total data streams is one:\n\nalt\n\n2\\. In the left-hand menu, click Analytics applications and under that click SQL applications:\n\nalt\n\n3\\. To start creating a Kinesis Data Analytics application, under Data Analytics, click Create SQL application (legacy):\n\nalt\n\nYou will be taken to the Create legacy SQL application form.\n\n4\\. In the Application configuration section, and enter lab-application in the Application name textbox:\n\nalt\n\n5\\. At the bottom of the page, click Create legacy SQL application:\n\nalt\n\nYou will be taken to a page displaying details of your application and you will see a notification that your application has been created:\n\nalt\n\nYou will come back to this page later in the lab to connect the pre-created Kinesis Data Stream as a data source for your Kinesis Data Analytics application.\n\n6\\. To navigate to the Kinesis Data Streams list page, in the left-hand side menu, click Data streams:\n\nalt\n\nYou will see one data stream listed called lab-stream.\n\n7\\. To view the details of the pre-created data stream, in the list, click lab-stream:\n\nalt\n\nYou will be taken to the Stream details page and you will see a series of tabs with Monitoring selected.\n\n8\\. To see the configuration details of the data stream, click Configuration:\n\nalt\n\nTake a moment to look at the details on this page, there are several Kinesis Data Stream configuration options that you should be aware of:\n\nData Stream capacity**: The number of shards in the Data Stream. Each shard has a maximum read and write capacity. To increase the total capacity of a data stream you can add shards.\nEncryption**: Kinesis Data Streams can be encrypted using an AWS managed or customer-managed, KMS key.\nData retention**: A Kinesis Data Stream can retain data for a configurable amount of time between 24 and 168 hours.\nEnhanced (shard-level) metrics**: More detailed CloudWatch metrics can be enabled for a Data Stream, these enhanced metrics have an extra cost.\n\nIn this lab, you will be working with a small amount of sample data, so there is one shard configured.\n\nLeave these options without changing them.\n\nConnecting to the Virtual Machine using EC2 Instance Connect\n\nIn the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\nalt\n\n2\\. To see available instances, click Instances in the left-hand menu:\n\nalt\n\nThe instances list page will open, and you will see an instance named cloudacademylabs:\n\nalt\n\nIf you don't see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n\n3\\. Right-click the cloudacademylabs instance, and click Connect:\n\nalt\n\nThe Connect to your instance form will load.\n\n4\\. In the form, ensure the EC2 Instance Connect tab is selected:\n\nalt\n\nYou will see the instance's Instance ID and Public IP address displayed.\n\n5\\. In the User name textbox, enter ec2-user:\n\nalt\n\nNote: Ensure there is no space after ec2-user or connect will fail. \n\n6\\. To open a browser-based shell, click Connect:\n\nalt\n\nIf you see an error it's likely that the environment hasn't finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\n\nalt\n\nA browser-based shell will open in a new window ready for you to use.\n\nKeep this window open, you will use it in later lab steps.\n\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\n\nSimulating a Real-Time Clickstream\n\n1\\. To create a template JSON file for a click event, enter the following command into the shell:\n\necho '{\n  \"user_id\": \"$USER_ID\",\n  \"event_timestamp\": \"$EVENT_TIMESTAMP\",\n  \"event_name\": \"$EVENT_NAME\",\n  \"event_type\": \"click\",\n  \"device_type\": \"desktop\"\n}' > click.json\n\n\nThere are two parts to this command, the first uses the built-in Bash command echo to print a JSON template. The second part uses a feature of the Bash shell called redirection, it redirects the output of the echo command to a file (creating it if doesn't exist) called click.json.\n\nThe template contains five fields, the event_type, and device_type fields are hardcoded, in a non-lab environment, you may encounter streams that come from different types of devices and streams that contain more than one type of event (clickstream events alongside sales or transaction data for example). The other fields will be populated dynamically.\n\n2\\. To put records into Kinesis and simulate a clickstream, enter the following command:\n\nUSER_IDS=(user1 user2 user3)\nEVENTS=(checkout search category detail navigate)\nfor i in $(seq 1 3000); do\n    echo \"Iteration: ${i}\"\n    export USER_ID=\"${USER_IDS[RANDOM%${#USER_IDS[@]}]}\";\n    export EVENT_NAME=\"${EVENTS[RANDOM%${#EVENTS[@]}]}\";\n    export EVENT_TIMESTAMP=$(($(date +%s) * 1000))\n    JSON=$(cat click.json | envsubst)\n    echo $JSON\n    aws kinesis put-record --stream-name lab-stream --data \"${JSON}\" --partition-key 1 --region us-west-2\n    session_interval=15\n    click_interval=2\n    if ! (($i%60)); then\n        echo \"Sleeping for ${session_interval} seconds\" && sleep ${session_interval}\n    else\n        echo \"Sleeping for ${click_interval} second(s)\" && sleep ${click_interval}\n    fi\ndone\n\nYou will see the templated JSON and also the JSON response from Kinesis for each record put into the Data Stream:\n\nalt\n\nThis command simulates a real-time click-stream with the following characteristics:\n\nCreates three thousand events\nEvents have a two-second interval between them\nAfter every sixty events (two minutes) there is a fifteen-second interval, later you will assume a gap of ten seconds or more is a session boundary\n\nThe command has a number of parts:\n\nSetup of sample user ids and event types at the beginning\nA loop that will execute three thousand times and a sleep statement\nStatements that randomly select a user id and an event type, and assign them along with the current timestamp to variables\nA statement that uses the envsubst command to substitute defined environment variables in the JSON template\nA statement invoking the AWS command-line interface tool, putting the templated JSON record into the Kinesis Data Stream\nA condition at the end of the loop that either sleeps for a few seconds or, periodically for longer, simulating the end of a session\n\nLeave the command running.\n\nNavigate to Kinesis Data Analytics in the AWS Management Console.\n\n4\\. In the list of applications, to expand the application, click lab-application:\n\nalt\n\n5\\. To connect your Data Analytics application to the pre-created Data Stream, click Configure **under Source stream **form:\n\nalt\n\nThe Configure source for lab-application form will load.\n\n6\\. Under Source, ensure Kinesis data stream is selected:\n\nalt\n\n7\\. In the Kinesis data stream, click Browse to select the radio button for lab-stream **and click Choose**:\n\nalt\n\n8\\. Under Access permissions, select Choose from IAM roles that Kinesis Data Analytics can assume:\n\nalt\n\n9\\. In the IAM role list, select the role beginning with cloudacademy-lab-data-analytics:\n\nalt\n\nIf you don't see the above role listed click the refresh button:\n\nalt\n\n10\\. To start discovering the schema of the records you added to the Data Stream, click Discover schema:\n\nalt\n\nAfter a moment or two, you will see a notification that the discovery was successful and below, some of the records will be displayed:\n\nalt\n\n11\\. To finish connecting your Data Analytics application to your Data Stream, click Save changes:\n\nalt\n\nYou will be redirected to the page for your Kinesis Data Analytics application. Leave this page open in a browser tab.\n\nSessionizing the Clickstream Data using Amazon Kinesis Data Analytics\n\n1\\. Return to the page for your Kinesis Data Analytics application in the AWS Management Console.\n\n2\\. To start your application and expand the Steps to configure your application, click Configure SQL:\n\n\n\n3\\. In the SQL code editor, replace the existing contents with the following SQL commands\n\nCREATE OR REPLACE STREAM \"INTERMEDIATE_SQL_STREAM\"\n(\n    \"event_timestamp\" TIMESTAMP,\n    \"user_id\" VARCHAR(7),\n    \"device_type\" VARCHAR(10),\n    \"session_timestamp\" TIMESTAMP\n);\n\n\nCREATE OR REPLACE  PUMP \"STREAM_PUMP1\" AS INSERT INTO \"INTERMEDIATE_SQL_STREAM\"\nSELECT  STREAM\n    TO_TIMESTAMP(\"event_timestamp\") as \"event_timestamp\",\n    \"user_id\",\n    \"device_type\",\n    CASE WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) > (10 * 1000) THEN\n            TO_TIMESTAMP(\"event_timestamp\")\n         WHEN (\"event_timestamp\" - lag(\"event_timestamp\", 1) OVER (PARTITION BY \"user_id\" ROWS 1 PRECEDING)) IS NULL THEN\n            TO_TIMESTAMP(\"event_timestamp\")\n         ELSE NULL\n    END AS \"session_timestamp\"\nFROM \"SOURCE_SQL_STREAM_001\";\n\nThese statements do the following:\n\nDefines an intermediate stream to insert data into called INTERMEDIATE\\_SQL\\_STREAM\nCreates a PUMP that selects data from the source stream\nThe SELECT statement uses the LAG function to determine if there is a ten-second interval between the last event and the current event\nThe LAG function statements are used with PARTITION statements to restrict the LAG function by the user\n\nYou should know that Kinesis Data Analytics natively assumes Unix timestamps include milliseconds. The stream you simulated is providing timestamps with milliseconds. This is why the CASE WHEN statement that checks for a ten-second interval includes (10 * 1000), it's multiplying ten by one thousand to get ten seconds in milliseconds.\n\nTip: you can increase the height of the SQL editor text-box by dragging the grey bar at the bottom.\n\n4\\. To execute the SQL statements, click Save and run application:\n\n\n\nThe query will take up to a couple of minutes to execute and start returning results.\n\nOccasionally you may see an error caused by the fifteen-second interval, if you do, re-run the query by clicking Save and run application again.\n\nTake a look at the results. Notice that only some records have a value for session_timestamp. This is because the CASE WHEN statement in the query supplies a value of null when:\n\nThe interval between event timestamps is less than ten seconds\nThere is no preceding event\n\nAlso notice that below the SQL Code editor, there are two streams, the INTERMEDIATE\\_SQL\\_STREAM, and an error_stream. The error stream is where any errors that occur during the execution of the SQL will be delivered to.\n\n5\\. In the SQL editor window, under the current SQL statements, add the following:\n\nCREATE OR REPLACE STREAM \"DESTINATION_SQL_STREAM\" (\n    \"user_id\" CHAR(7),\n    \"session_id\" VARCHAR(50),\n    \"session_time\" VARCHAR(20),\n    \"latest_time\" VARCHAR(20)\n);\n\n\nCREATE OR REPLACE  PUMP \"STREAM_PUMP2\" AS INSERT INTO \"DESTINATION_SQL_STREAM\"\nSELECT STREAM\n    \"user_id\",\n    \"user_id\"||''||\"device_type\"||''||TIMESTAMP_TO_CHAR('HH:mm:ss', LAST_VALUE(\"session_timestamp\") IGNORE NULLS OVER\n        (PARTITION BY \"user_id\" RANGE INTERVAL '24' HOUR PRECEDING)) AS \"session_id\",\n    TIMESTAMP_TO_CHAR('HH:mm:ss', \"session_timestamp\") AS \"session_time\",\n    TIMESTAMP_TO_CHAR('HH:mm:ss', \"event_timestamp\") AS \"latest_time\"\nFROM \"INTERMEDIATE_SQL_STREAM\"\nWHERE \"user_id\" = 'user1';\n\n\nThese SQL statements do the following:\n\nCreates a stream called DESTINATION\\_SQL\\_STREAM\nCreates a PUMP that selects from the INTERMEDIATE\\_SQL\\_STREAM\nConstructs a session_id by combining the user, device type and time\nRestricts the query to user1 using a WHERE clause\n\nSomething else to note about these statements is that the session and event timestamps are being converted to times.\n\n6\\. To run the updated query, click Save and run application.\n\nYou will see results similar to:\n\n\n\nYour times will be different.\n\nNotice that the session_time values are more than ten seconds apart. And that the seconds' interval of the latest_time column between the rows that have a session time, is ten seconds or less.\n\n7\\. To see only the rows for new sessions, replace the last line of the query with the following:\n\nWHERE \"session_timestamp\" IS NOT NULL;\n\nThis change to the WHERE clause of the last SQL statement removes the restriction of the query to user1, and removes rows where the value of session_timestamp is null.\n\n8\\. Click Save and run application to re-run your query.\n\nYou will see results similar to the following:\n\n\n\nYour results will be different.\n\nThe results now contain only session boundary rows for each of the users.\n\nLeave this browser tab open with the query running in Kinesis Data Analytics.\n\nCreating an AWS Lambda function to Store Sessions in an Amazon DynamoDB Table\n\n1\\. In the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\n\n\n\n2\\. To start creating your function, click Create function:\n\n\n\n3\\. Under Create function, ensure Author from scratch is selected:\n\n\n\n4\\. Under Basic information, in the Function name text-box, enter lab-function:\n\n\n\n5\\. In the Runtime drop-down, select the latest Python 3.x version available.\n\n6\\. To expand the role selection form, click Change default execution role.\n\n7\\. Under Execution role, select the Use an existing role radio button:\n\n\n\n8\\. To assign an execution role, in the Existing role drop-down, select the role called cloudacademy-lab-lambda:\n\n\n\n9\\. To create your function, click Create function:\n\n\n\nYou will be taken a page where you can configure your function, and you will see a notification that your function has been successfully created:\n\n\n\n10\\. Scroll down to the Code source section and in the code editor double-click the lambda_function.py file.\n\n11\\. To update your Lambda function's implementation, replace the code in the editor window with the following:\n\nfrom future import print_function\nimport boto3\nimport base64\nfrom json import loads\n\ndynamodb_client = boto3.client('dynamodb')\n\ntable_name = \"CloudAcademyLabs\"\n\ndef lambda_handler(event, context):\n    payload = event['records']\n    output = []\n    success = 0\n    failure = 0\n\n    for record in payload:\n        try:\n            payload = base64.b64decode(record['data'])\n            data_item = loads(payload)\n\n            ddb_item = {\n                'session_id': { 'S': data_item['session_id'] },\n                'session_time': { 'S': data_item['session_time'] },\n                'user_id': { 'S': data_item['user_id'] }\n            }\n\n            dynamodb_client.put_item(TableName=table_name, Item=ddb_item)\n\n            success += 1\n            output.append({'recordId': record['recordId'], 'result': 'Ok'})\n        except Exception:\n            failure += 1\n            output.append({'recordId': record['recordId'], 'result': 'DeliveryFailed'})\n\n    print('Successfully delivered {0} records, failed to deliver {1} records'.format(success, failure))\n    return {'records': output}\n\nThis python code processes a record from Kinesis Data Analytics and puts it into a DynamoDB table.\n\nThe implementation is based on one provided by AWS. The only change is the statements that construct the ddb_item. They have been modified to match the data being supplied by your Kinesis Data Analytics application.\n\n12\\. To deploy your function, at the top, click Deploy:\n\n\n\nYou will see a notification that your function has been deployed:\n\n\n\n13\\. To configure a timeout for your function, click the Configuration tab, and click Edit:\n\n\n\n14\\. Under Timeout, enter 1 in the min text-box, and 0 in the sec text-box:\n\n\n\nYou are updating the timeout because the default of three seconds is too low when processing data from Kinesis Data Analytics, and may lead to failures caused by the function timing out. AWS recommends setting a higher timeout to avoid such failures. \n\n15\\. To save your function's updated timeout, click Save:\n\n\n\nYou will see a notification that your change to the timeout has been saved:\n\nConfiguring Amazon Kinesis Data Analytics to Use Your AWS Lambda Function as a Destination\n\nNavigate to Kinesis Data Analytics in the AWS Management Console.\n\n2\\. In the list of applications, to expand the application, click lab-application:\n\nalt\n\n3\\. To begin configuring your Lambda as a destination, expand the Steps to configure your application and click Add destination:\n\nalt\n\nThe Configure destination form will load.\n\n4\\. Under Destination select AWS Lambda function:\n\nalt\n\n5\\. Under AWS Lambda function, click Browse and check radio box for lab-function **followed by clicking Choose**:\n\nalt\n\nThis is the Lambda function you created in the previous lab step.\n\n6\\. Under Access permissions, ensure Choose from IAM roles that Kinesis Data Analytics can assume is selected:\n\nalt\n\n7\\. In the IAM role drop-down, select the role called cloudacademy-lab-lambda:\n\nalt\n\nThis is a role that has been pre-created for this lab and allows Kinesis Data Analytics to invoke your Lambda function.\n\n8\\. In the In-application stream section, under Connect in-application stream, select Choose an existing in-application stream:\n\nalt\n\n9\\. In the In-application stream name drop-down, select DESTINATION\\_SQL\\_STREAM:\n\nalt\n\n10\\. To finish connecting your Kinesis Data Analytics application to your Lambda function, click Save changes:\n\nalt\n\nYour Kinesis Data Analytics application is being updated. Please be aware that it can take up to three minutes to complete.\n\nOnce complete the details page for Kinesis Data Analytics application will load.\n\n11\\. In the AWS Management Console search bar, enter DynamoDB, and click the DynamoDB result under Services:\n\nalt\n\n12\\. In the left-hand menu, click Tables:\n\nalt\n\n13\\. In the list of tables, click CloudAcademyLabs:\n\nalt\n\nThis table was pre-created as a part of this lab.\n\n14\\. To see items in the DynamoDB table, click the Explore Table *Items* button:\n\nalt\n\nYou will see the items in the table listed similar to:\n\nalt\n\nThese items have been inserted into the DyanmoDB table by your Lambda function, it's being invoked by your Kinesis Data Analytics application.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/kms/_index",
            "title": "Key Management Service",
            "description": "Amazon Key Management Service - Easily create and control the keys used to encrypt or digitally sign your data",
            "content": "\nAbout\n\nAWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.\n\nDocumentation\nUser Guide\n\nAWS KMS provides management of encryption keys for use with other AWS services (e.g. EBS, S3, RDS, etc.).\n\nAmazon Key Management Service Flow\n\nKey management on AWS is a broad range of activities from creating & storing public & private keys to creating, managing, and authorising access to AWS services with digital keys. This guide explains the key management solution on AWS that is easiest to use, most secure, and provides the most flexibility for you to create and manage your keys the way you need them.\n\nAlternatives\n\nHashiCorp Vaultc\nAzure Key Vault\nGoogle Cloud Key Management Service\nOpenSSH\nAkeyless Vault Platform\nVirtru\n\nConcepts\n\nKey types:\n\nCustomer managed keys\nAWS managed keys\nAWS owned keys\nIn May 2022, AWS KMS changed the rotation schedule for AWS managed keys from every three years (approximately 1,095 days) to every year (approximately 365 days)\n\n\n| Type of KMS key                                                                                          | Can view KMS key metadata | Can manage KMS key | Used only for my AWS account | Automatic rotation | Pricing                             |\n| -------------------------------------------------------------------------------------------------------- | ------------------------- | ------------------ | ---------------------------- | -------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |\n| Customer managed key | Yes                       | Yes                | Yes                          | Optional. Every year (approximately 365 days)                                                | Monthly fee (pro-rated hourly)Per-use fee                          |\n| AWS managed key   | Yes                       | No                 | Yes                          | Required. Every year (approximately 365 days)                                                | No monthly feePer-use fee (some AWS services pay this fee for you) |\n| AWS owned key       | No                        | No                 | No                           | Varies                                                                                       | Varies                                                                     |\n\n\nCustomer Managed Keys (CMK)\n\nThe primary resources in AWS KMS are customer master keys (CMKs). Typically, you use CMKs to protect data encryption keys (or data keys) which are then used to encrypt or decrypt larger amounts of data outside of the service. CMKs never leave AWS KMS unencrypted, but data keys can. AWS KMS does not store, manage, or track your data keys.\n\nThere is one AWS-managed CMK for each service that is integrated with AWS KMS. When you create an encrypted resource in these services, you can choose to protect that resource under the AWS-managed CMK for that service. This CMK is unique to your AWS account and the AWS region in which it is used, and it protects the data keys used by the AWS services to protect your data.\n\nData keys\n\nData keys are used to encrypt large data objects within an application outside AWS KMS.\n\nKey rotation and Backing Keys\n\nWhen you create a customer master key (CMK) in AWS KMS, the service creates a key ID for the CMK and key material, referred to as a backing key, that is tied to the key ID of the CMK. If you choose to enable key rotation for a given CMK, AWS KMS will create a new version of the backing key for each rotation. It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. CMK is simply a logical resource that does not change regardless of whether or of how many times the underlying backing keys have been rotated.\n\nA KMS key consists of\n\nAlias\nCreation date\nDescription\nKey state\nKey material (either customer provided or AWS provided)\n\nA KMS key can:\n\nencrypt data up to 4KB in size\ngenerate, encrypt, and decrypt Data Encryption Keys (DEKs)\nnever be exported from KMS (CloudHSM allows this).\n\nAliases\n\nUse an alias as a friendly name for a KMS key. For example, you can refer to a KMS key as test-key instead of 1234abcd-12ab-34cd-56ef-1234567890ab.\n\nCustom key stores\n\nA custom key store is an AWS KMS resource associated with FIPS 140-2 Level 3 hardware security modules (HSMs) in a AWS CloudHSM cluster that you own and manage.\n\nKey policy\n\nWhen you create a KMS keys, you determine who can use and manage that KMS keys.\n\nDigest\n\nKMS encrypts small pieces of data (usually data keys) MAX - 4 KB\n  Use Envelope Encryption for larger objects (CMK never leaves KMS)\n    Generate a data key (plain-text and encrypted) from KMS (GenerateDataKey)\n    Use data key to perform encryption/decryption on the object (within the service or client-side)\nYou can assign an encryption context with cryptographic operations\n  If encryption context is different, decryption will NOT succeed\nRequest quotas for KMS Cryptographic operations:\n  5,500 to 50,000 per second (varies with Region)\n  You might get a ThrottlingException if you exceed the limit\n  Lower your request rate to AWS KMS or Retry with Exponential Backoff\nUsage of KMS CMKs can be tracked in CloudTrail\nKey policies control access to CMKs (incl. cross account access)\nUse AWS Encryption SDK to interact with KMS(Provides Data Key Caching)\n\nPrice\n\nCurrent price\n\nKMS vs Cloud HSM\n\nGenerate, store, use and replace your keys(symmetric & asymmetric)\nKMS: **Multi-tenant Key Management Service\n  KMS integrates with all storage and database services in AWS\n  Define key usage permissions (including cross account access)\n  Automatically rotate master keys once a year\n  Schedule key deletion to verify if the key is used\n    Mandatory minimum wait period of 7 days (max-30 days)\nCloudHSM: Dedicated **single-tenant HSM for regulatory compliance\n  AWS KMS is a Multi-tenant service\n  AWS CANNOT access your encryption master keys in CloudHSM\n    (Recommendation) Be ultra safe with your keys. Use two or more HSMs in separate AZs.\n    AWS KMS can use CloudHSM cluster as \"custom key store\" to store the keys:\n      AWS Services can continue to talk to KMS for data encryption\n      (AND) KMS does the necessary integration with CloudHSM cluster\n\nUse Cases\n\nType: Data protection\n\nSame type services: Macie, Key Management Service (KMS), CloudHSM, Certif­icate Manager, Secrets Manager\n\nCloudHSM: (Web servers) Offload SSL processing, Certificate Authority etc\n\nPractice\n\nHow to encrypt S3 Objects Using SSE-KMS\n\nQuestions\n\nQ1\n\nKey rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?\n\nKMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key.\nKMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys.\nKey rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed.\nKey rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key.\n\n\nExplanation\n\n\nWhen you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key's older cryptographic material so it can be used to decrypt data that it has encrypted.\n\nQ2\n\nAlan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.\n\nWhich statements are true with regard to this scenario? (Choose 2 answers)\n\nCreate Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts.\nCan share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys.\nMake encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts.\nYou can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n\n1, 2\n",
            "tags": [
                "AWS",
                "Amazon Key Management Service"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/kms/encrypting-s3-objects-using-sse-kms/",
            "title": "Encrypting S3 Objects Using SSE-KMS",
            "description": "How to encrypt S3 Objects Using SSE-KMS",
            "content": "\nLab\n\nEncrypting S3 Objects Using SSE-KMS\n\nCreating a Customer Master Key (CMK)\n\n1\\. In the AWS Management Console search bar, enter KMS, and click the KMS result under Services:\n\nalt\n\n2\\. Select Customer managed* keys* in the left pane of the KMS console.\n\nWarning: Cloud Academy cleans up the lab environment for you after a lab is completed or terminated. As a precaution, AWS prevents keys from being deleted immediately. Rather, they are queued for deletion, and an expiration period is set (of 7-30 days). For this reason, you may see residual keys from other students within the last week. For this reason, you may need to append a unique number to the Alias field in the next instruction.\n\n3\\. Click Create Key, then expand *Advanced Options *and set the following values:\n\nKey type*: *Symmetric **(Symmetric keys are suitable for most data encryption applications. The same key is used for both encrypt and decrypt operations with symmetric key algorithms.)\nKey usage: **Encrypt and decrypt\nAdvanced options**:\n    Key Material Origin:  Leave as KMS (default). AWS will generate the key material for encryption. Note that another common use case is for customers to generate their own keys, and have AWS keep a back up encrypted copy and help manage them with KMS.\n    Regionality:* Single-Region key*\n\nalt\n\n4\\. Click Next to advance to the Add Labels page of the wizard.\n\n5\\. Set the following values before clicking Next (leave the default values for other fields)\n\nAlias**: calabs-CMK-key _(Append a unique number to the key's Alias if needed to be unique. For example, _calabs-CMK-key2.)\nDescription**: \n\n6\\. Click Next to advance to Define Key Administrative Permissions and leave the default values.\n\nAdministrative permissions allow users and roles to administer CMKs but not to perform cryptographic operations. In production environments, this is sometimes used to easily grant limited access to other users. The *Allow key administrators to delete this key *checkbox makes it explicit if deleting keys is allowed, since the key can't be recovered once deleted, making recovery of encrypted data impossible. Note that key deletion is not immediate and first enters into a pending state before the key is deleted. The delete operation can be canceled while in the pending state.\n\nThese settings generate a key policy. The default policy allows IAM policies to grant access the key, which is why you don't require selecting your student user as an administrator. The lab IAM policy of your student user allows you to perform the required actions of the lab.\n\n7\\. Click Next to advance to Define Key Usage Permissions.\n\nUsage permissions grant access to perform cryptographic operations such as encrypting and decrypting. Enterprises usually have different permissions for administrators and users, hence the wizard walks you through defining both.\n\nNotice that you can grant access to the key so other AWS accounts can use it for encryption/decryption. \n\n8\\. Click Next **to preview the **key policy and then click Finish when ready.\nThe CMK is created.\n\n9\\. Confirm the key created correctly and that the Status is Enabled:\n\nalt\n\nEncrypting S3 Data using Server-Side Encryption with KMS Managed Keys (SSE-KMS)\n\nYou will upload a file and encrypt it using SSE-KMS in this lab step.\n\n1\\. In the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n\nalt\n\n2\\. Click the name of the bucket the Cloud Academy lab environment created for you (name begins with cloudacademylabs-ssekms):\n\nalt\n\n3\\. Click Upload.\n\n4\\. Click Add files and select a small file, or download this sample file and select it.\n\n5\\. Expand the Properties tab and scroll until the Server-side encryption settings.\n\n6\\. Check the Specify an encryption key checkbox. \n\n7\\. Check the AWS Key Management Service key (SSE-KMS) checkbox and then the Choose from your AWS KMS keys checkbox:\n\nalt\n\n8\\. Choose the AWS KMS key you previously generated:\n\nalt\n\n9\\. Click on Upload.\n\n10\\. Click Close and then click the name of the object to open its properties panel: \n\nalt\n\nYou can verify the object is encrypted using SSE-KMS by checking that the Encryption field is AWS-KMS.\n\nEnforcing S3 Encryption Using Bucket Policies\n\n1\\. In the S3 bucket console, click the Permissions tab followed by Bucket Policy to open the Bucket policy editor:\n\nBucket policies are IAM policies applied to a bucket rather than to a user or role as is conventionally done with IAM policies. Similar to how a key policy applied to the CMK. These are examples of resource-based policies in AWS.\n\n2\\. Paste the following bucket policy into the policy editor:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Id\": \"RequireSSEKMS\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyUploadIfNotSSEKMSEncrypted\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::/*\",\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"s3:x-amz-server-side-encryption\": \"aws:kms\"\n                }\n            }\n        }\n    ]\n}\n\nThis policy denies (\"Effect\": \"Deny\") all users' (\"Principal\": \"\") uploads (\"Action\": \"s3:PutObject\") to the bucket (\"Resource\": \"arn:aws:s3:::/\") if the s3:x-amz-server-side-encryption is not set to aws:kms, which corresponds to SSE-KMS. The lab provides you with the policy but you could recreate it using the policy generator linked to beneath the policy editor.\n\n3\\. Replace &lt;Your\\Bucket\\_Name&gt; with the name of your lab bucket (it begins with _cloudacademylabs-ssekms- and can be copied from the S3 console):\n\nalt\n\n4\\. Click Save changes to save the policy and have it start being enforced.\n\n5\\. Click the Objects **tab followed by **Upload.\n\nClick Add files and select a small file, or download this sample file and select it.\n\n7\\. Click Upload and observe the image does not appear in the bucket contents table.\n\nClicking upload without configuring any properties of the object uses the default of no encryption.\n\nalt\n\nYou can see the upload Failed. \n\n8\\. Retry the upload but this time use the Set properties step to configure Encryption **to **AWS KMS master-key using your CMK.\n\nThe upload now succeeds since the bucket policy condition is satisfied:\n\nalt\n\nThe policy does not require the use of your CMK however, so the default S3 KMS key in the region is also allowed. You can change the policy condition to enforce a specific CMK is used.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/lambda/",
            "title": "Lambda",
            "description": "A step-by-step guide to AWS Lambda",
            "content": "\nAbout\n\nAWS Lambda\nAWS Lambda User Guide\n\nAWS Lambda is a serverless computing service that runs program code in response to certain events and is responsible for automatically allocating the necessary computing resources.\n\nAWS Lambda automatically runs program code in response to various events, such as HTTP requests through Amazon API Gateway, changing objects in Amazon Simple Storage Service garbage cans (Amazon S3), updating tables in Amazon DynamoDB or changing states in AWS Step Functions.\n\nSupports for Java, Go, PowerShell, Node.js, C#, Python and Ruby. It also provides a Runtime API which allows you to use any additional programming languages to author your functions. A runtime is a program that runs a Lambda function's handler method when the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file named bootstrap\n\n\n\nWhen you publish a version, AWS Lambda makes a snapshot copy of the Lambda function code (and configuration) in the $LATEST version. A published version is immutable.\n\nLambda execution role is an IAM role that grants the function permission to access AWS services and resources. Under Attach permissions policies, choose the AWS managed policies AWSLambdaBasicExecutionRole and AWSXRayDaemonWriteAccess.\n\nAWS managed policies for Lambda features\n\nDigest\n\nTypes of lambda invocation\n  RequestResponse.\n  Event.\n  Dryrun.\nLambda execution context is a temporary runtime environment that initializes any external dependencies of our Lambda function code, such as database connections or HTTP endpoints\nLambda Environment variables are variables that enable you to dynamically pass settings to your function code and libraries, without making changes to your code. Environment variables are key-value pairs that you create and modify as part of your function configuration.\nLambda concurrent executions = (invocations per second) x (average execution duration in seconds). Concurrency limit of lambda execution, Default 1000 Reserved - 900 unreserved 100. Will get throttled if it exceeds concurrency limit\nAWS_PROXY** in API gateway is primarily used for Lambda proxy integration.\nA Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. Lambda authorizer can be used for custom authorization scheme. 2 types:\n  Token based.\n  Request parameter based Lambda authorizer.\nLambda deployment configuration:\n  HalfAtATime\n  Canary\n  Linear.\nAWS Lambda compute platform deployments cannot use an in-place deployment type\nIncreasing memory in lambda will increase CPU in lambda\nLambda Versioning:\n  By default, each AWS Lambda function has a single current version of the code. Clients of Lambda function can call a specific version or at the latest implementation\nLambda Alias: You can create one or more aliases for our AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. Users can access the function version using the alias ARN\nLambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, which improves performance and reduces latency\nLambda Layer - Layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment package\nAmazon DynamoDB** is integrated with AWS Lambda so that you can trigger pieces of code that automatically respond to events in DynamoDB Streams. AWSLambdaDynamoDBExecutionRole is required to enable Lambda to work with DynamoDB\nAPI Gateway** - Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of a REST API.\nIntegrating Cloud Watch Events with lambda can be used for scheduling events\nIf there is an incompatible output returned from a Lambda proxy integration backend, it will return 502\nTo resolve lambda throttled exception when using Cognito events, perform retry on sync.\nLambda Event hook running order:\n  start -> BeforeAllowTraffic -> AllowTraffic -> After AllowTraffic -> End\nAWS Lambda runs function code securely within a VPC b default. To enable your Lambda function to access resources inside your private VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS\nLambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within your private VPC\nLambda Asynchronous invocation can be triggered by Amazon Simple Storage Service, Amazon Simple Notification Service, Amazon Simple Email Service, AWS CloudFormation, Amazon CloudWatch Logs, Amazon CloudWatch Events, AWS CodeCommit, AWS Config.\nLambda Limits: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html\nLambda provides 500 MB of additional disk space as a workspace.\nLambda logs all stout for a lambda function to CloudWatch Logs. Any additional logging calls used in the function will also be sent to CloudWatch Logs.\nTo connect to a VPC, lambda function execution role must have the following permissions: ec2:Create Networkinterface, ec2:DescribeNetworkinterfaces, ec2:Delete Networkinterface. These permissions are included in the AWSLambdaVPCAccessExecutionRole managed policy\nWhen lambda execution is hit by concurrency limit, you need to request AWS to increase concurrency limit\nFor stream-based services like Dynamo b streams, that don't invoke Lambda functions directly, the event source mapping configuration should be made on the Lambda side.\nA deployment package is a ZIP archive that contains your function code and dependencies.\nYou can unload the package directly to lambda. Or you can use an Amazon S3 bucket and then upload it to lambda. If the deployment package is larger than 50 MB. you must use Amazon 53\nLambda can incur a first run penalty also called cold starts. Cold starts can cause slower than expected behavior on infrequently run functions or functions with high concurrency demands\n\nPrice\n\nPrice\n\nPrice x86\n0.000016667 USD per gigabyte-second\n0,20 USD per 1 million requests\n\nArm price\n0,0000133334 USD for each gigabyte-second\n0,20 USD for 1 million queries\n\nPractice\n\n{{}}\n\nIn the AWS Management Console search bar, type Lambda and select Lambda under \"Services\":\n\n\n\nhttps://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#\n\nOn page Functions click Create a function\n\n\n\nAuthor from scratch is selected and enter the following values in the bottom form:\n\nFunction name**: *MyCustomFunc\nRuntime**: Node.js 16.X\n\nI select this section because I use the cloudacademy account. This role gives you permission to create functions\n\nPermissions: **Change default execution role.\n    Execution Role: Select Use an existing role.\n    Existing role: Select the role beginning with cloudacademylabs-LambdaExecutionRole\n\nLambda\n\nCreate function\n\nI'm writing a function to view the log, I'll add a print to the terminal. And I'll also add processing of the message I receive (In the next step in the testing section)\n\nThe function takes as an object event which contains an array of Records. On the 1st (0) position the object Sns (name of the service SNS Notifications).\n\nIn the object itself there will be 2 values:\n\ncook_secs - cooking time (microwave)\nreq_secs - cooking time (prepare)\n\nconsole.log('Loading function');\nexports.handler = function(event, context) {\n  console.log(JSON.stringify(event, null, 2));\n  const message = JSON.parse(event.Records[0].Sns.Message);\n  if (message.cook_secs\nExplanation\n\n\nAWS Secrets Manager\n\nC\n\nQ2\n\nA developer is building a streamlined development process for Lambda functions related to S3 storage.The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\n\nWhich of the following AWS solutions offers is best suited for this objective?\n\nAWS Software Development Kits (SDKs)\nAWS Serverless Application Model (SAM) templates\nAWS Systems Manager\nAWS Step Functions\n\n\nExplanation\n\n\nServerless Application Model\n\n2\n\nQ3\n\nA developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\n\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\n\nUse Amazon Cognito to provide the sign-up and sign-in functionality\nUse AWS IAM to provide the sign-up and sign-in functionality\nConfigure an AWS Config rule to make the API call triggered by the post-authentication event\nInvoke an Amazon API Gateway method to make the API call triggered by the post-authentication event\nExecute an AWS Lambda function to make the API call triggered by the post-authentication event\n\n\nExplanation\n\n\nAmazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n\n1, 5\n\nQ4\n\nA developer is designing a web application that allows the users to post comments and receive a real-time feedback.\n\nWhich architectures meet these requirements? (Select TWO.)\n\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store.\nCreate a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store\nCreate an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets.\nCreate a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store.\nEnable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store\n\n\nExplanation\n\n\nAWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\n\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\n\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n\n1, 2\n\nQ5\n\nA food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\n\nWhich approach best meets these requirements?\n\nUse EventBridge with Kinesis Data Streams to send messages. \nUse a Step Function to send SQS messages.\nUse a Lambda function to send SNS messages.\nUse AWS Batch and SNS to send messages.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html\n\n3\n\nResources\n\nCommunity posts\n",
            "tags": [
                "aws",
                "lambda"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/opensearch-service/_index",
            "title": "OpenSearch Service",
            "description": "Amazon",
            "content": "\nAbout\n\nOpenSearch Service is a fully managed service that makes it easy to deploy, secure\n\nAmazon OpenSearch Service is an open source, distributed search and analytics suite based on Elasticsearch.\n\nDocumentation\nUser Guide\n\nOpenSearch Service Flow\n\nYou can load streaming data from the following sources using AWS Lambda event handlers:\nAmazon S3\nAmazon Kinesis Data Streams and Data Firehose\nAmazon DynamoDB\nAmazon CloudWatch\nAWS IoT\n\nExposes three Elasticsearch logs through CloudWatch Logs:\nerror logs.\nsearch slow logs – These logs help fine tune the performance of any kind of search operation on Elasticsearch.\nindex slow logs – These logs provide insights into the indexing process and can be used to fine-tune the index setup.\nIndexing\n Before you can search data, you must index it. Indexing is the method by which search engines organize data for fast retrieval.\n the basic unit of data is a JSON document.\nWithin an index, Elasticsearch organizes documents into types (arbitrary data categories that you define) and identifies them using a unique ID.\n\nPrice\n\nCurrent price\n\nHas free tier.\nYou pay for each hour of use of an EC2 instance and for the cumulative size of any EBS storage volumes attached to your instances.\nYou can use Reserved Instances to reduce long term cost on your EC2 instances.\n\nUse Cases\n\nType: Analytics\n\nSame type services: Athena, EMR, Redshift, Kinesis, Elasti­csearch Service, Quicksight\n\nLog Analytics\nReal-Time Application Monitoring\nSecurity Analytics\nFull Text Search\nClickstream Analytics\n\nPractice\n\nBuild A Log Aggregation System in AWS\n",
            "tags": [
                "aws",
                "opensearch"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/opensearch-service/build-log-aggregation-system/",
            "title": "Build A Log Aggregation System in AWS",
            "description": "Will create a distributed, scalable log aggregation system within AWS running on Amazon OpenSearch Service. This Log Aggregation System will ingest as much of your CloudWatch log stream events as you want, events generated from AWS EC2 Instances, Lambda functions, Databases, and anything else you want to submit log events from.",
            "content": "\nLab\n\nMonitor Like a DevOps Pro: Build A Log Aggregation System in AWS\n\nNavigating to Your Cloud's Lambda Function\n\nStack Present State\n\n1\\. In the AWS Management Console search bar, enter Cloud Formation, and click the CloudFormation result under Services:\n\nalt\n\nThis will bring you to the CloudFormation Stacks table.\n\nThere will be one stack named cloudacademylabs in the table with a Status of CREATE_COMPLETE.\n\nNote: If the stack hasn't reached the Status **of **CREATE_COMPLETE, try refreshing the page after a minute. It only takes a minute for the stack to fully create.\n\n2\\. To view details of the stack, under Stack name, click the cloudacademylabs link.\n\n3\\. Click the Resources tab:\n\nalt\n\nYour Physical IDs will be different than in the supplied image. Note in the Type **column that a **DynamoDB Table, a Lambda Function, and *IAM *resources to grant the Lambda access to the DynamoDB Table have all been created. You will be querying the DynamoDB table via Lambda function invocations to create CloudWatch Logs, that will be aggregated and searchable via a user interface (UI).\n\n4\\. Click on the Outputs tab, and open the DynamoLambdaConsoleLink link in the Value column:\n\nalt\n\nThis takes you to the Lambda function Console.\n\nCreating Some Logs Using AWS Lambda\n\n1\\. Briefly look around the Lambda function console:\n\nalt\n\nThe Designer **gives a visual representation of the AWS resources that trigger the function (there are none in this case), and the AWS resources the function has access to (CloudWatch Logs, and **DynamoDB). The actual code that is executed by the function is farther down in the Function *code *section. You don't need to worry about the actual implementation details of the function for this Lab.\n\n2\\. To configure a test event to trigger the function, scroll down to the Code source section and click Test:\n\nalt\n\nIn the Configure test event form, enter the following values before scrolling to the bottom and click Save:\n\nEvent name**:TestPutEvent\nEnter the following in the code editor at the bottom of the form:\n\nCopy code\n\n{\n  \"fn\": \"PUT\",\n  \"data\": {\n    \"id\": \"12345\",\n    \"name\": \"foobar\"\n  }\n}\n\n\nalt\n\nThe PUT object event will update the DynamoDB database with an object with the given id.\n\n4\\. To run your function with your test event, click Test again:\n\nalt\n\nAfter a few seconds, in the code editor, a tab called Execution results will load:\n\nalt\n\nThe function succeeded and the Function Logsarea displays the logs that were generated and automatically sent to CloudWatch Logs by AWS Lambda.\n\n5\\. To view the Amazon CloudWatch logs, click the Monitor tab, and then click View logs in CloudWatch:\n\nalt\n\nNote: The Lab's CloudFormation stack outputs also include a link to the Log Group if you need to access it at a later time.\n\nManually Viewing Logs in Amazon CloudWatch\n\n1\\. Observe the Log Streams in the CloudWatch log group for the Lambda function you invoked:\n\nalt\n\nThe rows in the table are different Log Streams for the log group.\n\nEach log stream corresponds to log events from the same source. AWS Lambda creates a new log event for every Lambda invocation. However, it is possible to have multiple log streams for a single Lambda function since the log stream corresponds to the container actually running the function.\n\nBehind the scenes, a Lambda is run in a container. After a period of inactivity, the container is unloaded and the following requests will be served by a new container, thus creating a new log stream. Depending on how many times you invoked the test command in the previous step, you will see one or more rows in the log stream.\n\n2\\. Click on the latest Log Stream.\n\nThe log stream is a record of event Messages ordered in Time:\n\nalt\n\nEnter _PUT _into the *Filter events *search bar and click enter:\n\nalt\n\n4\\. Click the triangle to expand the event that matches the filter.\n\nYou will see the JSON formatted message:\n\nalt\n\nThe outermost data attribute wraps the test event you configured.\n\n5\\. Click custom to display the custom time range filter available in CloudWatch Logs:\n\nalt\n\nObserve the time-based options in the dialog box that displays:\n\nalt\n\nThe filter by text and by time capabilities are the tools that are available for sifting through logs in CloudWatch Logs. The text filters support some forms of conditions that can be expressed through a syntax specific to CloudWatch. These capabilities are handy, but you will see that there are more powerful tools available for log aggregation and retrieval.\n\nLaunching the OpenSearch Domain\n\nThe first thing you need is an Amazon OpenSearch cluster/domain. Using the Amazon OpenSearch Service has the following benefits:\n\nIt's distributed and resilient\nIt supports aggregations\nIt supports free-text search\nIt's managed and takes care of most of the operational complexities of operating a cluster\n\nIn 2021 AWS renamed Amazon ElasticSearch Service to Amazon OpenSearch Service. You may see references to ElasticSearch in the Amazon Management Console. You should assume that ElasticSearch and OpenSearch refer to the same AWS service.\n\nThe following diagram illustrates the overall design of the AWS Lab environment and the part that you are building in this lab step is highlighted in the lower-left corner in the AWS cloud:\n\nalt\n\n1\\. In the search bar at the top, enter OpenSearch, and under Services, click the Amazon OpenSearch Service result:\n\nalt\n\n2\\. To begin creating your cluster, on the right-hand side of the welcome page, click Create domain:\n\nalt\n\nThe terms OpenSearch domain and an OpenSearch cluster can be used almost interchangeably. The former is the logical search resource, and the latter is the actual servers that are launched to create a domain.\n\nThe Create domain form will load. \n\n3\\. In the Name section, in the Domain name textbox, enter ca-labs-domain-###, replacing ### with some random numbers:\n\nalt\n\n4\\. In the Deployment type section, select the following:\n\nDeployment type: Select **Development and testing\nVersion: Select **6.8 **under **ElasticSearch\n\nalt\n\nIn this short-lived lab, you are using a Development and testing deployment because it allows public access and reliability isn't a concern. In a production environment, you will want to use a Production deployment to get the full availability benefits and meet security requirements.\n\n5\\. In the Auto-Tune section, select Disable.\n\nIn this short-lived lab, Auto-Tune is not necessary.\n\n6\\. In the Data nodes section, enter and select the following and leave remaining defaults:\n\nInstance type: Select **t3.small.search\nNumber of nodes**: Enter 1\n\nalt\n\nThe storage type values correspond to the storage types available for Amazon EC2 instances.\n\nWhen deploying a cluster that uses multiple nodes, you can specify that the nodes are deployed in two or three availability zones. Deploying in multiple availability zones makes the cluster highly available and more reliable in the case of failures of outages.\n\n7\\. Scroll down to the Network section, and select Public access:\n\nalt\n\nIn this lab, you are creating a publicly available Amazon OpenSearch Service cluster for convenience. Be aware that you can also deploy a cluster into an Amazon Virtual Private Cloud (VPC) and receive the network isolation and security advantages of using a VPC.\n\nIn the *Fine-grained access control *section, uncheck the *Enable fine-grained access control *box.\n\n9\\. In a new browser tab, enter the following URL:\n\nhttps://checkip.amazonaws.com/\n\nYou will see an IP address displayed. This is the public IPv4 address of your internet connection. You will use this IP address to restrict access to your Amazon OpenSearch Service cluster.\n\n10\\. Scroll down to the Access Policy section and under Domain access policy, select Configure domain level access policy:\n\nalt\n\nYou will see a policy editor form display with the tabs Visual editor and JSON.\n\n11\\. In the Visual editor tab, enter and select the following:\n\nType: Select **IPv4 address\nPrincipal**: Enter the IP address you saw on the Check IP Page\nAction: Select **Allow\n\nalt\n\nYou are specifying an access policy that allows access to the cluster from your IP address. In a non-lab environment, you could deploy the cluster into an Amazon VPC and configure private or public access using a VPC's networking features.\n\n12\\. To finish creating your cluster, scroll to the bottom and click Create:\n\nalt\n\nA page displaying details of your cluster will load and you will see a green notification that you have successfully created a cluster.\n\n13\\. In the General information section, observe the Domain status:\n\nalt\n\nAWS is setting up and deploying your cluster. This process can take up to 15 or 30 minutes to complete.\n\n12\\. To see the latest status of your Amazon OpenSearch Service cluster, refresh the page in your browser.\n\nRefresh the page for your domain periodically to check if it has finished deploying.\n\nWhilst waiting for the domain to finish provisioning, feel free to consult the Amazon OpenSearch Service Developer Guide to learn more about the OpenSearch service.\n\nWhen the cluster has been provisioned you will see the Domain status change to Active:\n\nalt\n\nSending CloudWatch Logs to OpenSearch\n\nalt\n\n1\\. In the AWS Management Console search bar, enter CloudWatch, and click the CloudWatch result under Services:\n\nalt\n\n2\\. In the left-hand menu, under Logs, click on Log groups:\n\nalt\n\n3\\. Select the log group beginning with /aws/lambda/cloudacademylabs-DynamoLambda-:\n\nalt\n\nNext, you will create a subscription filter to send the log data to your ElasticSearch domain.\n\n4\\. Click Actions, in the menu that opens, under Subscription filters, click Create Amazon OpenSearch Service subscription filter:\n\nalt\n\nThe Create Amazon OpenSearch Service subscription filter form will load.\n\n5\\. In the Choose destination section, select the following:\n\nSelect account*: Ensure *This account **is selected\nAmazon OpenSearch Service cluster**: Select the cluster you created previously\n\nalt\n\nAfter selecting the Amazon OpenSearchService cluster, the Lambda function section will appear.\n\n6\\. In the Lambda IAM Execution Role drop-down select LambdaElasticSearch:\n\nalt\n\n7\\. In the Configure log format and filters section enter the following:\n\nLog Format: Select **Amazon Lambda\nSubscription filter name**: ca-lab-filter\n\nalt\n\nThe default Subscription Filter Pattern matches the timestamp, request_id, and event JSON. The *Test Pattern *button is available to see which events match the pattern.\n\n8\\. To start sending the logs to ElasticSearch, at the bottom, click Start streaming:\n\nalt\n\nMomentarily, you will see a notification that the subscription filter has been created and logs are being streamed to OpenSearch:\n\nalt\n\nDiscovering and Searching Events\n\n1\\. Navigate back to the Lambda function you invoked earlier and click the Test button a few times to submit more PUT events:\n\nalt\n\n2\\. Click the arrow on the Test button and click Configure test event:\n\nalt\n\n3\\. In the Configure test events **form, click the radio button for **Create new test event and enter the following non-default values:\n\nEvent name**:TestGetEvent\nEnter the following in the code editor at the bottom of the form:\n\n\n{\n\"fn\": \"GET\",\n\"id\": \"12345\"\n}\n\nYou will submit more test events of a different type - GET operations on the object that was PUT in the database. This gives two different event types to look at in Kibana (the Log Aggregation UI).\n\n4\\. Click Save.\n\n5\\. Click Test several times to make GET events.\n\n6\\. Return to the Amazon OpenSearch Search Console for the domain you created and click the link under Kibana URL:\n\nalt\n\n7\\. In the Add Data to Kibana section, on the right-hand side under Use Elasticsearch data, click Connect to your Elasticsearch index:\n\nalt\n\nThe log data is stored in OpenSearch, but you need to tell Kibana which index to use for discovering the data.\n\n8\\. In the Create an index pattern **wizard, enter the following value and click **Next step:\n\nIndex pattern*: cwl-\n\nalt\n\nThe pattern will match the daily CloudWatch Logs (cwl) indices that are created in Amazon OpenSearch.\n\n9\\. In the second step, enter the following value and click Create index pattern:\n\nIndex pattern: Select **@timestamp\n\nalt\n\nThe Time filter field name allows Kibana to determine which attribute represents the timestamp of each event. The confirmation page displays all of the fields in the log data index:\n\nalt\n\nNow that the index settings for Kibana are configured, you can begin using the Log Aggregation system!\n\n10\\. Click Discover in the sidebar menu on the left of the page.\n\n11\\. Explore the Discover interface:\n\nalt\n\nYou see some events and a graph. These are your aggregated log events! The system is online! Notice the search bar up top. It is initially empty so all log events will show up. But what if you only want to see the PUT events for objects containing 12345?\n\nEnter PUT 12345 in the search bar and press enter:\n\nalt\n\nThe matching terms in the event show up highlighted, and the bar graph updates to show only the count of PUT 12345 events that you made by clicking Test in the Lambda interface.\n\n13\\. Click on the timestamp range in the upper-right corner to display the time filter:\n\nalt\n\nJust as with CloudWatch Logs, you can filter the logs by time. However, in Kibana you can also drag on the bar chart to select a time range visually:alt\n\nVisualizing Aggregated Events\n\n1\\. Click Visualize in the Kibana sidebar menu.\n\n2\\. Click Create a visualization:\n\n \n\n\n3\\. Select *Area *chart visualization:\n\n \n\n4\\. In the *From a New Search, Select Index *area, click on the *cwl- **index name:\n\n\n\n\nIf you had any saved searches in the system, you could use them to make this Visualization from this step.\n\nOn the left-hand side, the visualization configuration tools will appear:\n\n\n\nEnter the following values in the visualization configuration:\n\nSelect buckets type**: X-Axis\nAggregation**: Date Histogram (to track log trends over time)\nField**: @timestamp\nInterval**: Auto\n\n\n\nTo make the graph more interesting, you will split the PUTs and GETs and display each stacked in on the chart with different colors. This requires a sub-buckets.\n\nClick Add sub-buckets below the rest of the X-Axis settings, and enter the following values:\n\nSelect buckets type**: Split Series\nSub Aggregation**: Terms (Terms splits the data based on the unique values of a field)\nField**: $event.data.fn.keyword (The test requests used the fn key for request type, which maps to the $event.data.fn.keyword field in OpenSearch)\n\n\n\n7\\. Click the play button to apply the changes and produce the visualization:\n\n\n\nIt will look something like the image below, with two regions in an area graph corresponding to GET and PUT event count over time:\n\n\n\nTo use the visualization in a Dashboard in the next step, you need to save the visualization.\n\nClick Save in the top toolbar:\n\n\n\n9\\. Enter PUTs and GETs Over Time in the Save Visualization field, and click Save:\n\nCreating a Kibana Dashboard\n\n1\\. Click on Dashboard in the sidebar menu.\n\n2\\. Click Create a dashboard:\n\n\n\n\n3\\. Click Add to add saved visualizations to the dashboard:\n\n\n\n4\\. Select the PUT and GETs Over Time visualization:\n\n\n\nThe visualization is added to the dashboard, but the size may not be what you like. You can adjust the size of the visualization by dragging the arrow in the lower-right corner:\n\n\n\n5\\. Click Save and enter the following values before clicking the revealed *Save *button:\n\nTitle**: Log Dashboard\nDescription**: Lambda API Logs\n\n\n\nYou've done it! The Dashboard will always contain the up-to-date statistics for your GET and PUT events that run through the Lambda function:\n\n\n6\\. Return to the Lambda console and create as many test events as you want.\n\nRefresh the Kibana dashboard and see the new requests in the visualization:\n\n \n\nYou can also configure Auto-refresh to avoid having to refresh the view:\n\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/questions",
            "title": "Questions",
            "description": "AWS exam questions Certified Developer 2023",
            "content": "\nOn this page you can find 50 random questions.\n\nTo get prepared for exam you can use cloud-exam-prepare.com\n\nQ1 - Q10\nQ1\n\nYou are developing an API in Amazon API Gateway that several mobile applications will use to interface with a back end service in AWS being written by another developer. You can use a(n)__ integration for your API methods to develop and test your client applications before the other developer has completed work on the back end.\n\nHTTP proxy\nmock\nAWS service proxy\nLambda function\n\n\nExplanation\n\n\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-method-settings-console.html\n\nAmazon API Gateway supports mock integrations for API methods.\n\n2\n\nQ2\n\nYou are creating multiple resources using multiple CloudFormation templates. One of the resources (Resource B) needs the ARN value of another resource (resource A) before it is created.\n\nWhat steps can you take in this situation? (Choose 2 answers)\n\nUse a template to first create Resource A with the ARN as an output value.\nUse a template to create Resource B and reference the ARN of Resource A using Fn::GetAtt.\nHard code the ARN value output from creating Resource A into the second template.\nJust create Resource B. \n\n\nExplanation\n\n\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n\n2\n\nQ3\n\nA company with global users is using a content delivery network service to ensure low latency for all customers. The company has several applications that require similar cache behavior.\n\nWhich API command can a developer use to ensure cache storage consistency with minimal duplication?\n\nCreateReusableDelegationSet with Route 53\nCreateStackSet with CloudFormation\nCreateGlobalReplicationGroup with ElastiCache\nCreateCachePolicy with CloudFront\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/cloudfront/latest/APIReference/API_CreateCachePolicy.html\n\n4\n\nQ4\n\nYou are creating a few test functions to demonstrate the ease of developing serverless applications. You want to use the command line to deploy AWS Lambda functions, an Amazon API Gateway, and Amazon DynamoDB tables.\n\nWhat is the easiest way to develop these simple applications?\n\nInstall AWS SAM CLI and run “sam init \\[options\\]” with the templates’ data. \nUse AWS step function visual workflow and insert your templates in the states\nSave your template in the Serverless Application Repository and use AWS SAM\n\n\nExplanation\n\n\nAWS SAM - AWS Serverless Application Model\n\nhttps://aws.amazon.com/serverless/sam/\n\n1\n\nQ5\n\nWhat will happen if you delete an unused custom deployment configuration in AWS CodeDeploy?\n\nYou will no longer be able to associate the deleted deployment configuration with new deployments and new deployment groups.\nNothing will happen, as the custom deployment configuration was unused.\nAll deployment groups associated with the custom deployment configuration will also be deleted.\nAll deployments associated with the custom deployment configuration will be terminated.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations-delete.html\n\nCan delete only if unused.\n\n1\n\nQ6\n\nWhat happens when you delete a deployment group with the AWS CLI in AWS CodeDeploy?\n\nAll details associated with that deployment group will be moved from AWS CodeDeploy to AWS OpsWorks.\nThe instances used in the deployment group will change.\nAll details associated with that deployment group will also be deleted from AWS CodeDeploy.\nThe instances that were participating in the deployment group will run once again.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-delete.html\n\nIf you delete a deployment group, all details associated with that deployment group will also be deleted from CodeDeploy. The instances used in the deployment group will remain unchanged. This action cannot be undone.\n\n3\n\nQ7\n\nYou are configuring a Jenkins project that is installed on an Amazon EC2 instance running a Windows operating system. You want this Jenkins project to integrate with AWS CodePipeline.\n\nWhich actions should you take to make this function perform correctly? (2 answers)\n\nRestart all Amazon EC2 instances that are running a Windows operating system.\nProvide the IAM user credentials to integrate AWS CodePipeline.\nFill out the required fields for your proxy host.\nModify the PATH variable to include the directory where you installed Jenkins on all Amazon EC2 instance that are running a Windows operating system.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-four-stage-pipeline.html\n\n2, 3\n\nQ8\n\nYou are deploying Multi-Factor Authentication (MFA) on Amazon Cognito. You have set the verification message to be by SMS. However, during testing, you do not receive the MFA SMS on your device.\n\nWhat action will best solve this issue?\n\nUse AWS Lambda to send the time-based one-time password by SMS\nIncrease the complexity of the password\nCreate and assign a role with a policy that enables Cognito to send SMS messages to users\nCreate and assign a role with a policy that enables Cognito to send Email messages to users\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n\n3\n\nQ9\n\nA developer is adding sign-up and sign-in functionality to an application. The application is required to make an API call to a custom analytics solution to log user sign-in events\n\nWhich combination of actions should the developer take to satisfy these requirements? (Select TWO.)\n\nUse Amazon Cognito to provide the sign-up and sign-in functionality\nUse AWS IAM to provide the sign-up and sign-in functionality\nConfigure an AWS Config rule to make the API call triggered by the post-authentication event\nInvoke an Amazon API Gateway method to make the API call triggered by the post-authentication event\nExecute an AWS Lambda function to make the API call triggered by the post-authentication event\n\n\nExplanation\n\n\nAmazon Cognito adds user sign-up, sign-in, and access control to web and mobile applications quickly and easily. Users can also create an AWS Lambda function to make an API call to a custom analytics solution and then trigger that function with an Amazon Cognito post authentication trigger.\n\n1, 5\n\nQ10\n\nA developer is designing a web application that allows the users to post comments and receive in a real-time feedback.\n\nWhich architectures meet these requirements? (Select TWO.)\n\nCreate an AWS AppSync schema and corresponding APIs. Use an Amazon DynamoDB table as the data store.\nCreate a WebSocket API in Amazon API Gateway. Use an AWS Lambda function as the backend and an Amazon DynamoDB table as the data store\nCreate an AWS Elastic Beanstalk application backed by an Amazon RDS database. Configure the application to allow long-lived TCP/IP sockets.\nCreate a GraphQL endpoint in Amazon API Gateway. Use an Amazon DynamoDB table as the data store.\nEnable WebSocket on Amazon CloudFront. Use an AWS Lambda function as the origin and an Amazon Aurora DB cluster as the data store\n\n\nExplanation\n\n\nAWS AppSync simplifies application development by letting users create a flexible API to securely access, manipulate, and combine data from one or more data sources. AWS AppSync is a managed service that uses GraphQL to make it easy for applications to get the exact data they need.\n\nAWS AppSync allows users to build scalable applications, including those requiring real-time updates, on a range of data sources, including Amazon DynamoDB. In Amazon API Gateway, users can create a WebSocket API as a stateful frontend for an AWS service (such as AWS Lambda or DynamoDB) or for an HTTP endpoint.\n\nThe WebSocket API invokes the backend based on the content of the messages it receives from client applications. Unlike a REST API, which receives and responds to requests, a WebSocket API supports two-way communication between client applications and the backend.\n\n1, 2\n\nQ11 - Q20\n\n1\n\nYou are asked to establish a baseline for normal Amazon ECS performance in your environment by measuring performance at various times and under different load conditions. To establish a baseline, Amazon recommends that you should at a minimum monitor the CPU and _ for your Amazon ECS clusters and the CPU and _ metrics for your Amazon ECS services.\n\nmemory reservation and utilization; concurrent connections\nmemory utilization; memory reservation and utilization\nconcurrent connections; memory reservation and utilization\nmemory reservation and utilization; memory utilization\n\n\nExplanation\n\n\n4\n\n2\n\nWhat is one reason that AWS does not recommend that you configure your ElastiCache so that it can be accessed from outside AWS?\n\nThe metrics reported by CloudWatch are more difficult to report.\nSecurity concerns and network latency over the public internet.\nThe ElastiCache cluster becomes more prone to failures.\nThe performance of the ElastiCache cluster is no longer controllable.\n\n\n\nExplanation\n\n\nElasticache is a service designed to be used internally to your VPC. External access is discouraged due to the latency of Internet traffic and security concerns. However, if external access to ElastiCache is required for test or development purposes, it can be done through a VPN.\n\n2\n\n3\n\nYou are building a web application that will run in an AWS ElasticBeanstalk environment. You need to add and configure an Amazon ElastiCache cluster into the environment immediately after the application is deployed.\n\nWhat is the most efficient method to ensure that the cluster is deployed immediately after the EB application is deployed?\n\nUse the AWS Management Console to create and configure the cluster.\nCreate a cron job to schedule the cluster deployment using the aws cloudformation deploy command\nCreate a configuration file with the .config extension and place it into the .ebextensions folder in the application package.\nBuild an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster.\n\n\nExplanation\n\n\n[AWS Secrets Manager](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-resources.html)\n\n3\n\n4\n\nEmily is building a web application using AWS ElasticBeanstalk. The application uses static images like icons, buttons and logos. Emily is looking for a way to serve these static images in a performant way that will not disrupt user sessions.\n\nWhich of the following options would meet this requirement?\n\nUse an Amazon Elastic File System (EFS) volume to serve the static image files.\nConfigure the AWS ElasticBeanstalk proxy server to serve the static image files.\nUse an Amazon S3 bucket to serve the static image files.\nUse an Amazon Elastic Block Store (EBS) volume to serve the static image files.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-cfg-staticfiles.html\n\nAn Amazon S3 bucket would work, but the AWS ElasticBeanstalk proxy server would need to route the requests to the static files to a different place anytime they need to be shown.\n\n2\n\n5\n\nA company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers\n\nWhich AWS service should be used to refactor this architecture?\n\nAWS X-Ray\nAmazon SQS\nAWS Lambda\nAmazon API Gateway\n\n\nExplanation\n\n\n4\n\n6\n\nWhich load balancer would you use for services which use HTTP or HTTPS traffic?\n\n\nExplanation\n\nApplication Load Balancer (ALB).\n\n7\n\nWhat are possible target groups for ALB (Application Load Balancer)?\n\n\nExplanation\n\n\nEC2 tasks\nECS instances\nLambda functions\nPrivate IP Addresses\n\n8\n\nYour would like to optimize the performance of their web application by routing inbound traffic to api.mysite.com to Compute Optimized EC2 instances and inbound traffic to mobile.mysite.com to Memory Optimized EC2 instances.\n\nWhich solution below would be best to implement for this?\n\nEnable X-Forwarded For on the web servers and use a Classic Load Balancer\nConfigure proxy servers to forward the traffic to the correct instances\nUse Classic Load Balancer with path-based routing rules to forward the traffic to the correct instances\nUse Application Load Balancer with host-based routing rules to forward the traffic to the correct instances\n\n\n\nExplanation\n\n\nApplication Load Balancer with host-based routing rules\n\nhttps://aws.amazon.com/blogs/aws/new-host-based-routing-support-for-aws-application-load-balancers/\n\n4\n\n9\n\nA company uses Amazon DynamoDB for managing and tracking orders. DynamoDB table is partitioned based on the order date. The company receives a huge increase in orders during a sales event, causing DynamoDB writes to throttle, and the consumed throughput is below the provisioned throughput.\n\nAccording to AWS best practices, how can this issue be resolved with MINIMAL costs?\n\nCreate a new Dynamo DB table for every order date\nAdd a random number suffix to the partition key values\nAdd a global secondary index to the DynamoDB table\nIncrease the read and write capacity units of the DynamoDB table\n\n\nExplanation\n\n\nA randomizing strategy can greatly improve write throughput. But it’s difficult to read a specific item because you don’t know which suffix value was used when writing the item.\n\nChoosing the Right DynamoDB Partition Key\n\nUsing write sharding to distribute workloads evenly\n\n2\n\n10\n\nA food delivery company is building a feature that requests reviews from customers after their orders are delivered. The solution should be a short-running process that can message customers simultaneously at various contact points including email, text, and mobile push notifications.\n\nWhich approach best meets these requirements?\n\nUse EventBridge with Kinesis Data Streams to send messages. \nUse a Step Function to send SQS messages.\nUse Lambda function to send SNS messages.\nUse AWS Batch and SNS to send messages.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html\n\n3\n\nQ21 - Q30\n\n1\n\nHow AWS Fargate different from AWS ECS?\n\n\nExplanation\n\n\nIn AWS ECS, you manage the infrastructure - you need to provision and configure the EC2 instances.\nWhile in AWS Fargate, you don't provision or manage the infrastructure, you simply focus on launching Docker containers. You can think of it as the serverless version of AWS ECS.\n\n2\n\nWhat is Chaos Engineering?\n\n\nExplanation\n\n\nChaos engineering is the process of stressing an application in testing or production environments by creating disruptive events, such as server outages or API throttling, observing how the system responds, and implementing improvements.\n\nChaos engineering helps teams create the real-world conditions needed to uncover the hidden issues, monitoring blind spots, and performance bottlenecks that are difficult to find in distributed systems.\n\nIt starts with analyzing the steady-state behavior, building an experiment hypothesis (e.g., terminating x number of instances will lead to x% more retries), executing the experiment by injecting fault actions, monitoring roll back conditions, and addressing the weaknesses.\n\n3\n\nA client has contracted you to review their existing AWS environment and recommend and implement best practice changes. You begin by reviewing existing users and Identity Access Management. You found out improvements that can be made with the use of the root account and Identity Access Management.\n\nWhat are the best practice guidelines for use of the root account?\n\nNever use the root account.\nUse the root account only to create administrator accounts.\nUse the root account to create your first IAM user and then lock away the root account.\nUse the root account to create all other accounts, and share the root account with one backup administrator.\n\n\nExplanation\n\nlock-away-credentials\n\n1\n\n4\n\nVeronika is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list stems are sent as query string parameters in the method request.\n\nHow should Veronika convert the query string parameters to arguments for the Lambda function?\n\nEnable request validation\nInclude the Amazon Resource Name (ARN) of the Lambda function\nChange the integration type\nCreate a mapping template\n\n\nExplanation\n\n\nAPI Gateway mapping template\n\n4\n\n5\n\nYour organization has an AWS setup and planning to build Single Sign-On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users log in to the AWS console using AWS STS Enterprise Identity Federation.\n\nWhich of the following services do you need to call from AWS STS service after you authenticate with your on-premise?\n\nAssumeRoleWithSAML\nGetFederationToken\nAssumeRoleWithWebIdentity\nGetCallerIdentity\n\n\nExplanation\n\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\n\n1\n\n6\n\nAlice is building a mobile application. She planned to use Multi-Factor Authentication (MFA) when accessing some AWS resources.\n\nWhich of the following APIs will be leveraged to provide temporary security credentials?\n\nAssumeRoleWithSAML\nGetFederationToken\nGetSessionToken\nAssumeRoleWithWebIdentity\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\n\n(AssumeRoleWithWebIdentity)[https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html] - does not support MFA\n\n3\n\n7\n\nYou built a data analysis application to collect and process real-time data from smart meters. Amazon Kinesis Data Streams is the backbone of your design. You received an alert that a few shards are hot.\n\nWhat steps will you take to keep a strong performance?\n\nRemove the hot shards\nMerge the hot shards\nSplit the hot shards\nIncrease the shard capacity\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-resharding-strategies.html\n\nSplit the hot shards\n\n3\n\n8\n\nJasmin needs to perform ad-hoc business analytics queries on well-structured dat1. Data comes in constantly at a high velocity. Jasmin's team can understand SQL.\n\nWhat AWS service(s) should Jasmin look to first?\n\nEMR using Hive\nEMR running Apache Spark\nKinesis Firehose + RDS\nKinesis Firehose + RedShift\n\n\nExplanation\n\n\nRedShift supports ad-hoc queries over well-structured data using a SQL-compliant wire protocol\n\nhttps://aws.amazon.com/kinesis/data-firehose/features/\n\n4\n\n9\n\nKey rotation is an important concept of key management. How does Key Management Service (KMS) implement key rotation?\n\nKMS supports manual Key Rotation only; you can create new keys any time you want and all data will be re-encrypted with the new key.\nKMS creates new cryptographic material for your KMS keys every rotation period, and uses the new keys for any upcoming encryption; it also maintains old keys to be able to decrypt data encrypted with those keys.\nKey rotation is the process of synchronizing keys between configured regions; KMS will synchronize key changes in near-real time once keys are changed.\nKey rotation is supported through the re-importing of new KMS keys; once you import a new key all data keys will be re-encrypted with the new KMS key.\n\n\nExplanation\n\n\nWhen you enable automatic key rotation for a customer-managed KMS key, AWS KMS generates new cryptographic material for the KMS key every year. AWS KMS also saves the KMS key's older cryptographic material so it can be used to decrypt data that it has encrypted.\n\n10\n\nAlan is managing an environment with regulation and compliance requirements that mandate encryption at rest and in transit. The environment covers multiple accounts (Management, Development, and Production) and at some point in time, Alan might need to move encrypted snapshots and AMIs with encrypted volumes across accounts.\n\nWhich statements are true with regard to this scenario? (Choose 2 answers)\n\nCreate Master keys in management account and assign Development and Production accounts as users of these keys, then any media encrypted using these keys can be shared between the three accounts.\nCan share AMIs with encrypted volumes across accounts, even with the use of custom encryption keys.\nMake encryption keys for development and production accounts then anything encrypted using these keys can be moved across accounts.\nYou can not move encrypted snapshots across accounts if data migration is required some third-party tools must be used.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n\n1, 2\n\nQ31 - Q40\n\n1\n\nWhen working with a published version of the AWS Lambda function, you should note that the _.\n\nUse the AWS Management Console to create and configure the cluster.\nCreate a cron job to schedule the cluster deployment using the aws cloudformation deploy command\nCreate a configuration file with the .config extension and place it into the .ebextensions folder in the application package.\nBuild an AWS Lambda function that polls to the ElasticBeanstalk environment deployments and create and configure the Amazon ElastiCache cluster.\n\n\nExplanation\n\n\nAWS Secrets Manager\n\n3\n\n2\n\nA Developer wants access to the log data of an application running on an EC2 instance available to systems administrators.\n\nWhich of the following enables monitoring of the metric in Amazon CloudWatch?\n\nRetrieve the log data from AWS CloudTrail using the LookupEvents API Call\nRetrieve the log data from CloudWatch using the GetMetricData API call\nLaunch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application\nInstall the Amazon CloudWatch logs agent on the EC2 instance that the application is running on\n\nExplanation\n\n\n4\n\n3\n\nA developer is building a streamlined development process for Lambda functions related to S3 storage. The developer needs a consistent, reusable code blueprint that can be easily customized to manage Lambda function definition and deployment, the S3 events to be managed and the Identity Access Management (IAM) policies definition.\n\nWhich of the following AWS solutions offers is best suited for this objective?\n\nAWS Software Development Kits (SDKs)\nAWS Serverless Application Model (SAM) templates\nAWS Systems Manager\nAWS Step Functions\n\n\nExplanation\n\n\nServerless Application Model\n\n2\n\n4\n\nExplain RDS Multi Availability Zone\n\n\nExplanation\n\n\nRDS multi AZ used mainly for disaster recovery purposes\nThere is an RDS master instance and in another AZ an RDS standby instance\nThe data is synced synchronously between them\nThe user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically\n\n5\n\nDeveloper wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.\n\nWhich methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)\n\nControl access based on the time of day\nControl access based on IP Address\nControl access based on Active Directory group\nControl access based on CIDR block\n\n\nExplanation\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html\n\nCIDRs - A set of Classless Inter-Domain Routings\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html\n\n1, 2, 4\n\n6\n\nTo ensure that an encryption key was not corrupted in transit, Elastic Transcoder  uses a(n) __ digest of the decryption key as a checksum.\n\nBLAKE2\nSHA-1\nSHA-2\nMD5\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html\n\nMD5 digest (or checksum)\n\n4\n\n7\n\nDan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.\n\nWhat are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)\n\nUse the encrypted SSL/TLS endpoint.\nEnable encryption in the bucket policy.\nEncrypt it on the client-side before uploading.\nSet the server-side encryption option on upload.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n\n1, 3\n\n8\n\nIn a move toward using microservices, a company’s Management team has asked all Development teams to build their services so that API requests depend only on that services data store. One team is building a Payments service that has its own database. The service floods data that originates in the Accounts database. Both are using Amazon DynamoDB.\n\nWhat approach will result in the simplest, decoupled, and reliable method to get near-real-time updates from the Accounts database?\n\nUse Amazon Glue to perform frequent updates from the Accounts database to the Payments database\nUse Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database.\nUse Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database.\nUse Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database.\n\n\nExplanation\n\n\n3\n\n9\n\nYou’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.\n\nWhich options are supported notification methods? (Choose 3 answers)\n\nHTTP or HTTPS POST notifications\nEmail using SMTP or plain text\nKinesis Stream\nInvoking of a Lambda function\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html\n\n1, 2, 4\n\n10\n\nWhich endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?\n\nSNS\nE-Mail\nSQS\nKinesis\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html\n\n3\n\nQ41 - Q50\n\n1\n\nA developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\n\nWhich of the following Step Function workflows can be used to meet requirements?\n\nAsynchronous Express Workflow\nSynchronous Express Workflow\nStandard Workflow\nStandard Express Workflow\n\n\nExplanation\n\n\nhttps://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n\n2\n\n2\n\nYou joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\n\nWhich AWS service will be your main tool?\n\nAWS X-Ray\nAWS IAM\nAWS CloudTrail\nAWS Athena\n\n\nExplanation\n\n\nAWS X-Ray provides graphs of system performance and identifies bottlenecks\n\n1\n\n3\n\nA team of Developers must migrate an application running inside an AWS Elastic Beastalk environment from a Classic Load Balancer to an Application Load Balancer.\n\nWhich steps should be taken to accomplish the task using the AWS Management Console?\n\n1\n   Select a new load balancer type before running the deployment.\n   Update the application code in the existing deployment.\n   Deploy the new version of the application code to the environment.\n2\n   Create a new environment with the same configurations except for the load balancer type.\n   Deploy the same application versions as used in the original environment.\n   Run the Swap-environment-cnames action.\n3\n   Clone the existing environment, changing the associated load balancer type.\n   Deploy the same application version as used in the original environment.\n   Run the Swap-environment-cnames action.\n4\n   Edit the environment definitions in the existing deployment.\n   Change the associated load balancer type according to the requirements.\n   Rebuild the environment with the new load balancer type.\n\n\nExplanation\n\n\n4\n\n4\n\nA developer is deploying an application that will store files in an Amazon S3 bucket. The files must be encrypted at rest. The developer wants to automatically replicate the files to an S3 bucket in a different AWS Region for disaster recovery.\n\nHow can the developer accomplish this task with the LEAST amount of configuration?\n\nEncrypt the files by using server-side encryption with S3 managed encryption keys (SSE-S3). Enable S3 bucket replication.\nEncrypt the files by using server-side encryption (SSE) with an AWS Key Management Service (AWS KMS) customer master key (CMK). Enable S3 bucket replication.\nUse the s3 sync command to sync the files to the S3 bucket in the other Region.\nConfigure an S3 Lifecycle configuration to automatically transfer files to the S3 bucket in the other Region.\n\n\nExplanation\n\n\n2\n\n5\n\nA serverless application is using AWS Step Functions to process data and save it to a database. The application needs to validate some data with an external service before saving the dat1. The application will call the external service from an AWS Lambda function, and the external service will take a few hours to validate the dat1. The external service will respond to a webhook when the validation is complete.\n\nA developer needs to pause the Step Functions workflow and wait for the response from the external service.\n\nWhat should the developer do to meet this requirement?\n\nUse the .waitForTaskToken option in the Lambda function task state. Pass the token in the body.\nUse the .waitForTaskToken option in the Lambda function task state. Pass the invocation request.\nCall the Lambda function in synchronous mode. Wait for the external service to complete the processing.\nCall the Lambda function in asynchronous mode. Use the Wait state until the external service completes the processing.\n\n\nExplanation\n\n\n4\n\n6\n\nA company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.\n\nWhat is causing the duplicate entries?\n\nThe S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket.\nThe Lambda function did not run correctly, and Lambda retried the invocation with a delay.\nAmazon S3 is delivering the same event multiple times.\nThe application stopped intermittently and then resumed, splitting the logs into multiple smaller files.\n\n\nExplanation\n\n\n1\n\n7\n\nAn AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function.\n\nHow can the developer inspect the timing of the DynamoDB API calls?\n\nAdd DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics\nPlace an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs\nLimit Lambda to no more than five concurrent invocations. Monitor from the Lambda console.\nEnable AWS X-Ray tracing for the function. View the traces from the X-Ray service.\n\n\nExplanation\n\n\n4\n\n8\n\nA developer deployed an application to an Amazon EC2 instance. The application needs to know the public IPv4 address of the instance.\nHow can the application find this information?\n\nQuery the instance metadata from http://169.254.169.254/latest/meta-data/.\nQuery the instance user data from http://169.254.169.254/latest/user-data/.\nQuery the Amazon Machine Image (AMI) information from http://169.254 169.254/latest/meta-data/ami/.\nCheck the hosts file of the operating system.\n\n\nExplanation\n\n\n1\n\n9\n\nA developer is creating a serverless application that uses an AWS Lambda function The developer will use AWS CloudFormation to deploy the application. The application will write logs to Amazon CloudWatch Logs. The developer has created a log group in a CloudFormation template for the application to use. The developer needs to modify the CloudFormation template to make the name of the log group available to the application at runtime.\n\nWhich solution will meet this requirement?\n\nUse the AWS::Include transform in CloudFormation to provide the log group's name to the application.\nPass the log group's name to the application in the user data section of the CloudFormation template\nUse the CloudFormation template's Mappings section to specify the log group's name for the application.\nPass the log group's Amazon Resource Name (ARN) as an environment variable to the Lambda function.\n\n\nExplanation\n\n\n4\n\n10\n\nA developer needs to use the AWS CLI on an on-premises development server temporarily to access AWS services while performing maintenance. The developer needs to authenticate to AWS with their identity for several hours.\n\nWhat is the MOST secure way to call AWS CLI commands with the developer's IAM identity?\n\nSpecify the developer's IAM access key ID and secret access key as parameters for each CLI command.\nRun the AWS configure CLI command. Provide the developer's IAM access key ID and secret access key.\nSpecify the developer's IAM profile as a parameter for each CLI command.\nRun the get-session-token CLI command with the developer's IAM user. Use the returned credentials to call the CLI.\n\n\nExplanation\n\n\n4\n\nQ51 - Q60\n\n6\n\nA developer notices timeouts from the AWS CLI when the developer runs list commands.\n\nWhat should the developer do to avoid these timeouts?\n\nUse the --page-size parameter to request a smaller number of items.\nUse shorthand syntax to separate the list by a single space.\nUse the yaml-stream output for faster viewing of large datasets.\nUse quotation marks around strings to enclose data structure.\n\n\nExplanation\n\n\n1\n\n7\n\nA company is planning to use AWS CodeDeploy to deploy an application to Amazon Elastic Container Service (Amazon ECS). During the deployment of a new version of the application, the company initially must expose only 10% of live traffic to the new version of the deployed application. Then, after 15 minutes elapse, the company must route all the remaining live traffic to the new version of the deployed application.\n\nWhich CodeDeploy predefined configuration will meet these requirements?\n\nCodeDeployDefault.ECSCanary10Percent15Minutes\nCodeDeployDefault.LambdaCanary10Percent5Minutes\nCodeDeployDefault.LambdaCanary10Percent15Minutes\nCodeDeployDefault.ECSLinear10PercentEvery1 Minutes\n\n\nExplanation\n\n\n1\n\n8\n\n\nExplanation\n\n\n9\n\nA microservices application is deployed across multiple containers in Amazon Elastic Container Service (Amazon ECS). To improve performance, a developer wants to capture trace information between the microservices and visualize the microservices architecture.\n\nWhich solution will meet these requirements?\n\nBuild the container from the amazon/aws-xray-daemon base image. Use the AWS X-Ray SDK to instrument the application.\nInstall the Amazon CloudWatch agent on the container image. Use the CloudWatch SDK to publish custom metrics from each of the microservices.\nInstall the AWS X-Ray daemon on each of the ECS instances.\nConfigure AWS CloudTrail data events to capture the traffic between the microservices.\n\n\nExplanation\n\n\n3\n\n10\n\nA company is running an application on Amazon Elastic Container Service (Amazon ECS). When the company deploys a new version of the application, the company initially needs to expose 10% of live traffic to the new version. After a period of time, the company needs to immediately route all the remaining live traffic to the new version.\n\nWhich ECS deployment should the company use to meet these requirements?\n\nRolling update\nBlue/green with canary\nBlue/green with all at once\nBlue/green with linear\n\n\nExplanation\n\n\n2\n\nExplanation\n\n\n2\n\n\nExplanation\n\n\n3\n\n\nExplanation\n\n\n-->\n\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/rds/",
            "title": "RDS",
            "description": "Set up, operate, and scale a relational database in the cloud with just a few clicks.",
            "content": "\nAbout\n\nRelational Database Service\nManaged DB service that uses SQL as query language\n\nAmazon Relational Database Service (Amazon RDS) is a collection of managed services that makes it simple to set up, operate, and scale databases in the cloud.\n\nDocumentation\nUser Guide\n\n\n\nSupports engines:\n\nAmazon Aurora with MySQL compatibility: 5432\nAmazon Aurora with PostgreSQL compatibility: 5432\nMySQL: 3306\nMariaDB: 3306\nPostgreSQL: 5432\nOracle: 1521\nSQL Server: 1433\n\nEngine modes:\n\nUsed in CreateDBCluster\n\nglobal\nparallelquery\nserverless\nmultimaster\n\nBackups\n\nBackups are enabled by default in RDS\n\nAutomated backups\n\nDaily full backup (during maintenance window)\nBackups of transaction logs (every 5 minutes)\n7 days retention (can increase upto 35)\n\nDB Snapshots\n\nManually triggered by the user\nCan retain backup as long as you want\n\nAuto scaling\n\nWhen RDS detects you're running out of space, it scales automatically\n\nDigest\n\nTo verify slowly running queries enable slow query log.\nTDE (Transparent data encryption) supports encryption on Microsoft SQL server\nAWS Systems Manager Parameter Store provides secure, hierarchical storage for confiquration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as\nparameter values\nAWS Secrets Manager is an AWS service that can be used to securely store, retrieve, and automatically rotate database credentials. AWS Secrets Manager has built-in integration for RDS databases.\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Relational\n\nThis type services: Aurora, Redshift, RDS\n\nEcommerce websites, Traditional sites etc.\n\nAmazon Relational Database Service (Amazon RDS) on AWS Outposts allows you to deploy fully managed database instances in your on-premises environment\n\nQuestions\n\nQ1\n\nExplain RDS Multi Availability Zone\n\n\nExplanation\n\n\nRDS multi AZ used mainly for disaster recovery purposes\nThere is an RDS master instance and in another AZ an RDS standby instance\nThe data is synced synchronously between them\nThe user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically\n\nQ2\n\nA company is migrating a legacy application to Amazon EC2. The application uses a username and password stored in the source code to connect to a MySQL database. The database will be migrated to an Amazon RDS for MySQL DB instance. As part of the migration, the company wants to implement a secure way to store and automatically rotate the database credentials.\n\nWhich approach meets these requirements?\n\nStore the database credentials in environment variables in an Amazon Machine Image (AMI). Rotate the credentials by replacing the AMI.\nStore the database credentials in AWS Systems Manager Parameter Store. Configure Parameter Store to automatically rotate the credentials.\nStore the database credentials in environment variables on the EC2 instances. Rotate the credentials by relaunching the EC2 instances.\nStore the database credentials in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials\n\n\nExplanation\n\n\nAWS Secrets Manager\n\nSecrets Manager offers secret rotation\n\n4\n\nQ3\n\nExplain RDS Multi Availability Zone\n\n\nExplanation\n\n\nRDS multi AZ used mainly for disaster recovery purposes\nThere is an RDS master instance and in another AZ an RDS standby instance\nThe data is synced synchronously between them\nThe user, application is accessing one DNS name and where there is a failure with the master instance, the DNS name moves to the standby instance, so the failover done automatically\n\n\n\n",
            "tags": [
                "aws",
                "rds"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/route53/",
            "title": "Route 53",
            "description": "Amazon Route 53 - A reliable and cost-effective way to route end users to Internet applications",
            "content": "\nAbout\n\nAmazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-premises.\n\nDocumentation\nUser Guide\n\n\n\nA highly available and scalable Domain Name System (DNS) web service used for domain registration, DNS routing, and health checking.\n\nCan create and manage your public DNS records.\n\nWhat is the difference between Route 53 and DNS?\n\nYour DNS is the service that translates your domain name into an IP address. AWS Route 53 is a smart DNS system that can dynamically change your origin address based on load, and even perform load balancing before traffic even reaches your servers.\n\nAlternatives\n\nCloudflare DNS.\nGoogle Cloud DNS.\nAzure DNS.\nGoDaddy Premium DNS.\nDNSMadeEasy.\nClouDNS.\nUltraDNS.\nNS1.\n\nRouting Policies\n\nSimple routing policy – route internet traffic to a single resource that performs a given function for your domain. You can’t create multiple records that have the same name and type, but you can specify multiple values in the same record, such as multiple IP addresses.\nFailover routing policy – use when you want to configure active-passive failover.\nGeolocation routing policy – use when you want to route internet traffic to your resources based on the location of your users.\nGeoproximity routing policy – use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.\n  You can also optionally choose to route more traffic or less to a given resource by specifying a value, known as a bias. A bias expands or shrinks the size of the geographic region from which traffic is routed to a resource.\n  The effect of changing the bias for your resources depends on a number of factors, including the following:\n  The number of resources that you have.\n  How close the resources are to one another.\n    The number of users that you have near the border area between geographic regions.\nLatency routing policy – use when you have resources in multiple locations and you want to route traffic to the resource that provides the best latency.\nIP-based routing policy – use when you want to route traffic based on your users’ locations, and know where the IP address or traffic is coming from.\nMultivalue answer routing policy – use when you want Route 53 to respond to DNS queries with up to eight healthy records selected at random.\nWeighted routing policy – use to route traffic to multiple resources in proportions that you specify.\n\nWhen you register a domain or transfer domain registration to Route 53, it configures the domain to renew automatically. The automatic renewal period is typically one year, although the registries for some top-level domains (TLDs) have longer renewal periods.\n\nWhen you register a domain with Route 53, it creates a hosted zone that has the same name as the domain, assigns four name servers to the hosted zone, and updates the domain to use those name servers.\n\nDigest\n\nRoute 53 is AWS DNS service\nMap domain names to EC2 instances, Load Balancers and 53 buckets\nRouting Policies\n  Simple - Traffic routed to a single resource\n  Weighted - Traffic routed to a resource = weight assigned to the resource/sum of all the weights\n  Latency - serves requests from the AWS region with low latency\n  Geographical - routes the traffic based on the location of the request origin\n  Failover - routes traffic to primary when primary healthy; secondary when primary is unhealthy\n  Multivalue Answer - routs randomly to multiple healthy resources\nVPC - private network on AWS platform\n  Subnet, NAT Instance, NAT Gatewav, Internet Gatewav, NACLs, Route Table\nVPC Wizard\n  Single public subnet\n  Public and Private subnet (NAT)\n  Public and private subnet and AWS managed VPN access\n  Private subnet only and AWS managed VPN access\nVPC Peering - helps transfer of data\nVPC Flow logs - helps capture information about incoming/outgoing traffic\nDirect Connect - dedicated connection from on premises network to VPC\n\nPrice\n\nPay only for what you use.\n\nCurrent price\n\nUse Cases\n\nDomain Registration / transfer\nManage network traffic globally\nSet up private DNS\n",
            "tags": [
                "aws",
                "Route 53"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/_index",
            "title": "S3",
            "description": "Object storage built to retrieve any amount of data from anywhere",
            "content": "\nAbout\n\nAmazon S3 (Simple Storage Service) provides object storage through a web service interface.\n\nAmazon S3\nAmazon S3 User Guide\n\nAmazon S3 Flow\n\nPrice\n\nPay only for what you use. There is no minimum charge.\n\nPrice\n\nS3 | EFS | EBS\n\nCompare S3 vs EFS vs EBS\n\nAmazon S3 is an object storage designed for storing large numbers of user files and backups.\n  Good for storing backups and other static data\t\n  Can be publicly accessible\n  Web interface\n  Object Storage\n  Scalable\n  Slower than EBS and EFS\n\nAmazon EBS (Amazon Elastic Block Store) is block storage for Amazon EC2 compute instances - it is similar to hard drives attached to your computers or laptops, but in a virtualized environment.\n  Is meant to be EC2 drive\t\n  Accessible only via the given EC2 Machine\n  File System interface\n  Block Storage\n  Hardly scalable\n  Faster than S3 and EFS\n\nAmazon EFS (Amazon Elastic File System) provides scalable network file storage for Amazon EC2 cloud computing service users.\n  Good for applications and shareable workloads\n  Accessible via several EC2 machines and AWS services\n  Web and file system interface\n  Object storage\n  Scalable\n  Faster than S3, slower than EBS\n\nFeatures\n\nAmazon S3 allows people to store objects (files) in “buckets” (directories)\nBuckets must have a globally unique name\n    Naming convention:\n        No uppercase\n        No underscore\n        3-63 characters long\n        Not an IP\n        Must start with lowercase letter or number\nObjects\n    Objects (files) have a Key. The key is the FULL path:\n        /my_file.txt\n        /my_folder/another_folder/my_file.txt\n    There’s no concept of “directories” within buckets (although the UI will trick you to think otherwise)\n    Just keys with very long names that contain slashes (“/“)\n    Object Values are the content of the body:\n        Max Size is 5TB\n        If uploading more than 5GB, must use “multi-part upload”\n    Metadata (list of text key / value pairs - system or user metadata)\n    Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle\n    Version ID (if versioning\n\nVersioning\n\nIt is enabled at the bucket level\nSame key overwrite will increment the “version”: 1, 2, 3\nIt is best practice to version your buckets\n    Protect against unintended deletes (ability to restore a version)\n    Easy roll back to previous versions\nAny file that is not version prior to enabling versioning will have the version “null”\n\nEncryption for Objects\n\nThere are 4 methods of encrypt objects in S3\n    SSE-S3: encrypts S3 objects\n        Encryption using keys handled & managed by AWS S3\n        Object is encrypted server side\n        AES-256 encryption type\n        Must set header: “x-amz-server-side-encryption”:”AES256”\n    SSE-KMS: encryption using keys handled & managed by KMS\n        KMS Advantages: user control + audit trail\n        Object is encrypted server side\n        Maintain control of the rotation policy for the encryption keys\n        Must set header: “x-amz-server-side-encryption”:”aws:kms”\n    SSE-C: server-side encryption using data keys fully managed by the customer outside of AWS\n        Amazon S3 does not store the encryption key you provide\n        HTTPS must be used\n        Encryption key must be provided in HTTP headers, for every HTTP request made\n    Client Side Encryption\n        Client library such as the amazon S3 Encryption Client\n        Clients must encrypt data themselves before sending to S3\n        Clients must decrypt data themselves when retrieving from S3\n        Customer fully manages the keys and encryption cycle\n\nEncryption in transit (SSL)\n\nexposes:\n    HTTP endpoint: non encrypted\n    HTTPS endpoint: encryption in flight\nYou’re free to use the endpoint your ant, but HTTPS is recommended\nHTTPS is mandatory for SSE-C\nEncryption in flight is also called SSL / TLS\n\nSecurity\n\nBy default, all S3 objects are private\n\nA user who does not have AWS credentials or permission to access an S3 object can be granted temporary access by using a presigned URL. A presigned URL is generated by an AWS user who has access to the object. The generated URL is then given to the unauthorized user\n\nUser based\n    IAM policies - which API calls should be allowed for a specific user from IAM console\nResource based\n    Bucket policies - bucket wide rules from the S3 console - allows cross account\n    Object Access Control List (ACL) - finer grain\n    Bucket Access Control List (ACL) - less common\nNetworking\n    Support VPC endpoints (for instances in VPC without www internet)\nLogging and Audit:\n    S3 access logs can be stored in other S3 buckets\n    API calls can be logged in AWS CloudTrail\nUser Security:\nMFA (multi factor authentication) can be required in versioned buckets to delete objects\nSigned URLs: URLS that are valid only for a limited time (ex: premium video services for logged in users)\n\nBucket Policies\n\nJSON based policies\n    Resources: buckets and objects\n    Actions: Set of API to Allow or Deny\n    Effect: Allow / Deny\n    Principal: The account or user to apply the policy to\nUse S3 bucket for policy to:\n    Grant public access to the bucket\n    Force objects to be encrypted at upload\n    Grant access to another account (Cross Account)\n\nWebsites\n\nS3 can host static website sand have them accessible on the world wide web\nThe website URL will be:\n    .s3-website..amzonaws.com\n    OR\n    .s3-website..amazonaws.com\nIf you get a 403 (forbidden) error, make sure the bucket policy allows public reads!\n\nCORS\n\nIf you request data from another S3 bucket, you need to enable CORS\nCross Origin Resource Sharing allows you to limit the number of websites that can request your files in S3 (and limit your costs)\nThis is a popular exam question\n\nConsistency Model\n\nRead after write consistency for PUTS of new objects\n    As soon as an object is written, we can retrieve itex: (PUT 200 -> GET 200)\n    This is true, except if we did a GET before to see if the object existedex: (GET 404 -> PUT 200 -> GET 404) - eventually consistent\nEventual Consistency for DELETES and PUTS of existing objects\n    If we read an object after updating, we might get the older versionex: (PUT 200 -> PUT 200 -> GET 200 (might be older version))\n    If we delete an object, we might still be able to retrieve it for a short timeex: (DELETE 200 -> GET 200)\n\nPerformance\n\nFaster upload of large objects (>5GB), use multipart upload\n    Parallelizes PUTs for greater throughput\n    Maximize your network bandwidth\n    Decrease time to retry in case a part fails\nUse CloudFront to ache S3 objects around the world (improves reads)\nS3 Transfer Acceleration (uses edge locations) - just need to change the endpoint you write to, not the code\nIf using SSE-KMS encryption, you may be limited to your AWS limits for KMS usage (~100s - 1000s downloads / uploads per second)\n\nQuestions\n\nQ1\n\nDeveloper wants to implement a more fine-grained control of developers S3 buckets by restricting access to S3 buckets on a case-by-case basis using S3 bucket policies.\n\nWhich methods of access control can developer implement using S3 bucket policies? (Choose 3 answers)\n\nControl access based on the time of day\nControl access based on IP Address\nControl access based on Active Directory group\nControl access based on CIDR block\n\n\nExplanation\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html\n\nCIDRs - A set of Classless Inter-Domain Routings\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html\n\n1, 2, 4\n\nQ2\n\nTo ensure that an encryption key was not corrupted in transit, Elastic Transcoder  uses a(n) __ digest of the decryption key as a checksum.\n\nBLAKE2\nSHA-1\nSHA-2\nMD5\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/elastictranscoder/latest/developerguide/job-settings.html\n\nMD5 digest (or checksum)\n\n4\n\nQ3\n\nDan is responsible for supporting your company’s AWS infrastructure, consisting of multiple EC2 instances running in a VPC, DynamoDB, SQS, and S3. You are working on provisioning a new S3 bucket, which will ultimately contain sensitive data.\n\nWhat are two separate ways to ensure data is encrypted in-flight both into and out of S3? (Choose 2 answers)\n\nUse the encrypted SSL/TLS endpoint.\nEnable encryption in the bucket policy.\nEncrypt it on the client-side before uploading.\nSet the server-side encryption option on upload.\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html\n\n1, 3\n\nQ4\n\nA company has an application that writes files to an Amazon S3 bucket. Whenever there is a new file, an S3 notification event invokes an AWS Lambda function to process the file. The Lambda function code works as expected. However, when a developer checks the Lambda function logs, the developer finds that multiple invocations occur for every file.\n\nWhat is causing the duplicate entries?\n\nThe S3 bucket name is incorrectly specified in the application and is targeting another S3 bucket.\nThe Lambda function did not run correctly, and Lambda retried the invocation with a delay.\nAmazon S3 is delivering the same event multiple times.\nThe application stopped intermittently and then resumed, splitting the logs into multiple smaller files.\n\n\nExplanation\n\n\n1\n",
            "tags": [
                "aws",
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/create-folder-s3/",
            "title": "Create a folder inside S3 Bucket",
            "description": "Create a Folder inside an Amazon S3 Bucket",
            "content": "\nCreating a Folder inside an Amazon S3 Bucket\n\nIntroduction\n\nThe AWS S3 console allows you to create folders for grouping objects. This can be a very helpful organizational tool. However, in Amazon S3, buckets and objects are the primary resources. A folder simply becomes a prefix for object key names that are virtually archived into it.\n\nInstructions\n\nReturn to the Buckets menu by clicking here, and click on the calabs-bucket you created earlier. (Reminder: Your bucket name will differ slightly.)\n\nClick Create folder:\n\nalt\n\nIn the Folder name textbox, enter cloudfolder:\n\nScroll to the bottom and click Create folder:\n\nThe folder is created inside your S3 bucket:\n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/create-s3-bucket/",
            "title": "Create S3 Bucket",
            "description": "Create an Amazon S3 Bucket",
            "content": "\nCreating an Amazon S3 Bucket\n\nIntroduction\n\nYou can create an Amazon S3 bucket using the AWS Management Console. As with many other AWS services, you can use the AWS API or CLI (command-line interface) as well.\n\nIn this lab step, you will create a new Amazon S3 bucket.\n\nInstructions\n\nIn the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n\nalt\n\nYou will be placed in the S3 console.\n\nFrom the S3 console, click the orange Create Bucket button:\n\nalt\n\nEnter a unique Bucket name on the Name and region screen of the wizard:\n\nalt\n\nRegion**: US West (Oregon) (This should be set for you. If not, please select this region.)\n\nImportant!_**Bucket names must be globally unique, regardless of the AWS region in which you create the bucket. Buckets must also be DNS-compliant.\n\nThe rules for DNS-compliant bucket names are:\n\nBucket names must be at least 3 and no more than 63 characters long.\nBucket names can contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number.\nBucket names must not be formatted as an IP address (for example, 192.168.1.1).\n\nThe following examples are valid bucket names: calabs-bucket-1, cloudacademybucket , cloudacademy.bucket , calabs.1 or ca-labs-bucket.\n\nTroubleshooting Tip: If you receive an error because your bucket name is not unique, append a unique number to the bucket name in order to guarantee its uniqueness:\n\nalt\n\nFor example, change \"calabs-bucket\" to \"calabs-bucket-1\" (or a unique number/character string) and try again. \n\nLeave the Block public access (bucket settings) at the default values:\n\nalt\n\nNo changes are needed. This is where you can set public access permissions.\n\nClick on Create bucket:\n\nalt\n\nA page with a table listing buckets will load and you will see a green notification that your bucket was created successfully.\n\nIn the Buckets table, click the name of your bucket in the Name column:\n\nalt\n\nA page will load with a row of tabs at the top.\n\nTo see details and options for your bucket, click on the Properties:\n\nalt\n\nThis page allows you to configure your Amazon S3 bucket in many different ways. No changes are needed in this lab at this time.\n\nFeel free to look at the other tabs and see the configuration options that are available.\n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/delete-from-s3/",
            "title": "Delete S3 Bucket",
            "description": "Delete an Amazon S3 Bucket",
            "content": "\nDeleting an Amazon S3 Bucket\n\nIntroduction\n\nYou can delete an Amazon S3 bucket using the S3 console. You will delete all objects within the bucket as well.\n\nInstructions\n\nIn the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n\nFrom the top level of the S3 console, notice the Delete button is not actionable.\n\nCheck the name of your bucket to select it:\n\nWith the bucket selected, click Empty:\n\nThe Empty bucket form page will load.\n\nIt's not possible to delete a bucket that contains objects.\n\nTo confirm that you want to delete all objects in this bucket, in the textbox at the bottom, enter permanently delete and click Empty:\n\nTo exit the empty bucket page, at the top-right, click Exit:\n\nYou will be returned to the Buckets page.\n\nTo delete your bucket, select it in the list, and click Delete\n\nTo confirm that you want to delete the bucket, in the textbox, enter the name of your bucket:\n\nClick Delete bucket to delete the bucket.\n\nWarning_**: Make sure to delete all the files/folders inside the bucket before deleting it, otherwise AWS won't allow you to delete the S3 bucket.\n\nImportant!_** Notice the message from AWS: \"Amazon S3 buckets are unique. If you delete this bucket, you may lose the bucket name to another AWS user.\"\n\nIf retaining the bucket name is important to you, consider using the Empty bucket feature and not actually deleting the bucket.\n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/grant-access-s3/",
            "title": "Grant public access to S3 Object",
            "description": "Grant Public Access to an Amazon S3 Object",
            "content": "\nGranting Public Access to an Amazon S3 Object\n\nIntroduction\n\nAll uploaded files are private by default and can only be viewed, edited, or downloaded by you. In order to illustrate this point, complete the instructions below.\n\nNote: The terms \"file\" and \"object\" are often used interchangeably when discussing Amazon S3. Technically, Amazon S3 is an object-store. It is not a block storage device and does not contain a file system as your local computer does. However, files such as images, movies, and sound clips are often uploaded from your file system to Amazon S3.\n\nInstructions\n\nClick on the object you just uploaded to the S3 bucket.\n\nTake a look at the Object overview section:\n\nUnder Object URL, right-click the link and open the URL in a new browser tab:\n\n You will see an XML (eXtensible Markup Language) response telling you that access is denied for this object:\n\nNote: The response may appear differently depending upon your web browser.\n\nLeave the browser tab open. You will return to it shortly.\n\nTo allow public access to objects, you need to disable the default safety guards that prevent them from being made publicly accessible.\n\nTo return to the bucket view, at the top of the page, click the name of your bucket in the bread crumb trail:\n\nClick the Permissions tab and click Edit in the Block public access section:\n\nUncheck all of the options to allow all kinds of public access:\n\nYou should carefully consider anytime you allow public access to S3 buckets. AWS has implemented these security features to help prevent data breaches. For this lab, there is no sensitive data and you do want to allow public access.\n\nPoorly managed Amazon S3 permissions have been a contributing factor to many unauthorized data access events. AWS is making sure you understand the implications of allowing public access to an Amazon S3 bucket.\n\nAt the bottom of the page, click Save changes:\n\nA confirmation dialog box will appear.\n\nEnter confirm in the confirmation dialog box and click Confirm:\n\nYou will see a green notification that the public access settings have been edited.\n\nTurning off Block all public access does not automatically make objects in an Amazon S3 bucket public. There are several ways of of explicitly granting public access including:\n\nBucket policies\nIAM policies\nAccess control lists\nPre-signed URLs\n\nIn this lab, you will use a bucket policy to grant public access to your Amazon S3 bucket.\n\nScroll down to the Bucket policy section and click Edit:\n\nThe Edit bucket policy page will load. Here you can specify a JSON (JavaScript Object Notation) policy to control access to your Amazon S3 bucket.\n\nReplace the contents of the Policy editor with the following:\n\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:GetObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"BUCKET_ARN/*\",\n      \"Principal\": \"*\"\n    }\n  ]\n}\n\nThis is a permissive policy that allows GetObject access to anyone. More restrictive policies are possible such as\n\nRestricting access to specific principals\nAllow cross AWS account access\nUsing conditions to restrict access to a specific IP address\n\nNotice the Resource is currently \"BUCKET_ARN/*\",  which is causing an error.  We need to replace this with the ARN of the bucket we created:\n\nClick the copy icon under *Bucket ARN *and replace BUCKET_ARN in the value of the Resource key with the ARN you just copied :\n\nNote: Ensure that you preserve the /* at the end of the value. This means that the policy will apply to all objects inside the bucket recursively. Public access won't be granted if this is not present.\n\nAt the bottom of the page, click Save changes:\n\nYou will see a green notification that the bucket policy was edited.\n\nReturn to the browser tab where access was denied and fresh the browser tab.\n\nYou will see the response change from “Access Denied” to the logo: \n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/how-to-change-metadata-s3/",
            "title": "Change metadata of S3 Object",
            "description": "Changing the Metadata of an Amazon S3 Object",
            "content": "\nChanging the Metadata of an Amazon S3 Object\n\nIntroduction\n\nEach object in Amazon S3 has a set of key/value pairs representing its metadata. There are two types of metadata: \"System metadata\" (for example, Content-Type and Content-Length) and custom \"User metadata\". User metadata is stored with the object and returned with it.\n\nAs an example, you might have your own taxonomy for various images, such as “logo”, “screenshot”, “diagram”, \"flowchart\" and so on. In this lab step, you will change the Content-Type of your image to \"text/plain\". You will also create custom user metadata.\n\nNote: With the new Amazon S3 UI you can set the metadata as part of the upload process, or add it later.\n\nInstructions\n\nReturn to the cloudfolder/ and delete the cloudacademy-logo.png from your Amazon S3 bucket by checking the checkbox and clicking Delete:\n\nThe Delete objects form page will load. Because a deleted object is not retrievable, AWS asks you to confirm that you want to delete the object before deletion.\n\nIn the textbox at the bottom of the page, enter permanently delete and click Delete objects:\n\nTo return to the bucket object view, at the top-right, click Close.\n\nClick Upload, then Add files and browse to the logo file (or drag-and-drop it into the Upload wizard) in order to upload it again.\n\nNear the bottom of the page, expand the Properties section:\n\nScroll down to the Metadata section and click Add metadata:\n\nA row of form elements will appear.\n\nEnter the following:\n\nType: Select **System defined\nKey: Select **Content-Type\nValue**: Enter text/plain\n\nThe drop-down list contains the System metadata that you can set.\n\nIn this lab, you have set the content type to text/plain as an example to see how to add metadata to an object when uploading to Amazon S3.\n\nNext you will add custom user metadata. User metadata must be prefaced with \"x-amz-meta-\". The remaining instructions will add a custom user type for imagetype, and imagestatus.\n\nClick Add metadata again to add another row. \n\nEnter the following to define custom metadata:\n\nType: Select **User defined\nKey: Enter imagetype after **x-amz-meta\nValue**: Enter logo\n\nYou have added two metadata key-value pairs to the object you are going to upload. One system metadata and one user-defined.\n\nAt the bottom of the page, click Upload:\n\nTo exit the upload form, at the top-right, click Close.\n\nIn the Objects table click the cloudacademy-logo.png object:\n\nScroll down to the Metadata section and observe the key-value pairs you added:\n\nThis is also where you can add, remove, and edit metadata after you have uploaded objects to Amazon S3.\n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/s3/upload-file-to-s3/",
            "title": "Upload a file to S3",
            "description": "Uploading a File to Amazon S3",
            "content": "\nUploading a File to Amazon S3\n\nIntroduction\n\nWhen you upload a folder from your local system or another machine, Amazon S3 uploads all the files and subfolders from the specified local folder to your bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. In this lab step, you will upload a file to your bucket. The process is similar to uploading a single file, multiple files, or a folder with files in it.\n\nIn order to complete this lab step, you have to upload the cloudacademy-logo.png file from your local file storage into an S3 folder you created earlier.\n\nDownload the Cloud Academy logo from the following location: https://s3-us-west-2.amazonaws.com/clouda-labs/scripts/s3/cloudacademy-logo.png (If the image is not downloaded for you, simply right-click the image and select Save image as to download it to your local file system.)\n\nInstructions\n\nClick on the cloudfolder folder. You are placed within the empty folder in your S3 bucket:\n\nNote: Click the folder name itself, not the checkbox for the folder name. If you select the folder checkbox then upload a file, it will be placed above the folder (not inside it).\n\nClick the Upload button.\n\nClick Add Files:\n\nA file picker will appear.\n\nBrowse to and select the local copy of cloudacademy-logo.png file that you downloaded earlier:\n\nThe logo is added to the list of files that are ready to upload. You have several options at this point:\n\nAdd more files**\nUpload**\n\nHowever, there is another method that some users prefer to add files for upload.\n\nCheck the file and click on Remove:\n\nThis time, rather than browsing to a file, drag and drop the logo file onto the wizard. The wizard adds it to the list of files to upload.\n\nScroll to the bottom of the page and click Upload to upload the file:\n\nYou will see a blue notification that the file is uploading and then a green notification that the upload has been completed successfully.\n\nThe file is placed in the folder in your bucket:\n",
            "tags": [
                "s3"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/sns/_index",
            "title": "Simple Notification Service",
            "description": "Amazon Simple Notification Service",
            "content": "\nAbout\n\nAmazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication.\n\nDocumentation\nUser Guide\n\nSimple Notification Service Flow\n\nSNS is a fully managed messaging service for both system­-to­-system & app-to­-person (A2P) commun­ica­tion.\n\nAmazon SNS can also send notifications via SMS text message, email, SQS queues or to any HTTP endpoint.\n\nAmazon SNS notifications can also trigger Lambda functions.\n\nAmazon SNS is inexpensive and based on a pay-as-you-go model with no upfront costs.\n\nAlternatives\n\nAirship\nApple Push Notification\nBeamer\nDrift\nExpo\nFirebase FCM\nMagicBell\nOneSignal Push\n\nTerminology\n\nSNS Topics\n\nA topic is an “access point” for allowing recipients to dynamically subscribe for identical copies of the same notification.\n\nAn SNS topic is a named communication channel.\n\nSNS Subscribers and Endpoints\n\nWhen subscribing to an SNS topic the following endpoint types are supported:\n\nHTTP/HTTPS\nEmail/Email-JSON\nAmazon Kinesis Data Firehose\nAmazon SQS\nAWS Lambda\nPlatform application endpoint (mobile push)\nSMS\n\nTopic types:\n\nStandard Topics\nFIFO Topics\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Applic­ation integr­ation\n\nSame type services: SNS, SQS, AppSync, EventB­ridge\t\n\nExample: Send email notification when CloudWatch alarm is triggered\n\nPractice\n\nProcess Amazon SNS Notifications with AWS Lambda\n\nQuestions\n\nQ1\n\nYou’ve decided to use autoscaling in conjunction with SNS to alert you when your auto-scaling group scales. Notifications can be delivered using SNS in several ways.\n\nWhich options are supported notification methods? (Choose 3 answers)\n\nHTTP or HTTPS POST notifications\nEmail using SMTP or plain text\nKinesis Stream\nInvoking of a Lambda function\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html\n\n1, 2, 4\n",
            "tags": [
                "AWS",
                "SNS"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/sns/aws-lambda-sns-notifications/",
            "title": "Process Amazon SNS Notifications with AWS Lambda",
            "description": "Process Amazon SNS Notifications with AWS Lambda",
            "content": "\nLab\n\nProcess Amazon SNS Notifications with AWS Lambda\n\nCreating an Amazon SNS Topic\n\nIn the AWS Management Console search bar, enter SNS, and click the Simple Notification Service result under Services:\n\nalt\n\nIn the left-hand side menu, click Topics:\n\nalt\n\nIf you can't see the left-hand menu, to expand it, click the following:\n\nalt\n\nClick Create topic:\n\nalt\n\nIn the Create topic form, ensure to have selected the Standard type, and enter the following values accepting the defaults for values not specified:\n\nName:** lab-topic\n\nalt\n\nYou can leave the Display name field empty for this Lab. When you create topics where the recipients receive messages over SMS (Short Message Service) you are required to provide a value.\n\nAt the bottom of the form, click Create topic:\n\nalt\n\nCreating an AWS Lambda Function\n\nIn the AWS Management Console search bar, enter Lambda, and click the Lambda result under Services:\n\nalt\n\nYou will see the Functions list page.\n\nClick Create function:\n\nalt\n\nIn the Create function form, ensure Author from scratch is selected:\n\nalt\n\nIn the Create function form, enter lab-function in the Function name field:\n\nalt\n\nIn the Create function form, in the Runtime drop-down, select Python 3.8:\n\nalt\n\nIn the Create function form, click Change default execution role and select Use an existing role:\n\nalt\n\nIn the Existing role drop-down, select lambda\\_s3\\_put:\n\nalt\n\nThe role you have selected has been pre-populated for this Lab. Usually when using Lambda you will create a specific role for your function.\n\nTo create your function, click Create function:\n\nalt\n\nImplementing an AWS Lambda Function to Upload to S3\n\nScroll down to the Code source section and double-click lambda_function.py.\n\nIn the code editor, replace the contents with the following Python code:\n\nfrom datetime import datetime\nimport boto3\n\naccount_id = boto3.client('sts').get_caller_identity()[\"Account\"]\ns3 = boto3.resource('s3')\n\n\ndef lambda_handler(event, context):\n    record = event'Records'['Sns']\n    message = record['Message']\n    subject = record['Subject']\n\n    print(\"Subject: %s\" % subject)\n    print(\"Message: %s\" % message)\n\n    s3.Object(f\"sns-lab-bucket-{account_id}\", subject).put(Body=message)\n\n    return \"SUCCESS\"\n\nThe function code you entered processes a message from SNS. The code uploads a file into an S3 Bucket which was pre-created as a part of this lab. The name of the file will be the subject of the message and the content of the file will be the message body.\n\nYou can use a Lambda function to do many different things. Some examples include:\n\nProcess web-requests\nPut a custom metric into AWS CloudWatch\nAdd or update a record in a database\nPost a web-request to an external service\n\nTo save your changes and deploy your function, at the top of the Code source section, click Deploy:\n\nalt\n\nYou will see a notification that your function has been deployed:\n\nalt\n\n To add an SNS trigger, in the Function overview section, click Add trigger:\n\naltalt​\n\nIn the Select a trigger dropdown, enter SNS, and click the SNS result:\n\naltalt​\n\nIn the SNS topic drop-down, select lab-topic:\n\naltalt​\n\nThe SNS topic field will be filled with the ARN (Amazon Resource Name) of your SNS topic.\n\nTo add your SNS trigger, click Add:\n\naltalt​\n\nYou will see a notification that your trigger has been added:\n\nalt\n\nIn SNS terminology, by adding an SNS trigger you have \"subscribed\" your Lambda function to the SNS topic.\n\nPublishing a Message to an Amazon SNS Topic\n\nNavigate to the AWS SNS service.\n\nIn the left-hand side menu, click Topics:\n\nalt\n\nIn the list of topics, click lab-topic:\n\nalt\n\nClick Publish message:\n\nalt\n\nIn the Message details section of the form, in the Subject field, enter lab-subject:\n\nalt\n\nIn the Message body section of the form, in the Message body to send to the endpoint textbox, enter Lab Message:\n\nalt\n\nUsually when you publish a message to an SNS topic, you would include meaningful data in the message body. The content of the message body is often called the \"payload\" of a message. In SNS, the payload can be plain text, or it can be a structured payload such as JSON, XML, or some other format. The service or device subscribed to your topic can use the data in the payload to determine what action to take in response to receiving a message.\n\nTo publish your message, click Publish message:\n\nalt\n\nYou will see a notification, similar to the following, confirming your message has been published:\n\nalt\n\nVerifying the AWS Lambda Function Processed the Message\n\nIn the AWS Management Console search bar, enter S3, and click the S3 result under Services:\n\nalt\n\nIn the list of S3 Buckets, click the Bucket beginning with sns-lab-bucket-:\n\nalt\n\nIn the list of objects you will see a file called lab-subject:\n\nalt\n\nThis file was uploaded to the S3 bucket by your Lambda function.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/sqs/_index",
            "title": "Simple Queue Service",
            "description": "Amazon Simple Queue Service",
            "content": "\nAbout\n\nFully managed message queues for microservices, distributed systems, and serverless applications\n\nDocumentation\nUser Guide\n\nSQS offers two types of message queues. Standard queues offer maximum throughput, best-effort ordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that messages are processed exactly once, in the exact order that they are sent.\n\nSQS is a fully managed message queuing service that enables to decouple & scale micros­erv­ices, distri­buted systems, & serverless applic­ations.\n\nMessages are published to an Amazon SNS topic and then pushed to Amazon SQS subscriber queues. This eliminates the need for periodic polling and allows for messages to be processed in parallel asynchronously by the subscribers.\n\nAlternatives\n\nApache Kafka\nGoogle Cloud Pub/Sub\nRabbitMQ\nIBM MQ\nTIBCO Enterprise Message Service\nAWS IoT Core\nAmazon CloudWatch\nAzure Service Bus\n\nConfiguration\n\nVisibility timeout**:\n  Other consumers will not receive a message being processed for the configured time period (default 30 seconds, min - 0, max - 12 hours)\n  Consumer processing a message can call ChangeMessageVisibility to increase visibility timeout of a message (before visibility timeout)\nDelaySeconds**:\n  Time period before a new message is visible on the queue\n  Delay Queue = Create Queue + Delay Seconds default - 0, max - 15 minutes\n  Can be set at Queue creation or updated using SetQueueAttributes\n  Use message timers to configure a message specific DelaySeconds value\nMessage retention period:\n  Maximum period a message can be on the queue Default - 4 days, Min - 60 seconds, Max - 14 days\nMaxReceiveCount**:\n  Maximum number of failures in processing a message\n\nDigest\n\nLong polling**: Long polling will reduce the overhead of the CPU and not require excessive dolls.\nVisibility timeout**: a period of time during which Amazon SOS prevents other consumers from receiving and processing the message. The default visibility timeout for a message is 30 seconds. The minimum is O seconds. The maximum is 12 hours\nDelay queue**: Delay queues let you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period\nKnow FIFO vs Standard Queue\nSQS** is a fully managed message queuing service that enables you to decouple and scale micro-services, distributed systems, and Serverless applications\nSNS**: To receive subset of messages, subscriber can apply filter policy to topic subscription.\nIf use case has subscribed to email notification, go with SNS as opposed to SQS.\nBy default only the queue owner is allowed to use the queue. Configure SQS Queue Access Policy to provide access to other AWS accounts\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Applic­ation integr­ation\n\nSame type services: SNS, SQS, AppSync, EventB­ridge\t\n\nPractice\n\nFan-Out Orders using Amazon SNS and SQS\n\nQuestions\n\nQ1\n\nWhich endpoint is considered to be best practice when analyzing data within a Configuration Stream of AWS Config?\n\nSNS\nE-Mail\nSQS\nKinesis\n\n\nExplanation\n\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/monitor-resource-changes.html\n\n3\n",
            "tags": [
                "AWS",
                "SQS"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/sqs/fan-out-orders-with-sns-sqs/",
            "title": "Fan-Out Orders using Amazon SNS and SQS",
            "description": "Fan-Out Orders using Amazon SNS and SQS",
            "content": "\nLab\n\nFan-Out Orders using Amazon SNS and SQS\n\nCreating an Amazon SNS Topic and Amazon SQS Queues\n\nHere's a diagram of what you will build and configure in this lab step:\n\nalt\n\nIn the search bar at the top, enter SNS and under Services, click the Simple Notification Service result:\n\nalt\n\nIn the Create topic card on the right, in the Topic name textbox, enter new-orders and click Next step:\n\n\nalt\n\nThe Create topic form will load.\n\nBy default, the Type of topic selected will be Standard. This is the most scalable topic type. The cost of this scalability is that message order and exactly-once delivery attempts can not be guaranteed.\n\nIf you are building a solution requires strict message ordering and exactly-once message delivery, you should use a FIFO type topic.\n\nStandard is fine for this lab.\n\nClick the black triangle next to Access policy - optional to expand the section:\n\nalt\n\nIn the Access policy section, under Define who can publish messages to the topic, select Everyone:\n\nalt\n\nUnder Define who can subscribe to this topic, select Everyone:\n\nalt\n\nYou are using a permissive access policy to save time and because the focus of this lab is on demonstrating the fan-out scenario.\n\nIn a non-lab environment, you should carefully consider the access policy required and make sure if conforms with your company or organization's security requirements.\n\nScroll to the bottom of the page, and click Create topic:\n\nalt\n\nYou will see a page load displaying details of your newly created topic:\n\nalt\n\nIn the order processing system your are building, this Amazon SNS topic is where orders are published to. In a non-lab environment it would most likely be a web application or other application that accepts orders that will publish messages to this topic.\n\nNext, you will create two queues using Amazon Simple Queue Service and subscribe them to your Amazon SNS topic.\n\nOpen a new tab by right-clicking the AWS icon in the top-left and selecting Open in new tab.\n\nNote: The above instruction may vary slightly depending upon the web browser you are using.\n\nIn the search bar at the top, enter SQS, and under Services, click the Simple Queue Service result:\n\nalt\n\nIn the middle right of the screen, in the Get started card, click Create queue:\n\nalt\n\nThe Create queue form will open.\n\nIn the Name textbox, enter orders-for-inventory:\n\nalt\n\nScroll down to the bottom, click Create queue:\n\nalt\n\nYou will see a web page load showing you details of your newly created Amazon SQS queue:\n\nalt\n\nYou will now create a second Amazon SQS queue for analytics.\n\nTo navigate to the Queues list page, at the top-left, click Queues:\n\nalt\n\nOn the right-hand side, click Create queue.\n\nRepeat the queue creation process, only this time enter orders-for-analytics as the Name of the queue.\n\nReturn to the Queues list page by clicking Queues in the top-left.\n\nYou will see the two queues you have created:\n\nalt\n\nClick the radio button for the orders-for-analytics queue.\n\nOn the right-hand side, click Actions and click Subscribe to Amazon SNS topic:\n\nalt\n\nThe Subscribe to Amazon SNS topic form will load.\n\nIn the Choose a topic drop down, select the topic ending with new-orders:\n\nalt\n\nThis is the Amazon SNS topic you created earlier.\n\nClick Save to finish subscribing this queue to your topic.\n\nAt the top-left, click Queues again.\n\nRepeat the topic subscription process for your orders-for-inventory Amazon SQS queue.\n\nYou now have both of your Amazon SQS queues subscribed to your Amazon SNS topic. Any messages published to the topic will fan-out to both queues.\n\nConnecting to the Virtual Machine using EC2 Instance Connect\n\nIn the AWS Management Console search bar, enter EC2, and click the EC2 result under Services:\n\nalt\n\n2\\. To see available instances, click Instances in the left-hand menu:\n\nalt\n\nThe instances list page will open, and you will see an instance named cloudacademylabs:\n\nalt\n\nIf you don't see a running instance then the lab environment is still loading. Wait until the Instance state is Running.\n\n3\\. Right-click the cloudacademylabs instance, and click Connect:\n\nalt\n\nThe Connect to your instance form will load.\n\n4\\. In the form, ensure the EC2 Instance Connect tab is selected:\n\nalt\n\nYou will see the instance's Instance ID and Public IP address displayed.\n\n5\\. In the User name textbox, enter ec2-user:\n\nalt\n\nNote: Ensure there is no space after ec2-user or connect will fail. \n\n6\\. To open a browser-based shell, click Connect:\n\nalt\n\nIf you see an error it's likely that the environment hasn't finished setting up. Check for Setup completed at the top-left corner of the lab and try connecting again:\n\nalt\n\nA browser-based shell will open in a new window ready for you to use.\n\nKeep this window open, you will use it in later lab steps.\n\nYou can also connect to the instance using your preferred SSH client and the PPK (Windows) or PEM (Mac/Linux) key files in the Credentials section of this lab.\n\nPublishing and Processing Messages\n\nIn the terminal, enter the following command:\n\naws sns list-topics\n\nYou will see one topic displayed:\n\nalt\n\nNote: Your TopicArn will have a different account identifier.\n\nBy default, the AWS command-line interface tool uses the JSON format for responses. This response contains an array of Topics with one element. The element consists of a TopicArn.\n\nArn is short for Amazon Resource Name. An ARN is used to uniquely identify resources in AWS.\n\nIn this lab, the EC2 instance has been configured with an IAM role that has permissions to interact with Amazon SNS topics and Amazon SQS queues.\n\nStore the value of the TopicArn attribute in a shell variable (topic_arn):\n\ntopic_arn=$(aws sns list-topics --query 'Topics[0].TopicArn' --output text)\n\nThe above command uses the --query option to select only the value of the TopicArn and the --output option is used to specify plaintext format which removes the quotation marks from the value.\n\nTo publish a message, enter the following, utilizing the ARN you stored in the topic_arn shell variable:\n\naws sns publish \\\n--topic-arn $topic_arn \\\n--message  \"1 x Widget @ 21.99 USD\\n2 x Widget Cables @ 5.99 USD\"\n\nIn response, you will see a MessageId:\n\nalt\n\nNote: Your message identifier will be different.\n\nYou have successfully published an order message to your Amazon Simple Notification Service topic.\n\nIn this lab, you are using the AWS command-line interface tool to simulate an application publishing an order message.\n\nIn a non-lab environment, the message could be published by a web application that accepts orders from customers.\n\nTo list Amazon Simple Queue Service queues, enter the following command:\n\naws sqs list-queues\n\nYou will see a JSON response:\n\nalt\n\nThe queues that you created earlier are listed.\n\nStore each of the QueueUrls in shell variables:\n\nanalytics_queue_url=$(aws sqs list-queues --query 'QueueUrls[0]' --output text)\ninventory_queue_url=$(aws sqs list-queues --query 'QueueUrls[1]' --output text)\n\nTo retrieve a message from the orders-for-analytics queue, enter the following command, utilizing the analytics queue URL you stored previously:\n\naws sqs receive-message \\\n    --queue-url $analytics_queue_url\n\nYou will see a JSON response containing an array with one Message:\n\nalt\n\nThe response contains the following fields:\n\nBody**: A JSON representation of the message\nReceiptHandle**: You are required to supply this to delete a message after processing\nMD5OfBody**: An MD5 hash of the message body\nMessageId**: The message identifier that Amazon SNS saw when pushing the message to the queues\n    Note that this is not the same as the MessageId that Amazon SNS returned to you when you published to the topic\nRepeat the previous instruction, using the orders-for-inventory queue but store the message response in a shell variable (for use later) and output the shell variable (using Python's JSON tool to pretty print it):\n\ninventory_message=$(aws sqs receive-message --queue-url $inventory_queue_url)\necho $inventory_message | python -m json.tool\n\nYou will see the same message displayed again.\n\nThe message you published to the Amazon SNS topic has been sent to the Amazon SQS queues you subscribed the topic. This is an example of fanning out a message to multiple receivers.\n\nIn a non-lab environment, you could have worker applications constantly running and asking the Amazon SQS queues for more messages. One worker may be updating an inventory database for the order, whilst another worker could be recording the order details in a data lake for future analysis.\n\nUsing Amazon SNS and Amazon SQS like this allows you to build scalable systems that are decoupled and resilient. If a worker went offline, messages would queue up in the Amazon SQS queues. When the worker is available again, it can pick up new messages where it left off.\n\nYou can also have multiple worker applications, to help ensure there's no downtime in message processing.\n\nAfter successfully processing a message, a worker application should delete the message to prevent it from being processed again.\n\nStore the value of the ReceiptHandle attribute in a shell variable:\n\nreceipt_handle=$(echo $inventory_message | python -m json.tool | grep ReceiptHandle | cut -d\\\" -f 4)\n\nTo delete a message, enter the following command for the orders-for-inventory queue:\n\naws sqs delete-message \\\n    --queue-url $inventory_queue_url \\\n    --receipt-handle $receipt_handle\n\nReturn to your browser tab with the Amazon SQS management console open.\n\nNote: If the SQS management console appears to only have one SQS queue, click the refresh button above the table:\nalt\nThe correct number of SQS queues will be displayed after a refresh.\n\nNavigate to the Queues list and click the orders-for-inventory queue.\n\nIn the top-right, click Send and receive messages:\n\nalt\n\nVerify that in that Receive messages section, under Messages available, it says 0.\n\nThis is the queue you deleted a message for, simulating a long-running background application that receives an Amazon SQS message and then deletes the message after processing.\n\nRepeat the last three instructions for the orders-for-analytics queue and verify Messages available is 1:\n\nalt\n\nThis is the queue you did not delete the message for. The message is still available to be picked up for processing by an application receiving messages from the queue.\n",
            "tags": [],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/step-functions/",
            "title": "Step Functions",
            "description": "Amazon Step Functions",
            "content": "\nAbout\n\nAWS Step Functions is a low-code, visual workflow service that developers use to build distributed applications, automate IT and business processes, and build data and machine learning pipelines using AWS services.\n\nDocumentation\nUser Guide\n\nStep Functions Flow\n\nStep Functions is a serverless function orches­trator that makes it easy to sequence Lambda functions & multiple AWS services into busine­ss-­cri­tical applic­ations.\n\nStep Functions Flow\n\nAlternatives\n\nAWS lambda\nAirflow\nGoogle Cloud Workflows\nMicrosoft Flow\n\nPrice\n\nPay only for what you use\n\nCurrent price\n\nFree Tier: 4,000 state transitions per month\n\nUse Cases\n\nStep Functions is an easy-to-use function orchestra that makes it possible to string Lambda functions and multiple AWS services into business-critical applications.\n\nStep Functions manages the operations and underlying infrastructure for you to ensure your application is available at any scale.\n\nWith Step Functions, you are able to easily coordinate complex processes composed of different tasks.\n\nWithout using this service you have to coordinate each Lambda Function yourself and manage every kind of error in all steps of this complex process.\n\nAWS Step Functions is a useful service for breaking down complex processes into smaller and easier tasks\n\nAutomate Extract, Transform, and Load (ETL) process\nOrchestrate microservices\nWorkflow configuration\nAWS service integrations\nComponent reuse\nBuilt-in error handling\n\nType: Orches­tration, Workflows\n\nStep Function Standard Workflows are optimized for long-running processes.\n\nExpress Workflows are better for event-driven workloads.\n\nPractice\n\nIntroduction to AWS Step Functions\n\nQuestions\n\nQ1\n\nA developer is adding a feedback form to a website. Upon user submission, the form should create a discount code, email the user the code and display a message on the website that tells the user to check their email. The developer wants to use separate Lambda functions to manage these processes and use a Step Function to orchestrate the interactions with minimal custom scripting.\n\nWhich of the following Step Function workflows can be used to meet requirements?\n\nAsynchronous Express Workflow\nStandard Workflow\nSynchronous Express Workflow\nStandard Express Workflow\n\n\nExplanation\n\n\nhttps://aws.amazon.com/blogs/compute/new-synchronous-express-workflows-for-aws-step-functions/\n\n3\n",
            "tags": [
                "AWS",
                "Step Functions"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/aws-certified-developer-associate/xray/",
            "title": "X-Ray",
            "description": "Analyze and debug production, distributed applications",
            "content": "\nAbout\n\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\n\nDocumentation\nUser Guide\n\nX-Ray Flow\n\nX-Ray allows software engineers to view the state of a system at a glance, identify potential bottlenecks, and make informed operational decisions to improve performance and reliability. X-Ray inspects application code using a combination of machine and customer-provided data to identify potential bottlenecks and analyze performance and performance trends for each test scenario.\n\nTerminology\n\nAWS X-Ray receives data from services as segments. X-Ray then groups segments that have a common request into traces. X-Ray processes the traces to generate a service graph that provides a visual representation of your application\n\nX-Ray Trace Hierarchy: Trace > Segment > Sub Segment\n\nTrace\n\nAn X-Ray trace is a set of data points that share the same trace ID.\n\nSegments\n\nA segment is a JSON representation of a request that your application serves.\n\nA trace segment records information about the original request, information about the work that your application does locally, and subsegments with information about downstream calls that your application makes to AWS resources, HTTP APIs, and SQL databases.\n\nSubsegments\n\nSubsegments provide more granular timing information and details about downstream calls that your application made to fulfill the original request.\n\nAnnotations\n\nAn X-Ray annotation is system-defined, or user-defined data associated with a segment\nA segment can contain multiple annotations.\nAnnotations are used to describe the request, the response, and other information about the segment\nCan be used for adding system or user-defined data to segments and subsegments that you want to index for search.\n\nSampling\n\nX-Ray traces are sampled at a rate that you specify. The rate is specified in the sampling_rate field of the sampling object in the config object.\n\nMetadata\n\nX-Ray traces contain metadata that is useful for understanding the trace.\nMetadata (Key / value pairs) is not indexed and cannot be used for searching\n\nDigest\n\nTrace request across microservices/AWS services\n  Analyze, Troubleshoot errors, Solve performance issues\n  Gather tracing information\n    From applications/components/AWS Services\n  Tools to view, filter and gain insights (Ex: Service Map)\nHow does Tracing work?\n  Unique trace ID assigned to every client request\n    X-Amzn-Trace-Id:Root=1-5759e988-bd862e3fe\n  Each service in request chain sends traces to X-Ray with trace ID\n    X-Ray gathers all the information and provides visualization\n  How do you reduce performance impact due to tracing?\n    Sampling - Only a sub set of requests are sampled (Configurable)\n  How can AWS Services and your applications send tracing info?\n    Step 1 : Update Application Code Using X-Ray SDK\n    Step 2: Use X-Ray agents (EASY to use in some services! Ex: AWS Lambda)\n\nSegments and Sub-segments can include an annotations object containing one or more fields that X-Ray indexes for use with Filter Expressions. It is indexed. Use up to 50 annotations per trace.\nTotal sampled request per second = Reservoir size + ((incoming requests per second - reservoir size) * fixed rate)\nDefault sampling X-ray SDK first request each second and 5% of any additional requests\nTracing header can be added in http request header\nAnnotations vs Segments vs Subsegments vs metadata\nX-ray daemon listens for traffic on UDP port 2000\nX-ray SDK provides interceptors to add your code to trace incoming HTTP requests.\nX-ray in EC2: You need the X-Ray daemon to be running on your EC2 instances in order to send data to X-Ray. User data script could be used to install the X-Ray daemon in EC2 instance.\nX-ray in ECS: In Amazon ECS, create a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your Amazon ECS cluster.\nX-ray in elastic beanstalk: Enable the X-Ray daemon by including the xray-daemon.config configuration file in the .ebextensions directory of your source code\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a micro-service architecture.\nA segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details about - downstream calls that your application made to fulfill the original request.\nAdd annotations to subsegment document if you want to trace downstream calls.\nSegments and subsegment can include a metadata object containing one or more fields with values of any type, including objects and arrays.\nTracing header is added in the HTTP request header. A tracing header (X-Amzn-Trace-ld) can originate from the X-Ray SDK, an AWS service, or the - client request.\nUse the GetTraceSummaries API to get the list of trace IDs of the application and then retrieve the list of traces using BatchGetTraces API in - order to develop the custom debug tool\n\nPrice\n\nCurrent price\n\nUse Cases\n\nType: Developer Tools\n\nAlternatives\n\nGoogle Stackdriver\nAzure Monitor\nElastic Observability\nDatadog\nSplunk\n\nAWS X-Ray supports applications running on:\n\nAmazon Elastic Compute Cloud (Amazon EC2)\nAmazon EC2 Container Service (Amazon ECS)\nAWS Lambda\nWS Elastic Beanstalk\n\nPractice\n\nQuestions\n\nQ1\n\nYou joined an application monitoring team. Your role focuses on finding system performance and bottlenecks in Lambda functions and providing specific solutions. Another teammate focuses on auditing the systems.\n\nWhich AWS service will be your main tool?\n\nAWS X-Ray\nAWS IAM\nAWS CloudTrail\nAWS Athena\n\n\nExplanation\n\n\nAWS X-Ray provides graphs of system performance and identifies bottlenecks\n\n1\n",
            "tags": [
                "aws",
                "xray"
            ],
            "lang": "en"
        },
        {
            "uri": "/tracks/disser/utils/text_2_short",
            "title": "Short description from article",
            "description": "Generate short description from article",
            "content": "\n\n            Create\n\n",
            "tags": [],
            "lang": "en"
        }
    ]
}